id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/broadinstitute/cromwell/issues/4513:170,Deployability,upgrade,upgrade,170,"#4490 introduces a couple of new builds to confirm the PAPI v1 => v2 upgrade plan will work as intended. But it's not clear what the fate of these changes should be post-upgrade. The code as written now is specific to this one-time upgrade so some choice will need to be made, whether it's trying to model upgrade scenarios in different environments or just deleting it altogether.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4513
https://github.com/broadinstitute/cromwell/issues/4513:232,Deployability,upgrade,upgrade,232,"#4490 introduces a couple of new builds to confirm the PAPI v1 => v2 upgrade plan will work as intended. But it's not clear what the fate of these changes should be post-upgrade. The code as written now is specific to this one-time upgrade so some choice will need to be made, whether it's trying to model upgrade scenarios in different environments or just deleting it altogether.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4513
https://github.com/broadinstitute/cromwell/issues/4513:306,Deployability,upgrade,upgrade,306,"#4490 introduces a couple of new builds to confirm the PAPI v1 => v2 upgrade plan will work as intended. But it's not clear what the fate of these changes should be post-upgrade. The code as written now is specific to this one-time upgrade so some choice will need to be made, whether it's trying to model upgrade scenarios in different environments or just deleting it altogether.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4513
https://github.com/broadinstitute/cromwell/issues/4513:118,Usability,clear,clear,118,"#4490 introduces a couple of new builds to confirm the PAPI v1 => v2 upgrade plan will work as intended. But it's not clear what the fate of these changes should be post-upgrade. The code as written now is specific to this one-time upgrade so some choice will need to be made, whether it's trying to model upgrade scenarios in different environments or just deleting it altogether.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4513
https://github.com/broadinstitute/cromwell/issues/4515:544,Security,validat,validation,544,"When using relative path-based imports in a WDL, and imported WDL which also imports a WDL does not look in the right places for relative imports.; For example, given the directory structure:; ```; import_issue; | Issue.wdl; |; a___; | |___A.wdl; |; |___b; |___B.wdl; |; |___c; |___C.wdl; ```; where `Issue.wdl` imports `""a/A.wdl"" as WorkflowA` and `""b/B.wdl"" as WorkflowB` and `B.wdl` imports `import ""b/c/C.wdl"" as CTasks`, Cromwell is able to process and run the WDL in v34, but not in v36. It fails both submission to a Cromwell server and validation in Womtool. Attached is a zip file containing this toy example.; [import_issue.zip](https://github.com/broadinstitute/cromwell/files/2728020/import_issue.zip)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4515
https://github.com/broadinstitute/cromwell/pull/4516:18,Availability,down,down,18,Another CTKS test down.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4516
https://github.com/broadinstitute/cromwell/pull/4516:13,Testability,test,test,13,Another CTKS test down.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4516
https://github.com/broadinstitute/cromwell/issues/4518:202,Availability,down,download,202,"I am struggling to make read_json work on any data that is more complex than a flat json-map. For instance, a json like; ```json; [{""series"":""GSE69360"",""name"":""Biochain_Adult_Liver"",""path"":""https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238"",""model"":""Illumina HiSeq 2000"",""run"":""SRR2014238"",""gsm"":""GSM1698568"",""characteristics"":""number of donors -> 1;age -> 64 years old;tissue -> Liver;vendor -> Biochain;isolate -> Lot no.: B510092;gender -> Male"",""strategy"":""RNA-Seq"",""organism"":""Homo sapiens"",""layout"":""PAIRED"",""title"":""Biochain_Adult_Liver""}]; ```; cannot be read by ; ```; Array[Map[String,String]] runs = read_json(path_to_json_file); ```; even through it is clearly Array[Map[String, String]]; I get the following failure:; ```; Workflow failed; WorkflowFailure(Failed to evaluate job outputs,List(WorkflowFailure(Bad output 'get_gsm.runs': Failed to read_json(""/data/cromwell-executions/test/f8f591dc-3797-46de-9846-dbd2a902ff65/call-get_gsm/execution/GSM1698568_runs.json"") (reason 1 of 1): No coercion defined from '[{""series"":""GSE69360"",""name"":""Biochain_Adult_Liver"",""path"":""https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238"",""model"":""Illumina HiSeq 2000"",""run"":""SRR2014238"",""gsm"":""GSM1698568"",""characteristics"":""number of donors -> 1;age -> 64 years old;tissue -> Liver;vendor -> Biochain;isolate -> Lot no.: B510092;gender -> Male"",""strategy"":""RNA-Seq"",""organism"":""Homo sapiens"",""layout"":""PAIRED"",""title"":""Biochain_Adult_Liver""}]' of type 'spray.json.JsArray' to 'Object'.,List()))); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4518
https://github.com/broadinstitute/cromwell/issues/4518:744,Availability,failure,failure,744,"I am struggling to make read_json work on any data that is more complex than a flat json-map. For instance, a json like; ```json; [{""series"":""GSE69360"",""name"":""Biochain_Adult_Liver"",""path"":""https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238"",""model"":""Illumina HiSeq 2000"",""run"":""SRR2014238"",""gsm"":""GSM1698568"",""characteristics"":""number of donors -> 1;age -> 64 years old;tissue -> Liver;vendor -> Biochain;isolate -> Lot no.: B510092;gender -> Male"",""strategy"":""RNA-Seq"",""organism"":""Homo sapiens"",""layout"":""PAIRED"",""title"":""Biochain_Adult_Liver""}]; ```; cannot be read by ; ```; Array[Map[String,String]] runs = read_json(path_to_json_file); ```; even through it is clearly Array[Map[String, String]]; I get the following failure:; ```; Workflow failed; WorkflowFailure(Failed to evaluate job outputs,List(WorkflowFailure(Bad output 'get_gsm.runs': Failed to read_json(""/data/cromwell-executions/test/f8f591dc-3797-46de-9846-dbd2a902ff65/call-get_gsm/execution/GSM1698568_runs.json"") (reason 1 of 1): No coercion defined from '[{""series"":""GSE69360"",""name"":""Biochain_Adult_Liver"",""path"":""https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238"",""model"":""Illumina HiSeq 2000"",""run"":""SRR2014238"",""gsm"":""GSM1698568"",""characteristics"":""number of donors -> 1;age -> 64 years old;tissue -> Liver;vendor -> Biochain;isolate -> Lot no.: B510092;gender -> Male"",""strategy"":""RNA-Seq"",""organism"":""Homo sapiens"",""layout"":""PAIRED"",""title"":""Biochain_Adult_Liver""}]' of type 'spray.json.JsArray' to 'Object'.,List()))); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4518
https://github.com/broadinstitute/cromwell/issues/4518:1121,Availability,down,download,1121,"I am struggling to make read_json work on any data that is more complex than a flat json-map. For instance, a json like; ```json; [{""series"":""GSE69360"",""name"":""Biochain_Adult_Liver"",""path"":""https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238"",""model"":""Illumina HiSeq 2000"",""run"":""SRR2014238"",""gsm"":""GSM1698568"",""characteristics"":""number of donors -> 1;age -> 64 years old;tissue -> Liver;vendor -> Biochain;isolate -> Lot no.: B510092;gender -> Male"",""strategy"":""RNA-Seq"",""organism"":""Homo sapiens"",""layout"":""PAIRED"",""title"":""Biochain_Adult_Liver""}]; ```; cannot be read by ; ```; Array[Map[String,String]] runs = read_json(path_to_json_file); ```; even through it is clearly Array[Map[String, String]]; I get the following failure:; ```; Workflow failed; WorkflowFailure(Failed to evaluate job outputs,List(WorkflowFailure(Bad output 'get_gsm.runs': Failed to read_json(""/data/cromwell-executions/test/f8f591dc-3797-46de-9846-dbd2a902ff65/call-get_gsm/execution/GSM1698568_runs.json"") (reason 1 of 1): No coercion defined from '[{""series"":""GSE69360"",""name"":""Biochain_Adult_Liver"",""path"":""https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238"",""model"":""Illumina HiSeq 2000"",""run"":""SRR2014238"",""gsm"":""GSM1698568"",""characteristics"":""number of donors -> 1;age -> 64 years old;tissue -> Liver;vendor -> Biochain;isolate -> Lot no.: B510092;gender -> Male"",""strategy"":""RNA-Seq"",""organism"":""Homo sapiens"",""layout"":""PAIRED"",""title"":""Biochain_Adult_Liver""}]' of type 'spray.json.JsArray' to 'Object'.,List()))); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4518
https://github.com/broadinstitute/cromwell/issues/4518:918,Testability,test,test,918,"I am struggling to make read_json work on any data that is more complex than a flat json-map. For instance, a json like; ```json; [{""series"":""GSE69360"",""name"":""Biochain_Adult_Liver"",""path"":""https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238"",""model"":""Illumina HiSeq 2000"",""run"":""SRR2014238"",""gsm"":""GSM1698568"",""characteristics"":""number of donors -> 1;age -> 64 years old;tissue -> Liver;vendor -> Biochain;isolate -> Lot no.: B510092;gender -> Male"",""strategy"":""RNA-Seq"",""organism"":""Homo sapiens"",""layout"":""PAIRED"",""title"":""Biochain_Adult_Liver""}]; ```; cannot be read by ; ```; Array[Map[String,String]] runs = read_json(path_to_json_file); ```; even through it is clearly Array[Map[String, String]]; I get the following failure:; ```; Workflow failed; WorkflowFailure(Failed to evaluate job outputs,List(WorkflowFailure(Bad output 'get_gsm.runs': Failed to read_json(""/data/cromwell-executions/test/f8f591dc-3797-46de-9846-dbd2a902ff65/call-get_gsm/execution/GSM1698568_runs.json"") (reason 1 of 1): No coercion defined from '[{""series"":""GSE69360"",""name"":""Biochain_Adult_Liver"",""path"":""https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238"",""model"":""Illumina HiSeq 2000"",""run"":""SRR2014238"",""gsm"":""GSM1698568"",""characteristics"":""number of donors -> 1;age -> 64 years old;tissue -> Liver;vendor -> Biochain;isolate -> Lot no.: B510092;gender -> Male"",""strategy"":""RNA-Seq"",""organism"":""Homo sapiens"",""layout"":""PAIRED"",""title"":""Biochain_Adult_Liver""}]' of type 'spray.json.JsArray' to 'Object'.,List()))); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4518
https://github.com/broadinstitute/cromwell/issues/4518:688,Usability,clear,clearly,688,"I am struggling to make read_json work on any data that is more complex than a flat json-map. For instance, a json like; ```json; [{""series"":""GSE69360"",""name"":""Biochain_Adult_Liver"",""path"":""https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238"",""model"":""Illumina HiSeq 2000"",""run"":""SRR2014238"",""gsm"":""GSM1698568"",""characteristics"":""number of donors -> 1;age -> 64 years old;tissue -> Liver;vendor -> Biochain;isolate -> Lot no.: B510092;gender -> Male"",""strategy"":""RNA-Seq"",""organism"":""Homo sapiens"",""layout"":""PAIRED"",""title"":""Biochain_Adult_Liver""}]; ```; cannot be read by ; ```; Array[Map[String,String]] runs = read_json(path_to_json_file); ```; even through it is clearly Array[Map[String, String]]; I get the following failure:; ```; Workflow failed; WorkflowFailure(Failed to evaluate job outputs,List(WorkflowFailure(Bad output 'get_gsm.runs': Failed to read_json(""/data/cromwell-executions/test/f8f591dc-3797-46de-9846-dbd2a902ff65/call-get_gsm/execution/GSM1698568_runs.json"") (reason 1 of 1): No coercion defined from '[{""series"":""GSE69360"",""name"":""Biochain_Adult_Liver"",""path"":""https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238"",""model"":""Illumina HiSeq 2000"",""run"":""SRR2014238"",""gsm"":""GSM1698568"",""characteristics"":""number of donors -> 1;age -> 64 years old;tissue -> Liver;vendor -> Biochain;isolate -> Lot no.: B510092;gender -> Male"",""strategy"":""RNA-Seq"",""organism"":""Homo sapiens"",""layout"":""PAIRED"",""title"":""Biochain_Adult_Liver""}]' of type 'spray.json.JsArray' to 'Object'.,List()))); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4518
https://github.com/broadinstitute/cromwell/issues/4519:3711,Availability,down,download,3711,"lot the multidimensional scaling plot using edgeR version 3.1.3.\nThe *.osc.gz files were loaded into the genome browser ZENBU and was used visualize the transcripts. Screen shots were captured.\nGenome_build: hg19 with Gencode V19 annotation\nSupplementary_files_format_and_content: .osc files are simple tab delimited files. They were generated by combining the isoform.results files outputed by RSEM with the gencode v19 .gtf file. It contains abundance measurements and transcript isoforms. It also contains metadata that is inputed into ZENBU.\nSupplementary_files_format_and_content: RNAseq.counts is a simple tab delimited file containing the counts for all the RNA-seq libraries for each gene (summary file of counts).""; },; ""relations"" : {; ""BioSample"" : ""https://www.ncbi.nlm.nih.gov/biosample/SAMN03610550"",; ""SRA"" : ""https://www.ncbi.nlm.nih.gov/sra?term=SRX1020495""; },; ""status"" : {; ""submitted"" : ""May 29 2015"",; ""updated"" : ""Jun 01 2015""; },; ""runs"" : [; {; ""run"" : {; ""Run"" : ""SRR2014238"",; ""ReleaseDate"" : ""2015-05-25 05:44:11"",; ""LoadDate"" : ""2015-05-25 05:38:29"",; ""AssemblyName"" : """",; ""download_path"" : ""https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238"",; ""Experiment"" : ""SRX1020495""; },; ""stats"" : {; ""spots"" : 85220810,; ""bases"" : 12953563120,; ""spots_with_mates"" : 85220810,; ""avgLength"" : 152,; ""size_MB"" : 7790.0; },; ""library"" : {; ""LibraryName"" : ""Biochain_Adult_Liver"",; ""LibraryStrategy"" : ""RNA-Seq"",; ""LibrarySelection"" : ""other"",; ""LibrarySource"" : ""TRANSCRIPTOMIC"",; ""LibraryLayout"" : ""PAIRED""; },; ""sample"" : {; ""Platform"" : ""ILLUMINA"",; ""Model"" : ""Illumina HiSeq 2000"",; ""SRAStudy"" : ""SRP058036"",; ""BioProject"" : ""PRJNA283012"",; ""Study_Pubmed_id"" : """",; ""ProjectID"" : ""283012"",; ""Sample"" : ""SRS931038"",; ""BioSample"" : ""SAMN03610550"",; ""SampleType"" : ""simple"",; ""TaxID"" : ""9606"",; ""ScientificName"" : ""Homo sapiens"",; ""SampleName"" : ""Biochain_Adult_Liver""; },; ""subject"" : {; ""Subject_ID"" : """",; ""Sex"" : ""male"",; ""Disease"" : """",; ""Tumor"" : ""no",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519
https://github.com/broadinstitute/cromwell/issues/4519:2261,Deployability,pipeline,pipeline,2261,"A, non-polyadenylated RNA, pre-processed RNA, tRNA, and may also contain regulatory RNA molecules such as microRNA (miRNA) and short interfering RNA (siRNA), snRNA, and other RNA transcripts of yet unknown function. Ambion RiboMinus rRNA depletion was performed as described in the manufacturer’s protocol (Pub. Part no.: 100004590, Rev. date 2 December 2011) following the standard protocol.\nTruSeq RNA Sample Preparation was performed on the RiboMinus™ RNA fraction as described in the manufacturer’s protocol (Pub. Part no.: 15026495 Rev. F March 2014) following the low sample protocol.\nThe libraries were sequenced on Illumina’s HiSeq 2000 instrument following standard protocol."",; ""processing"" : ""Data quality check using fastQC version 0.11.2.\nAlignment of unpaired unstranded reads using STAR version 2.4.0.\nQuantification of transcripts and isoforms using RSEM version 1.2.21 using rsem-calculate-expression, both alignment and quantification was done using the STAR_RSEM.sh pipeline (https://github.com/ENCODE-DCC/long-rna-seq-pipeline/blob/master/DAC/STAR_RSEM.sh)\nThe programe featurecounts version 1.4.6-p2 from the SourceForge Subread package was used to produce a summary file of counts from all the alignement .bam files.\nThe summary file of counts (RNAseq.counts) was used to plot the multidimensional scaling plot using edgeR version 3.1.3.\nThe *.osc.gz files were loaded into the genome browser ZENBU and was used visualize the transcripts. Screen shots were captured.\nGenome_build: hg19 with Gencode V19 annotation\nSupplementary_files_format_and_content: .osc files are simple tab delimited files. They were generated by combining the isoform.results files outputed by RSEM with the gencode v19 .gtf file. It contains abundance measurements and transcript isoforms. It also contains metadata that is inputed into ZENBU.\nSupplementary_files_format_and_content: RNAseq.counts is a simple tab delimited file containing the counts for all the RNA-seq libraries for each gen",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519
https://github.com/broadinstitute/cromwell/issues/4519:2314,Deployability,pipeline,pipeline,2314," tRNA, and may also contain regulatory RNA molecules such as microRNA (miRNA) and short interfering RNA (siRNA), snRNA, and other RNA transcripts of yet unknown function. Ambion RiboMinus rRNA depletion was performed as described in the manufacturer’s protocol (Pub. Part no.: 100004590, Rev. date 2 December 2011) following the standard protocol.\nTruSeq RNA Sample Preparation was performed on the RiboMinus™ RNA fraction as described in the manufacturer’s protocol (Pub. Part no.: 15026495 Rev. F March 2014) following the low sample protocol.\nThe libraries were sequenced on Illumina’s HiSeq 2000 instrument following standard protocol."",; ""processing"" : ""Data quality check using fastQC version 0.11.2.\nAlignment of unpaired unstranded reads using STAR version 2.4.0.\nQuantification of transcripts and isoforms using RSEM version 1.2.21 using rsem-calculate-expression, both alignment and quantification was done using the STAR_RSEM.sh pipeline (https://github.com/ENCODE-DCC/long-rna-seq-pipeline/blob/master/DAC/STAR_RSEM.sh)\nThe programe featurecounts version 1.4.6-p2 from the SourceForge Subread package was used to produce a summary file of counts from all the alignement .bam files.\nThe summary file of counts (RNAseq.counts) was used to plot the multidimensional scaling plot using edgeR version 3.1.3.\nThe *.osc.gz files were loaded into the genome browser ZENBU and was used visualize the transcripts. Screen shots were captured.\nGenome_build: hg19 with Gencode V19 annotation\nSupplementary_files_format_and_content: .osc files are simple tab delimited files. They were generated by combining the isoform.results files outputed by RSEM with the gencode v19 .gtf file. It contains abundance measurements and transcript isoforms. It also contains metadata that is inputed into ZENBU.\nSupplementary_files_format_and_content: RNAseq.counts is a simple tab delimited file containing the counts for all the RNA-seq libraries for each gene (summary file of counts).""; },; ""relations"" ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519
https://github.com/broadinstitute/cromwell/issues/4519:3502,Deployability,update,updated,3502,"lot the multidimensional scaling plot using edgeR version 3.1.3.\nThe *.osc.gz files were loaded into the genome browser ZENBU and was used visualize the transcripts. Screen shots were captured.\nGenome_build: hg19 with Gencode V19 annotation\nSupplementary_files_format_and_content: .osc files are simple tab delimited files. They were generated by combining the isoform.results files outputed by RSEM with the gencode v19 .gtf file. It contains abundance measurements and transcript isoforms. It also contains metadata that is inputed into ZENBU.\nSupplementary_files_format_and_content: RNAseq.counts is a simple tab delimited file containing the counts for all the RNA-seq libraries for each gene (summary file of counts).""; },; ""relations"" : {; ""BioSample"" : ""https://www.ncbi.nlm.nih.gov/biosample/SAMN03610550"",; ""SRA"" : ""https://www.ncbi.nlm.nih.gov/sra?term=SRX1020495""; },; ""status"" : {; ""submitted"" : ""May 29 2015"",; ""updated"" : ""Jun 01 2015""; },; ""runs"" : [; {; ""run"" : {; ""Run"" : ""SRR2014238"",; ""ReleaseDate"" : ""2015-05-25 05:44:11"",; ""LoadDate"" : ""2015-05-25 05:38:29"",; ""AssemblyName"" : """",; ""download_path"" : ""https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238"",; ""Experiment"" : ""SRX1020495""; },; ""stats"" : {; ""spots"" : 85220810,; ""bases"" : 12953563120,; ""spots_with_mates"" : 85220810,; ""avgLength"" : 152,; ""size_MB"" : 7790.0; },; ""library"" : {; ""LibraryName"" : ""Biochain_Adult_Liver"",; ""LibraryStrategy"" : ""RNA-Seq"",; ""LibrarySelection"" : ""other"",; ""LibrarySource"" : ""TRANSCRIPTOMIC"",; ""LibraryLayout"" : ""PAIRED""; },; ""sample"" : {; ""Platform"" : ""ILLUMINA"",; ""Model"" : ""Illumina HiSeq 2000"",; ""SRAStudy"" : ""SRP058036"",; ""BioProject"" : ""PRJNA283012"",; ""Study_Pubmed_id"" : """",; ""ProjectID"" : ""283012"",; ""Sample"" : ""SRS931038"",; ""BioSample"" : ""SAMN03610550"",; ""SampleType"" : ""simple"",; ""TaxID"" : ""9606"",; ""ScientificName"" : ""Homo sapiens"",; ""SampleName"" : ""Biochain_Adult_Liver""; },; ""subject"" : {; ""Subject_ID"" : """",; ""Sex"" : ""male"",; ""Disease"" : """",; ""Tumor"" : ""no",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519
https://github.com/broadinstitute/cromwell/issues/4519:3582,Deployability,Release,ReleaseDate,3582,"lot the multidimensional scaling plot using edgeR version 3.1.3.\nThe *.osc.gz files were loaded into the genome browser ZENBU and was used visualize the transcripts. Screen shots were captured.\nGenome_build: hg19 with Gencode V19 annotation\nSupplementary_files_format_and_content: .osc files are simple tab delimited files. They were generated by combining the isoform.results files outputed by RSEM with the gencode v19 .gtf file. It contains abundance measurements and transcript isoforms. It also contains metadata that is inputed into ZENBU.\nSupplementary_files_format_and_content: RNAseq.counts is a simple tab delimited file containing the counts for all the RNA-seq libraries for each gene (summary file of counts).""; },; ""relations"" : {; ""BioSample"" : ""https://www.ncbi.nlm.nih.gov/biosample/SAMN03610550"",; ""SRA"" : ""https://www.ncbi.nlm.nih.gov/sra?term=SRX1020495""; },; ""status"" : {; ""submitted"" : ""May 29 2015"",; ""updated"" : ""Jun 01 2015""; },; ""runs"" : [; {; ""run"" : {; ""Run"" : ""SRR2014238"",; ""ReleaseDate"" : ""2015-05-25 05:44:11"",; ""LoadDate"" : ""2015-05-25 05:38:29"",; ""AssemblyName"" : """",; ""download_path"" : ""https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238"",; ""Experiment"" : ""SRX1020495""; },; ""stats"" : {; ""spots"" : 85220810,; ""bases"" : 12953563120,; ""spots_with_mates"" : 85220810,; ""avgLength"" : 152,; ""size_MB"" : 7790.0; },; ""library"" : {; ""LibraryName"" : ""Biochain_Adult_Liver"",; ""LibraryStrategy"" : ""RNA-Seq"",; ""LibrarySelection"" : ""other"",; ""LibrarySource"" : ""TRANSCRIPTOMIC"",; ""LibraryLayout"" : ""PAIRED""; },; ""sample"" : {; ""Platform"" : ""ILLUMINA"",; ""Model"" : ""Illumina HiSeq 2000"",; ""SRAStudy"" : ""SRP058036"",; ""BioProject"" : ""PRJNA283012"",; ""Study_Pubmed_id"" : """",; ""ProjectID"" : ""283012"",; ""Sample"" : ""SRS931038"",; ""BioSample"" : ""SAMN03610550"",; ""SampleType"" : ""simple"",; ""TaxID"" : ""9606"",; ""ScientificName"" : ""Homo sapiens"",; ""SampleName"" : ""Biochain_Adult_Liver""; },; ""subject"" : {; ""Subject_ID"" : """",; ""Sex"" : ""male"",; ""Disease"" : """",; ""Tumor"" : ""no",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519
https://github.com/broadinstitute/cromwell/issues/4519:679,Integrability,protocol,protocol,679,"read_json almost never work for me. Here is for instance a json that is totally valid but breaks read_json; ```; {; ""id"" : ""GSM1698568"",; ""gse"" : [; ""GSE69360""; ],; ""title"" : ""Biochain_Adult_Liver"",; ""sampleType"" : ""SRA"",; ""organism"" : {; ""name"" : ""Homo sapiens"",; ""taxid"" : ""9606""; },; ""sequencer"" : ""Illumina HiSeq 2000"",; ""characteristics"" : {; ""number of donors"" : ""1"",; ""age"" : ""64 years old"",; ""tissue"" : ""Liver"",; ""vendor"" : ""Biochain"",; ""isolate"" : ""Lot no.: B510092"",; ""gender"" : ""Male""; },; ""library"" : {; ""strategy"" : ""RNA-Seq"",; ""selection"" : ""cDNA"",; ""source"" : ""transcriptomic""; },; ""extraction"" : {; ""source"" : ""Biochain Adult Liver"",; ""molecule"" : ""total RNA"",; ""protocol"" : ""2 different fetal normal tissues and 6 different adult normal tissues were purchased from different sources (Agilent, Biochain and OriGene). The qualities of these total RNA were tested using the Agilent Bioanalyzer 2100 Eukaryote Total RNA Nano Series II. Only total RNAs with a RIN score of more than 7 were used for RNA-Seq library construction\nRibosomal RNA (rRNA) was removed from total RNA using the RiboMinus™ Eukaryote Kit for RNA-Seq from Ambion. The ribosomal RNA depleted RNA fraction is termed the RiboMinus™ RNA fraction and is enriched in polyadenylated (polyA) mRNA, non-polyadenylated RNA, pre-processed RNA, tRNA, and may also contain regulatory RNA molecules such as microRNA (miRNA) and short interfering RNA (siRNA), snRNA, and other RNA transcripts of yet unknown function. Ambion RiboMinus rRNA depletion was performed as described in the manufacturer’s protocol (Pub. Part no.: 100004590, Rev. date 2 December 2011) following the standard protocol.\nTruSeq RNA Sample Preparation was performed on the RiboMinus™ RNA fraction as described in the manufacturer’s protocol (Pub. Part no.: 15026495 Rev. F March 2014) following the low sample protocol.\nThe libraries were sequenced on Illumina’s HiSeq 2000 instrument following standard protocol."",; ""processing"" : ""Data quality check usin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519
https://github.com/broadinstitute/cromwell/issues/4519:1569,Integrability,protocol,protocol,1569,"eq"",; ""selection"" : ""cDNA"",; ""source"" : ""transcriptomic""; },; ""extraction"" : {; ""source"" : ""Biochain Adult Liver"",; ""molecule"" : ""total RNA"",; ""protocol"" : ""2 different fetal normal tissues and 6 different adult normal tissues were purchased from different sources (Agilent, Biochain and OriGene). The qualities of these total RNA were tested using the Agilent Bioanalyzer 2100 Eukaryote Total RNA Nano Series II. Only total RNAs with a RIN score of more than 7 were used for RNA-Seq library construction\nRibosomal RNA (rRNA) was removed from total RNA using the RiboMinus™ Eukaryote Kit for RNA-Seq from Ambion. The ribosomal RNA depleted RNA fraction is termed the RiboMinus™ RNA fraction and is enriched in polyadenylated (polyA) mRNA, non-polyadenylated RNA, pre-processed RNA, tRNA, and may also contain regulatory RNA molecules such as microRNA (miRNA) and short interfering RNA (siRNA), snRNA, and other RNA transcripts of yet unknown function. Ambion RiboMinus rRNA depletion was performed as described in the manufacturer’s protocol (Pub. Part no.: 100004590, Rev. date 2 December 2011) following the standard protocol.\nTruSeq RNA Sample Preparation was performed on the RiboMinus™ RNA fraction as described in the manufacturer’s protocol (Pub. Part no.: 15026495 Rev. F March 2014) following the low sample protocol.\nThe libraries were sequenced on Illumina’s HiSeq 2000 instrument following standard protocol."",; ""processing"" : ""Data quality check using fastQC version 0.11.2.\nAlignment of unpaired unstranded reads using STAR version 2.4.0.\nQuantification of transcripts and isoforms using RSEM version 1.2.21 using rsem-calculate-expression, both alignment and quantification was done using the STAR_RSEM.sh pipeline (https://github.com/ENCODE-DCC/long-rna-seq-pipeline/blob/master/DAC/STAR_RSEM.sh)\nThe programe featurecounts version 1.4.6-p2 from the SourceForge Subread package was used to produce a summary file of counts from all the alignement .bam files.\nThe summary file o",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519
https://github.com/broadinstitute/cromwell/issues/4519:1655,Integrability,protocol,protocol,1655,"Adult Liver"",; ""molecule"" : ""total RNA"",; ""protocol"" : ""2 different fetal normal tissues and 6 different adult normal tissues were purchased from different sources (Agilent, Biochain and OriGene). The qualities of these total RNA were tested using the Agilent Bioanalyzer 2100 Eukaryote Total RNA Nano Series II. Only total RNAs with a RIN score of more than 7 were used for RNA-Seq library construction\nRibosomal RNA (rRNA) was removed from total RNA using the RiboMinus™ Eukaryote Kit for RNA-Seq from Ambion. The ribosomal RNA depleted RNA fraction is termed the RiboMinus™ RNA fraction and is enriched in polyadenylated (polyA) mRNA, non-polyadenylated RNA, pre-processed RNA, tRNA, and may also contain regulatory RNA molecules such as microRNA (miRNA) and short interfering RNA (siRNA), snRNA, and other RNA transcripts of yet unknown function. Ambion RiboMinus rRNA depletion was performed as described in the manufacturer’s protocol (Pub. Part no.: 100004590, Rev. date 2 December 2011) following the standard protocol.\nTruSeq RNA Sample Preparation was performed on the RiboMinus™ RNA fraction as described in the manufacturer’s protocol (Pub. Part no.: 15026495 Rev. F March 2014) following the low sample protocol.\nThe libraries were sequenced on Illumina’s HiSeq 2000 instrument following standard protocol."",; ""processing"" : ""Data quality check using fastQC version 0.11.2.\nAlignment of unpaired unstranded reads using STAR version 2.4.0.\nQuantification of transcripts and isoforms using RSEM version 1.2.21 using rsem-calculate-expression, both alignment and quantification was done using the STAR_RSEM.sh pipeline (https://github.com/ENCODE-DCC/long-rna-seq-pipeline/blob/master/DAC/STAR_RSEM.sh)\nThe programe featurecounts version 1.4.6-p2 from the SourceForge Subread package was used to produce a summary file of counts from all the alignement .bam files.\nThe summary file of counts (RNAseq.counts) was used to plot the multidimensional scaling plot using edgeR version 3.1.3.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519
https://github.com/broadinstitute/cromwell/issues/4519:1776,Integrability,protocol,protocol,1776,"d 6 different adult normal tissues were purchased from different sources (Agilent, Biochain and OriGene). The qualities of these total RNA were tested using the Agilent Bioanalyzer 2100 Eukaryote Total RNA Nano Series II. Only total RNAs with a RIN score of more than 7 were used for RNA-Seq library construction\nRibosomal RNA (rRNA) was removed from total RNA using the RiboMinus™ Eukaryote Kit for RNA-Seq from Ambion. The ribosomal RNA depleted RNA fraction is termed the RiboMinus™ RNA fraction and is enriched in polyadenylated (polyA) mRNA, non-polyadenylated RNA, pre-processed RNA, tRNA, and may also contain regulatory RNA molecules such as microRNA (miRNA) and short interfering RNA (siRNA), snRNA, and other RNA transcripts of yet unknown function. Ambion RiboMinus rRNA depletion was performed as described in the manufacturer’s protocol (Pub. Part no.: 100004590, Rev. date 2 December 2011) following the standard protocol.\nTruSeq RNA Sample Preparation was performed on the RiboMinus™ RNA fraction as described in the manufacturer’s protocol (Pub. Part no.: 15026495 Rev. F March 2014) following the low sample protocol.\nThe libraries were sequenced on Illumina’s HiSeq 2000 instrument following standard protocol."",; ""processing"" : ""Data quality check using fastQC version 0.11.2.\nAlignment of unpaired unstranded reads using STAR version 2.4.0.\nQuantification of transcripts and isoforms using RSEM version 1.2.21 using rsem-calculate-expression, both alignment and quantification was done using the STAR_RSEM.sh pipeline (https://github.com/ENCODE-DCC/long-rna-seq-pipeline/blob/master/DAC/STAR_RSEM.sh)\nThe programe featurecounts version 1.4.6-p2 from the SourceForge Subread package was used to produce a summary file of counts from all the alignement .bam files.\nThe summary file of counts (RNAseq.counts) was used to plot the multidimensional scaling plot using edgeR version 3.1.3.\nThe *.osc.gz files were loaded into the genome browser ZENBU and was used visualize the ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519
https://github.com/broadinstitute/cromwell/issues/4519:1854,Integrability,protocol,protocol,1854,"ualities of these total RNA were tested using the Agilent Bioanalyzer 2100 Eukaryote Total RNA Nano Series II. Only total RNAs with a RIN score of more than 7 were used for RNA-Seq library construction\nRibosomal RNA (rRNA) was removed from total RNA using the RiboMinus™ Eukaryote Kit for RNA-Seq from Ambion. The ribosomal RNA depleted RNA fraction is termed the RiboMinus™ RNA fraction and is enriched in polyadenylated (polyA) mRNA, non-polyadenylated RNA, pre-processed RNA, tRNA, and may also contain regulatory RNA molecules such as microRNA (miRNA) and short interfering RNA (siRNA), snRNA, and other RNA transcripts of yet unknown function. Ambion RiboMinus rRNA depletion was performed as described in the manufacturer’s protocol (Pub. Part no.: 100004590, Rev. date 2 December 2011) following the standard protocol.\nTruSeq RNA Sample Preparation was performed on the RiboMinus™ RNA fraction as described in the manufacturer’s protocol (Pub. Part no.: 15026495 Rev. F March 2014) following the low sample protocol.\nThe libraries were sequenced on Illumina’s HiSeq 2000 instrument following standard protocol."",; ""processing"" : ""Data quality check using fastQC version 0.11.2.\nAlignment of unpaired unstranded reads using STAR version 2.4.0.\nQuantification of transcripts and isoforms using RSEM version 1.2.21 using rsem-calculate-expression, both alignment and quantification was done using the STAR_RSEM.sh pipeline (https://github.com/ENCODE-DCC/long-rna-seq-pipeline/blob/master/DAC/STAR_RSEM.sh)\nThe programe featurecounts version 1.4.6-p2 from the SourceForge Subread package was used to produce a summary file of counts from all the alignement .bam files.\nThe summary file of counts (RNAseq.counts) was used to plot the multidimensional scaling plot using edgeR version 3.1.3.\nThe *.osc.gz files were loaded into the genome browser ZENBU and was used visualize the transcripts. Screen shots were captured.\nGenome_build: hg19 with Gencode V19 annotation\nSupplementary_files_fo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519
https://github.com/broadinstitute/cromwell/issues/4519:1949,Integrability,protocol,protocol,1949,"00 Eukaryote Total RNA Nano Series II. Only total RNAs with a RIN score of more than 7 were used for RNA-Seq library construction\nRibosomal RNA (rRNA) was removed from total RNA using the RiboMinus™ Eukaryote Kit for RNA-Seq from Ambion. The ribosomal RNA depleted RNA fraction is termed the RiboMinus™ RNA fraction and is enriched in polyadenylated (polyA) mRNA, non-polyadenylated RNA, pre-processed RNA, tRNA, and may also contain regulatory RNA molecules such as microRNA (miRNA) and short interfering RNA (siRNA), snRNA, and other RNA transcripts of yet unknown function. Ambion RiboMinus rRNA depletion was performed as described in the manufacturer’s protocol (Pub. Part no.: 100004590, Rev. date 2 December 2011) following the standard protocol.\nTruSeq RNA Sample Preparation was performed on the RiboMinus™ RNA fraction as described in the manufacturer’s protocol (Pub. Part no.: 15026495 Rev. F March 2014) following the low sample protocol.\nThe libraries were sequenced on Illumina’s HiSeq 2000 instrument following standard protocol."",; ""processing"" : ""Data quality check using fastQC version 0.11.2.\nAlignment of unpaired unstranded reads using STAR version 2.4.0.\nQuantification of transcripts and isoforms using RSEM version 1.2.21 using rsem-calculate-expression, both alignment and quantification was done using the STAR_RSEM.sh pipeline (https://github.com/ENCODE-DCC/long-rna-seq-pipeline/blob/master/DAC/STAR_RSEM.sh)\nThe programe featurecounts version 1.4.6-p2 from the SourceForge Subread package was used to produce a summary file of counts from all the alignement .bam files.\nThe summary file of counts (RNAseq.counts) was used to plot the multidimensional scaling plot using edgeR version 3.1.3.\nThe *.osc.gz files were loaded into the genome browser ZENBU and was used visualize the transcripts. Screen shots were captured.\nGenome_build: hg19 with Gencode V19 annotation\nSupplementary_files_format_and_content: .osc files are simple tab delimited files. They were g",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519
https://github.com/broadinstitute/cromwell/issues/4519:1524,Performance,perform,performed,1524,"eq"",; ""selection"" : ""cDNA"",; ""source"" : ""transcriptomic""; },; ""extraction"" : {; ""source"" : ""Biochain Adult Liver"",; ""molecule"" : ""total RNA"",; ""protocol"" : ""2 different fetal normal tissues and 6 different adult normal tissues were purchased from different sources (Agilent, Biochain and OriGene). The qualities of these total RNA were tested using the Agilent Bioanalyzer 2100 Eukaryote Total RNA Nano Series II. Only total RNAs with a RIN score of more than 7 were used for RNA-Seq library construction\nRibosomal RNA (rRNA) was removed from total RNA using the RiboMinus™ Eukaryote Kit for RNA-Seq from Ambion. The ribosomal RNA depleted RNA fraction is termed the RiboMinus™ RNA fraction and is enriched in polyadenylated (polyA) mRNA, non-polyadenylated RNA, pre-processed RNA, tRNA, and may also contain regulatory RNA molecules such as microRNA (miRNA) and short interfering RNA (siRNA), snRNA, and other RNA transcripts of yet unknown function. Ambion RiboMinus rRNA depletion was performed as described in the manufacturer’s protocol (Pub. Part no.: 100004590, Rev. date 2 December 2011) following the standard protocol.\nTruSeq RNA Sample Preparation was performed on the RiboMinus™ RNA fraction as described in the manufacturer’s protocol (Pub. Part no.: 15026495 Rev. F March 2014) following the low sample protocol.\nThe libraries were sequenced on Illumina’s HiSeq 2000 instrument following standard protocol."",; ""processing"" : ""Data quality check using fastQC version 0.11.2.\nAlignment of unpaired unstranded reads using STAR version 2.4.0.\nQuantification of transcripts and isoforms using RSEM version 1.2.21 using rsem-calculate-expression, both alignment and quantification was done using the STAR_RSEM.sh pipeline (https://github.com/ENCODE-DCC/long-rna-seq-pipeline/blob/master/DAC/STAR_RSEM.sh)\nThe programe featurecounts version 1.4.6-p2 from the SourceForge Subread package was used to produce a summary file of counts from all the alignement .bam files.\nThe summary file o",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519
https://github.com/broadinstitute/cromwell/issues/4519:1700,Performance,perform,performed,1700,"d 6 different adult normal tissues were purchased from different sources (Agilent, Biochain and OriGene). The qualities of these total RNA were tested using the Agilent Bioanalyzer 2100 Eukaryote Total RNA Nano Series II. Only total RNAs with a RIN score of more than 7 were used for RNA-Seq library construction\nRibosomal RNA (rRNA) was removed from total RNA using the RiboMinus™ Eukaryote Kit for RNA-Seq from Ambion. The ribosomal RNA depleted RNA fraction is termed the RiboMinus™ RNA fraction and is enriched in polyadenylated (polyA) mRNA, non-polyadenylated RNA, pre-processed RNA, tRNA, and may also contain regulatory RNA molecules such as microRNA (miRNA) and short interfering RNA (siRNA), snRNA, and other RNA transcripts of yet unknown function. Ambion RiboMinus rRNA depletion was performed as described in the manufacturer’s protocol (Pub. Part no.: 100004590, Rev. date 2 December 2011) following the standard protocol.\nTruSeq RNA Sample Preparation was performed on the RiboMinus™ RNA fraction as described in the manufacturer’s protocol (Pub. Part no.: 15026495 Rev. F March 2014) following the low sample protocol.\nThe libraries were sequenced on Illumina’s HiSeq 2000 instrument following standard protocol."",; ""processing"" : ""Data quality check using fastQC version 0.11.2.\nAlignment of unpaired unstranded reads using STAR version 2.4.0.\nQuantification of transcripts and isoforms using RSEM version 1.2.21 using rsem-calculate-expression, both alignment and quantification was done using the STAR_RSEM.sh pipeline (https://github.com/ENCODE-DCC/long-rna-seq-pipeline/blob/master/DAC/STAR_RSEM.sh)\nThe programe featurecounts version 1.4.6-p2 from the SourceForge Subread package was used to produce a summary file of counts from all the alignement .bam files.\nThe summary file of counts (RNAseq.counts) was used to plot the multidimensional scaling plot using edgeR version 3.1.3.\nThe *.osc.gz files were loaded into the genome browser ZENBU and was used visualize the ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519
https://github.com/broadinstitute/cromwell/issues/4519:2663,Performance,load,loaded,2663,"n was performed on the RiboMinus™ RNA fraction as described in the manufacturer’s protocol (Pub. Part no.: 15026495 Rev. F March 2014) following the low sample protocol.\nThe libraries were sequenced on Illumina’s HiSeq 2000 instrument following standard protocol."",; ""processing"" : ""Data quality check using fastQC version 0.11.2.\nAlignment of unpaired unstranded reads using STAR version 2.4.0.\nQuantification of transcripts and isoforms using RSEM version 1.2.21 using rsem-calculate-expression, both alignment and quantification was done using the STAR_RSEM.sh pipeline (https://github.com/ENCODE-DCC/long-rna-seq-pipeline/blob/master/DAC/STAR_RSEM.sh)\nThe programe featurecounts version 1.4.6-p2 from the SourceForge Subread package was used to produce a summary file of counts from all the alignement .bam files.\nThe summary file of counts (RNAseq.counts) was used to plot the multidimensional scaling plot using edgeR version 3.1.3.\nThe *.osc.gz files were loaded into the genome browser ZENBU and was used visualize the transcripts. Screen shots were captured.\nGenome_build: hg19 with Gencode V19 annotation\nSupplementary_files_format_and_content: .osc files are simple tab delimited files. They were generated by combining the isoform.results files outputed by RSEM with the gencode v19 .gtf file. It contains abundance measurements and transcript isoforms. It also contains metadata that is inputed into ZENBU.\nSupplementary_files_format_and_content: RNAseq.counts is a simple tab delimited file containing the counts for all the RNA-seq libraries for each gene (summary file of counts).""; },; ""relations"" : {; ""BioSample"" : ""https://www.ncbi.nlm.nih.gov/biosample/SAMN03610550"",; ""SRA"" : ""https://www.ncbi.nlm.nih.gov/sra?term=SRX1020495""; },; ""status"" : {; ""submitted"" : ""May 29 2015"",; ""updated"" : ""Jun 01 2015""; },; ""runs"" : [; {; ""run"" : {; ""Run"" : ""SRR2014238"",; ""ReleaseDate"" : ""2015-05-25 05:44:11"",; ""LoadDate"" : ""2015-05-25 05:38:29"",; ""AssemblyName"" : """",; ""download_path",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519
https://github.com/broadinstitute/cromwell/issues/4519:3622,Performance,Load,LoadDate,3622,"lot the multidimensional scaling plot using edgeR version 3.1.3.\nThe *.osc.gz files were loaded into the genome browser ZENBU and was used visualize the transcripts. Screen shots were captured.\nGenome_build: hg19 with Gencode V19 annotation\nSupplementary_files_format_and_content: .osc files are simple tab delimited files. They were generated by combining the isoform.results files outputed by RSEM with the gencode v19 .gtf file. It contains abundance measurements and transcript isoforms. It also contains metadata that is inputed into ZENBU.\nSupplementary_files_format_and_content: RNAseq.counts is a simple tab delimited file containing the counts for all the RNA-seq libraries for each gene (summary file of counts).""; },; ""relations"" : {; ""BioSample"" : ""https://www.ncbi.nlm.nih.gov/biosample/SAMN03610550"",; ""SRA"" : ""https://www.ncbi.nlm.nih.gov/sra?term=SRX1020495""; },; ""status"" : {; ""submitted"" : ""May 29 2015"",; ""updated"" : ""Jun 01 2015""; },; ""runs"" : [; {; ""run"" : {; ""Run"" : ""SRR2014238"",; ""ReleaseDate"" : ""2015-05-25 05:44:11"",; ""LoadDate"" : ""2015-05-25 05:38:29"",; ""AssemblyName"" : """",; ""download_path"" : ""https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238"",; ""Experiment"" : ""SRX1020495""; },; ""stats"" : {; ""spots"" : 85220810,; ""bases"" : 12953563120,; ""spots_with_mates"" : 85220810,; ""avgLength"" : 152,; ""size_MB"" : 7790.0; },; ""library"" : {; ""LibraryName"" : ""Biochain_Adult_Liver"",; ""LibraryStrategy"" : ""RNA-Seq"",; ""LibrarySelection"" : ""other"",; ""LibrarySource"" : ""TRANSCRIPTOMIC"",; ""LibraryLayout"" : ""PAIRED""; },; ""sample"" : {; ""Platform"" : ""ILLUMINA"",; ""Model"" : ""Illumina HiSeq 2000"",; ""SRAStudy"" : ""SRP058036"",; ""BioProject"" : ""PRJNA283012"",; ""Study_Pubmed_id"" : """",; ""ProjectID"" : ""283012"",; ""Sample"" : ""SRS931038"",; ""BioSample"" : ""SAMN03610550"",; ""SampleType"" : ""simple"",; ""TaxID"" : ""9606"",; ""ScientificName"" : ""Homo sapiens"",; ""SampleName"" : ""Biochain_Adult_Liver""; },; ""subject"" : {; ""Subject_ID"" : """",; ""Sex"" : ""male"",; ""Disease"" : """",; ""Tumor"" : ""no",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519
https://github.com/broadinstitute/cromwell/issues/4519:871,Testability,test,tested,871,"read_json almost never work for me. Here is for instance a json that is totally valid but breaks read_json; ```; {; ""id"" : ""GSM1698568"",; ""gse"" : [; ""GSE69360""; ],; ""title"" : ""Biochain_Adult_Liver"",; ""sampleType"" : ""SRA"",; ""organism"" : {; ""name"" : ""Homo sapiens"",; ""taxid"" : ""9606""; },; ""sequencer"" : ""Illumina HiSeq 2000"",; ""characteristics"" : {; ""number of donors"" : ""1"",; ""age"" : ""64 years old"",; ""tissue"" : ""Liver"",; ""vendor"" : ""Biochain"",; ""isolate"" : ""Lot no.: B510092"",; ""gender"" : ""Male""; },; ""library"" : {; ""strategy"" : ""RNA-Seq"",; ""selection"" : ""cDNA"",; ""source"" : ""transcriptomic""; },; ""extraction"" : {; ""source"" : ""Biochain Adult Liver"",; ""molecule"" : ""total RNA"",; ""protocol"" : ""2 different fetal normal tissues and 6 different adult normal tissues were purchased from different sources (Agilent, Biochain and OriGene). The qualities of these total RNA were tested using the Agilent Bioanalyzer 2100 Eukaryote Total RNA Nano Series II. Only total RNAs with a RIN score of more than 7 were used for RNA-Seq library construction\nRibosomal RNA (rRNA) was removed from total RNA using the RiboMinus™ Eukaryote Kit for RNA-Seq from Ambion. The ribosomal RNA depleted RNA fraction is termed the RiboMinus™ RNA fraction and is enriched in polyadenylated (polyA) mRNA, non-polyadenylated RNA, pre-processed RNA, tRNA, and may also contain regulatory RNA molecules such as microRNA (miRNA) and short interfering RNA (siRNA), snRNA, and other RNA transcripts of yet unknown function. Ambion RiboMinus rRNA depletion was performed as described in the manufacturer’s protocol (Pub. Part no.: 100004590, Rev. date 2 December 2011) following the standard protocol.\nTruSeq RNA Sample Preparation was performed on the RiboMinus™ RNA fraction as described in the manufacturer’s protocol (Pub. Part no.: 15026495 Rev. F March 2014) following the low sample protocol.\nThe libraries were sequenced on Illumina’s HiSeq 2000 instrument following standard protocol."",; ""processing"" : ""Data quality check usin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519
https://github.com/broadinstitute/cromwell/issues/4519:2872,Usability,simpl,simple,2872," were sequenced on Illumina’s HiSeq 2000 instrument following standard protocol."",; ""processing"" : ""Data quality check using fastQC version 0.11.2.\nAlignment of unpaired unstranded reads using STAR version 2.4.0.\nQuantification of transcripts and isoforms using RSEM version 1.2.21 using rsem-calculate-expression, both alignment and quantification was done using the STAR_RSEM.sh pipeline (https://github.com/ENCODE-DCC/long-rna-seq-pipeline/blob/master/DAC/STAR_RSEM.sh)\nThe programe featurecounts version 1.4.6-p2 from the SourceForge Subread package was used to produce a summary file of counts from all the alignement .bam files.\nThe summary file of counts (RNAseq.counts) was used to plot the multidimensional scaling plot using edgeR version 3.1.3.\nThe *.osc.gz files were loaded into the genome browser ZENBU and was used visualize the transcripts. Screen shots were captured.\nGenome_build: hg19 with Gencode V19 annotation\nSupplementary_files_format_and_content: .osc files are simple tab delimited files. They were generated by combining the isoform.results files outputed by RSEM with the gencode v19 .gtf file. It contains abundance measurements and transcript isoforms. It also contains metadata that is inputed into ZENBU.\nSupplementary_files_format_and_content: RNAseq.counts is a simple tab delimited file containing the counts for all the RNA-seq libraries for each gene (summary file of counts).""; },; ""relations"" : {; ""BioSample"" : ""https://www.ncbi.nlm.nih.gov/biosample/SAMN03610550"",; ""SRA"" : ""https://www.ncbi.nlm.nih.gov/sra?term=SRX1020495""; },; ""status"" : {; ""submitted"" : ""May 29 2015"",; ""updated"" : ""Jun 01 2015""; },; ""runs"" : [; {; ""run"" : {; ""Run"" : ""SRR2014238"",; ""ReleaseDate"" : ""2015-05-25 05:44:11"",; ""LoadDate"" : ""2015-05-25 05:38:29"",; ""AssemblyName"" : """",; ""download_path"" : ""https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238"",; ""Experiment"" : ""SRX1020495""; },; ""stats"" : {; ""spots"" : 85220810,; ""bases"" : 12953563120,; ""spots_with_m",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519
https://github.com/broadinstitute/cromwell/issues/4519:3182,Usability,simpl,simple,3182,"one using the STAR_RSEM.sh pipeline (https://github.com/ENCODE-DCC/long-rna-seq-pipeline/blob/master/DAC/STAR_RSEM.sh)\nThe programe featurecounts version 1.4.6-p2 from the SourceForge Subread package was used to produce a summary file of counts from all the alignement .bam files.\nThe summary file of counts (RNAseq.counts) was used to plot the multidimensional scaling plot using edgeR version 3.1.3.\nThe *.osc.gz files were loaded into the genome browser ZENBU and was used visualize the transcripts. Screen shots were captured.\nGenome_build: hg19 with Gencode V19 annotation\nSupplementary_files_format_and_content: .osc files are simple tab delimited files. They were generated by combining the isoform.results files outputed by RSEM with the gencode v19 .gtf file. It contains abundance measurements and transcript isoforms. It also contains metadata that is inputed into ZENBU.\nSupplementary_files_format_and_content: RNAseq.counts is a simple tab delimited file containing the counts for all the RNA-seq libraries for each gene (summary file of counts).""; },; ""relations"" : {; ""BioSample"" : ""https://www.ncbi.nlm.nih.gov/biosample/SAMN03610550"",; ""SRA"" : ""https://www.ncbi.nlm.nih.gov/sra?term=SRX1020495""; },; ""status"" : {; ""submitted"" : ""May 29 2015"",; ""updated"" : ""Jun 01 2015""; },; ""runs"" : [; {; ""run"" : {; ""Run"" : ""SRR2014238"",; ""ReleaseDate"" : ""2015-05-25 05:44:11"",; ""LoadDate"" : ""2015-05-25 05:38:29"",; ""AssemblyName"" : """",; ""download_path"" : ""https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238"",; ""Experiment"" : ""SRX1020495""; },; ""stats"" : {; ""spots"" : 85220810,; ""bases"" : 12953563120,; ""spots_with_mates"" : 85220810,; ""avgLength"" : 152,; ""size_MB"" : 7790.0; },; ""library"" : {; ""LibraryName"" : ""Biochain_Adult_Liver"",; ""LibraryStrategy"" : ""RNA-Seq"",; ""LibrarySelection"" : ""other"",; ""LibrarySource"" : ""TRANSCRIPTOMIC"",; ""LibraryLayout"" : ""PAIRED""; },; ""sample"" : {; ""Platform"" : ""ILLUMINA"",; ""Model"" : ""Illumina HiSeq 2000"",; ""SRAStudy"" : ""SRP058036"",; ""Bio",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519
https://github.com/broadinstitute/cromwell/issues/4519:4384,Usability,simpl,simple,4384,"script isoforms. It also contains metadata that is inputed into ZENBU.\nSupplementary_files_format_and_content: RNAseq.counts is a simple tab delimited file containing the counts for all the RNA-seq libraries for each gene (summary file of counts).""; },; ""relations"" : {; ""BioSample"" : ""https://www.ncbi.nlm.nih.gov/biosample/SAMN03610550"",; ""SRA"" : ""https://www.ncbi.nlm.nih.gov/sra?term=SRX1020495""; },; ""status"" : {; ""submitted"" : ""May 29 2015"",; ""updated"" : ""Jun 01 2015""; },; ""runs"" : [; {; ""run"" : {; ""Run"" : ""SRR2014238"",; ""ReleaseDate"" : ""2015-05-25 05:44:11"",; ""LoadDate"" : ""2015-05-25 05:38:29"",; ""AssemblyName"" : """",; ""download_path"" : ""https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238"",; ""Experiment"" : ""SRX1020495""; },; ""stats"" : {; ""spots"" : 85220810,; ""bases"" : 12953563120,; ""spots_with_mates"" : 85220810,; ""avgLength"" : 152,; ""size_MB"" : 7790.0; },; ""library"" : {; ""LibraryName"" : ""Biochain_Adult_Liver"",; ""LibraryStrategy"" : ""RNA-Seq"",; ""LibrarySelection"" : ""other"",; ""LibrarySource"" : ""TRANSCRIPTOMIC"",; ""LibraryLayout"" : ""PAIRED""; },; ""sample"" : {; ""Platform"" : ""ILLUMINA"",; ""Model"" : ""Illumina HiSeq 2000"",; ""SRAStudy"" : ""SRP058036"",; ""BioProject"" : ""PRJNA283012"",; ""Study_Pubmed_id"" : """",; ""ProjectID"" : ""283012"",; ""Sample"" : ""SRS931038"",; ""BioSample"" : ""SAMN03610550"",; ""SampleType"" : ""simple"",; ""TaxID"" : ""9606"",; ""ScientificName"" : ""Homo sapiens"",; ""SampleName"" : ""Biochain_Adult_Liver""; },; ""subject"" : {; ""Subject_ID"" : """",; ""Sex"" : ""male"",; ""Disease"" : """",; ""Tumor"" : ""no"",; ""Affection_Status"" : """",; ""Analyte_Type"" : """",; ""Histological_Type"" : """",; ""Body_Site"" : """"; },; ""other"" : {; ""InsertSize"" : ""158"",; ""InsertDev"" : ""41"",; ""g1k_pop_code"" : """",; ""source"" : """",; ""g1k_analysis_group"" : """",; ""CenterName"" : ""CANCER SCIENCE INSTITUTE OF SINGAPORE"",; ""Submission"" : ""SRA266153"",; ""dbgap_study_accession"" : """",; ""Consent"" : ""public"",; ""RunHash"" : ""0189DDD0D225B2E8DEA03FC1EEFCB0F5"",; ""ReadHash"" : ""98D4DE007275783AF1596BEDD6502C11""; }; }; ]; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519
https://github.com/broadinstitute/cromwell/issues/4520:83,Availability,error,errors,83,"Straight from the log: ""Something has gone horribly wrong!"". Jenkins build 2475 (2 errors); Jenkins build 2470; Jenkins build 2426",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4520
https://github.com/broadinstitute/cromwell/issues/4520:18,Testability,log,log,18,"Straight from the log: ""Something has gone horribly wrong!"". Jenkins build 2475 (2 errors); Jenkins build 2470; Jenkins build 2426",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4520
https://github.com/broadinstitute/cromwell/pull/4522:336,Integrability,depend,dependencies,336,"From the hackathon:. Does a few potentially positive things:; - Adds a `dot` format graph to metadata; - Implements `wom graph` for WOM (ie so it should work equally for all languages - CWL, WDL 1.0, et al); - Provides a live status DAG endpoint, showing the current status of each step of a workflow as nodes within the graph of other dependencies. And a potentially negative one:; - Changes up the `wom graph` output for draft-2 in potentially backwards incompatible ways (and is not tested so might be outright broken)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4522
https://github.com/broadinstitute/cromwell/pull/4522:486,Testability,test,tested,486,"From the hackathon:. Does a few potentially positive things:; - Adds a `dot` format graph to metadata; - Implements `wom graph` for WOM (ie so it should work equally for all languages - CWL, WDL 1.0, et al); - Provides a live status DAG endpoint, showing the current status of each step of a workflow as nodes within the graph of other dependencies. And a potentially negative one:; - Changes up the `wom graph` output for draft-2 in potentially backwards incompatible ways (and is not tested so might be outright broken)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4522
https://github.com/broadinstitute/cromwell/issues/4526:45,Availability,error,error,45,"When I try to use globs, I get the following error:. ```; Evaluating glob(file_pattern) failed: glob(path, pattern) not implemented yet; ```. I've created a minimal reproducible example:. ## File `glob.wdl`; ```; task hello {; File in. command {; egrep 'CROMWELL' '${in}'; }. output {; Array[String] matches = read_lines(stdout()); }; }. workflow example {; String file_pattern = '*.sh' ; Array[File] files = glob(file_pattern); scatter(path in files) {; call hello {input: in=path}; }; }; ```. When run with all-default paramters as follows:. ```; java -jar cromwell-36.jar run glob.wdl; ```. The following output results:. ```; [2019-01-07 16:21:06,14] [info] Running with database db.url = jdbc:hsqldb:mem:094e8bf9-be0f-4d7c-854a-0cf1a15dc0d7;shutdown=false;hsqldb.tx=mvcc; [2019-01-07 16:21:16,40] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-07 16:21:16,42] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-07 16:21:16,62] [info] Running with database db.url = jdbc:hsqldb:mem:2efc8123-f7e8-4fe3-abed-48d1bcf8eb97;shutdown=false;hsqldb.tx=mvcc; [2019-01-07 16:21:17,27] [info] Slf4jLogger started; [2019-01-07 16:21:17,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-231ef13"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-01-07 16:21:17,87] [info] Metadata summary refreshing every 2 seconds.; [2019-01-07 16:21:17,94] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,00] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-01-07 16:21:19,53] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-01-07 16:21:19,58] [info] SingleWorkflowRunner",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:1239,Availability,heartbeat,heartbeat,1239,"_lines(stdout()); }; }. workflow example {; String file_pattern = '*.sh' ; Array[File] files = glob(file_pattern); scatter(path in files) {; call hello {input: in=path}; }; }; ```. When run with all-default paramters as follows:. ```; java -jar cromwell-36.jar run glob.wdl; ```. The following output results:. ```; [2019-01-07 16:21:06,14] [info] Running with database db.url = jdbc:hsqldb:mem:094e8bf9-be0f-4d7c-854a-0cf1a15dc0d7;shutdown=false;hsqldb.tx=mvcc; [2019-01-07 16:21:16,40] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-07 16:21:16,42] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-07 16:21:16,62] [info] Running with database db.url = jdbc:hsqldb:mem:2efc8123-f7e8-4fe3-abed-48d1bcf8eb97;shutdown=false;hsqldb.tx=mvcc; [2019-01-07 16:21:17,27] [info] Slf4jLogger started; [2019-01-07 16:21:17,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-231ef13"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-01-07 16:21:17,87] [info] Metadata summary refreshing every 2 seconds.; [2019-01-07 16:21:17,94] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,00] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-01-07 16:21:19,53] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-01-07 16:21:19,58] [info] SingleWorkflowRunnerActor: Version 36; [2019-01-07 16:21:19,61] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-01-07 16:21:19,69] [info] Unspecified type (Unspecified version) workflow 18de8166-5f29-4288-9fa4-6741565446fd submitted; [2019-01-07 16:21:19,74] [info] SingleWorkflowRunnerActor: Workflow submitted [38;5;2m",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:1303,Availability,heartbeat,heartbeatInterval,1303,"_lines(stdout()); }; }. workflow example {; String file_pattern = '*.sh' ; Array[File] files = glob(file_pattern); scatter(path in files) {; call hello {input: in=path}; }; }; ```. When run with all-default paramters as follows:. ```; java -jar cromwell-36.jar run glob.wdl; ```. The following output results:. ```; [2019-01-07 16:21:06,14] [info] Running with database db.url = jdbc:hsqldb:mem:094e8bf9-be0f-4d7c-854a-0cf1a15dc0d7;shutdown=false;hsqldb.tx=mvcc; [2019-01-07 16:21:16,40] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-07 16:21:16,42] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-07 16:21:16,62] [info] Running with database db.url = jdbc:hsqldb:mem:2efc8123-f7e8-4fe3-abed-48d1bcf8eb97;shutdown=false;hsqldb.tx=mvcc; [2019-01-07 16:21:17,27] [info] Slf4jLogger started; [2019-01-07 16:21:17,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-231ef13"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-01-07 16:21:17,87] [info] Metadata summary refreshing every 2 seconds.; [2019-01-07 16:21:17,94] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,00] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-01-07 16:21:19,53] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-01-07 16:21:19,58] [info] SingleWorkflowRunnerActor: Version 36; [2019-01-07 16:21:19,61] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-01-07 16:21:19,69] [info] Unspecified type (Unspecified version) workflow 18de8166-5f29-4288-9fa4-6741565446fd submitted; [2019-01-07 16:21:19,74] [info] SingleWorkflowRunnerActor: Workflow submitted [38;5;2m",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:7084,Availability,down,down,7084,".scala:49); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2019-01-07 16:21:23,91] [info] WorkflowManagerActor WorkflowActor-18de8166-5f29-4288-9fa4-6741565446fd is in a terminal state: WorkflowFailedState; [2019-01-07 16:21:30,36] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-01-07 16:21:32,96] [info] Workflow polling stopped; [2019-01-07 16:21:32,99] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-01-07 16:21:32,99] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-01-07 16:21:33,02] [info] Aborting all running workflows.; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 secon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:7172,Availability,down,down,7172,".scala:49); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2019-01-07 16:21:23,91] [info] WorkflowManagerActor WorkflowActor-18de8166-5f29-4288-9fa4-6741565446fd is in a terminal state: WorkflowFailedState; [2019-01-07 16:21:30,36] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-01-07 16:21:32,96] [info] Workflow polling stopped; [2019-01-07 16:21:32,99] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-01-07 16:21:32,99] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-01-07 16:21:33,02] [info] Aborting all running workflows.; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 secon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:7388,Availability,down,down,7388,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:7615,Availability,down,down,7615,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:7860,Availability,down,down,7860,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:7907,Availability,down,down,7907,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:8001,Availability,down,down,8001,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:8087,Availability,down,down,8087,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:8179,Availability,down,down,8179,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:8272,Availability,down,down,8272,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:8360,Availability,down,down,8360,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:8578,Availability,down,down,8578,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:8674,Availability,down,down,8674,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:8764,Availability,down,down,8764,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:9135,Availability,down,down,9135,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:1249,Deployability,configurat,configuration,1249,"_lines(stdout()); }; }. workflow example {; String file_pattern = '*.sh' ; Array[File] files = glob(file_pattern); scatter(path in files) {; call hello {input: in=path}; }; }; ```. When run with all-default paramters as follows:. ```; java -jar cromwell-36.jar run glob.wdl; ```. The following output results:. ```; [2019-01-07 16:21:06,14] [info] Running with database db.url = jdbc:hsqldb:mem:094e8bf9-be0f-4d7c-854a-0cf1a15dc0d7;shutdown=false;hsqldb.tx=mvcc; [2019-01-07 16:21:16,40] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-07 16:21:16,42] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-07 16:21:16,62] [info] Running with database db.url = jdbc:hsqldb:mem:2efc8123-f7e8-4fe3-abed-48d1bcf8eb97;shutdown=false;hsqldb.tx=mvcc; [2019-01-07 16:21:17,27] [info] Slf4jLogger started; [2019-01-07 16:21:17,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-231ef13"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-01-07 16:21:17,87] [info] Metadata summary refreshing every 2 seconds.; [2019-01-07 16:21:17,94] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,00] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-01-07 16:21:19,53] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-01-07 16:21:19,58] [info] SingleWorkflowRunnerActor: Version 36; [2019-01-07 16:21:19,61] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-01-07 16:21:19,69] [info] Unspecified type (Unspecified version) workflow 18de8166-5f29-4288-9fa4-6741565446fd submitted; [2019-01-07 16:21:19,74] [info] SingleWorkflowRunnerActor: Workflow submitted [38;5;2m",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:2975,Integrability,message,message,2975,"tor: Submitting workflow; [2019-01-07 16:21:19,69] [info] Unspecified type (Unspecified version) workflow 18de8166-5f29-4288-9fa4-6741565446fd submitted; [2019-01-07 16:21:19,74] [info] SingleWorkflowRunnerActor: Workflow submitted [38;5;2m18de8166-5f29-4288-9fa4-6741565446fd[0m; [2019-01-07 16:21:19,75] [info] 1 new workflows fetched; [2019-01-07 16:21:19,77] [info] WorkflowManagerActor Starting workflow [38;5;2m18de8166-5f29-4288-9fa4-6741565446fd[0m; [2019-01-07 16:21:19,77] [info] WorkflowManagerActor Successfully started WorkflowActor-18de8166-5f29-4288-9fa4-6741565446fd; [2019-01-07 16:21:19,77] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2019-01-07 16:21:19,78] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2019-01-07 16:21:19,80] [[38;5;220mwarn[0m] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2019-01-07 16:21:19,98] [info] MaterializeWorkflowDescriptorActor [[38;5;2m18de8166[0m]: Parsing workflow as WDL draft-2; [2019-01-07 16:21:21,18] [info] MaterializeWorkflowDescriptorActor [[38;5;2m18de8166[0m]: Call-to-Backend assignments: example.hello -> Local; [2019-01-07 16:21:23,89] [[38;5;1merror[0m] WorkflowManagerActor Workflow 18de8166-5f29-4288-9fa4-6741565446fd failed (during ExecutingWorkflowState): java.lang.RuntimeException: Failed to evaluate 'example.files' (reason 1 of 1): Evaluating glob(file_pattern) failed: glob(path, pattern) not implemented yet; 	at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:29); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$startRunnableNodes$7(WorkflowExecutionActor.scala:510); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:73); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:63); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$2(list.scala:65); 	at cats.Eva",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:8593,Integrability,message,messages,8593,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:8689,Integrability,message,messages,8689,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:8779,Integrability,message,messages,8779,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:1249,Modifiability,config,configuration,1249,"_lines(stdout()); }; }. workflow example {; String file_pattern = '*.sh' ; Array[File] files = glob(file_pattern); scatter(path in files) {; call hello {input: in=path}; }; }; ```. When run with all-default paramters as follows:. ```; java -jar cromwell-36.jar run glob.wdl; ```. The following output results:. ```; [2019-01-07 16:21:06,14] [info] Running with database db.url = jdbc:hsqldb:mem:094e8bf9-be0f-4d7c-854a-0cf1a15dc0d7;shutdown=false;hsqldb.tx=mvcc; [2019-01-07 16:21:16,40] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-07 16:21:16,42] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-07 16:21:16,62] [info] Running with database db.url = jdbc:hsqldb:mem:2efc8123-f7e8-4fe3-abed-48d1bcf8eb97;shutdown=false;hsqldb.tx=mvcc; [2019-01-07 16:21:17,27] [info] Slf4jLogger started; [2019-01-07 16:21:17,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-231ef13"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-01-07 16:21:17,87] [info] Metadata summary refreshing every 2 seconds.; [2019-01-07 16:21:17,94] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,00] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-01-07 16:21:19,53] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-01-07 16:21:19,58] [info] SingleWorkflowRunnerActor: Version 36; [2019-01-07 16:21:19,61] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-01-07 16:21:19,69] [info] Unspecified type (Unspecified version) workflow 18de8166-5f29-4288-9fa4-6741565446fd submitted; [2019-01-07 16:21:19,74] [info] SingleWorkflowRunnerActor: Workflow submitted [38;5;2m",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:1546,Modifiability,config,configured,1546," -jar cromwell-36.jar run glob.wdl; ```. The following output results:. ```; [2019-01-07 16:21:06,14] [info] Running with database db.url = jdbc:hsqldb:mem:094e8bf9-be0f-4d7c-854a-0cf1a15dc0d7;shutdown=false;hsqldb.tx=mvcc; [2019-01-07 16:21:16,40] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-07 16:21:16,42] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-07 16:21:16,62] [info] Running with database db.url = jdbc:hsqldb:mem:2efc8123-f7e8-4fe3-abed-48d1bcf8eb97;shutdown=false;hsqldb.tx=mvcc; [2019-01-07 16:21:17,27] [info] Slf4jLogger started; [2019-01-07 16:21:17,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-231ef13"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-01-07 16:21:17,87] [info] Metadata summary refreshing every 2 seconds.; [2019-01-07 16:21:17,94] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,00] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-01-07 16:21:19,53] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-01-07 16:21:19,58] [info] SingleWorkflowRunnerActor: Version 36; [2019-01-07 16:21:19,61] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-01-07 16:21:19,69] [info] Unspecified type (Unspecified version) workflow 18de8166-5f29-4288-9fa4-6741565446fd submitted; [2019-01-07 16:21:19,74] [info] SingleWorkflowRunnerActor: Workflow submitted [38;5;2m18de8166-5f29-4288-9fa4-6741565446fd[0m; [2019-01-07 16:21:19,75] [info] 1 new workflows fetched; [2019-01-07 16:21:19,77] [info] WorkflowManagerActor Starting workflow [38;5;2m18de8166-5f29-4288-9fa4-6741565446fd[0m; [2019-01-07 16:21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:1660,Modifiability,config,configured,1660,"with database db.url = jdbc:hsqldb:mem:094e8bf9-be0f-4d7c-854a-0cf1a15dc0d7;shutdown=false;hsqldb.tx=mvcc; [2019-01-07 16:21:16,40] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-07 16:21:16,42] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-07 16:21:16,62] [info] Running with database db.url = jdbc:hsqldb:mem:2efc8123-f7e8-4fe3-abed-48d1bcf8eb97;shutdown=false;hsqldb.tx=mvcc; [2019-01-07 16:21:17,27] [info] Slf4jLogger started; [2019-01-07 16:21:17,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-231ef13"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-01-07 16:21:17,87] [info] Metadata summary refreshing every 2 seconds.; [2019-01-07 16:21:17,94] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,00] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-01-07 16:21:19,53] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-01-07 16:21:19,58] [info] SingleWorkflowRunnerActor: Version 36; [2019-01-07 16:21:19,61] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-01-07 16:21:19,69] [info] Unspecified type (Unspecified version) workflow 18de8166-5f29-4288-9fa4-6741565446fd submitted; [2019-01-07 16:21:19,74] [info] SingleWorkflowRunnerActor: Workflow submitted [38;5;2m18de8166-5f29-4288-9fa4-6741565446fd[0m; [2019-01-07 16:21:19,75] [info] 1 new workflows fetched; [2019-01-07 16:21:19,77] [info] WorkflowManagerActor Starting workflow [38;5;2m18de8166-5f29-4288-9fa4-6741565446fd[0m; [2019-01-07 16:21:19,77] [info] WorkflowManagerActor Successfully started WorkflowActor-18de8166-5f29-4288-9fa4-6741565446fd; [2019-01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:1781,Modifiability,config,configured,1781,"7 16:21:16,40] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-07 16:21:16,42] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-07 16:21:16,62] [info] Running with database db.url = jdbc:hsqldb:mem:2efc8123-f7e8-4fe3-abed-48d1bcf8eb97;shutdown=false;hsqldb.tx=mvcc; [2019-01-07 16:21:17,27] [info] Slf4jLogger started; [2019-01-07 16:21:17,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-231ef13"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-01-07 16:21:17,87] [info] Metadata summary refreshing every 2 seconds.; [2019-01-07 16:21:17,94] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,00] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-01-07 16:21:19,53] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-01-07 16:21:19,58] [info] SingleWorkflowRunnerActor: Version 36; [2019-01-07 16:21:19,61] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-01-07 16:21:19,69] [info] Unspecified type (Unspecified version) workflow 18de8166-5f29-4288-9fa4-6741565446fd submitted; [2019-01-07 16:21:19,74] [info] SingleWorkflowRunnerActor: Workflow submitted [38;5;2m18de8166-5f29-4288-9fa4-6741565446fd[0m; [2019-01-07 16:21:19,75] [info] 1 new workflows fetched; [2019-01-07 16:21:19,77] [info] WorkflowManagerActor Starting workflow [38;5;2m18de8166-5f29-4288-9fa4-6741565446fd[0m; [2019-01-07 16:21:19,77] [info] WorkflowManagerActor Successfully started WorkflowActor-18de8166-5f29-4288-9fa4-6741565446fd; [2019-01-07 16:21:19,77] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2019-01-07 16:21:19,78] [info] WorkflowSto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:2810,Modifiability,config,configured,2810," }; [2019-01-07 16:21:17,87] [info] Metadata summary refreshing every 2 seconds.; [2019-01-07 16:21:17,94] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,00] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-01-07 16:21:19,53] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-01-07 16:21:19,58] [info] SingleWorkflowRunnerActor: Version 36; [2019-01-07 16:21:19,61] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-01-07 16:21:19,69] [info] Unspecified type (Unspecified version) workflow 18de8166-5f29-4288-9fa4-6741565446fd submitted; [2019-01-07 16:21:19,74] [info] SingleWorkflowRunnerActor: Workflow submitted [38;5;2m18de8166-5f29-4288-9fa4-6741565446fd[0m; [2019-01-07 16:21:19,75] [info] 1 new workflows fetched; [2019-01-07 16:21:19,77] [info] WorkflowManagerActor Starting workflow [38;5;2m18de8166-5f29-4288-9fa4-6741565446fd[0m; [2019-01-07 16:21:19,77] [info] WorkflowManagerActor Successfully started WorkflowActor-18de8166-5f29-4288-9fa4-6741565446fd; [2019-01-07 16:21:19,77] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2019-01-07 16:21:19,78] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2019-01-07 16:21:19,80] [[38;5;220mwarn[0m] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2019-01-07 16:21:19,98] [info] MaterializeWorkflowDescriptorActor [[38;5;2m18de8166[0m]: Parsing workflow as WDL draft-2; [2019-01-07 16:21:21,18] [info] MaterializeWorkflowDescriptorActor [[38;5;2m18de8166[0m]: Call-to-Backend assignments: example.hello -> Local; [2019-01-07 16:21:23,89] [[38;5;1merror[0m] WorkflowManagerActor Workflow 18de8166-5f29-4288-9fa4-6741565446fd failed (during ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:8586,Performance,queue,queued,8586,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:8682,Performance,queue,queued,8682,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:8772,Performance,queue,queued,8772,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:7110,Safety,Timeout,Timeout,7110,".scala:49); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2019-01-07 16:21:23,91] [info] WorkflowManagerActor WorkflowActor-18de8166-5f29-4288-9fa4-6741565446fd is in a terminal state: WorkflowFailedState; [2019-01-07 16:21:30,36] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-01-07 16:21:32,96] [info] Workflow polling stopped; [2019-01-07 16:21:32,99] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-01-07 16:21:32,99] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-01-07 16:21:33,02] [info] Aborting all running workflows.; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 secon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:7201,Safety,Timeout,Timeout,7201,".scala:49); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2019-01-07 16:21:23,91] [info] WorkflowManagerActor WorkflowActor-18de8166-5f29-4288-9fa4-6741565446fd is in a terminal state: WorkflowFailedState; [2019-01-07 16:21:30,36] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-01-07 16:21:32,96] [info] Workflow polling stopped; [2019-01-07 16:21:32,99] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-01-07 16:21:32,99] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-01-07 16:21:33,02] [info] Aborting all running workflows.; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 secon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:7254,Safety,Abort,Aborting,7254,".scala:49); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2019-01-07 16:21:23,91] [info] WorkflowManagerActor WorkflowActor-18de8166-5f29-4288-9fa4-6741565446fd is in a terminal state: WorkflowFailedState; [2019-01-07 16:21:30,36] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-01-07 16:21:32,96] [info] Workflow polling stopped; [2019-01-07 16:21:32,99] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-01-07 16:21:32,99] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-01-07 16:21:33,02] [info] Aborting all running workflows.; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 secon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:7422,Safety,Timeout,Timeout,7422,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:7643,Safety,Timeout,Timeout,7643,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:7936,Safety,Timeout,Timeout,7936,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:8022,Safety,Timeout,Timeout,8022,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:8114,Safety,Timeout,Timeout,8114,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:8207,Safety,Timeout,Timeout,8207,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:8295,Safety,Timeout,Timeout,8295,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:8375,Safety,Timeout,Timeout,8375,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:5242,Testability,Log,LoggingFSM,5242,ances$$anon$1.traverse(list.scala:72); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:12); 	at cats.Traverse$Ops.traverse(Traverse.scala:19); 	at cats.Traverse$Ops.traverse$(Traverse.scala:19); 	at cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableNodes(WorkflowExecutionActor.scala:504); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:186); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:184); 	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:168); 	at akka.actor.FSM.processEvent(FSM.scala:687); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:49); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:49); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$Timers$$super$aroundReceive(WorkflowExecutionActor.scala:49); 	at akka.actor.Timers.aroundReceive(Timers.scala:51); 	at akka.actor.Timers.aroundReceive$(Timers.scala:40); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:49); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:5322,Testability,Log,LoggingFSM,5322,stances$$anon$1.traverse(list.scala:12); 	at cats.Traverse$Ops.traverse(Traverse.scala:19); 	at cats.Traverse$Ops.traverse$(Traverse.scala:19); 	at cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableNodes(WorkflowExecutionActor.scala:504); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:186); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:184); 	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:168); 	at akka.actor.FSM.processEvent(FSM.scala:687); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:49); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:49); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$Timers$$super$aroundReceive(WorkflowExecutionActor.scala:49); 	at akka.actor.Timers.aroundReceive(Timers.scala:51); 	at akka.actor.Timers.aroundReceive$(Timers.scala:40); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:49); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/issues/4526:5377,Testability,Log,LoggingFSM,5377,rse$Ops.traverse(Traverse.scala:19); 	at cats.Traverse$Ops.traverse$(Traverse.scala:19); 	at cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableNodes(WorkflowExecutionActor.scala:504); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:186); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:184); 	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:168); 	at akka.actor.FSM.processEvent(FSM.scala:687); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:49); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:49); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$Timers$$super$aroundReceive(WorkflowExecutionActor.scala:49); 	at akka.actor.Timers.aroundReceive(Timers.scala:51); 	at akka.actor.Timers.aroundReceive$(Timers.scala:40); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:49); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.d,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526
https://github.com/broadinstitute/cromwell/pull/4527:131,Testability,test,test,131,Succeeded on this branch at https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-papiv2/224/ (VPN required). This test is identical to the one in the [test_bcbio_cwl repo](https://github.com/bcbio/test_bcbio_cwl) with one exception: all storage locations have been converted to `gs://` paths (true before this PR also). Brad's comment about `gs://` conversion:; >I believe Thibault was mapping over the relative file-based references into the equivalent gs:// URLs for Cromwell testing.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4527
https://github.com/broadinstitute/cromwell/pull/4527:495,Testability,test,testing,495,Succeeded on this branch at https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-papiv2/224/ (VPN required). This test is identical to the one in the [test_bcbio_cwl repo](https://github.com/bcbio/test_bcbio_cwl) with one exception: all storage locations have been converted to `gs://` paths (true before this PR also). Brad's comment about `gs://` conversion:; >I believe Thibault was mapping over the relative file-based references into the equivalent gs:// URLs for Cromwell testing.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4527
https://github.com/broadinstitute/cromwell/pull/4528:131,Testability,test,test,131,Succeeded on this branch at https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-papiv2/222/ (VPN required). This test is identical to the one in the [test_bcbio_cwl repo](https://github.com/bcbio/test_bcbio_cwl) with one exception: all storage locations have been converted to `gs://` paths (true before this PR also). Brad's comment about `gs://` conversion:; >I believe Thibault was mapping over the relative file-based references into the equivalent gs:// URLs for Cromwell testing.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4528
https://github.com/broadinstitute/cromwell/pull/4528:495,Testability,test,testing,495,Succeeded on this branch at https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-papiv2/222/ (VPN required). This test is identical to the one in the [test_bcbio_cwl repo](https://github.com/bcbio/test_bcbio_cwl) with one exception: all storage locations have been converted to `gs://` paths (true before this PR also). Brad's comment about `gs://` conversion:; >I believe Thibault was mapping over the relative file-based references into the equivalent gs:// URLs for Cromwell testing.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4528
https://github.com/broadinstitute/cromwell/pull/4529:13,Testability,test,tests,13,Unitized the tests since it appears the coercions under test happen during the building of the executables so running the workflow isn't really necessary.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4529
https://github.com/broadinstitute/cromwell/pull/4529:56,Testability,test,test,56,Unitized the tests since it appears the coercions under test happen during the building of the executables so running the workflow isn't really necessary.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4529
https://github.com/broadinstitute/cromwell/pull/4530:5,Testability,test,test,5,CTKS test converted to two unit tests and one Centaur test.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4530
https://github.com/broadinstitute/cromwell/pull/4530:32,Testability,test,tests,32,CTKS test converted to two unit tests and one Centaur test.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4530
https://github.com/broadinstitute/cromwell/pull/4530:54,Testability,test,test,54,CTKS test converted to two unit tests and one Centaur test.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4530
https://github.com/broadinstitute/cromwell/pull/4532:131,Testability,test,test,131,Succeeded on this branch at https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-papiv2/230/ (VPN required). This test is identical to the one in the test_bcbio_cwl repo.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4532
https://github.com/broadinstitute/cromwell/issues/4533:7,Energy Efficiency,reduce,reduce,7,"I want reduce the length of bash command line when perform the joint-genotyping on thousands samples, so that I'm using InitialWorkDirRequirement to create a file path list. Here's the example CWL:; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0. class: CommandLineTool. requirements:; - class: InlineJavascriptRequirement; - class: DockerRequirement; dockerPull: quay.io/shenglai/alpine_with_bash:1.0; - class: InitialWorkDirRequirement; listing:; - entryname: ""files.json""; entry: $(inputs.files.basename); inputs:; files: File. outputs:; output:; type: File; outputBinding:; glob: ""files.json"". baseCommand: []; ```; the input is; ```; {; ""files"": {""class"": ""File"", ""path"": ""/mnt/glusterfs/a.file""}; }; ```; the cwltool output `files.json` is. ```; {""basename"":""a.file"",""nameroot"":""a"",""nameext"":"".file"",""location"":""file:///mnt/glusterfs/a.file"",""path"":""/var/lib/cwl/stg1a204114-8318-47ee-b5d0-7454ab87f7df/a.file"",""dirname"":""/var/lib/cwl/stg1a204114-8318-47ee-b5d0-7454ab87f7df"",""class"":""File"",""size"":0}; ```. the cromwell one is; ```; {""nameext"":"".file"",""location"":""/mnt/glusterfs/a.file"",""path"":""/mnt/glusterfs/a.file"",""size"":0,""dirname"":""/mnt/glusterfs"",""secondaryFiles"":[],""basename"":""a.file"",""class"":""File"",""nameroot"":""a""}; ```. As you can see, from cwltool, it actually returns a mapped path under `path`, so the later docker command can pick that up.; On the other hand, cromwell only returns the host path, which can not be found in docker. I only tested the piece with SLURM. I would assume it also fails on local run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4533
https://github.com/broadinstitute/cromwell/issues/4533:51,Performance,perform,perform,51,"I want reduce the length of bash command line when perform the joint-genotyping on thousands samples, so that I'm using InitialWorkDirRequirement to create a file path list. Here's the example CWL:; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0. class: CommandLineTool. requirements:; - class: InlineJavascriptRequirement; - class: DockerRequirement; dockerPull: quay.io/shenglai/alpine_with_bash:1.0; - class: InitialWorkDirRequirement; listing:; - entryname: ""files.json""; entry: $(inputs.files.basename); inputs:; files: File. outputs:; output:; type: File; outputBinding:; glob: ""files.json"". baseCommand: []; ```; the input is; ```; {; ""files"": {""class"": ""File"", ""path"": ""/mnt/glusterfs/a.file""}; }; ```; the cwltool output `files.json` is. ```; {""basename"":""a.file"",""nameroot"":""a"",""nameext"":"".file"",""location"":""file:///mnt/glusterfs/a.file"",""path"":""/var/lib/cwl/stg1a204114-8318-47ee-b5d0-7454ab87f7df/a.file"",""dirname"":""/var/lib/cwl/stg1a204114-8318-47ee-b5d0-7454ab87f7df"",""class"":""File"",""size"":0}; ```. the cromwell one is; ```; {""nameext"":"".file"",""location"":""/mnt/glusterfs/a.file"",""path"":""/mnt/glusterfs/a.file"",""size"":0,""dirname"":""/mnt/glusterfs"",""secondaryFiles"":[],""basename"":""a.file"",""class"":""File"",""nameroot"":""a""}; ```. As you can see, from cwltool, it actually returns a mapped path under `path`, so the later docker command can pick that up.; On the other hand, cromwell only returns the host path, which can not be found in docker. I only tested the piece with SLURM. I would assume it also fails on local run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4533
https://github.com/broadinstitute/cromwell/issues/4533:1461,Testability,test,tested,1461,"I want reduce the length of bash command line when perform the joint-genotyping on thousands samples, so that I'm using InitialWorkDirRequirement to create a file path list. Here's the example CWL:; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0. class: CommandLineTool. requirements:; - class: InlineJavascriptRequirement; - class: DockerRequirement; dockerPull: quay.io/shenglai/alpine_with_bash:1.0; - class: InitialWorkDirRequirement; listing:; - entryname: ""files.json""; entry: $(inputs.files.basename); inputs:; files: File. outputs:; output:; type: File; outputBinding:; glob: ""files.json"". baseCommand: []; ```; the input is; ```; {; ""files"": {""class"": ""File"", ""path"": ""/mnt/glusterfs/a.file""}; }; ```; the cwltool output `files.json` is. ```; {""basename"":""a.file"",""nameroot"":""a"",""nameext"":"".file"",""location"":""file:///mnt/glusterfs/a.file"",""path"":""/var/lib/cwl/stg1a204114-8318-47ee-b5d0-7454ab87f7df/a.file"",""dirname"":""/var/lib/cwl/stg1a204114-8318-47ee-b5d0-7454ab87f7df"",""class"":""File"",""size"":0}; ```. the cromwell one is; ```; {""nameext"":"".file"",""location"":""/mnt/glusterfs/a.file"",""path"":""/mnt/glusterfs/a.file"",""size"":0,""dirname"":""/mnt/glusterfs"",""secondaryFiles"":[],""basename"":""a.file"",""class"":""File"",""nameroot"":""a""}; ```. As you can see, from cwltool, it actually returns a mapped path under `path`, so the later docker command can pick that up.; On the other hand, cromwell only returns the host path, which can not be found in docker. I only tested the piece with SLURM. I would assume it also fails on local run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4533
https://github.com/broadinstitute/cromwell/pull/4534:182,Availability,down,downstream,182,"Part 2 of implementing the ""Remove Object"" [PR in OpenWDL](https://github.com/openwdl/wdl/pull/228). Allows us to treat calls as structs of their outputs, for purposes of evaluating downstream expressions, eg:; ```wdl; workflow blah { ; call foo; FooOutputStruct foo_outputs = foo; }. task foo {; # ... output {; Int i = ...; String s = ...; }; }. struct FooOutputStruct {; Int i; String s; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4534
https://github.com/broadinstitute/cromwell/issues/4535:181,Availability,down,downstream,181,"Part 2 of implementing the ""Remove Object"" [PR in OpenWDL](https://github.com/openwdl/wdl/pull/228). Allow us to treat calls as structs of their outputs, for purposes of evaluating downstream expressions, eg:; ```wdl; workflow blah { ; call foo; FooOutputStruct foo_outputs = foo; }. task foo {; # ... output {; Int i = ...; String s = ...; }; }. struct FooOutputStruct {; Int i; String s; }; ```. TODOs:; - [x] Simple use case (eg the example above); - [x] Within `scatter`s and `if`s; - [x] Don't disrupt the `after` logic; - [ ] Make sure WDL draft-2 and 1.0 still can't do this",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4535
https://github.com/broadinstitute/cromwell/issues/4535:519,Testability,log,logic,519,"Part 2 of implementing the ""Remove Object"" [PR in OpenWDL](https://github.com/openwdl/wdl/pull/228). Allow us to treat calls as structs of their outputs, for purposes of evaluating downstream expressions, eg:; ```wdl; workflow blah { ; call foo; FooOutputStruct foo_outputs = foo; }. task foo {; # ... output {; Int i = ...; String s = ...; }; }. struct FooOutputStruct {; Int i; String s; }; ```. TODOs:; - [x] Simple use case (eg the example above); - [x] Within `scatter`s and `if`s; - [x] Don't disrupt the `after` logic; - [ ] Make sure WDL draft-2 and 1.0 still can't do this",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4535
https://github.com/broadinstitute/cromwell/issues/4535:412,Usability,Simpl,Simple,412,"Part 2 of implementing the ""Remove Object"" [PR in OpenWDL](https://github.com/openwdl/wdl/pull/228). Allow us to treat calls as structs of their outputs, for purposes of evaluating downstream expressions, eg:; ```wdl; workflow blah { ; call foo; FooOutputStruct foo_outputs = foo; }. task foo {; # ... output {; Int i = ...; String s = ...; }; }. struct FooOutputStruct {; Int i; String s; }; ```. TODOs:; - [x] Simple use case (eg the example above); - [x] Within `scatter`s and `if`s; - [x] Don't disrupt the `after` logic; - [ ] Make sure WDL draft-2 and 1.0 still can't do this",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4535
https://github.com/broadinstitute/cromwell/issues/4536:193,Availability,error,errors,193,"Cromwell: 36; Backend: Google Cloud. When running a CWL snpEff tool using Cromwell ([CWL is here](https://github.com/broadinstitute/cromwell/files/2743866/snpEff.cwl.txt)), I get the following errors: ([full error log](https://github.com/broadinstitute/cromwell/files/2743887/snpeff_indels-stderr.log)). ```; xargs: invalid option -- 'I'; find: unrecognized: -empty; ```. In other words, Cromwell is generating a script that uses `xargs -I` and `find -empty`, flags which are not compatible with Busybox ([full Cromwell-generated script](https://github.com/broadinstitute/cromwell/files/2743877/script.txt)). The reason this matters is that all [Biocontainers](https://biocontainers.pro/) are based on Busybox, and I would say they represent a majority of the bioinformatics containers, so ensuring compatibility is in everyone's interest. Might it be possible to edit the Cromwell script to use a smaller subset of these command line flags?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4536
https://github.com/broadinstitute/cromwell/issues/4536:208,Availability,error,error,208,"Cromwell: 36; Backend: Google Cloud. When running a CWL snpEff tool using Cromwell ([CWL is here](https://github.com/broadinstitute/cromwell/files/2743866/snpEff.cwl.txt)), I get the following errors: ([full error log](https://github.com/broadinstitute/cromwell/files/2743887/snpeff_indels-stderr.log)). ```; xargs: invalid option -- 'I'; find: unrecognized: -empty; ```. In other words, Cromwell is generating a script that uses `xargs -I` and `find -empty`, flags which are not compatible with Busybox ([full Cromwell-generated script](https://github.com/broadinstitute/cromwell/files/2743877/script.txt)). The reason this matters is that all [Biocontainers](https://biocontainers.pro/) are based on Busybox, and I would say they represent a majority of the bioinformatics containers, so ensuring compatibility is in everyone's interest. Might it be possible to edit the Cromwell script to use a smaller subset of these command line flags?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4536
https://github.com/broadinstitute/cromwell/issues/4536:214,Testability,log,log,214,"Cromwell: 36; Backend: Google Cloud. When running a CWL snpEff tool using Cromwell ([CWL is here](https://github.com/broadinstitute/cromwell/files/2743866/snpEff.cwl.txt)), I get the following errors: ([full error log](https://github.com/broadinstitute/cromwell/files/2743887/snpeff_indels-stderr.log)). ```; xargs: invalid option -- 'I'; find: unrecognized: -empty; ```. In other words, Cromwell is generating a script that uses `xargs -I` and `find -empty`, flags which are not compatible with Busybox ([full Cromwell-generated script](https://github.com/broadinstitute/cromwell/files/2743877/script.txt)). The reason this matters is that all [Biocontainers](https://biocontainers.pro/) are based on Busybox, and I would say they represent a majority of the bioinformatics containers, so ensuring compatibility is in everyone's interest. Might it be possible to edit the Cromwell script to use a smaller subset of these command line flags?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4536
https://github.com/broadinstitute/cromwell/issues/4536:297,Testability,log,log,297,"Cromwell: 36; Backend: Google Cloud. When running a CWL snpEff tool using Cromwell ([CWL is here](https://github.com/broadinstitute/cromwell/files/2743866/snpEff.cwl.txt)), I get the following errors: ([full error log](https://github.com/broadinstitute/cromwell/files/2743887/snpeff_indels-stderr.log)). ```; xargs: invalid option -- 'I'; find: unrecognized: -empty; ```. In other words, Cromwell is generating a script that uses `xargs -I` and `find -empty`, flags which are not compatible with Busybox ([full Cromwell-generated script](https://github.com/broadinstitute/cromwell/files/2743877/script.txt)). The reason this matters is that all [Biocontainers](https://biocontainers.pro/) are based on Busybox, and I would say they represent a majority of the bioinformatics containers, so ensuring compatibility is in everyone's interest. Might it be possible to edit the Cromwell script to use a smaller subset of these command line flags?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4536
https://github.com/broadinstitute/cromwell/issues/4537:3915,Availability,error,error,3915,"-306de3037265; [2019-01-10 17:34:35,81] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:0:1]: Status change from - to Initializing; [2019-01-10 17:34:35,81] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: Status change from - to Initializing; [2019-01-10 17:36:24,60] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:0:1]: Status change from Initializing to Running; [2019-01-10 17:36:32,19] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: Status change from Initializing to Running; [2019-01-10 18:21:56,23] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: Status change from Running to Succeeded; [2019-01-10 18:23:09,43] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:0:1]: Status change from Running to Succeeded; [2019-01-10 18:23:10,45] [error] WorkflowManagerActor Workflow 00d0c2df-8f87-42af-9439-b45593930c84 failed (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://s4-branford-dev-nphi-valinor-v2/cromwell-execution/tigris_workflow/00d0c2df-8f87-42af-9439-b45593930c84/call-SomaticSNVInDel/vc.SomaticSNVInDel/127f691e-ea2e-441f-9d58-deba01a84c5e/call-MarkDuplicates/shard-0/MarkDuplicates-0-rc.txt: s3://s3.amazonaws.com/s4-branford-dev-nphi-valinor-v2/cromwell-execution/tigris_workflow/00d0c2df-8f87-42af-9439-b45593930c84/call-SomaticSNVInDel/vc.SomaticSNVInDel/127f691e-ea2e-441f-9d58-deba01a84c5e/call-MarkDuplicates/shard-0/MarkDuplicates-0-rc.txt; Caused by: java.io.IOException: Could not read from s3://s4-branford-dev-nphi-valinor-v2/cromwell-execution/tigris_workflow/00d0c2df-8f87-42af-9439-b45593930c84/call-SomaticSNVInDel/vc.SomaticSNVInDel/127f691e-ea2e-441f-9d58-deba01a84c5e/call-MarkDuplicates/shard-0/MarkDuplicates-0-rc.txt: s3",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4537
https://github.com/broadinstitute/cromwell/issues/4537:4059,Modifiability,Enhance,EnhancedCromwellIoException,4059,"onActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: Status change from - to Initializing; [2019-01-10 17:36:24,60] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:0:1]: Status change from Initializing to Running; [2019-01-10 17:36:32,19] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: Status change from Initializing to Running; [2019-01-10 18:21:56,23] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: Status change from Running to Succeeded; [2019-01-10 18:23:09,43] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:0:1]: Status change from Running to Succeeded; [2019-01-10 18:23:10,45] [error] WorkflowManagerActor Workflow 00d0c2df-8f87-42af-9439-b45593930c84 failed (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://s4-branford-dev-nphi-valinor-v2/cromwell-execution/tigris_workflow/00d0c2df-8f87-42af-9439-b45593930c84/call-SomaticSNVInDel/vc.SomaticSNVInDel/127f691e-ea2e-441f-9d58-deba01a84c5e/call-MarkDuplicates/shard-0/MarkDuplicates-0-rc.txt: s3://s3.amazonaws.com/s4-branford-dev-nphi-valinor-v2/cromwell-execution/tigris_workflow/00d0c2df-8f87-42af-9439-b45593930c84/call-SomaticSNVInDel/vc.SomaticSNVInDel/127f691e-ea2e-441f-9d58-deba01a84c5e/call-MarkDuplicates/shard-0/MarkDuplicates-0-rc.txt; Caused by: java.io.IOException: Could not read from s3://s4-branford-dev-nphi-valinor-v2/cromwell-execution/tigris_workflow/00d0c2df-8f87-42af-9439-b45593930c84/call-SomaticSNVInDel/vc.SomaticSNVInDel/127f691e-ea2e-441f-9d58-deba01a84c5e/call-MarkDuplicates/shard-0/MarkDuplicates-0-rc.txt: s3://s3.amazonaws.com/s4-branford-dev-nphi-valinor-v2/cromwell-execution/tigris_workflow/00d0c2df-8f87-42af-9439-b45593930c84/call-SomaticSNVInDel/vc.SomaticSNVInDel/127f691e-ea2e-441f-9d58-deba01a84c5e/call-MarkDuplicates/shard-0/M",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4537
https://github.com/broadinstitute/cromwell/issues/4537:1669,Performance,queue,queue,1669,"TPUT JML005-M254-NORMAL.dedup.bam \; --METRICS_FILE JML005-M254-NORMAL.dupmetric \. sambamba index -t 4 JML005-M254-NORMAL.dedup.bam; [2019-01-10 17:34:31,80] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: gatk MarkDuplicates \; --java-options -Djava.io.tmpdir='' \; --INPUT /cromwell_root/s4-branford-dev-nphi-valinor-v2/cromwell-execution/tigris_workflow/00d0c2df-8f87-42af-9439-b45593930c84/call-SomaticSNVInDel/vc.SomaticSNVInDel/127f691e-ea2e-441f-9d58-deba01a84c5e/call-mergeBam/shard-1/JML005-M253-TUMOR.merged.bam \; --OUTPUT JML005-M253-TUMOR.dedup.bam \; --METRICS_FILE JML005-M253-TUMOR.dupmetric \. sambamba index -t 4 JML005-M253-TUMOR.dedup.bam; [2019-01-10 17:34:31,81] [info] Submitting job to AWS Batch; [2019-01-10 17:34:31,81] [info] dockerImage: 267795504649.dkr.ecr.us-east-1.amazonaws.com/s4-radbinf-somaticgenomicsrd-tigris:1.2.0; [2019-01-10 17:34:31,81] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-63f905e0-1459-11e9-8f03-0a4a1f15abc2; [2019-01-10 17:34:31,81] [info] taskId: SomaticSNVInDel.MarkDuplicates-Some(1)-1; [2019-01-10 17:34:31,81] [info] hostpath root: vc.SomaticSNVInDel/vc.MarkDuplicates/127f691e-ea2e-441f-9d58-deba01a84c5e/Some(1)/1; [2019-01-10 17:34:31,81] [info] Submitting job to AWS Batch; [2019-01-10 17:34:31,81] [info] dockerImage: 267795504649.dkr.ecr.us-east-1.amazonaws.com/s4-radbinf-somaticgenomicsrd-tigris:1.2.0; [2019-01-10 17:34:31,81] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-63f905e0-1459-11e9-8f03-0a4a1f15abc2; [2019-01-10 17:34:31,82] [info] taskId: SomaticSNVInDel.MarkDuplicates-Some(0)-1; [2019-01-10 17:34:31,82] [info] hostpath root: vc.SomaticSNVInDel/vc.MarkDuplicates/127f691e-ea2e-441f-9d58-deba01a84c5e/Some(0)/1; [2019-01-10 17:34:31,99] [warn] Job definition already exists. Performing describe and retrieving latest revision.; [2019-01-10 17:34:35,76] [info] AwsBatchAsyncBackendJobExecuti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4537
https://github.com/broadinstitute/cromwell/issues/4537:2228,Performance,queue,queue,2228,"ed.bam \; --OUTPUT JML005-M253-TUMOR.dedup.bam \; --METRICS_FILE JML005-M253-TUMOR.dupmetric \. sambamba index -t 4 JML005-M253-TUMOR.dedup.bam; [2019-01-10 17:34:31,81] [info] Submitting job to AWS Batch; [2019-01-10 17:34:31,81] [info] dockerImage: 267795504649.dkr.ecr.us-east-1.amazonaws.com/s4-radbinf-somaticgenomicsrd-tigris:1.2.0; [2019-01-10 17:34:31,81] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-63f905e0-1459-11e9-8f03-0a4a1f15abc2; [2019-01-10 17:34:31,81] [info] taskId: SomaticSNVInDel.MarkDuplicates-Some(1)-1; [2019-01-10 17:34:31,81] [info] hostpath root: vc.SomaticSNVInDel/vc.MarkDuplicates/127f691e-ea2e-441f-9d58-deba01a84c5e/Some(1)/1; [2019-01-10 17:34:31,81] [info] Submitting job to AWS Batch; [2019-01-10 17:34:31,81] [info] dockerImage: 267795504649.dkr.ecr.us-east-1.amazonaws.com/s4-radbinf-somaticgenomicsrd-tigris:1.2.0; [2019-01-10 17:34:31,81] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-63f905e0-1459-11e9-8f03-0a4a1f15abc2; [2019-01-10 17:34:31,82] [info] taskId: SomaticSNVInDel.MarkDuplicates-Some(0)-1; [2019-01-10 17:34:31,82] [info] hostpath root: vc.SomaticSNVInDel/vc.MarkDuplicates/127f691e-ea2e-441f-9d58-deba01a84c5e/Some(0)/1; [2019-01-10 17:34:31,99] [warn] Job definition already exists. Performing describe and retrieving latest revision.; [2019-01-10 17:34:35,76] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:0:1]: job id: b632db00-aef4-4463-8ccf-74be73b3c251; [2019-01-10 17:34:35,76] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: job id: 26c8aab7-48f7-4a6c-9bf0-306de3037265; [2019-01-10 17:34:35,81] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:0:1]: Status change from - to Initializing; [2019-01-10 17:34:35,81] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: Status change from - to Ini",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4537
https://github.com/broadinstitute/cromwell/issues/4537:2570,Performance,Perform,Performing,2570,"17:34:31,81] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-63f905e0-1459-11e9-8f03-0a4a1f15abc2; [2019-01-10 17:34:31,81] [info] taskId: SomaticSNVInDel.MarkDuplicates-Some(1)-1; [2019-01-10 17:34:31,81] [info] hostpath root: vc.SomaticSNVInDel/vc.MarkDuplicates/127f691e-ea2e-441f-9d58-deba01a84c5e/Some(1)/1; [2019-01-10 17:34:31,81] [info] Submitting job to AWS Batch; [2019-01-10 17:34:31,81] [info] dockerImage: 267795504649.dkr.ecr.us-east-1.amazonaws.com/s4-radbinf-somaticgenomicsrd-tigris:1.2.0; [2019-01-10 17:34:31,81] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-63f905e0-1459-11e9-8f03-0a4a1f15abc2; [2019-01-10 17:34:31,82] [info] taskId: SomaticSNVInDel.MarkDuplicates-Some(0)-1; [2019-01-10 17:34:31,82] [info] hostpath root: vc.SomaticSNVInDel/vc.MarkDuplicates/127f691e-ea2e-441f-9d58-deba01a84c5e/Some(0)/1; [2019-01-10 17:34:31,99] [warn] Job definition already exists. Performing describe and retrieving latest revision.; [2019-01-10 17:34:35,76] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:0:1]: job id: b632db00-aef4-4463-8ccf-74be73b3c251; [2019-01-10 17:34:35,76] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: job id: 26c8aab7-48f7-4a6c-9bf0-306de3037265; [2019-01-10 17:34:35,81] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:0:1]: Status change from - to Initializing; [2019-01-10 17:34:35,81] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: Status change from - to Initializing; [2019-01-10 17:36:24,60] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:0:1]: Status change from Initializing to Running; [2019-01-10 17:36:32,19] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: Status change from Initializing to Running; [2019-01-10 18:21:56",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4537
https://github.com/broadinstitute/cromwell/issues/4537:239,Testability,log,log,239,"I am not sure what is going on here, I am running cromwell with aws batch enabled. In MarkDuplicate step, the cromwell server shows submit two job: tumor and normal, but it turns out it submitted tumor twice, no normal. Here is the server log:. [2019-01-10 17:34:31,80] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:0:1]: gatk MarkDuplicates \; --java-options -Djava.io.tmpdir='' \; --INPUT /cromwell_root/s4-branford-dev-nphi-valinor-v2/cromwell-execution/tigris_workflow/00d0c2df-8f87-42af-9439-b45593930c84/call-SomaticSNVInDel/vc.SomaticSNVInDel/127f691e-ea2e-441f-9d58-deba01a84c5e/call-mergeBam/shard-0/JML005-M254-NORMAL.merged.bam \; --OUTPUT JML005-M254-NORMAL.dedup.bam \; --METRICS_FILE JML005-M254-NORMAL.dupmetric \. sambamba index -t 4 JML005-M254-NORMAL.dedup.bam; [2019-01-10 17:34:31,80] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: gatk MarkDuplicates \; --java-options -Djava.io.tmpdir='' \; --INPUT /cromwell_root/s4-branford-dev-nphi-valinor-v2/cromwell-execution/tigris_workflow/00d0c2df-8f87-42af-9439-b45593930c84/call-SomaticSNVInDel/vc.SomaticSNVInDel/127f691e-ea2e-441f-9d58-deba01a84c5e/call-mergeBam/shard-1/JML005-M253-TUMOR.merged.bam \; --OUTPUT JML005-M253-TUMOR.dedup.bam \; --METRICS_FILE JML005-M253-TUMOR.dupmetric \. sambamba index -t 4 JML005-M253-TUMOR.dedup.bam; [2019-01-10 17:34:31,81] [info] Submitting job to AWS Batch; [2019-01-10 17:34:31,81] [info] dockerImage: 267795504649.dkr.ecr.us-east-1.amazonaws.com/s4-radbinf-somaticgenomicsrd-tigris:1.2.0; [2019-01-10 17:34:31,81] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-63f905e0-1459-11e9-8f03-0a4a1f15abc2; [2019-01-10 17:34:31,81] [info] taskId: SomaticSNVInDel.MarkDuplicates-Some(1)-1; [2019-01-10 17:34:31,81] [info] hostpath root: vc.SomaticSNVInDel/vc.MarkDuplicates/127f691e-ea2e-441f-9d58-deba01a84c5e/Some(1)/1; [2019-01-10 17:34:31,81] [info] Submitting job to AWS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4537
https://github.com/broadinstitute/cromwell/issues/4538:156,Availability,error,error,156,"Hi, ; I have a CWL workflow which works well when no output is specified but fails with the following when output is added:. ```; [2019-01-09 17:56:27,87] [error] WorkflowManagerActor Workflow b240bd3e-cdfd-45c8-be1e-046794929e90 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Workflow output parameters such as WorkflowOutputParameter(file:///var/folders/34/kzv1pzl57s92dbz54rvr1px00000gn/T/b240bd3e-cdfd-45c8-be1e-046794929e90.temp.4180597054306144316/b240bd3e-cdfd-45c8-be1e-046794929e90.cwl#outputwf,None,None,None,None,None,None,Some(Inr(Inl([Ljava.lang.String;@2ff95103))),None,Some(Inl(Inl(File)))) are not supported.; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.FSM.akka$actor$FSM$$processMsg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4538
https://github.com/broadinstitute/cromwell/issues/4538:1617,Testability,Log,LoggingFSM,1617,"e,None,None,None,None,None,Some(Inr(Inl([Ljava.lang.String;@2ff95103))),None,Some(Inl(Inl(File)))) are not supported.; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4538
https://github.com/broadinstitute/cromwell/issues/4538:1710,Testability,Log,LoggingFSM,1710,",None,Some(Inl(Inl(File)))) are not supported.; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4538
https://github.com/broadinstitute/cromwell/issues/4538:1765,Testability,Log,LoggingFSM,1765,mwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4538
https://github.com/broadinstitute/cromwell/issues/4538:3390,Testability,test,test,3390,"terialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. The same workflow works on cwltool. The command I used is `java -jar cromwell-35.jar run -i exampleInput/cat.yml -p ../CWL/workflows/cat.zip ../CWL/workflows/wf_cat.cwl` . the CWL worfklow:; ```; class: Workflow; cwlVersion: v1.0; id: wf_cat; label: wf_cat; inputs:; - id: input; type: File; outputs:; - id: outputwf; outputSource:; - cat_test/output; type: File; steps:; - id: cat_test; in:; - id: input; source: input; out:; - id: output; run: ./cat-test.cwl; label: cat-test; requirements: []; ```. When in the above wflow the outputs section is replaced with; ```; outputs: []; ```; The workflow is executed to completion (and the output is stored in the execution directory); So it seems that cromwell cannot accept the outputs section in the workflow description, is this expected? . . The command line tool which is then zipped in cat.zip and provided as input to the cromwell command line:; ```; class: CommandLineTool; cwlVersion: v1.0; id: cat_test; baseCommand:; - cat; inputs:; - id: input; type: File; inputBinding:; position: 0; outputs:; - id: output; type: File; outputBinding:; glob: '*.out'; label: cat-test; requirements:; - class: DockerRequirement; dockerPull: 'ubuntu:latest'; - class: InlineJavascriptRequirement; stdout: '${ return inputs.input.basename + "".out"" }'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4538
https://github.com/broadinstitute/cromwell/issues/4538:3411,Testability,test,test,3411,"terialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. The same workflow works on cwltool. The command I used is `java -jar cromwell-35.jar run -i exampleInput/cat.yml -p ../CWL/workflows/cat.zip ../CWL/workflows/wf_cat.cwl` . the CWL worfklow:; ```; class: Workflow; cwlVersion: v1.0; id: wf_cat; label: wf_cat; inputs:; - id: input; type: File; outputs:; - id: outputwf; outputSource:; - cat_test/output; type: File; steps:; - id: cat_test; in:; - id: input; source: input; out:; - id: output; run: ./cat-test.cwl; label: cat-test; requirements: []; ```. When in the above wflow the outputs section is replaced with; ```; outputs: []; ```; The workflow is executed to completion (and the output is stored in the execution directory); So it seems that cromwell cannot accept the outputs section in the workflow description, is this expected? . . The command line tool which is then zipped in cat.zip and provided as input to the cromwell command line:; ```; class: CommandLineTool; cwlVersion: v1.0; id: cat_test; baseCommand:; - cat; inputs:; - id: input; type: File; inputBinding:; position: 0; outputs:; - id: output; type: File; outputBinding:; glob: '*.out'; label: cat-test; requirements:; - class: DockerRequirement; dockerPull: 'ubuntu:latest'; - class: InlineJavascriptRequirement; stdout: '${ return inputs.input.basename + "".out"" }'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4538
https://github.com/broadinstitute/cromwell/issues/4538:4059,Testability,test,test,4059,"terialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. The same workflow works on cwltool. The command I used is `java -jar cromwell-35.jar run -i exampleInput/cat.yml -p ../CWL/workflows/cat.zip ../CWL/workflows/wf_cat.cwl` . the CWL worfklow:; ```; class: Workflow; cwlVersion: v1.0; id: wf_cat; label: wf_cat; inputs:; - id: input; type: File; outputs:; - id: outputwf; outputSource:; - cat_test/output; type: File; steps:; - id: cat_test; in:; - id: input; source: input; out:; - id: output; run: ./cat-test.cwl; label: cat-test; requirements: []; ```. When in the above wflow the outputs section is replaced with; ```; outputs: []; ```; The workflow is executed to completion (and the output is stored in the execution directory); So it seems that cromwell cannot accept the outputs section in the workflow description, is this expected? . . The command line tool which is then zipped in cat.zip and provided as input to the cromwell command line:; ```; class: CommandLineTool; cwlVersion: v1.0; id: cat_test; baseCommand:; - cat; inputs:; - id: input; type: File; inputBinding:; position: 0; outputs:; - id: output; type: File; outputBinding:; glob: '*.out'; label: cat-test; requirements:; - class: DockerRequirement; dockerPull: 'ubuntu:latest'; - class: InlineJavascriptRequirement; stdout: '${ return inputs.input.basename + "".out"" }'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4538
https://github.com/broadinstitute/cromwell/issues/4540:9,Performance,Queue,Queued,9,"The `Job Queued` and `Jobs Running` in the [Cromwell summary Grafana dashboard ](https://www.hostedgraphite.com/e2dc6eb6/grafana/dashboard/db/cromwell-summary) often reports 0 even when jobs are clearly running. Consider:. * The behavior is intermittent. Will report for a while, then off for a while.; * IIRC We report counters as gauges somehow (need to verify). These are counters.; * Hosted graphite themselves may be able to help us troubleshoot; * StatsD is UDP, thus may lose packets. Job Queued metric is: `gauges.$env.cromwell.job.QueuedInCromwell`; Jobs Running metric is: `gauges.$env.cromwell.job.Running`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4540
https://github.com/broadinstitute/cromwell/issues/4540:496,Performance,Queue,Queued,496,"The `Job Queued` and `Jobs Running` in the [Cromwell summary Grafana dashboard ](https://www.hostedgraphite.com/e2dc6eb6/grafana/dashboard/db/cromwell-summary) often reports 0 even when jobs are clearly running. Consider:. * The behavior is intermittent. Will report for a while, then off for a while.; * IIRC We report counters as gauges somehow (need to verify). These are counters.; * Hosted graphite themselves may be able to help us troubleshoot; * StatsD is UDP, thus may lose packets. Job Queued metric is: `gauges.$env.cromwell.job.QueuedInCromwell`; Jobs Running metric is: `gauges.$env.cromwell.job.Running`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4540
https://github.com/broadinstitute/cromwell/issues/4540:540,Performance,Queue,QueuedInCromwell,540,"The `Job Queued` and `Jobs Running` in the [Cromwell summary Grafana dashboard ](https://www.hostedgraphite.com/e2dc6eb6/grafana/dashboard/db/cromwell-summary) often reports 0 even when jobs are clearly running. Consider:. * The behavior is intermittent. Will report for a while, then off for a while.; * IIRC We report counters as gauges somehow (need to verify). These are counters.; * Hosted graphite themselves may be able to help us troubleshoot; * StatsD is UDP, thus may lose packets. Job Queued metric is: `gauges.$env.cromwell.job.QueuedInCromwell`; Jobs Running metric is: `gauges.$env.cromwell.job.Running`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4540
https://github.com/broadinstitute/cromwell/issues/4540:195,Usability,clear,clearly,195,"The `Job Queued` and `Jobs Running` in the [Cromwell summary Grafana dashboard ](https://www.hostedgraphite.com/e2dc6eb6/grafana/dashboard/db/cromwell-summary) often reports 0 even when jobs are clearly running. Consider:. * The behavior is intermittent. Will report for a while, then off for a while.; * IIRC We report counters as gauges somehow (need to verify). These are counters.; * Hosted graphite themselves may be able to help us troubleshoot; * StatsD is UDP, thus may lose packets. Job Queued metric is: `gauges.$env.cromwell.job.QueuedInCromwell`; Jobs Running metric is: `gauges.$env.cromwell.job.Running`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4540
https://github.com/broadinstitute/cromwell/issues/4541:566,Availability,error,error,566,"Hi,; following the example on how to use AWS Batch I am able to run the tests using the suggested edits to aws.conf. I then try to specify the output directories using the following `options.json` file. ```; {; ""final_workflow_outputs_dir"": ""s3://bucket/cromwell/outputs"",; ""final_call_logs_dir"": ""s3:/bucket/cromwell/call_logs"",; ""final_workflow_log_dir"": ""s3://bucket/cromwell/wf_logs""; }; ```. When running cromwell : `java -Dconfig.file=awsbatch/aws.conf -jar cromwell-36.jar run awsbatch/hello.wdl -i awsbatch/hello.inputs -o options.json`. it results with the error:. ```; [2019-01-12 00:31:03,94] [info] $a [866d19d0]: Copying workflow logs from /rcecloud/kmavrommatis/workspace/Workflows/cromwell/cromwell-workflow-logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log to s3://celgene-rnd-riku-researchanalytics/cromwell/wf_logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log; [2019-01-12 00:31:04,03] [error] Key cannot be empty; java.lang.IllegalArgumentException: Key cannot be empty; 	at software.amazon.awssdk.core.util.ValidationUtils.assertStringNotEmpty(ValidationUtils.java:111); 	at software.amazon.awssdk.core.runtime.transform.PathMarshallers$GreedyPathMarshaller.marshall(PathMarshallers.java:109); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:87); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:31); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:88); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.headObject(DefaultS3Client.java:1628); 	at org.lerch.s3fs.util.S3Utils.getS3ObjectSummary(S3Utils.java:47); 	at org.lerch.s3fs.S3FileSystemProvider.exists(S3FileSystemProvider.java:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4541
https://github.com/broadinstitute/cromwell/issues/4541:915,Availability,error,error,915,"Hi,; following the example on how to use AWS Batch I am able to run the tests using the suggested edits to aws.conf. I then try to specify the output directories using the following `options.json` file. ```; {; ""final_workflow_outputs_dir"": ""s3://bucket/cromwell/outputs"",; ""final_call_logs_dir"": ""s3:/bucket/cromwell/call_logs"",; ""final_workflow_log_dir"": ""s3://bucket/cromwell/wf_logs""; }; ```. When running cromwell : `java -Dconfig.file=awsbatch/aws.conf -jar cromwell-36.jar run awsbatch/hello.wdl -i awsbatch/hello.inputs -o options.json`. it results with the error:. ```; [2019-01-12 00:31:03,94] [info] $a [866d19d0]: Copying workflow logs from /rcecloud/kmavrommatis/workspace/Workflows/cromwell/cromwell-workflow-logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log to s3://celgene-rnd-riku-researchanalytics/cromwell/wf_logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log; [2019-01-12 00:31:04,03] [error] Key cannot be empty; java.lang.IllegalArgumentException: Key cannot be empty; 	at software.amazon.awssdk.core.util.ValidationUtils.assertStringNotEmpty(ValidationUtils.java:111); 	at software.amazon.awssdk.core.runtime.transform.PathMarshallers$GreedyPathMarshaller.marshall(PathMarshallers.java:109); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:87); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:31); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:88); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.headObject(DefaultS3Client.java:1628); 	at org.lerch.s3fs.util.S3Utils.getS3ObjectSummary(S3Utils.java:47); 	at org.lerch.s3fs.S3FileSystemProvider.exists(S3FileSystemProvider.java:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4541
https://github.com/broadinstitute/cromwell/issues/4541:4774,Deployability,configurat,configuration,4774,erPathMethods.createPermissionedDirectories$(EvenBetterPathMethods.scala:62); 	at cromwell.filesystems.s3.S3Path.createPermissionedDirectories(S3PathBuilder.scala:156); 	at cromwell.core.path.EvenBetterPathMethods.createPermissionedDirectories(EvenBetterPathMethods.scala:64); 	at cromwell.core.path.EvenBetterPathMethods.createPermissionedDirectories$(EvenBetterPathMethods.scala:62); 	at cromwell.filesystems.s3.S3Path.createPermissionedDirectories(S3PathBuilder.scala:156); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowLogsActor.copyLog(CopyWorkflowLogsActor.scala:36); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowLogsActor$$anonfun$copyLogsReceive$1.$anonfun$applyOrElse$1(CopyWorkflowLogsActor.scala:67); 	at scala.Option.foreach(Option.scala:257); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowLogsActor$$anonfun$copyLogsReceive$1.applyOrElse(CopyWorkflowLogsActor.scala:62); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowLogsActor.aroundReceive(CopyWorkflowLogsActor.scala:30); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); ```. Is there additional configuration required to specify outputs on s3? . Thanks in advance for your help,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4541
https://github.com/broadinstitute/cromwell/issues/4541:4774,Modifiability,config,configuration,4774,erPathMethods.createPermissionedDirectories$(EvenBetterPathMethods.scala:62); 	at cromwell.filesystems.s3.S3Path.createPermissionedDirectories(S3PathBuilder.scala:156); 	at cromwell.core.path.EvenBetterPathMethods.createPermissionedDirectories(EvenBetterPathMethods.scala:64); 	at cromwell.core.path.EvenBetterPathMethods.createPermissionedDirectories$(EvenBetterPathMethods.scala:62); 	at cromwell.filesystems.s3.S3Path.createPermissionedDirectories(S3PathBuilder.scala:156); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowLogsActor.copyLog(CopyWorkflowLogsActor.scala:36); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowLogsActor$$anonfun$copyLogsReceive$1.$anonfun$applyOrElse$1(CopyWorkflowLogsActor.scala:67); 	at scala.Option.foreach(Option.scala:257); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowLogsActor$$anonfun$copyLogsReceive$1.applyOrElse(CopyWorkflowLogsActor.scala:62); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowLogsActor.aroundReceive(CopyWorkflowLogsActor.scala:30); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); ```. Is there additional configuration required to specify outputs on s3? . Thanks in advance for your help,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4541
https://github.com/broadinstitute/cromwell/issues/4541:1037,Security,Validat,ValidationUtils,1037,"Batch I am able to run the tests using the suggested edits to aws.conf. I then try to specify the output directories using the following `options.json` file. ```; {; ""final_workflow_outputs_dir"": ""s3://bucket/cromwell/outputs"",; ""final_call_logs_dir"": ""s3:/bucket/cromwell/call_logs"",; ""final_workflow_log_dir"": ""s3://bucket/cromwell/wf_logs""; }; ```. When running cromwell : `java -Dconfig.file=awsbatch/aws.conf -jar cromwell-36.jar run awsbatch/hello.wdl -i awsbatch/hello.inputs -o options.json`. it results with the error:. ```; [2019-01-12 00:31:03,94] [info] $a [866d19d0]: Copying workflow logs from /rcecloud/kmavrommatis/workspace/Workflows/cromwell/cromwell-workflow-logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log to s3://celgene-rnd-riku-researchanalytics/cromwell/wf_logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log; [2019-01-12 00:31:04,03] [error] Key cannot be empty; java.lang.IllegalArgumentException: Key cannot be empty; 	at software.amazon.awssdk.core.util.ValidationUtils.assertStringNotEmpty(ValidationUtils.java:111); 	at software.amazon.awssdk.core.runtime.transform.PathMarshallers$GreedyPathMarshaller.marshall(PathMarshallers.java:109); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:87); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:31); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:88); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.headObject(DefaultS3Client.java:1628); 	at org.lerch.s3fs.util.S3Utils.getS3ObjectSummary(S3Utils.java:47); 	at org.lerch.s3fs.S3FileSystemProvider.exists(S3FileSystemProvider.java:624); 	at org.lerch.s3fs.S3FileSystemProvide",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4541
https://github.com/broadinstitute/cromwell/issues/4541:1074,Security,Validat,ValidationUtils,1074," tests using the suggested edits to aws.conf. I then try to specify the output directories using the following `options.json` file. ```; {; ""final_workflow_outputs_dir"": ""s3://bucket/cromwell/outputs"",; ""final_call_logs_dir"": ""s3:/bucket/cromwell/call_logs"",; ""final_workflow_log_dir"": ""s3://bucket/cromwell/wf_logs""; }; ```. When running cromwell : `java -Dconfig.file=awsbatch/aws.conf -jar cromwell-36.jar run awsbatch/hello.wdl -i awsbatch/hello.inputs -o options.json`. it results with the error:. ```; [2019-01-12 00:31:03,94] [info] $a [866d19d0]: Copying workflow logs from /rcecloud/kmavrommatis/workspace/Workflows/cromwell/cromwell-workflow-logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log to s3://celgene-rnd-riku-researchanalytics/cromwell/wf_logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log; [2019-01-12 00:31:04,03] [error] Key cannot be empty; java.lang.IllegalArgumentException: Key cannot be empty; 	at software.amazon.awssdk.core.util.ValidationUtils.assertStringNotEmpty(ValidationUtils.java:111); 	at software.amazon.awssdk.core.runtime.transform.PathMarshallers$GreedyPathMarshaller.marshall(PathMarshallers.java:109); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:87); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:31); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:88); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.headObject(DefaultS3Client.java:1628); 	at org.lerch.s3fs.util.S3Utils.getS3ObjectSummary(S3Utils.java:47); 	at org.lerch.s3fs.S3FileSystemProvider.exists(S3FileSystemProvider.java:624); 	at org.lerch.s3fs.S3FileSystemProvider.checkAccess(S3FileSystemP",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4541
https://github.com/broadinstitute/cromwell/issues/4541:72,Testability,test,tests,72,"Hi,; following the example on how to use AWS Batch I am able to run the tests using the suggested edits to aws.conf. I then try to specify the output directories using the following `options.json` file. ```; {; ""final_workflow_outputs_dir"": ""s3://bucket/cromwell/outputs"",; ""final_call_logs_dir"": ""s3:/bucket/cromwell/call_logs"",; ""final_workflow_log_dir"": ""s3://bucket/cromwell/wf_logs""; }; ```. When running cromwell : `java -Dconfig.file=awsbatch/aws.conf -jar cromwell-36.jar run awsbatch/hello.wdl -i awsbatch/hello.inputs -o options.json`. it results with the error:. ```; [2019-01-12 00:31:03,94] [info] $a [866d19d0]: Copying workflow logs from /rcecloud/kmavrommatis/workspace/Workflows/cromwell/cromwell-workflow-logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log to s3://celgene-rnd-riku-researchanalytics/cromwell/wf_logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log; [2019-01-12 00:31:04,03] [error] Key cannot be empty; java.lang.IllegalArgumentException: Key cannot be empty; 	at software.amazon.awssdk.core.util.ValidationUtils.assertStringNotEmpty(ValidationUtils.java:111); 	at software.amazon.awssdk.core.runtime.transform.PathMarshallers$GreedyPathMarshaller.marshall(PathMarshallers.java:109); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:87); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:31); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:88); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.headObject(DefaultS3Client.java:1628); 	at org.lerch.s3fs.util.S3Utils.getS3ObjectSummary(S3Utils.java:47); 	at org.lerch.s3fs.S3FileSystemProvider.exists(S3FileSystemProvider.java:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4541
https://github.com/broadinstitute/cromwell/issues/4541:643,Testability,log,logs,643,"Hi,; following the example on how to use AWS Batch I am able to run the tests using the suggested edits to aws.conf. I then try to specify the output directories using the following `options.json` file. ```; {; ""final_workflow_outputs_dir"": ""s3://bucket/cromwell/outputs"",; ""final_call_logs_dir"": ""s3:/bucket/cromwell/call_logs"",; ""final_workflow_log_dir"": ""s3://bucket/cromwell/wf_logs""; }; ```. When running cromwell : `java -Dconfig.file=awsbatch/aws.conf -jar cromwell-36.jar run awsbatch/hello.wdl -i awsbatch/hello.inputs -o options.json`. it results with the error:. ```; [2019-01-12 00:31:03,94] [info] $a [866d19d0]: Copying workflow logs from /rcecloud/kmavrommatis/workspace/Workflows/cromwell/cromwell-workflow-logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log to s3://celgene-rnd-riku-researchanalytics/cromwell/wf_logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log; [2019-01-12 00:31:04,03] [error] Key cannot be empty; java.lang.IllegalArgumentException: Key cannot be empty; 	at software.amazon.awssdk.core.util.ValidationUtils.assertStringNotEmpty(ValidationUtils.java:111); 	at software.amazon.awssdk.core.runtime.transform.PathMarshallers$GreedyPathMarshaller.marshall(PathMarshallers.java:109); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:87); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:31); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:88); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.headObject(DefaultS3Client.java:1628); 	at org.lerch.s3fs.util.S3Utils.getS3ObjectSummary(S3Utils.java:47); 	at org.lerch.s3fs.S3FileSystemProvider.exists(S3FileSystemProvider.java:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4541
https://github.com/broadinstitute/cromwell/issues/4541:723,Testability,log,logs,723,"Hi,; following the example on how to use AWS Batch I am able to run the tests using the suggested edits to aws.conf. I then try to specify the output directories using the following `options.json` file. ```; {; ""final_workflow_outputs_dir"": ""s3://bucket/cromwell/outputs"",; ""final_call_logs_dir"": ""s3:/bucket/cromwell/call_logs"",; ""final_workflow_log_dir"": ""s3://bucket/cromwell/wf_logs""; }; ```. When running cromwell : `java -Dconfig.file=awsbatch/aws.conf -jar cromwell-36.jar run awsbatch/hello.wdl -i awsbatch/hello.inputs -o options.json`. it results with the error:. ```; [2019-01-12 00:31:03,94] [info] $a [866d19d0]: Copying workflow logs from /rcecloud/kmavrommatis/workspace/Workflows/cromwell/cromwell-workflow-logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log to s3://celgene-rnd-riku-researchanalytics/cromwell/wf_logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log; [2019-01-12 00:31:04,03] [error] Key cannot be empty; java.lang.IllegalArgumentException: Key cannot be empty; 	at software.amazon.awssdk.core.util.ValidationUtils.assertStringNotEmpty(ValidationUtils.java:111); 	at software.amazon.awssdk.core.runtime.transform.PathMarshallers$GreedyPathMarshaller.marshall(PathMarshallers.java:109); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:87); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:31); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:88); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.headObject(DefaultS3Client.java:1628); 	at org.lerch.s3fs.util.S3Utils.getS3ObjectSummary(S3Utils.java:47); 	at org.lerch.s3fs.S3FileSystemProvider.exists(S3FileSystemProvider.java:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4541
https://github.com/broadinstitute/cromwell/issues/4541:774,Testability,log,log,774,"Hi,; following the example on how to use AWS Batch I am able to run the tests using the suggested edits to aws.conf. I then try to specify the output directories using the following `options.json` file. ```; {; ""final_workflow_outputs_dir"": ""s3://bucket/cromwell/outputs"",; ""final_call_logs_dir"": ""s3:/bucket/cromwell/call_logs"",; ""final_workflow_log_dir"": ""s3://bucket/cromwell/wf_logs""; }; ```. When running cromwell : `java -Dconfig.file=awsbatch/aws.conf -jar cromwell-36.jar run awsbatch/hello.wdl -i awsbatch/hello.inputs -o options.json`. it results with the error:. ```; [2019-01-12 00:31:03,94] [info] $a [866d19d0]: Copying workflow logs from /rcecloud/kmavrommatis/workspace/Workflows/cromwell/cromwell-workflow-logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log to s3://celgene-rnd-riku-researchanalytics/cromwell/wf_logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log; [2019-01-12 00:31:04,03] [error] Key cannot be empty; java.lang.IllegalArgumentException: Key cannot be empty; 	at software.amazon.awssdk.core.util.ValidationUtils.assertStringNotEmpty(ValidationUtils.java:111); 	at software.amazon.awssdk.core.runtime.transform.PathMarshallers$GreedyPathMarshaller.marshall(PathMarshallers.java:109); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:87); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:31); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:88); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.headObject(DefaultS3Client.java:1628); 	at org.lerch.s3fs.util.S3Utils.getS3ObjectSummary(S3Utils.java:47); 	at org.lerch.s3fs.S3FileSystemProvider.exists(S3FileSystemProvider.java:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4541
https://github.com/broadinstitute/cromwell/issues/4541:884,Testability,log,log,884,"Hi,; following the example on how to use AWS Batch I am able to run the tests using the suggested edits to aws.conf. I then try to specify the output directories using the following `options.json` file. ```; {; ""final_workflow_outputs_dir"": ""s3://bucket/cromwell/outputs"",; ""final_call_logs_dir"": ""s3:/bucket/cromwell/call_logs"",; ""final_workflow_log_dir"": ""s3://bucket/cromwell/wf_logs""; }; ```. When running cromwell : `java -Dconfig.file=awsbatch/aws.conf -jar cromwell-36.jar run awsbatch/hello.wdl -i awsbatch/hello.inputs -o options.json`. it results with the error:. ```; [2019-01-12 00:31:03,94] [info] $a [866d19d0]: Copying workflow logs from /rcecloud/kmavrommatis/workspace/Workflows/cromwell/cromwell-workflow-logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log to s3://celgene-rnd-riku-researchanalytics/cromwell/wf_logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log; [2019-01-12 00:31:04,03] [error] Key cannot be empty; java.lang.IllegalArgumentException: Key cannot be empty; 	at software.amazon.awssdk.core.util.ValidationUtils.assertStringNotEmpty(ValidationUtils.java:111); 	at software.amazon.awssdk.core.runtime.transform.PathMarshallers$GreedyPathMarshaller.marshall(PathMarshallers.java:109); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:87); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:31); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:88); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.headObject(DefaultS3Client.java:1628); 	at org.lerch.s3fs.util.S3Utils.getS3ObjectSummary(S3Utils.java:47); 	at org.lerch.s3fs.S3FileSystemProvider.exists(S3FileSystemProvider.java:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4541
https://github.com/broadinstitute/cromwell/issues/4541:1053,Testability,assert,assertStringNotEmpty,1053," tests using the suggested edits to aws.conf. I then try to specify the output directories using the following `options.json` file. ```; {; ""final_workflow_outputs_dir"": ""s3://bucket/cromwell/outputs"",; ""final_call_logs_dir"": ""s3:/bucket/cromwell/call_logs"",; ""final_workflow_log_dir"": ""s3://bucket/cromwell/wf_logs""; }; ```. When running cromwell : `java -Dconfig.file=awsbatch/aws.conf -jar cromwell-36.jar run awsbatch/hello.wdl -i awsbatch/hello.inputs -o options.json`. it results with the error:. ```; [2019-01-12 00:31:03,94] [info] $a [866d19d0]: Copying workflow logs from /rcecloud/kmavrommatis/workspace/Workflows/cromwell/cromwell-workflow-logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log to s3://celgene-rnd-riku-researchanalytics/cromwell/wf_logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log; [2019-01-12 00:31:04,03] [error] Key cannot be empty; java.lang.IllegalArgumentException: Key cannot be empty; 	at software.amazon.awssdk.core.util.ValidationUtils.assertStringNotEmpty(ValidationUtils.java:111); 	at software.amazon.awssdk.core.runtime.transform.PathMarshallers$GreedyPathMarshaller.marshall(PathMarshallers.java:109); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:87); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:31); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:88); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.headObject(DefaultS3Client.java:1628); 	at org.lerch.s3fs.util.S3Utils.getS3ObjectSummary(S3Utils.java:47); 	at org.lerch.s3fs.S3FileSystemProvider.exists(S3FileSystemProvider.java:624); 	at org.lerch.s3fs.S3FileSystemProvider.checkAccess(S3FileSystemP",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4541
https://github.com/broadinstitute/cromwell/issues/4542:403,Availability,error,errors,403,"Hi,; I am trying to run a workflow on AWS Batch using the genomics-ami.; The ami was built following the instructions in the relevant pages and i have confirmed that it contains a /cromwell-root mount point and has rw access to the bucket we use.; The AWS batch backpoint was tested with the hello.wdl workflow and it went through. When running the workflow on the local filesystem it completes without errors but when running it using the AWS Batch backend the first step fails with the following error:. ```; 2019-01-11 20:27:06,80] [error] WorkflowManagerActor Workflow 8fa7a9e4-f30d-4c19-b8cb-68be6442f317 failed (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; Caused by: java.io.IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542
https://github.com/broadinstitute/cromwell/issues/4542:498,Availability,error,error,498,"Hi,; I am trying to run a workflow on AWS Batch using the genomics-ami.; The ami was built following the instructions in the relevant pages and i have confirmed that it contains a /cromwell-root mount point and has rw access to the bucket we use.; The AWS batch backpoint was tested with the hello.wdl workflow and it went through. When running the workflow on the local filesystem it completes without errors but when running it using the AWS Batch backend the first step fails with the following error:. ```; 2019-01-11 20:27:06,80] [error] WorkflowManagerActor Workflow 8fa7a9e4-f30d-4c19-b8cb-68be6442f317 failed (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; Caused by: java.io.IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542
https://github.com/broadinstitute/cromwell/issues/4542:536,Availability,error,error,536,"Hi,; I am trying to run a workflow on AWS Batch using the genomics-ami.; The ami was built following the instructions in the relevant pages and i have confirmed that it contains a /cromwell-root mount point and has rw access to the bucket we use.; The AWS batch backpoint was tested with the hello.wdl workflow and it went through. When running the workflow on the local filesystem it completes without errors but when running it using the AWS Batch backend the first step fails with the following error:. ```; 2019-01-11 20:27:06,80] [error] WorkflowManagerActor Workflow 8fa7a9e4-f30d-4c19-b8cb-68be6442f317 failed (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; Caused by: java.io.IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542
https://github.com/broadinstitute/cromwell/issues/4542:1644,Availability,Failure,Failure,1644,: cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; Caused by: java.io.IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkj,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542
https://github.com/broadinstitute/cromwell/issues/4542:1652,Availability,recover,recoverWith,1652,ngine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; Caused by: java.io.IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoin,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542
https://github.com/broadinstitute/cromwell/issues/4542:8017,Deployability,configurat,configuration,8017,"ate directory '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': Permission denied; ; 04:26:11; /bin/bash: line 66: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/cwl.output.json': No such file or directory; ; 04:26:11; /bin/bash: line 72: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': No such file or directory; ; 04:26:11; mv: cannot stat '/cromwell_root/bbmap-rc.txt.tmp': No such file or directory; ; 04:26:11; MIME-Version: 1.0; ; 04:26:11; Content-Type: multipart/alternative; boundary=278185423cec5467d351ab751807c36a; ; 04:26:11; --278185423cec5467d351ab751807c36a; ; 04:26:11; Content-Type: text/plain; ; 04:26:11; Content-Disposition: attachment; filename=rc.txt; ; 04:26:11; cat: /cromwell_root/bbmap-rc.txt: No such file or directory; ; 04:26:11; --278185423cec5467d351ab751807c36a; ; 04:26:11; Content-Type: text/plain; ; 04:26:11; Content-Disposition: attachment; filename=stdout.txt; ; 04:26:11; cat: /cromwell_root/bbmap-stdout.log: No such file or directory; ; 04:26:11; --278185423cec5467d351ab751807c36a; ; 04:26:11; Content-Type: text/plain; ; 04:26:11; Content-Disposition: attachment; filename=stderr.txt; ; 04:26:11; cat: /cromwell_root/DA0000317_WSU-DLCL.qcstats: No such file or directory; ; 04:26:11; --278185423cec5467d351ab751807c36a--; ; 04:26:11; cat: /cromwell_root/bbmap-rc.txt: No such file or directory; ; 04:26:11; rm: cannot remove '/out.1': No such file or directory; ; 04:26:11; rm: cannot remove '/err.1': No such file or directory; ```; I have also tried to login to the node, and explicitly specify 777 permissions to /cromwell-root but the result was the same.; Are there any specific considerations regarding the docker image or any additional configuration required? . Thanks in advance for your help",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542
https://github.com/broadinstitute/cromwell/issues/4542:680,Modifiability,Enhance,EnhancedCromwellIoException,680,"Hi,; I am trying to run a workflow on AWS Batch using the genomics-ami.; The ami was built following the instructions in the relevant pages and i have confirmed that it contains a /cromwell-root mount point and has rw access to the bucket we use.; The AWS batch backpoint was tested with the hello.wdl workflow and it went through. When running the workflow on the local filesystem it completes without errors but when running it using the AWS Batch backend the first step fails with the following error:. ```; 2019-01-11 20:27:06,80] [error] WorkflowManagerActor Workflow 8fa7a9e4-f30d-4c19-b8cb-68be6442f317 failed (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; Caused by: java.io.IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542
https://github.com/broadinstitute/cromwell/issues/4542:8017,Modifiability,config,configuration,8017,"ate directory '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': Permission denied; ; 04:26:11; /bin/bash: line 66: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/cwl.output.json': No such file or directory; ; 04:26:11; /bin/bash: line 72: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': No such file or directory; ; 04:26:11; mv: cannot stat '/cromwell_root/bbmap-rc.txt.tmp': No such file or directory; ; 04:26:11; MIME-Version: 1.0; ; 04:26:11; Content-Type: multipart/alternative; boundary=278185423cec5467d351ab751807c36a; ; 04:26:11; --278185423cec5467d351ab751807c36a; ; 04:26:11; Content-Type: text/plain; ; 04:26:11; Content-Disposition: attachment; filename=rc.txt; ; 04:26:11; cat: /cromwell_root/bbmap-rc.txt: No such file or directory; ; 04:26:11; --278185423cec5467d351ab751807c36a; ; 04:26:11; Content-Type: text/plain; ; 04:26:11; Content-Disposition: attachment; filename=stdout.txt; ; 04:26:11; cat: /cromwell_root/bbmap-stdout.log: No such file or directory; ; 04:26:11; --278185423cec5467d351ab751807c36a; ; 04:26:11; Content-Type: text/plain; ; 04:26:11; Content-Disposition: attachment; filename=stderr.txt; ; 04:26:11; cat: /cromwell_root/DA0000317_WSU-DLCL.qcstats: No such file or directory; ; 04:26:11; --278185423cec5467d351ab751807c36a--; ; 04:26:11; cat: /cromwell_root/bbmap-rc.txt: No such file or directory; ; 04:26:11; rm: cannot remove '/out.1': No such file or directory; ; 04:26:11; rm: cannot remove '/err.1': No such file or directory; ```; I have also tried to login to the node, and explicitly specify 777 permissions to /cromwell-root but the result was the same.; Are there any specific considerations regarding the docker image or any additional configuration required? . Thanks in advance for your help",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542
https://github.com/broadinstitute/cromwell/issues/4542:1652,Safety,recover,recoverWith,1652,ngine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; Caused by: java.io.IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoin,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542
https://github.com/broadinstitute/cromwell/issues/4542:218,Security,access,access,218,"Hi,; I am trying to run a workflow on AWS Batch using the genomics-ami.; The ami was built following the instructions in the relevant pages and i have confirmed that it contains a /cromwell-root mount point and has rw access to the bucket we use.; The AWS batch backpoint was tested with the hello.wdl workflow and it went through. When running the workflow on the local filesystem it completes without errors but when running it using the AWS Batch backend the first step fails with the following error:. ```; 2019-01-11 20:27:06,80] [error] WorkflowManagerActor Workflow 8fa7a9e4-f30d-4c19-b8cb-68be6442f317 failed (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; Caused by: java.io.IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542
https://github.com/broadinstitute/cromwell/issues/4542:4439,Security,access,access,4439,"romwell.core.path.EvenBetterPathMethods.mediaInputStream(EvenBetterPathMethods.scala:94); 	at cromwell.core.path.EvenBetterPathMethods.mediaInputStream$(EvenBetterPathMethods.scala:91); 	at cromwell.filesystems.s3.S3Path.mediaInputStream(S3PathBuilder.scala:156); 	at cromwell.engine.io.nio.NioFlow.$anonfun$withReader$1(NioFlow.scala:145); 	at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:14); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); 	... 14 more. [2019-01-11 20:27:06,80] [info] WorkflowManagerActor WorkflowActor-8fa7a9e4-f30d-4c19-b8cb-68be6442f317 is in a terminal state: WorkflowFailedState. ```. Looking at the cloudwatch logs it appears that the problem is with permission on the node. ```; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl': Permission denied; ; 04:26:11; chmod: cannot access '': No such file or directory; ; 04:26:11; mkfifo: cannot create fifo '/out.1': Permission denied; ; 04:26:11; mkfifo: cannot create fifo '/err.1': Permission denied; ; 04:26:11; /bin/bash: line 15: /out.1: No such file or directory; ; 04:26:11; /bin/bash: line 16: /err.1: No such file or directory; ; 04:26:11; /bin/bash: line 22: /out.1: Permission denied; ; 04:26:11; /bin/bash: line 23: /cromwell_root/bbmap-rc.txt.tmp: Permission denied; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa': Permission denied; ; 04:26:11; /bin/bash: line 38: /cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*R?.fq.gz': No such file or directory; ; 04:26:11; /bin/bash: line 44: /cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa': No such file",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542
https://github.com/broadinstitute/cromwell/issues/4542:5183,Security,access,access,5183,0] [info] WorkflowManagerActor WorkflowActor-8fa7a9e4-f30d-4c19-b8cb-68be6442f317 is in a terminal state: WorkflowFailedState. ```. Looking at the cloudwatch logs it appears that the problem is with permission on the node. ```; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl': Permission denied; ; 04:26:11; chmod: cannot access '': No such file or directory; ; 04:26:11; mkfifo: cannot create fifo '/out.1': Permission denied; ; 04:26:11; mkfifo: cannot create fifo '/err.1': Permission denied; ; 04:26:11; /bin/bash: line 15: /out.1: No such file or directory; ; 04:26:11; /bin/bash: line 16: /err.1: No such file or directory; ; 04:26:11; /bin/bash: line 22: /out.1: Permission denied; ; 04:26:11; /bin/bash: line 23: /cromwell_root/bbmap-rc.txt.tmp: Permission denied; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa': Permission denied; ; 04:26:11; /bin/bash: line 38: /cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*R?.fq.gz': No such file or directory; ; 04:26:11; /bin/bash: line 44: /cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa': No such file or directory; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-560912e697c3494360223c7ca65aa3e8': Permission denied; ; 04:26:11; /bin/bash: line 52: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*.qcstats': No such file or directory; ; 04:26:11; /bin/bash: line 58: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-560912e697c3494360223c7ca65aa3e8': No such file or director,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542
https://github.com/broadinstitute/cromwell/issues/4542:5380,Security,access,access,5380,1; mkfifo: cannot create fifo '/err.1': Permission denied; ; 04:26:11; /bin/bash: line 15: /out.1: No such file or directory; ; 04:26:11; /bin/bash: line 16: /err.1: No such file or directory; ; 04:26:11; /bin/bash: line 22: /out.1: Permission denied; ; 04:26:11; /bin/bash: line 23: /cromwell_root/bbmap-rc.txt.tmp: Permission denied; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa': Permission denied; ; 04:26:11; /bin/bash: line 38: /cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*R?.fq.gz': No such file or directory; ; 04:26:11; /bin/bash: line 44: /cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa': No such file or directory; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-560912e697c3494360223c7ca65aa3e8': Permission denied; ; 04:26:11; /bin/bash: line 52: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*.qcstats': No such file or directory; ; 04:26:11; /bin/bash: line 58: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-560912e697c3494360223c7ca65aa3e8': No such file or directory; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': Permission denied; ; 04:26:11; /bin/bash: line 66: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/cwl.output.json': No such file or directory; ; 04:26:11; /bin/bash: line 72: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542
https://github.com/broadinstitute/cromwell/issues/4542:5757,Security,access,access,5757,1; mkfifo: cannot create fifo '/err.1': Permission denied; ; 04:26:11; /bin/bash: line 15: /out.1: No such file or directory; ; 04:26:11; /bin/bash: line 16: /err.1: No such file or directory; ; 04:26:11; /bin/bash: line 22: /out.1: Permission denied; ; 04:26:11; /bin/bash: line 23: /cromwell_root/bbmap-rc.txt.tmp: Permission denied; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa': Permission denied; ; 04:26:11; /bin/bash: line 38: /cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*R?.fq.gz': No such file or directory; ; 04:26:11; /bin/bash: line 44: /cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa': No such file or directory; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-560912e697c3494360223c7ca65aa3e8': Permission denied; ; 04:26:11; /bin/bash: line 52: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*.qcstats': No such file or directory; ; 04:26:11; /bin/bash: line 58: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-560912e697c3494360223c7ca65aa3e8': No such file or directory; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': Permission denied; ; 04:26:11; /bin/bash: line 66: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/cwl.output.json': No such file or directory; ; 04:26:11; /bin/bash: line 72: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542
https://github.com/broadinstitute/cromwell/issues/4542:5954,Security,access,access,5954, such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*R?.fq.gz': No such file or directory; ; 04:26:11; /bin/bash: line 44: /cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa': No such file or directory; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-560912e697c3494360223c7ca65aa3e8': Permission denied; ; 04:26:11; /bin/bash: line 52: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*.qcstats': No such file or directory; ; 04:26:11; /bin/bash: line 58: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-560912e697c3494360223c7ca65aa3e8': No such file or directory; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': Permission denied; ; 04:26:11; /bin/bash: line 66: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/cwl.output.json': No such file or directory; ; 04:26:11; /bin/bash: line 72: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': No such file or directory; ; 04:26:11; mv: cannot stat '/cromwell_root/bbmap-rc.txt.tmp': No such file or directory; ; 04:26:11; MIME-Version: 1.0; ; 04:26:11; Content-Type: multipart/alternative; boundary=278185423cec5467d351ab751807c36a; ; 04:26:11; --278185423cec5467d351ab751807c36a; ; 04:26:11; Content-Type: text/plain; ; 04:26:11; Content-Disposition: attachment; filename=rc.txt; ; 04:26:11; cat: /cromwell_root/bbmap-rc.txt: No such file or directory; ; 04:26:11; --278185423cec5467d351ab751807c36a; ; 04:26:11; Conte,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542
https://github.com/broadinstitute/cromwell/issues/4542:6331,Security,access,access,6331, such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*R?.fq.gz': No such file or directory; ; 04:26:11; /bin/bash: line 44: /cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa': No such file or directory; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-560912e697c3494360223c7ca65aa3e8': Permission denied; ; 04:26:11; /bin/bash: line 52: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*.qcstats': No such file or directory; ; 04:26:11; /bin/bash: line 58: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-560912e697c3494360223c7ca65aa3e8': No such file or directory; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': Permission denied; ; 04:26:11; /bin/bash: line 66: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/cwl.output.json': No such file or directory; ; 04:26:11; /bin/bash: line 72: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': No such file or directory; ; 04:26:11; mv: cannot stat '/cromwell_root/bbmap-rc.txt.tmp': No such file or directory; ; 04:26:11; MIME-Version: 1.0; ; 04:26:11; Content-Type: multipart/alternative; boundary=278185423cec5467d351ab751807c36a; ; 04:26:11; --278185423cec5467d351ab751807c36a; ; 04:26:11; Content-Type: text/plain; ; 04:26:11; Content-Disposition: attachment; filename=rc.txt; ; 04:26:11; cat: /cromwell_root/bbmap-rc.txt: No such file or directory; ; 04:26:11; --278185423cec5467d351ab751807c36a; ; 04:26:11; Conte,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542
https://github.com/broadinstitute/cromwell/issues/4542:6534,Security,access,access,6534,denied; ; 04:26:11; /bin/bash: line 52: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*.qcstats': No such file or directory; ; 04:26:11; /bin/bash: line 58: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-560912e697c3494360223c7ca65aa3e8': No such file or directory; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': Permission denied; ; 04:26:11; /bin/bash: line 66: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/cwl.output.json': No such file or directory; ; 04:26:11; /bin/bash: line 72: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': No such file or directory; ; 04:26:11; mv: cannot stat '/cromwell_root/bbmap-rc.txt.tmp': No such file or directory; ; 04:26:11; MIME-Version: 1.0; ; 04:26:11; Content-Type: multipart/alternative; boundary=278185423cec5467d351ab751807c36a; ; 04:26:11; --278185423cec5467d351ab751807c36a; ; 04:26:11; Content-Type: text/plain; ; 04:26:11; Content-Disposition: attachment; filename=rc.txt; ; 04:26:11; cat: /cromwell_root/bbmap-rc.txt: No such file or directory; ; 04:26:11; --278185423cec5467d351ab751807c36a; ; 04:26:11; Content-Type: text/plain; ; 04:26:11; Content-Disposition: attachment; filename=stdout.txt; ; 04:26:11; cat: /cromwell_root/bbmap-stdout.log: No such file or directory; ; 04:26:11; --278185423cec5467d351ab751807c36a; ; 04:26:11; Content-Type: text/plain; ; 04:26:11; Content-Disposition: attachment; filename=stderr.txt; ; 04:26:11; cat: /cromwell_root/DA0000317_WSU-DLCL.qcstats: No such file or directory; ; 04:26:11; --278185423cec5467d351ab7518,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542
https://github.com/broadinstitute/cromwell/issues/4542:276,Testability,test,tested,276,"Hi,; I am trying to run a workflow on AWS Batch using the genomics-ami.; The ami was built following the instructions in the relevant pages and i have confirmed that it contains a /cromwell-root mount point and has rw access to the bucket we use.; The AWS batch backpoint was tested with the hello.wdl workflow and it went through. When running the workflow on the local filesystem it completes without errors but when running it using the AWS Batch backend the first step fails with the following error:. ```; 2019-01-11 20:27:06,80] [error] WorkflowManagerActor Workflow 8fa7a9e4-f30d-4c19-b8cb-68be6442f317 failed (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; Caused by: java.io.IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542
https://github.com/broadinstitute/cromwell/issues/4542:4199,Testability,log,logs,4199,"e.path.BetterFileMethods.newInputStream(BetterFileMethods.scala:240); 	at cromwell.core.path.BetterFileMethods.newInputStream$(BetterFileMethods.scala:239); 	at cromwell.filesystems.s3.S3Path.newInputStream(S3PathBuilder.scala:156); 	at cromwell.core.path.EvenBetterPathMethods.mediaInputStream(EvenBetterPathMethods.scala:94); 	at cromwell.core.path.EvenBetterPathMethods.mediaInputStream$(EvenBetterPathMethods.scala:91); 	at cromwell.filesystems.s3.S3Path.mediaInputStream(S3PathBuilder.scala:156); 	at cromwell.engine.io.nio.NioFlow.$anonfun$withReader$1(NioFlow.scala:145); 	at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:14); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); 	... 14 more. [2019-01-11 20:27:06,80] [info] WorkflowManagerActor WorkflowActor-8fa7a9e4-f30d-4c19-b8cb-68be6442f317 is in a terminal state: WorkflowFailedState. ```. Looking at the cloudwatch logs it appears that the problem is with permission on the node. ```; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl': Permission denied; ; 04:26:11; chmod: cannot access '': No such file or directory; ; 04:26:11; mkfifo: cannot create fifo '/out.1': Permission denied; ; 04:26:11; mkfifo: cannot create fifo '/err.1': Permission denied; ; 04:26:11; /bin/bash: line 15: /out.1: No such file or directory; ; 04:26:11; /bin/bash: line 16: /err.1: No such file or directory; ; 04:26:11; /bin/bash: line 22: /out.1: Permission denied; ; 04:26:11; /bin/bash: line 23: /cromwell_root/bbmap-rc.txt.tmp: Permission denied; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa': Permission denied; ; 04:26:11; /bin/bash: line 38: /cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*R?.fq.gz': ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542
https://github.com/broadinstitute/cromwell/issues/4542:7266,Testability,log,log,7266,"ate directory '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': Permission denied; ; 04:26:11; /bin/bash: line 66: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/cwl.output.json': No such file or directory; ; 04:26:11; /bin/bash: line 72: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': No such file or directory; ; 04:26:11; mv: cannot stat '/cromwell_root/bbmap-rc.txt.tmp': No such file or directory; ; 04:26:11; MIME-Version: 1.0; ; 04:26:11; Content-Type: multipart/alternative; boundary=278185423cec5467d351ab751807c36a; ; 04:26:11; --278185423cec5467d351ab751807c36a; ; 04:26:11; Content-Type: text/plain; ; 04:26:11; Content-Disposition: attachment; filename=rc.txt; ; 04:26:11; cat: /cromwell_root/bbmap-rc.txt: No such file or directory; ; 04:26:11; --278185423cec5467d351ab751807c36a; ; 04:26:11; Content-Type: text/plain; ; 04:26:11; Content-Disposition: attachment; filename=stdout.txt; ; 04:26:11; cat: /cromwell_root/bbmap-stdout.log: No such file or directory; ; 04:26:11; --278185423cec5467d351ab751807c36a; ; 04:26:11; Content-Type: text/plain; ; 04:26:11; Content-Disposition: attachment; filename=stderr.txt; ; 04:26:11; cat: /cromwell_root/DA0000317_WSU-DLCL.qcstats: No such file or directory; ; 04:26:11; --278185423cec5467d351ab751807c36a--; ; 04:26:11; cat: /cromwell_root/bbmap-rc.txt: No such file or directory; ; 04:26:11; rm: cannot remove '/out.1': No such file or directory; ; 04:26:11; rm: cannot remove '/err.1': No such file or directory; ```; I have also tried to login to the node, and explicitly specify 777 permissions to /cromwell-root but the result was the same.; Are there any specific considerations regarding the docker image or any additional configuration required? . Thanks in advance for your help",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542
https://github.com/broadinstitute/cromwell/issues/4542:7828,Testability,log,login,7828,"ate directory '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': Permission denied; ; 04:26:11; /bin/bash: line 66: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/cwl.output.json': No such file or directory; ; 04:26:11; /bin/bash: line 72: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': No such file or directory; ; 04:26:11; mv: cannot stat '/cromwell_root/bbmap-rc.txt.tmp': No such file or directory; ; 04:26:11; MIME-Version: 1.0; ; 04:26:11; Content-Type: multipart/alternative; boundary=278185423cec5467d351ab751807c36a; ; 04:26:11; --278185423cec5467d351ab751807c36a; ; 04:26:11; Content-Type: text/plain; ; 04:26:11; Content-Disposition: attachment; filename=rc.txt; ; 04:26:11; cat: /cromwell_root/bbmap-rc.txt: No such file or directory; ; 04:26:11; --278185423cec5467d351ab751807c36a; ; 04:26:11; Content-Type: text/plain; ; 04:26:11; Content-Disposition: attachment; filename=stdout.txt; ; 04:26:11; cat: /cromwell_root/bbmap-stdout.log: No such file or directory; ; 04:26:11; --278185423cec5467d351ab751807c36a; ; 04:26:11; Content-Type: text/plain; ; 04:26:11; Content-Disposition: attachment; filename=stderr.txt; ; 04:26:11; cat: /cromwell_root/DA0000317_WSU-DLCL.qcstats: No such file or directory; ; 04:26:11; --278185423cec5467d351ab751807c36a--; ; 04:26:11; cat: /cromwell_root/bbmap-rc.txt: No such file or directory; ; 04:26:11; rm: cannot remove '/out.1': No such file or directory; ; 04:26:11; rm: cannot remove '/err.1': No such file or directory; ```; I have also tried to login to the node, and explicitly specify 777 permissions to /cromwell-root but the result was the same.; Are there any specific considerations regarding the docker image or any additional configuration required? . Thanks in advance for your help",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542
https://github.com/broadinstitute/cromwell/pull/4543:13,Availability,down,down,13,"another CTKS down, maybe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4543
https://github.com/broadinstitute/cromwell/issues/4544:61,Modifiability,inherit,inherited,61,"Per the CWL Specifications, section 3.3:. > Requirements are inherited. A requirement specified in a Workflow applies to all workflow steps; a requirement specified on a workflow step will apply to the process implementation of that step and any of its substeps. It appears that Cromwell does not correctly implement the above. For example, a `dockerPull` requirement specified in a workflow, does not get applied to a command in a subworkflow. Cromwell version: `cromwell-36`. `test.cwl`; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: Workflow. requirements:; - class: SubworkflowFeatureRequirement; - class: DockerRequirement; dockerPull: ubuntu. inputs: []. steps:; substep:; run: subworkflow.cwl; in: []; out: [container]; ; outputs:; container:; type: File; outputSource: substep/container; ```. `subworkflow.cwl`; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: Workflow. requirements:; - class: SubworkflowFeatureRequirement. inputs: []. steps:; substep:; run: command.cwl; in: []; out: [container]; ; outputs:; container:; type: File; outputSource: substep/container; ```. `command.cwl`; ``` ; cwlVersion: v1.0 ; class: CommandLineTool. baseCommand: ['grep', 'docker', '/proc/1/cgroup']. inputs: []; outputs:; container:; type: stdout; ```. `test.yaml`; ```; {}; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4544
https://github.com/broadinstitute/cromwell/issues/4544:479,Testability,test,test,479,"Per the CWL Specifications, section 3.3:. > Requirements are inherited. A requirement specified in a Workflow applies to all workflow steps; a requirement specified on a workflow step will apply to the process implementation of that step and any of its substeps. It appears that Cromwell does not correctly implement the above. For example, a `dockerPull` requirement specified in a workflow, does not get applied to a command in a subworkflow. Cromwell version: `cromwell-36`. `test.cwl`; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: Workflow. requirements:; - class: SubworkflowFeatureRequirement; - class: DockerRequirement; dockerPull: ubuntu. inputs: []. steps:; substep:; run: subworkflow.cwl; in: []; out: [container]; ; outputs:; container:; type: File; outputSource: substep/container; ```. `subworkflow.cwl`; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: Workflow. requirements:; - class: SubworkflowFeatureRequirement. inputs: []. steps:; substep:; run: command.cwl; in: []; out: [container]; ; outputs:; container:; type: File; outputSource: substep/container; ```. `command.cwl`; ``` ; cwlVersion: v1.0 ; class: CommandLineTool. baseCommand: ['grep', 'docker', '/proc/1/cgroup']. inputs: []; outputs:; container:; type: stdout; ```. `test.yaml`; ```; {}; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4544
https://github.com/broadinstitute/cromwell/issues/4544:1270,Testability,test,test,1270,"Per the CWL Specifications, section 3.3:. > Requirements are inherited. A requirement specified in a Workflow applies to all workflow steps; a requirement specified on a workflow step will apply to the process implementation of that step and any of its substeps. It appears that Cromwell does not correctly implement the above. For example, a `dockerPull` requirement specified in a workflow, does not get applied to a command in a subworkflow. Cromwell version: `cromwell-36`. `test.cwl`; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: Workflow. requirements:; - class: SubworkflowFeatureRequirement; - class: DockerRequirement; dockerPull: ubuntu. inputs: []. steps:; substep:; run: subworkflow.cwl; in: []; out: [container]; ; outputs:; container:; type: File; outputSource: substep/container; ```. `subworkflow.cwl`; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: Workflow. requirements:; - class: SubworkflowFeatureRequirement. inputs: []. steps:; substep:; run: command.cwl; in: []; out: [container]; ; outputs:; container:; type: File; outputSource: substep/container; ```. `command.cwl`; ``` ; cwlVersion: v1.0 ; class: CommandLineTool. baseCommand: ['grep', 'docker', '/proc/1/cgroup']. inputs: []; outputs:; container:; type: stdout; ```. `test.yaml`; ```; {}; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4544
https://github.com/broadinstitute/cromwell/issues/4545:12,Deployability,deploy,deploy,12,"In order to deploy CromIAM, we need to add the deployment configurations to `firecloud-develop`. This means, at a minimum:; * a consul-template-ized `docker-compose.yml` for environments `live`, `fiab`, and `local`; * consul-template-ized `cromiam.conf` file ; * A stable-versioned, published docker image of cromiam. This should be submitted as a PR to `dev` branch, whereupon it makes its way through QA to the production environment. I've submitted my initial work(a skeleton) to the `db_add_cromiam` branch in the firecloud-develop repo.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4545
https://github.com/broadinstitute/cromwell/issues/4545:47,Deployability,deploy,deployment,47,"In order to deploy CromIAM, we need to add the deployment configurations to `firecloud-develop`. This means, at a minimum:; * a consul-template-ized `docker-compose.yml` for environments `live`, `fiab`, and `local`; * consul-template-ized `cromiam.conf` file ; * A stable-versioned, published docker image of cromiam. This should be submitted as a PR to `dev` branch, whereupon it makes its way through QA to the production environment. I've submitted my initial work(a skeleton) to the `db_add_cromiam` branch in the firecloud-develop repo.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4545
https://github.com/broadinstitute/cromwell/issues/4545:58,Deployability,configurat,configurations,58,"In order to deploy CromIAM, we need to add the deployment configurations to `firecloud-develop`. This means, at a minimum:; * a consul-template-ized `docker-compose.yml` for environments `live`, `fiab`, and `local`; * consul-template-ized `cromiam.conf` file ; * A stable-versioned, published docker image of cromiam. This should be submitted as a PR to `dev` branch, whereupon it makes its way through QA to the production environment. I've submitted my initial work(a skeleton) to the `db_add_cromiam` branch in the firecloud-develop repo.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4545
https://github.com/broadinstitute/cromwell/issues/4545:58,Modifiability,config,configurations,58,"In order to deploy CromIAM, we need to add the deployment configurations to `firecloud-develop`. This means, at a minimum:; * a consul-template-ized `docker-compose.yml` for environments `live`, `fiab`, and `local`; * consul-template-ized `cromiam.conf` file ; * A stable-versioned, published docker image of cromiam. This should be submitted as a PR to `dev` branch, whereupon it makes its way through QA to the production environment. I've submitted my initial work(a skeleton) to the `db_add_cromiam` branch in the firecloud-develop repo.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4545
https://github.com/broadinstitute/cromwell/issues/4546:463,Availability,error,error,463,"Hi, I was trying to have a VCF related workflow, which involves gatk4, picard tools. As an example, lets say I want to call gatk4 first to get some VCF files, and use picard to sort them. if i have `gatk4.cwl` output as; ```; outputs:; vcf_list:; type: File[]; outputBinding:; glob: '*.vcf.gz'; secondaryFiles: [.tbi]; ```; and next `picard sort` has input array (w/ or w/o `secondaryFiles` here doesn’t matter from my tests. Neither works and will have the same error); ```; inputs:; vcf:; type:; type: array; items: File; inputBinding:; prefix: I=; separate: false; ```; After gatk4 finishes, the `execution` dir will look like; ``` ; drwx------ 3 root root 4.0K Jan 14 19:16 genomicsdb-0; -rw-r--r-- 3 root root 5.7K Jan 14 20:17 genomicsdb-0.vcf.gz; -rw-r--r-- 2 root root 105 Jan 14 20:17 genomicsdb-0.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 19:17 genomicsdb-1; -rw-r--r-- 3 root root 927K Jan 14 20:32 genomicsdb-1.vcf.gz; -rw-r--r-- 2 root root 7.6K Jan 14 20:32 genomicsdb-1.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 19:29 genomicsdb-2; -rw-r--r-- 3 root root 554K Jan 14 20:31 genomicsdb-2.vcf.gz; -rw-r--r-- 2 root root 11K Jan 14 20:31 genomicsdb-2.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 19:41 genomicsdb-3; -rw-r--r-- 3 root root 813K Jan 14 20:30 genomicsdb-3.vcf.gz; -rw-r--r-- 2 root root 11K Jan 14 20:30 genomicsdb-3.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 19:52 genomicsdb-4; -rw-r--r-- 3 root root 620K Jan 14 20:32 genomicsdb-4.vcf.gz; -rw-r--r-- 2 root root 12K Jan 14 20:32 genomicsdb-4.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 20:04 genomicsdb-5; -rw-r--r-- 3 root root 50K Jan 14 20:17 genomicsdb-5.vcf.gz; -rw-r--r-- 2 root root 746 Jan 14 20:17 genomicsdb-5.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 20:05 genomicsdb-6; -rw-r--r-- 3 root root 673K Jan 14 20:31 genomicsdb-6.vcf.gz; -rw-r--r-- 2 root root 13K Jan 14 20:31 genomicsdb-6.vcf.gz.tbi; drwxr-xr-x 2 root root 4.0K Jan 14 20:32 glob-330eecb06b4c0ad6b45febf0c8001b04; -rw-r--r--",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4546
https://github.com/broadinstitute/cromwell/issues/4546:4497,Availability,error,error,4497,"-r--r-- 2 root root 13K Jan 14 20:31 genomicsdb-6.vcf.gz.tbi. glob-b34dfc006a981a93d6da067cf50036fe:; total 512; -rw-r--r-- 1 root root 277 Jan 14 20:32 cromwell_glob_control_file. glob-ce2a0ab5d8c37a6d061c814f835853ee:; total 3.6M; -rw-r--r-- 1 root root 277 Jan 14 20:32 cromwell_glob_control_file; -rw-r--r-- 3 root root 5.7K Jan 14 20:17 genomicsdb-0.vcf.gz; -rw-r--r-- 3 root root 927K Jan 14 20:32 genomicsdb-1.vcf.gz; -rw-r--r-- 3 root root 554K Jan 14 20:31 genomicsdb-2.vcf.gz; -rw-r--r-- 3 root root 813K Jan 14 20:30 genomicsdb-3.vcf.gz; -rw-r--r-- 3 root root 620K Jan 14 20:32 genomicsdb-4.vcf.gz; -rw-r--r-- 3 root root 50K Jan 14 20:17 genomicsdb-5.vcf.gz; -rw-r--r-- 3 root root 673K Jan 14 20:31 genomicsdb-6.vcf.gz; ```; As you can see, here `vcf.gz` and `vcf.gz.tbi` are stored under different directories.; However, the next `picard sort` step will be only looking at the directory where all `vcf.gz` live, which leads to the error:; ```; Could not localize /mnt/glusterfs/genomel-cohort-cwl/cromwell-executions/cwl_temp_file_6dfd1508-a107-491d-9cc2-8984f8e84977.cwl/6dfd1508-a107-491d-9cc2-8984f8e84977/call-gatk4_cohort_genotyping/shard-0/gatk4_cohort_genotyping.cwl/09c59ed5-8631-415c-97bc-896553cd775a/call-genomel_pdc_gatk4_cohort_genotyping/execution/glob-ce2a0ab5d8c37a6d061c814f835853ee/genomicsdb-0.vcf.gz.tbi -> /mnt/glusterfs/genomel-cohort-cwl/cromwell-executions/cwl_temp_file_6dfd1508-a107-491d-9cc2-8984f8e84977.cwl/6dfd1508-a107-491d-9cc2-8984f8e84977/call-gatk4_cohort_genotyping/shard-0/gatk4_cohort_genotyping.cwl/09c59ed5-8631-415c-97bc-896553cd775a/call-picard_sortvcf/inputs/2004815296/genomicsdb-0.vcf.gz.tbi:',; u""\t/mnt/glusterfs/genomel-cohort-cwl/cromwell-executions/cwl_temp_file_6dfd1508-a107-491d-9cc2-8984f8e84977.cwl/6dfd1508-a107-491d-9cc2-8984f8e84977/call-gatk4_cohort_genotyping/shard-0/gatk4_cohort_genotyping.cwl/09c59ed5-8631-415c-97bc-896553cd775a/call-genomel_pdc_gatk4_cohort_genotyping/execution/glob-ce2a0ab5d8c37a6d061c814f835853ee/gen",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4546
https://github.com/broadinstitute/cromwell/issues/4546:419,Testability,test,tests,419,"Hi, I was trying to have a VCF related workflow, which involves gatk4, picard tools. As an example, lets say I want to call gatk4 first to get some VCF files, and use picard to sort them. if i have `gatk4.cwl` output as; ```; outputs:; vcf_list:; type: File[]; outputBinding:; glob: '*.vcf.gz'; secondaryFiles: [.tbi]; ```; and next `picard sort` has input array (w/ or w/o `secondaryFiles` here doesn’t matter from my tests. Neither works and will have the same error); ```; inputs:; vcf:; type:; type: array; items: File; inputBinding:; prefix: I=; separate: false; ```; After gatk4 finishes, the `execution` dir will look like; ``` ; drwx------ 3 root root 4.0K Jan 14 19:16 genomicsdb-0; -rw-r--r-- 3 root root 5.7K Jan 14 20:17 genomicsdb-0.vcf.gz; -rw-r--r-- 2 root root 105 Jan 14 20:17 genomicsdb-0.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 19:17 genomicsdb-1; -rw-r--r-- 3 root root 927K Jan 14 20:32 genomicsdb-1.vcf.gz; -rw-r--r-- 2 root root 7.6K Jan 14 20:32 genomicsdb-1.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 19:29 genomicsdb-2; -rw-r--r-- 3 root root 554K Jan 14 20:31 genomicsdb-2.vcf.gz; -rw-r--r-- 2 root root 11K Jan 14 20:31 genomicsdb-2.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 19:41 genomicsdb-3; -rw-r--r-- 3 root root 813K Jan 14 20:30 genomicsdb-3.vcf.gz; -rw-r--r-- 2 root root 11K Jan 14 20:30 genomicsdb-3.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 19:52 genomicsdb-4; -rw-r--r-- 3 root root 620K Jan 14 20:32 genomicsdb-4.vcf.gz; -rw-r--r-- 2 root root 12K Jan 14 20:32 genomicsdb-4.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 20:04 genomicsdb-5; -rw-r--r-- 3 root root 50K Jan 14 20:17 genomicsdb-5.vcf.gz; -rw-r--r-- 2 root root 746 Jan 14 20:17 genomicsdb-5.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 20:05 genomicsdb-6; -rw-r--r-- 3 root root 673K Jan 14 20:31 genomicsdb-6.vcf.gz; -rw-r--r-- 2 root root 13K Jan 14 20:31 genomicsdb-6.vcf.gz.tbi; drwxr-xr-x 2 root root 4.0K Jan 14 20:32 glob-330eecb06b4c0ad6b45febf0c8001b04; -rw-r--r--",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4546
https://github.com/broadinstitute/cromwell/pull/4547:14,Testability,test,tests,14,13 fewer CTKS tests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4547
https://github.com/broadinstitute/cromwell/pull/4548:17,Integrability,message,messages,17,Adds much better messages for debugging cyclic dependencies. - Closes #3885 ; - Closes #3143,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4548
https://github.com/broadinstitute/cromwell/pull/4548:47,Integrability,depend,dependencies,47,Adds much better messages for debugging cyclic dependencies. - Closes #3885 ; - Closes #3143,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4548
https://github.com/broadinstitute/cromwell/issues/4549:598,Availability,alive,alive,598,"Cromwell: 36. I had the following config file, missing a brace:. ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ; }; }; }; ```. When I ran `java -Dconfig.file=$(realpath spartan.conf) -jar cromwell-36.jar`, the error it printed was:. ```; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(Con",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:775,Availability,error,error,775,"Cromwell: 36. I had the following config file, missing a brace:. ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ; }; }; }; ```. When I ran `java -Dconfig.file=$(realpath spartan.conf) -jar cromwell-36.jar`, the error it printed was:. ```; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(Con",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:1305,Energy Efficiency,adapt,adapted,1305,"emory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ; }; }; }; ```. When I ran `java -Dconfig.file=$(realpath spartan.conf) -jar cromwell-36.jar`, the error it printed was:. ```; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseConte",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:531,Integrability,wrap,wrap,531,"Cromwell: 36. I had the following config file, missing a brace:. ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ; }; }; }; ```. When I ran `java -Dconfig.file=$(realpath spartan.conf) -jar cromwell-36.jar`, the error it printed was:. ```; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(Con",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:34,Modifiability,config,config,34,"Cromwell: 36. I had the following config file, missing a brace:. ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ; }; }; }; ```. When I ran `java -Dconfig.file=$(realpath spartan.conf) -jar cromwell-36.jar`, the error it printed was:. ```; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(Con",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:167,Modifiability,config,config,167,"Cromwell: 36. I had the following config file, missing a brace:. ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ; }; }; }; ```. When I ran `java -Dconfig.file=$(realpath spartan.conf) -jar cromwell-36.jar`, the error it printed was:. ```; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(Con",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:174,Modifiability,Config,ConfigBackendLifecycleActorFactory,174,"Cromwell: 36. I had the following config file, missing a brace:. ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ; }; }; }; ```. When I ran `java -Dconfig.file=$(realpath spartan.conf) -jar cromwell-36.jar`, the error it printed was:. ```; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(Con",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:211,Modifiability,config,config,211,"Cromwell: 36. I had the following config file, missing a brace:. ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ; }; }; }; ```. When I ran `java -Dconfig.file=$(realpath spartan.conf) -jar cromwell-36.jar`, the error it printed was:. ```; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(Con",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:1305,Modifiability,adapt,adapted,1305,"emory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ; }; }; }; ```. When I ran `java -Dconfig.file=$(realpath spartan.conf) -jar cromwell-36.jar`, the error it printed was:. ```; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseConte",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:1585,Modifiability,config,config,1585,"}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ; }; }; }; ```. When I ran `java -Dconfig.file=$(realpath spartan.conf) -jar cromwell-36.jar`, the error it printed was:. ```; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:1592,Modifiability,Config,ConfigException,1592,"job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ; }; }; }; ```. When I ran `java -Dconfig.file=$(realpath spartan.conf) -jar cromwell-36.jar`, the error it printed was:. ```; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:1731,Modifiability,config,config,1731,"spartan.conf) -jar cromwell-36.jar`, the error it printed was:. ```; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:1743,Modifiability,Config,ConfigDocumentParser,1743,"l-36.jar`, the error it printed was:. ```; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:1788,Modifiability,Config,ConfigDocumentParser,1788,"as:. ```; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseab",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:1837,Modifiability,config,config,1837,"ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.types",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:1849,Modifiability,Config,ConfigDocumentParser,1849,"r; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:1894,Modifiability,Config,ConfigDocumentParser,1894,"romwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:1943,Modifiability,config,config,1943,"mwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(Confi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:1955,Modifiability,Config,ConfigDocumentParser,1955,"cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:1998,Modifiability,Config,ConfigDocumentParser,1998,"pp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfig",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2047,Modifiability,config,config,2047,"Init$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2059,Modifiability,Config,ConfigDocumentParser,2059,"p.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2105,Modifiability,Config,ConfigDocumentParser,2105,"pply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.Conf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2154,Modifiability,config,config,2154,"ion0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2166,Modifiability,Config,ConfigDocumentParser,2166,"n0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.co",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2211,Modifiability,Config,ConfigDocumentParser,2211,"bstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigF",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2260,Modifiability,config,config,2260,".scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.Config",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2272,Modifiability,Config,ConfigDocumentParser,2272,"$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFact",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2318,Modifiability,Config,ConfigDocumentParser,2318,":76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.co",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2367,Modifiability,config,config,2367,"h(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2379,Modifiability,Config,ConfigDocumentParser,2379,"la.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2419,Modifiability,Config,ConfigDocumentParser,2419," scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.Confi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2468,Modifiability,config,config,2468,"CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2480,Modifiability,Config,ConfigDocumentParser,2480,"romwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2507,Modifiability,Config,ConfigDocumentParser,2507," cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.loa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2555,Modifiability,config,config,2555,"); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2629,Modifiability,config,config,2629,"m0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEn",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2703,Modifiability,config,config,2703,f file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.Cro,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2774,Modifiability,config,config,2774,parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.Crom,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2845,Modifiability,config,config,2845,.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.Cromw,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2911,Modifiability,config,config,2911,.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other w,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2918,Modifiability,Config,ConfigFactory,2918, 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it te,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2942,Modifiability,Config,ConfigFactory,2942,onfig.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:2984,Modifiability,config,config,2984,"t.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3099,Modifiability,config,config,3099,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3106,Modifiability,Config,ConfigFactory,3106,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3139,Modifiability,Config,ConfigFactory,3139,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3181,Modifiability,config,config,3181,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3188,Modifiability,Config,ConfigFactory,3188,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3209,Modifiability,Config,ConfigFactory,3209,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3251,Modifiability,config,config,3251,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3258,Modifiability,Config,ConfigFactory,3258,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3279,Modifiability,Config,ConfigFactory,3279,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3321,Modifiability,config,config,3321,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3333,Modifiability,Config,ConfigImpl,3333,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3372,Modifiability,Config,ConfigImpl,3372,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3410,Modifiability,config,config,3410,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3422,Modifiability,Config,ConfigImpl,3422,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3453,Modifiability,Config,ConfigImpl,3453,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3491,Modifiability,config,config,3491,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3498,Modifiability,Config,ConfigFactory,3498,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3517,Modifiability,Config,ConfigFactory,3517,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3559,Modifiability,config,config,3559,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3566,Modifiability,Config,ConfigFactory,3566,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3585,Modifiability,Config,ConfigFactory,3585,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3643,Modifiability,config,config,3643,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3724,Modifiability,config,config,3724,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:336,Performance,queue,queue,336,"Cromwell: 36. I had the following config file, missing a brace:. ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ; }; }; }; ```. When I ran `java -Dconfig.file=$(realpath spartan.conf) -jar cromwell-36.jar`, the error it printed was:. ```; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(Con",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:451,Performance,queue,queue,451,"Cromwell: 36. I had the following config file, missing a brace:. ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ; }; }; }; ```. When I ran `java -Dconfig.file=$(realpath spartan.conf) -jar cromwell-36.jar`, the error it printed was:. ```; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(Con",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3344,Performance,Load,LoaderCache,3344,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3512,Performance,load,load,3512,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4549:3580,Performance,load,load,3580,"igDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missing a brace `}`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549
https://github.com/broadinstitute/cromwell/issues/4550:233,Availability,error,error,233,"I ran into the following problem when upgrading from cromwell 31.1 to 36. ; It looks like in cromwell 31.1, it is possible to pass a single File to a task that expects an Array[File]. In cromwell 36 however, this gives the following error: `Failed to evaluate input 'files' (reason 1 of 1): No coercion defined from wom value(s) '""cromwell-36.jar""' of type 'File' to 'Array[File]'.`; However, both womtool 31.1 and womtool 36 give no errors when validating the workflow. I'm not sure which of the two is the correct behaviour according to the WDL spec, but I think womtool and cromwell should agree on whether or not the wdl file is valid or not. Cromwell Womtool 31. $ java -jar womtool-31.jar validate wf.wdl ; ; $ java -jar cromwell-31.1.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:07:53,81] [info] Running with database db.url = jdbc:hsqldb:mem:6b74f862-dc18-4cf1-8e2e-0b9002bba0bf;shutdown=false;hsqldb.tx=mvcc; .; .; [2019-01-15 15:08:11,54] [info] WorkflowExecutionActor-977d0c47-9cf5-4893-8dcf-465c27da13d7 [977d0c47]: Workflow wf complete. Final Outputs:; {; ""wf.F"": [[""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-0/inputs/home/redmar/devel/wdl/test/issue/cromwell-31.1.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-1/inputs/home/redmar/devel/wdl/test/issue/cromwell-36.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550
https://github.com/broadinstitute/cromwell/issues/4550:434,Availability,error,errors,434,"I ran into the following problem when upgrading from cromwell 31.1 to 36. ; It looks like in cromwell 31.1, it is possible to pass a single File to a task that expects an Array[File]. In cromwell 36 however, this gives the following error: `Failed to evaluate input 'files' (reason 1 of 1): No coercion defined from wom value(s) '""cromwell-36.jar""' of type 'File' to 'Array[File]'.`; However, both womtool 31.1 and womtool 36 give no errors when validating the workflow. I'm not sure which of the two is the correct behaviour according to the WDL spec, but I think womtool and cromwell should agree on whether or not the wdl file is valid or not. Cromwell Womtool 31. $ java -jar womtool-31.jar validate wf.wdl ; ; $ java -jar cromwell-31.1.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:07:53,81] [info] Running with database db.url = jdbc:hsqldb:mem:6b74f862-dc18-4cf1-8e2e-0b9002bba0bf;shutdown=false;hsqldb.tx=mvcc; .; .; [2019-01-15 15:08:11,54] [info] WorkflowExecutionActor-977d0c47-9cf5-4893-8dcf-465c27da13d7 [977d0c47]: Workflow wf complete. Final Outputs:; {; ""wf.F"": [[""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-0/inputs/home/redmar/devel/wdl/test/issue/cromwell-31.1.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-1/inputs/home/redmar/devel/wdl/test/issue/cromwell-36.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550
https://github.com/broadinstitute/cromwell/issues/4550:2505,Availability,heartbeat,heartbeat,2505,"4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d915b85b25fc;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,59] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-15 15:09:22,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-15 15:09:22,67] [info] Running with database db.url = jdbc:hsqldb:mem:52af65c3-a08f-4d3a-a6bc-c97a3d7e1a3c;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,97] [info] Slf4jLogger started; [2019-01-15 15:09:23,24] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-d961aae"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; .; .; [2019-01-15 15:09:29,85] [error] WorkflowManagerActor Workflow 5bc372e9-61f6-45fd-b178-60ed25529216 failed (during ExecutingWorkflowState): cromwell.engine.workflow.lifecycle.execution.job.preparation.JobPreparationActor$$anonfun$1$$anon$1: Call input and runtime attributes evaluation failed for ls:; Failed to evaluate input 'files' (reason 1 of 1): No coercion defined from wom value(s) '""womtool-31.jar""' of type 'File' to 'Array[File]'.; .; .; Workflow 5bc372e9-61f6-45fd-b178-60ed25529216 transitioned to state Failed. [issue.zip](https://github.com/broadinstitute/cromwell/files/2759990/issue.zip). I've attached the `wdl` and `json` files I've used, the input filenames are the `jar` files of both cromwell and womtool used to run the test workflow. I'm not attaching those because they are very large.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550
https://github.com/broadinstitute/cromwell/issues/4550:2569,Availability,heartbeat,heartbeatInterval,2569,"4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d915b85b25fc;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,59] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-15 15:09:22,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-15 15:09:22,67] [info] Running with database db.url = jdbc:hsqldb:mem:52af65c3-a08f-4d3a-a6bc-c97a3d7e1a3c;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,97] [info] Slf4jLogger started; [2019-01-15 15:09:23,24] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-d961aae"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; .; .; [2019-01-15 15:09:29,85] [error] WorkflowManagerActor Workflow 5bc372e9-61f6-45fd-b178-60ed25529216 failed (during ExecutingWorkflowState): cromwell.engine.workflow.lifecycle.execution.job.preparation.JobPreparationActor$$anonfun$1$$anon$1: Call input and runtime attributes evaluation failed for ls:; Failed to evaluate input 'files' (reason 1 of 1): No coercion defined from wom value(s) '""womtool-31.jar""' of type 'File' to 'Array[File]'.; .; .; Workflow 5bc372e9-61f6-45fd-b178-60ed25529216 transitioned to state Failed. [issue.zip](https://github.com/broadinstitute/cromwell/files/2759990/issue.zip). I've attached the `wdl` and `json` files I've used, the input filenames are the `jar` files of both cromwell and womtool used to run the test workflow. I'm not attaching those because they are very large.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550
https://github.com/broadinstitute/cromwell/issues/4550:2715,Availability,error,error,2715,"4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d915b85b25fc;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,59] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-15 15:09:22,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-15 15:09:22,67] [info] Running with database db.url = jdbc:hsqldb:mem:52af65c3-a08f-4d3a-a6bc-c97a3d7e1a3c;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,97] [info] Slf4jLogger started; [2019-01-15 15:09:23,24] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-d961aae"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; .; .; [2019-01-15 15:09:29,85] [error] WorkflowManagerActor Workflow 5bc372e9-61f6-45fd-b178-60ed25529216 failed (during ExecutingWorkflowState): cromwell.engine.workflow.lifecycle.execution.job.preparation.JobPreparationActor$$anonfun$1$$anon$1: Call input and runtime attributes evaluation failed for ls:; Failed to evaluate input 'files' (reason 1 of 1): No coercion defined from wom value(s) '""womtool-31.jar""' of type 'File' to 'Array[File]'.; .; .; Workflow 5bc372e9-61f6-45fd-b178-60ed25529216 transitioned to state Failed. [issue.zip](https://github.com/broadinstitute/cromwell/files/2759990/issue.zip). I've attached the `wdl` and `json` files I've used, the input filenames are the `jar` files of both cromwell and womtool used to run the test workflow. I'm not attaching those because they are very large.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550
https://github.com/broadinstitute/cromwell/issues/4550:2515,Deployability,configurat,configuration,2515,"4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d915b85b25fc;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,59] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-15 15:09:22,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-15 15:09:22,67] [info] Running with database db.url = jdbc:hsqldb:mem:52af65c3-a08f-4d3a-a6bc-c97a3d7e1a3c;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,97] [info] Slf4jLogger started; [2019-01-15 15:09:23,24] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-d961aae"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; .; .; [2019-01-15 15:09:29,85] [error] WorkflowManagerActor Workflow 5bc372e9-61f6-45fd-b178-60ed25529216 failed (during ExecutingWorkflowState): cromwell.engine.workflow.lifecycle.execution.job.preparation.JobPreparationActor$$anonfun$1$$anon$1: Call input and runtime attributes evaluation failed for ls:; Failed to evaluate input 'files' (reason 1 of 1): No coercion defined from wom value(s) '""womtool-31.jar""' of type 'File' to 'Array[File]'.; .; .; Workflow 5bc372e9-61f6-45fd-b178-60ed25529216 transitioned to state Failed. [issue.zip](https://github.com/broadinstitute/cromwell/files/2759990/issue.zip). I've attached the `wdl` and `json` files I've used, the input filenames are the `jar` files of both cromwell and womtool used to run the test workflow. I'm not attaching those because they are very large.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550
https://github.com/broadinstitute/cromwell/issues/4550:2515,Modifiability,config,configuration,2515,"4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d915b85b25fc;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,59] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-15 15:09:22,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-15 15:09:22,67] [info] Running with database db.url = jdbc:hsqldb:mem:52af65c3-a08f-4d3a-a6bc-c97a3d7e1a3c;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,97] [info] Slf4jLogger started; [2019-01-15 15:09:23,24] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-d961aae"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; .; .; [2019-01-15 15:09:29,85] [error] WorkflowManagerActor Workflow 5bc372e9-61f6-45fd-b178-60ed25529216 failed (during ExecutingWorkflowState): cromwell.engine.workflow.lifecycle.execution.job.preparation.JobPreparationActor$$anonfun$1$$anon$1: Call input and runtime attributes evaluation failed for ls:; Failed to evaluate input 'files' (reason 1 of 1): No coercion defined from wom value(s) '""womtool-31.jar""' of type 'File' to 'Array[File]'.; .; .; Workflow 5bc372e9-61f6-45fd-b178-60ed25529216 transitioned to state Failed. [issue.zip](https://github.com/broadinstitute/cromwell/files/2759990/issue.zip). I've attached the `wdl` and `json` files I've used, the input filenames are the `jar` files of both cromwell and womtool used to run the test workflow. I'm not attaching those because they are very large.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550
https://github.com/broadinstitute/cromwell/issues/4550:446,Security,validat,validating,446,"I ran into the following problem when upgrading from cromwell 31.1 to 36. ; It looks like in cromwell 31.1, it is possible to pass a single File to a task that expects an Array[File]. In cromwell 36 however, this gives the following error: `Failed to evaluate input 'files' (reason 1 of 1): No coercion defined from wom value(s) '""cromwell-36.jar""' of type 'File' to 'Array[File]'.`; However, both womtool 31.1 and womtool 36 give no errors when validating the workflow. I'm not sure which of the two is the correct behaviour according to the WDL spec, but I think womtool and cromwell should agree on whether or not the wdl file is valid or not. Cromwell Womtool 31. $ java -jar womtool-31.jar validate wf.wdl ; ; $ java -jar cromwell-31.1.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:07:53,81] [info] Running with database db.url = jdbc:hsqldb:mem:6b74f862-dc18-4cf1-8e2e-0b9002bba0bf;shutdown=false;hsqldb.tx=mvcc; .; .; [2019-01-15 15:08:11,54] [info] WorkflowExecutionActor-977d0c47-9cf5-4893-8dcf-465c27da13d7 [977d0c47]: Workflow wf complete. Final Outputs:; {; ""wf.F"": [[""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-0/inputs/home/redmar/devel/wdl/test/issue/cromwell-31.1.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-1/inputs/home/redmar/devel/wdl/test/issue/cromwell-36.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550
https://github.com/broadinstitute/cromwell/issues/4550:695,Security,validat,validate,695,"I ran into the following problem when upgrading from cromwell 31.1 to 36. ; It looks like in cromwell 31.1, it is possible to pass a single File to a task that expects an Array[File]. In cromwell 36 however, this gives the following error: `Failed to evaluate input 'files' (reason 1 of 1): No coercion defined from wom value(s) '""cromwell-36.jar""' of type 'File' to 'Array[File]'.`; However, both womtool 31.1 and womtool 36 give no errors when validating the workflow. I'm not sure which of the two is the correct behaviour according to the WDL spec, but I think womtool and cromwell should agree on whether or not the wdl file is valid or not. Cromwell Womtool 31. $ java -jar womtool-31.jar validate wf.wdl ; ; $ java -jar cromwell-31.1.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:07:53,81] [info] Running with database db.url = jdbc:hsqldb:mem:6b74f862-dc18-4cf1-8e2e-0b9002bba0bf;shutdown=false;hsqldb.tx=mvcc; .; .; [2019-01-15 15:08:11,54] [info] WorkflowExecutionActor-977d0c47-9cf5-4893-8dcf-465c27da13d7 [977d0c47]: Workflow wf complete. Final Outputs:; {; ""wf.F"": [[""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-0/inputs/home/redmar/devel/wdl/test/issue/cromwell-31.1.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-1/inputs/home/redmar/devel/wdl/test/issue/cromwell-36.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550
https://github.com/broadinstitute/cromwell/issues/4550:1818,Security,validat,validate,1818,"tabase db.url = jdbc:hsqldb:mem:6b74f862-dc18-4cf1-8e2e-0b9002bba0bf;shutdown=false;hsqldb.tx=mvcc; .; .; [2019-01-15 15:08:11,54] [info] WorkflowExecutionActor-977d0c47-9cf5-4893-8dcf-465c27da13d7 [977d0c47]: Workflow wf complete. Final Outputs:; {; ""wf.F"": [[""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-0/inputs/home/redmar/devel/wdl/test/issue/cromwell-31.1.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-1/inputs/home/redmar/devel/wdl/test/issue/cromwell-36.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d915b85b25fc;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,59] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-15 15:09:22,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-15 15:09:22,67] [info] Running with database db.url = jdbc:hsqldb:mem:52af65c3-a08f-4d3a-a6bc-c97a3d7e1a3c;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,97] [info] Slf4jLogger started; [2019-01-15 15:09:23,24] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-d961aae"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; .; .; [2019-01-15 15:09:29,85] [error] WorkflowManagerActor Workflow 5bc372e9-61f6-45fd-b178-60ed25529216 failed (during ExecutingWorkflowS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550
https://github.com/broadinstitute/cromwell/issues/4550:1107,Testability,test,test,1107,"xpects an Array[File]. In cromwell 36 however, this gives the following error: `Failed to evaluate input 'files' (reason 1 of 1): No coercion defined from wom value(s) '""cromwell-36.jar""' of type 'File' to 'Array[File]'.`; However, both womtool 31.1 and womtool 36 give no errors when validating the workflow. I'm not sure which of the two is the correct behaviour according to the WDL spec, but I think womtool and cromwell should agree on whether or not the wdl file is valid or not. Cromwell Womtool 31. $ java -jar womtool-31.jar validate wf.wdl ; ; $ java -jar cromwell-31.1.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:07:53,81] [info] Running with database db.url = jdbc:hsqldb:mem:6b74f862-dc18-4cf1-8e2e-0b9002bba0bf;shutdown=false;hsqldb.tx=mvcc; .; .; [2019-01-15 15:08:11,54] [info] WorkflowExecutionActor-977d0c47-9cf5-4893-8dcf-465c27da13d7 [977d0c47]: Workflow wf complete. Final Outputs:; {; ""wf.F"": [[""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-0/inputs/home/redmar/devel/wdl/test/issue/cromwell-31.1.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-1/inputs/home/redmar/devel/wdl/test/issue/cromwell-36.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d915b85b25fc;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,59] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550
https://github.com/broadinstitute/cromwell/issues/4550:1223,Testability,test,test,1223,"xpects an Array[File]. In cromwell 36 however, this gives the following error: `Failed to evaluate input 'files' (reason 1 of 1): No coercion defined from wom value(s) '""cromwell-36.jar""' of type 'File' to 'Array[File]'.`; However, both womtool 31.1 and womtool 36 give no errors when validating the workflow. I'm not sure which of the two is the correct behaviour according to the WDL spec, but I think womtool and cromwell should agree on whether or not the wdl file is valid or not. Cromwell Womtool 31. $ java -jar womtool-31.jar validate wf.wdl ; ; $ java -jar cromwell-31.1.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:07:53,81] [info] Running with database db.url = jdbc:hsqldb:mem:6b74f862-dc18-4cf1-8e2e-0b9002bba0bf;shutdown=false;hsqldb.tx=mvcc; .; .; [2019-01-15 15:08:11,54] [info] WorkflowExecutionActor-977d0c47-9cf5-4893-8dcf-465c27da13d7 [977d0c47]: Workflow wf complete. Final Outputs:; {; ""wf.F"": [[""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-0/inputs/home/redmar/devel/wdl/test/issue/cromwell-31.1.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-1/inputs/home/redmar/devel/wdl/test/issue/cromwell-36.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d915b85b25fc;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,59] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550
https://github.com/broadinstitute/cromwell/issues/4550:1280,Testability,test,test,1280,"omwell-36.jar""' of type 'File' to 'Array[File]'.`; However, both womtool 31.1 and womtool 36 give no errors when validating the workflow. I'm not sure which of the two is the correct behaviour according to the WDL spec, but I think womtool and cromwell should agree on whether or not the wdl file is valid or not. Cromwell Womtool 31. $ java -jar womtool-31.jar validate wf.wdl ; ; $ java -jar cromwell-31.1.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:07:53,81] [info] Running with database db.url = jdbc:hsqldb:mem:6b74f862-dc18-4cf1-8e2e-0b9002bba0bf;shutdown=false;hsqldb.tx=mvcc; .; .; [2019-01-15 15:08:11,54] [info] WorkflowExecutionActor-977d0c47-9cf5-4893-8dcf-465c27da13d7 [977d0c47]: Workflow wf complete. Final Outputs:; {; ""wf.F"": [[""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-0/inputs/home/redmar/devel/wdl/test/issue/cromwell-31.1.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-1/inputs/home/redmar/devel/wdl/test/issue/cromwell-36.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d915b85b25fc;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,59] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-15 15:09:22,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-15 15:09:22,67] [info] Running with database db.url = jdbc:hs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550
https://github.com/broadinstitute/cromwell/issues/4550:1396,Testability,test,test,1396,"omwell-36.jar""' of type 'File' to 'Array[File]'.`; However, both womtool 31.1 and womtool 36 give no errors when validating the workflow. I'm not sure which of the two is the correct behaviour according to the WDL spec, but I think womtool and cromwell should agree on whether or not the wdl file is valid or not. Cromwell Womtool 31. $ java -jar womtool-31.jar validate wf.wdl ; ; $ java -jar cromwell-31.1.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:07:53,81] [info] Running with database db.url = jdbc:hsqldb:mem:6b74f862-dc18-4cf1-8e2e-0b9002bba0bf;shutdown=false;hsqldb.tx=mvcc; .; .; [2019-01-15 15:08:11,54] [info] WorkflowExecutionActor-977d0c47-9cf5-4893-8dcf-465c27da13d7 [977d0c47]: Workflow wf complete. Final Outputs:; {; ""wf.F"": [[""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-0/inputs/home/redmar/devel/wdl/test/issue/cromwell-31.1.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-1/inputs/home/redmar/devel/wdl/test/issue/cromwell-36.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d915b85b25fc;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,59] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-15 15:09:22,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-15 15:09:22,67] [info] Running with database db.url = jdbc:hs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550
https://github.com/broadinstitute/cromwell/issues/4550:1451,Testability,test,test,1451,"the correct behaviour according to the WDL spec, but I think womtool and cromwell should agree on whether or not the wdl file is valid or not. Cromwell Womtool 31. $ java -jar womtool-31.jar validate wf.wdl ; ; $ java -jar cromwell-31.1.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:07:53,81] [info] Running with database db.url = jdbc:hsqldb:mem:6b74f862-dc18-4cf1-8e2e-0b9002bba0bf;shutdown=false;hsqldb.tx=mvcc; .; .; [2019-01-15 15:08:11,54] [info] WorkflowExecutionActor-977d0c47-9cf5-4893-8dcf-465c27da13d7 [977d0c47]: Workflow wf complete. Final Outputs:; {; ""wf.F"": [[""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-0/inputs/home/redmar/devel/wdl/test/issue/cromwell-31.1.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-1/inputs/home/redmar/devel/wdl/test/issue/cromwell-36.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d915b85b25fc;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,59] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-15 15:09:22,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-15 15:09:22,67] [info] Running with database db.url = jdbc:hsqldb:mem:52af65c3-a08f-4d3a-a6bc-c97a3d7e1a3c;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,97] [info] Slf4jLogger started; [2019-01-15 15:09:23,24] [info] Workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550
https://github.com/broadinstitute/cromwell/issues/4550:1567,Testability,test,test,1567,"the correct behaviour according to the WDL spec, but I think womtool and cromwell should agree on whether or not the wdl file is valid or not. Cromwell Womtool 31. $ java -jar womtool-31.jar validate wf.wdl ; ; $ java -jar cromwell-31.1.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:07:53,81] [info] Running with database db.url = jdbc:hsqldb:mem:6b74f862-dc18-4cf1-8e2e-0b9002bba0bf;shutdown=false;hsqldb.tx=mvcc; .; .; [2019-01-15 15:08:11,54] [info] WorkflowExecutionActor-977d0c47-9cf5-4893-8dcf-465c27da13d7 [977d0c47]: Workflow wf complete. Final Outputs:; {; ""wf.F"": [[""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-0/inputs/home/redmar/devel/wdl/test/issue/cromwell-31.1.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-1/inputs/home/redmar/devel/wdl/test/issue/cromwell-36.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d915b85b25fc;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,59] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-15 15:09:22,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-15 15:09:22,67] [info] Running with database db.url = jdbc:hsqldb:mem:52af65c3-a08f-4d3a-a6bc-c97a3d7e1a3c;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,97] [info] Slf4jLogger started; [2019-01-15 15:09:23,24] [info] Workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550
https://github.com/broadinstitute/cromwell/issues/4550:1621,Testability,test,test,1621," -jar womtool-31.jar validate wf.wdl ; ; $ java -jar cromwell-31.1.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:07:53,81] [info] Running with database db.url = jdbc:hsqldb:mem:6b74f862-dc18-4cf1-8e2e-0b9002bba0bf;shutdown=false;hsqldb.tx=mvcc; .; .; [2019-01-15 15:08:11,54] [info] WorkflowExecutionActor-977d0c47-9cf5-4893-8dcf-465c27da13d7 [977d0c47]: Workflow wf complete. Final Outputs:; {; ""wf.F"": [[""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-0/inputs/home/redmar/devel/wdl/test/issue/cromwell-31.1.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-1/inputs/home/redmar/devel/wdl/test/issue/cromwell-36.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d915b85b25fc;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,59] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-15 15:09:22,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-15 15:09:22,67] [info] Running with database db.url = jdbc:hsqldb:mem:52af65c3-a08f-4d3a-a6bc-c97a3d7e1a3c;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,97] [info] Slf4jLogger started; [2019-01-15 15:09:23,24] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-d961aae"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550
https://github.com/broadinstitute/cromwell/issues/4550:1737,Testability,test,test,1737," -jar womtool-31.jar validate wf.wdl ; ; $ java -jar cromwell-31.1.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:07:53,81] [info] Running with database db.url = jdbc:hsqldb:mem:6b74f862-dc18-4cf1-8e2e-0b9002bba0bf;shutdown=false;hsqldb.tx=mvcc; .; .; [2019-01-15 15:08:11,54] [info] WorkflowExecutionActor-977d0c47-9cf5-4893-8dcf-465c27da13d7 [977d0c47]: Workflow wf complete. Final Outputs:; {; ""wf.F"": [[""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-0/inputs/home/redmar/devel/wdl/test/issue/cromwell-31.1.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-1/inputs/home/redmar/devel/wdl/test/issue/cromwell-36.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d915b85b25fc;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,59] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-15 15:09:22,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-15 15:09:22,67] [info] Running with database db.url = jdbc:hsqldb:mem:52af65c3-a08f-4d3a-a6bc-c97a3d7e1a3c;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,97] [info] Slf4jLogger started; [2019-01-15 15:09:23,24] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-d961aae"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550
https://github.com/broadinstitute/cromwell/issues/4550:3432,Testability,test,test,3432,"4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d915b85b25fc;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,59] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-15 15:09:22,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-15 15:09:22,67] [info] Running with database db.url = jdbc:hsqldb:mem:52af65c3-a08f-4d3a-a6bc-c97a3d7e1a3c;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,97] [info] Slf4jLogger started; [2019-01-15 15:09:23,24] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-d961aae"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; .; .; [2019-01-15 15:09:29,85] [error] WorkflowManagerActor Workflow 5bc372e9-61f6-45fd-b178-60ed25529216 failed (during ExecutingWorkflowState): cromwell.engine.workflow.lifecycle.execution.job.preparation.JobPreparationActor$$anonfun$1$$anon$1: Call input and runtime attributes evaluation failed for ls:; Failed to evaluate input 'files' (reason 1 of 1): No coercion defined from wom value(s) '""womtool-31.jar""' of type 'File' to 'Array[File]'.; .; .; Workflow 5bc372e9-61f6-45fd-b178-60ed25529216 transitioned to state Failed. [issue.zip](https://github.com/broadinstitute/cromwell/files/2759990/issue.zip). I've attached the `wdl` and `json` files I've used, the input filenames are the `jar` files of both cromwell and womtool used to run the test workflow. I'm not attaching those because they are very large.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550
https://github.com/broadinstitute/cromwell/issues/4553:319,Availability,ERROR,ERROR,319,"Something appears to be wrong in the credential building for PAPI v2 private Docker as turned up in @marctalbott's testing. It's not clear why this is not replicated by the `docker_hash_dockerhub_private_wf_options` Centaur test but from reading the code it does appear to be a real issue. ```; 2019-01-14 20:20:22,530 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; common.exception.AggregatedException: :; null; 	Unrecognized token 'user_service_account_json': was expecting ('true', 'false' or 'null'); at [Source: (ByteArrayInputStream); line: 1, column: 51]; 	at common.util.TryUtil$.sequenceIterable(TryUtil.scala:29); 	at common.util.TryUtil$.sequenceMap(TryUtil.scala:47); 	at cromwell.engine.backend.CromwellBackends.<init>(CromwellBackends.scala:14); 	at cromwell.engine.backend.CromwellBackends$.initBackends(CromwellBackends.scala:42); 	at cromwell.server.CromwellSystem.$init$(CromwellSystem.scala:62); 	at cromwell.CromwellEntryPoint$$anon$2.<init>(CromwellEntryPoint.scala:96); 	at cromwell.CromwellEntryPoint$.$anonfun$buildCromwellSystem$1(CromwellEntryPoint.scala:96); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:96); 	at cromwell.CromwellEntryPoint$.runServer(CromwellEntryPoint.scala:50); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:15); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/issues/4553:375,Availability,down,down,375,"Something appears to be wrong in the credential building for PAPI v2 private Docker as turned up in @marctalbott's testing. It's not clear why this is not replicated by the `docker_hash_dockerhub_private_wf_options` Centaur test but from reading the code it does appear to be a real issue. ```; 2019-01-14 20:20:22,530 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; common.exception.AggregatedException: :; null; 	Unrecognized token 'user_service_account_json': was expecting ('true', 'false' or 'null'); at [Source: (ByteArrayInputStream); line: 1, column: 51]; 	at common.util.TryUtil$.sequenceIterable(TryUtil.scala:29); 	at common.util.TryUtil$.sequenceMap(TryUtil.scala:47); 	at cromwell.engine.backend.CromwellBackends.<init>(CromwellBackends.scala:14); 	at cromwell.engine.backend.CromwellBackends$.initBackends(CromwellBackends.scala:42); 	at cromwell.server.CromwellSystem.$init$(CromwellSystem.scala:62); 	at cromwell.CromwellEntryPoint$$anon$2.<init>(CromwellEntryPoint.scala:96); 	at cromwell.CromwellEntryPoint$.$anonfun$buildCromwellSystem$1(CromwellEntryPoint.scala:96); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:96); 	at cromwell.CromwellEntryPoint$.runServer(CromwellEntryPoint.scala:50); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:15); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/issues/4553:4828,Deployability,pipeline,pipelines,4828,tInObject(UTF8StreamJsonParser.java:826); 		at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:723); 		at com.google.api.client.json.jackson2.JacksonParser.nextToken(JacksonParser.java:55); 		at com.google.api.client.json.JsonParser.startParsing(JsonParser.java:221); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:380); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:355); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:87); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:81); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:250); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:226); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential$(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.apiClientGoogleCredential(GoogleAuthMode.scala:237); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.$anonfun$apply$1(PipelinesApiVMAuthentication.scala:41); 		at scala.Option.flatMap(Option.scala:171); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.apply(PipelinesApiVMAuthentication.scala:37); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.$anonfun$dockerCredentials$1(PipelinesApiConfiguration.scala:24); 		at scala.Option.map(Option.scala:146); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.<init>(PipelinesApiConfiguration.scala:23); 		at cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:12); 		... 29 common frames omitted; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/issues/4553:4860,Deployability,Pipeline,PipelinesApiDockerCredentials,4860,tInObject(UTF8StreamJsonParser.java:826); 		at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:723); 		at com.google.api.client.json.jackson2.JacksonParser.nextToken(JacksonParser.java:55); 		at com.google.api.client.json.JsonParser.startParsing(JsonParser.java:221); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:380); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:355); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:87); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:81); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:250); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:226); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential$(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.apiClientGoogleCredential(GoogleAuthMode.scala:237); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.$anonfun$apply$1(PipelinesApiVMAuthentication.scala:41); 		at scala.Option.flatMap(Option.scala:171); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.apply(PipelinesApiVMAuthentication.scala:37); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.$anonfun$dockerCredentials$1(PipelinesApiConfiguration.scala:24); 		at scala.Option.map(Option.scala:146); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.<init>(PipelinesApiConfiguration.scala:23); 		at cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:12); 		... 29 common frames omitted; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/issues/4553:4908,Deployability,Pipeline,PipelinesApiVMAuthentication,4908,tInObject(UTF8StreamJsonParser.java:826); 		at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:723); 		at com.google.api.client.json.jackson2.JacksonParser.nextToken(JacksonParser.java:55); 		at com.google.api.client.json.JsonParser.startParsing(JsonParser.java:221); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:380); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:355); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:87); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:81); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:250); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:226); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential$(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.apiClientGoogleCredential(GoogleAuthMode.scala:237); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.$anonfun$apply$1(PipelinesApiVMAuthentication.scala:41); 		at scala.Option.flatMap(Option.scala:171); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.apply(PipelinesApiVMAuthentication.scala:37); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.$anonfun$dockerCredentials$1(PipelinesApiConfiguration.scala:24); 		at scala.Option.map(Option.scala:146); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.<init>(PipelinesApiConfiguration.scala:23); 		at cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:12); 		... 29 common frames omitted; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/issues/4553:5022,Deployability,pipeline,pipelines,5022,tInObject(UTF8StreamJsonParser.java:826); 		at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:723); 		at com.google.api.client.json.jackson2.JacksonParser.nextToken(JacksonParser.java:55); 		at com.google.api.client.json.JsonParser.startParsing(JsonParser.java:221); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:380); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:355); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:87); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:81); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:250); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:226); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential$(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.apiClientGoogleCredential(GoogleAuthMode.scala:237); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.$anonfun$apply$1(PipelinesApiVMAuthentication.scala:41); 		at scala.Option.flatMap(Option.scala:171); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.apply(PipelinesApiVMAuthentication.scala:37); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.$anonfun$dockerCredentials$1(PipelinesApiConfiguration.scala:24); 		at scala.Option.map(Option.scala:146); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.<init>(PipelinesApiConfiguration.scala:23); 		at cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:12); 		... 29 common frames omitted; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/issues/4553:5054,Deployability,Pipeline,PipelinesApiDockerCredentials,5054,tInObject(UTF8StreamJsonParser.java:826); 		at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:723); 		at com.google.api.client.json.jackson2.JacksonParser.nextToken(JacksonParser.java:55); 		at com.google.api.client.json.JsonParser.startParsing(JsonParser.java:221); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:380); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:355); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:87); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:81); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:250); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:226); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential$(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.apiClientGoogleCredential(GoogleAuthMode.scala:237); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.$anonfun$apply$1(PipelinesApiVMAuthentication.scala:41); 		at scala.Option.flatMap(Option.scala:171); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.apply(PipelinesApiVMAuthentication.scala:37); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.$anonfun$dockerCredentials$1(PipelinesApiConfiguration.scala:24); 		at scala.Option.map(Option.scala:146); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.<init>(PipelinesApiConfiguration.scala:23); 		at cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:12); 		... 29 common frames omitted; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/issues/4553:5091,Deployability,Pipeline,PipelinesApiVMAuthentication,5091,tInObject(UTF8StreamJsonParser.java:826); 		at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:723); 		at com.google.api.client.json.jackson2.JacksonParser.nextToken(JacksonParser.java:55); 		at com.google.api.client.json.JsonParser.startParsing(JsonParser.java:221); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:380); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:355); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:87); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:81); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:250); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:226); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential$(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.apiClientGoogleCredential(GoogleAuthMode.scala:237); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.$anonfun$apply$1(PipelinesApiVMAuthentication.scala:41); 		at scala.Option.flatMap(Option.scala:171); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.apply(PipelinesApiVMAuthentication.scala:37); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.$anonfun$dockerCredentials$1(PipelinesApiConfiguration.scala:24); 		at scala.Option.map(Option.scala:146); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.<init>(PipelinesApiConfiguration.scala:23); 		at cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:12); 		... 29 common frames omitted; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/issues/4553:5160,Deployability,pipeline,pipelines,5160,tInObject(UTF8StreamJsonParser.java:826); 		at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:723); 		at com.google.api.client.json.jackson2.JacksonParser.nextToken(JacksonParser.java:55); 		at com.google.api.client.json.JsonParser.startParsing(JsonParser.java:221); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:380); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:355); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:87); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:81); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:250); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:226); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential$(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.apiClientGoogleCredential(GoogleAuthMode.scala:237); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.$anonfun$apply$1(PipelinesApiVMAuthentication.scala:41); 		at scala.Option.flatMap(Option.scala:171); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.apply(PipelinesApiVMAuthentication.scala:37); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.$anonfun$dockerCredentials$1(PipelinesApiConfiguration.scala:24); 		at scala.Option.map(Option.scala:146); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.<init>(PipelinesApiConfiguration.scala:23); 		at cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:12); 		... 29 common frames omitted; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/issues/4553:5177,Deployability,Pipeline,PipelinesApiConfiguration,5177,tInObject(UTF8StreamJsonParser.java:826); 		at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:723); 		at com.google.api.client.json.jackson2.JacksonParser.nextToken(JacksonParser.java:55); 		at com.google.api.client.json.JsonParser.startParsing(JsonParser.java:221); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:380); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:355); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:87); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:81); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:250); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:226); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential$(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.apiClientGoogleCredential(GoogleAuthMode.scala:237); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.$anonfun$apply$1(PipelinesApiVMAuthentication.scala:41); 		at scala.Option.flatMap(Option.scala:171); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.apply(PipelinesApiVMAuthentication.scala:37); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.$anonfun$dockerCredentials$1(PipelinesApiConfiguration.scala:24); 		at scala.Option.map(Option.scala:146); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.<init>(PipelinesApiConfiguration.scala:23); 		at cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:12); 		... 29 common frames omitted; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/issues/4553:5232,Deployability,Pipeline,PipelinesApiConfiguration,5232,tInObject(UTF8StreamJsonParser.java:826); 		at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:723); 		at com.google.api.client.json.jackson2.JacksonParser.nextToken(JacksonParser.java:55); 		at com.google.api.client.json.JsonParser.startParsing(JsonParser.java:221); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:380); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:355); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:87); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:81); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:250); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:226); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential$(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.apiClientGoogleCredential(GoogleAuthMode.scala:237); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.$anonfun$apply$1(PipelinesApiVMAuthentication.scala:41); 		at scala.Option.flatMap(Option.scala:171); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.apply(PipelinesApiVMAuthentication.scala:37); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.$anonfun$dockerCredentials$1(PipelinesApiConfiguration.scala:24); 		at scala.Option.map(Option.scala:146); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.<init>(PipelinesApiConfiguration.scala:23); 		at cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:12); 		... 29 common frames omitted; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/issues/4553:5339,Deployability,pipeline,pipelines,5339,tInObject(UTF8StreamJsonParser.java:826); 		at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:723); 		at com.google.api.client.json.jackson2.JacksonParser.nextToken(JacksonParser.java:55); 		at com.google.api.client.json.JsonParser.startParsing(JsonParser.java:221); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:380); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:355); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:87); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:81); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:250); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:226); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential$(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.apiClientGoogleCredential(GoogleAuthMode.scala:237); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.$anonfun$apply$1(PipelinesApiVMAuthentication.scala:41); 		at scala.Option.flatMap(Option.scala:171); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.apply(PipelinesApiVMAuthentication.scala:37); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.$anonfun$dockerCredentials$1(PipelinesApiConfiguration.scala:24); 		at scala.Option.map(Option.scala:146); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.<init>(PipelinesApiConfiguration.scala:23); 		at cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:12); 		... 29 common frames omitted; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/issues/4553:5356,Deployability,Pipeline,PipelinesApiConfiguration,5356,tInObject(UTF8StreamJsonParser.java:826); 		at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:723); 		at com.google.api.client.json.jackson2.JacksonParser.nextToken(JacksonParser.java:55); 		at com.google.api.client.json.JsonParser.startParsing(JsonParser.java:221); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:380); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:355); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:87); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:81); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:250); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:226); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential$(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.apiClientGoogleCredential(GoogleAuthMode.scala:237); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.$anonfun$apply$1(PipelinesApiVMAuthentication.scala:41); 		at scala.Option.flatMap(Option.scala:171); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.apply(PipelinesApiVMAuthentication.scala:37); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.$anonfun$dockerCredentials$1(PipelinesApiConfiguration.scala:24); 		at scala.Option.map(Option.scala:146); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.<init>(PipelinesApiConfiguration.scala:23); 		at cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:12); 		... 29 common frames omitted; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/issues/4553:5389,Deployability,Pipeline,PipelinesApiConfiguration,5389,tInObject(UTF8StreamJsonParser.java:826); 		at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:723); 		at com.google.api.client.json.jackson2.JacksonParser.nextToken(JacksonParser.java:55); 		at com.google.api.client.json.JsonParser.startParsing(JsonParser.java:221); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:380); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:355); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:87); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:81); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:250); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:226); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential$(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.apiClientGoogleCredential(GoogleAuthMode.scala:237); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.$anonfun$apply$1(PipelinesApiVMAuthentication.scala:41); 		at scala.Option.flatMap(Option.scala:171); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.apply(PipelinesApiVMAuthentication.scala:37); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.$anonfun$dockerCredentials$1(PipelinesApiConfiguration.scala:24); 		at scala.Option.map(Option.scala:146); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.<init>(PipelinesApiConfiguration.scala:23); 		at cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:12); 		... 29 common frames omitted; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/issues/4553:5455,Deployability,pipeline,pipelines,5455,tInObject(UTF8StreamJsonParser.java:826); 		at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:723); 		at com.google.api.client.json.jackson2.JacksonParser.nextToken(JacksonParser.java:55); 		at com.google.api.client.json.JsonParser.startParsing(JsonParser.java:221); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:380); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:355); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:87); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:81); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:250); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:226); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential$(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.apiClientGoogleCredential(GoogleAuthMode.scala:237); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.$anonfun$apply$1(PipelinesApiVMAuthentication.scala:41); 		at scala.Option.flatMap(Option.scala:171); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.apply(PipelinesApiVMAuthentication.scala:37); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.$anonfun$dockerCredentials$1(PipelinesApiConfiguration.scala:24); 		at scala.Option.map(Option.scala:146); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.<init>(PipelinesApiConfiguration.scala:23); 		at cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:12); 		... 29 common frames omitted; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/issues/4553:5474,Deployability,Pipeline,PipelinesApiLifecycleActorFactory,5474,tInObject(UTF8StreamJsonParser.java:826); 		at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:723); 		at com.google.api.client.json.jackson2.JacksonParser.nextToken(JacksonParser.java:55); 		at com.google.api.client.json.JsonParser.startParsing(JsonParser.java:221); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:380); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:355); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:87); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:81); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:250); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:226); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential$(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.apiClientGoogleCredential(GoogleAuthMode.scala:237); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.$anonfun$apply$1(PipelinesApiVMAuthentication.scala:41); 		at scala.Option.flatMap(Option.scala:171); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.apply(PipelinesApiVMAuthentication.scala:37); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.$anonfun$dockerCredentials$1(PipelinesApiConfiguration.scala:24); 		at scala.Option.map(Option.scala:146); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.<init>(PipelinesApiConfiguration.scala:23); 		at cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:12); 		... 29 common frames omitted; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/issues/4553:5515,Deployability,Pipeline,PipelinesApiLifecycleActorFactory,5515,tInObject(UTF8StreamJsonParser.java:826); 		at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:723); 		at com.google.api.client.json.jackson2.JacksonParser.nextToken(JacksonParser.java:55); 		at com.google.api.client.json.JsonParser.startParsing(JsonParser.java:221); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:380); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:355); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:87); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:81); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:250); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:226); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential$(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.apiClientGoogleCredential(GoogleAuthMode.scala:237); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.$anonfun$apply$1(PipelinesApiVMAuthentication.scala:41); 		at scala.Option.flatMap(Option.scala:171); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.apply(PipelinesApiVMAuthentication.scala:37); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.$anonfun$dockerCredentials$1(PipelinesApiConfiguration.scala:24); 		at scala.Option.map(Option.scala:146); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.<init>(PipelinesApiConfiguration.scala:23); 		at cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:12); 		... 29 common frames omitted; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/issues/4553:1745,Energy Efficiency,adapt,adapted,1745,ckends.<init>(CromwellBackends.scala:14); 	at cromwell.engine.backend.CromwellBackends$.initBackends(CromwellBackends.scala:42); 	at cromwell.server.CromwellSystem.$init$(CromwellSystem.scala:62); 	at cromwell.CromwellEntryPoint$$anon$2.<init>(CromwellEntryPoint.scala:96); 	at cromwell.CromwellEntryPoint$.$anonfun$buildCromwellSystem$1(CromwellEntryPoint.scala:96); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:96); 	at cromwell.CromwellEntryPoint$.runServer(CromwellEntryPoint.scala:50); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:15); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); 	Suppressed: java.lang.reflect.InvocationTargetException: null; 		at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 		at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 		at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 		at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 		at cromwell.engine.backend.BackendConfigurationEntry.$anonfun$asBackendLifecycleActorFactory$1(BackendConfiguration.scala:13); 		at scala.util.Try$.apply(Try.scala:209); 		at cromwell.engine.backend.BackendConfigurationEntry.asBackendLifecycleActorFactory(BackendConfiguration.scala:14); 		at cromwell.engine.backend.Cromwel,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/issues/4553:1745,Modifiability,adapt,adapted,1745,ckends.<init>(CromwellBackends.scala:14); 	at cromwell.engine.backend.CromwellBackends$.initBackends(CromwellBackends.scala:42); 	at cromwell.server.CromwellSystem.$init$(CromwellSystem.scala:62); 	at cromwell.CromwellEntryPoint$$anon$2.<init>(CromwellEntryPoint.scala:96); 	at cromwell.CromwellEntryPoint$.$anonfun$buildCromwellSystem$1(CromwellEntryPoint.scala:96); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:96); 	at cromwell.CromwellEntryPoint$.runServer(CromwellEntryPoint.scala:50); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:15); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); 	Suppressed: java.lang.reflect.InvocationTargetException: null; 		at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 		at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 		at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 		at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 		at cromwell.engine.backend.BackendConfigurationEntry.$anonfun$asBackendLifecycleActorFactory$1(BackendConfiguration.scala:13); 		at scala.util.Try$.apply(Try.scala:209); 		at cromwell.engine.backend.BackendConfigurationEntry.asBackendLifecycleActorFactory(BackendConfiguration.scala:14); 		at cromwell.engine.backend.Cromwel,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/issues/4553:4845,Security,authenticat,authentication,4845,tInObject(UTF8StreamJsonParser.java:826); 		at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:723); 		at com.google.api.client.json.jackson2.JacksonParser.nextToken(JacksonParser.java:55); 		at com.google.api.client.json.JsonParser.startParsing(JsonParser.java:221); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:380); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:355); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:87); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:81); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:250); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:226); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential$(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.apiClientGoogleCredential(GoogleAuthMode.scala:237); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.$anonfun$apply$1(PipelinesApiVMAuthentication.scala:41); 		at scala.Option.flatMap(Option.scala:171); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.apply(PipelinesApiVMAuthentication.scala:37); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.$anonfun$dockerCredentials$1(PipelinesApiConfiguration.scala:24); 		at scala.Option.map(Option.scala:146); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.<init>(PipelinesApiConfiguration.scala:23); 		at cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:12); 		... 29 common frames omitted; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/issues/4553:5039,Security,authenticat,authentication,5039,tInObject(UTF8StreamJsonParser.java:826); 		at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:723); 		at com.google.api.client.json.jackson2.JacksonParser.nextToken(JacksonParser.java:55); 		at com.google.api.client.json.JsonParser.startParsing(JsonParser.java:221); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:380); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:355); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:87); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:81); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:250); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:226); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential$(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.apiClientGoogleCredential(GoogleAuthMode.scala:237); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.$anonfun$apply$1(PipelinesApiVMAuthentication.scala:41); 		at scala.Option.flatMap(Option.scala:171); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.apply(PipelinesApiVMAuthentication.scala:37); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.$anonfun$dockerCredentials$1(PipelinesApiConfiguration.scala:24); 		at scala.Option.map(Option.scala:146); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.<init>(PipelinesApiConfiguration.scala:23); 		at cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:12); 		... 29 common frames omitted; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/issues/4553:115,Testability,test,testing,115,"Something appears to be wrong in the credential building for PAPI v2 private Docker as turned up in @marctalbott's testing. It's not clear why this is not replicated by the `docker_hash_dockerhub_private_wf_options` Centaur test but from reading the code it does appear to be a real issue. ```; 2019-01-14 20:20:22,530 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; common.exception.AggregatedException: :; null; 	Unrecognized token 'user_service_account_json': was expecting ('true', 'false' or 'null'); at [Source: (ByteArrayInputStream); line: 1, column: 51]; 	at common.util.TryUtil$.sequenceIterable(TryUtil.scala:29); 	at common.util.TryUtil$.sequenceMap(TryUtil.scala:47); 	at cromwell.engine.backend.CromwellBackends.<init>(CromwellBackends.scala:14); 	at cromwell.engine.backend.CromwellBackends$.initBackends(CromwellBackends.scala:42); 	at cromwell.server.CromwellSystem.$init$(CromwellSystem.scala:62); 	at cromwell.CromwellEntryPoint$$anon$2.<init>(CromwellEntryPoint.scala:96); 	at cromwell.CromwellEntryPoint$.$anonfun$buildCromwellSystem$1(CromwellEntryPoint.scala:96); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:96); 	at cromwell.CromwellEntryPoint$.runServer(CromwellEntryPoint.scala:50); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:15); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/issues/4553:224,Testability,test,test,224,"Something appears to be wrong in the credential building for PAPI v2 private Docker as turned up in @marctalbott's testing. It's not clear why this is not replicated by the `docker_hash_dockerhub_private_wf_options` Centaur test but from reading the code it does appear to be a real issue. ```; 2019-01-14 20:20:22,530 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; common.exception.AggregatedException: :; null; 	Unrecognized token 'user_service_account_json': was expecting ('true', 'false' or 'null'); at [Source: (ByteArrayInputStream); line: 1, column: 51]; 	at common.util.TryUtil$.sequenceIterable(TryUtil.scala:29); 	at common.util.TryUtil$.sequenceMap(TryUtil.scala:47); 	at cromwell.engine.backend.CromwellBackends.<init>(CromwellBackends.scala:14); 	at cromwell.engine.backend.CromwellBackends$.initBackends(CromwellBackends.scala:42); 	at cromwell.server.CromwellSystem.$init$(CromwellSystem.scala:62); 	at cromwell.CromwellEntryPoint$$anon$2.<init>(CromwellEntryPoint.scala:96); 	at cromwell.CromwellEntryPoint$.$anonfun$buildCromwellSystem$1(CromwellEntryPoint.scala:96); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:96); 	at cromwell.CromwellEntryPoint$.runServer(CromwellEntryPoint.scala:50); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:15); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/issues/4553:133,Usability,clear,clear,133,"Something appears to be wrong in the credential building for PAPI v2 private Docker as turned up in @marctalbott's testing. It's not clear why this is not replicated by the `docker_hash_dockerhub_private_wf_options` Centaur test but from reading the code it does appear to be a real issue. ```; 2019-01-14 20:20:22,530 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; common.exception.AggregatedException: :; null; 	Unrecognized token 'user_service_account_json': was expecting ('true', 'false' or 'null'); at [Source: (ByteArrayInputStream); line: 1, column: 51]; 	at common.util.TryUtil$.sequenceIterable(TryUtil.scala:29); 	at common.util.TryUtil$.sequenceMap(TryUtil.scala:47); 	at cromwell.engine.backend.CromwellBackends.<init>(CromwellBackends.scala:14); 	at cromwell.engine.backend.CromwellBackends$.initBackends(CromwellBackends.scala:42); 	at cromwell.server.CromwellSystem.$init$(CromwellSystem.scala:62); 	at cromwell.CromwellEntryPoint$$anon$2.<init>(CromwellEntryPoint.scala:96); 	at cromwell.CromwellEntryPoint$.$anonfun$buildCromwellSystem$1(CromwellEntryPoint.scala:96); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:96); 	at cromwell.CromwellEntryPoint$.runServer(CromwellEntryPoint.scala:50); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:15); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553
https://github.com/broadinstitute/cromwell/pull/4554:18,Security,validat,validate,18,As extra `womtool validate` test to make sure WDL 1.0 is catching this. Unfortunately there's no draft-2 equivalent of this test yet because of the bug spotted in #4550,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4554
https://github.com/broadinstitute/cromwell/pull/4554:28,Testability,test,test,28,As extra `womtool validate` test to make sure WDL 1.0 is catching this. Unfortunately there's no draft-2 equivalent of this test yet because of the bug spotted in #4550,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4554
https://github.com/broadinstitute/cromwell/pull/4554:124,Testability,test,test,124,As extra `womtool validate` test to make sure WDL 1.0 is catching this. Unfortunately there's no draft-2 equivalent of this test yet because of the bug spotted in #4550,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4554
https://github.com/broadinstitute/cromwell/issues/4555:188,Availability,error,error,188,"Totally valid workflow fails in the last stage in the latest development version of cromwell because of:; ```; Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types; ```; error.; I enclose both wdl and input. ; [quantification.zip](https://github.com/broadinstitute/cromwell/files/2761544/quantification.zip). The error is the following:. ```json; Workflow failed. WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:1:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2000), WomString(characteristics) -> WomString(number of donors -> 1;age -> 26 years old;tissue -> Kidney;vendor -> Biochain;isolate -> Lot no.: B106007;gender -> Male), WomString(series) -> WomString(GSE69360), WomString(organism) -> WomString(Homo sapiens), WomString(run) -> WomString(SRR2014240), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014240), WomString(name) -> WomString(Biochain_Adult_Kidney), WomString(gsm) -> WomString(GSM1698570), WomString(title) -> WomString(Biochain_Adult_Kidney))), WomString(run) -> WomString(SRR2014240), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/cal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555
https://github.com/broadinstitute/cromwell/issues/4555:331,Availability,error,error,331,"Totally valid workflow fails in the last stage in the latest development version of cromwell because of:; ```; Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types; ```; error.; I enclose both wdl and input. ; [quantification.zip](https://github.com/broadinstitute/cromwell/files/2761544/quantification.zip). The error is the following:. ```json; Workflow failed. WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:1:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2000), WomString(characteristics) -> WomString(number of donors -> 1;age -> 26 years old;tissue -> Kidney;vendor -> Biochain;isolate -> Lot no.: B106007;gender -> Male), WomString(series) -> WomString(GSE69360), WomString(organism) -> WomString(Homo sapiens), WomString(run) -> WomString(SRR2014240), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014240), WomString(name) -> WomString(Biochain_Adult_Kidney), WomString(gsm) -> WomString(GSM1698570), WomString(title) -> WomString(Biochain_Adult_Kidney))), WomString(run) -> WomString(SRR2014240), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/cal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555
https://github.com/broadinstitute/cromwell/issues/4555:409,Availability,failure,failure,409,"Totally valid workflow fails in the last stage in the latest development version of cromwell because of:; ```; Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types; ```; error.; I enclose both wdl and input. ; [quantification.zip](https://github.com/broadinstitute/cromwell/files/2761544/quantification.zip). The error is the following:. ```json; Workflow failed. WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:1:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2000), WomString(characteristics) -> WomString(number of donors -> 1;age -> 26 years old;tissue -> Kidney;vendor -> Biochain;isolate -> Lot no.: B106007;gender -> Male), WomString(series) -> WomString(GSE69360), WomString(organism) -> WomString(Homo sapiens), WomString(run) -> WomString(SRR2014240), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014240), WomString(name) -> WomString(Biochain_Adult_Kidney), WomString(gsm) -> WomString(GSM1698570), WomString(title) -> WomString(Biochain_Adult_Kidney))), WomString(run) -> WomString(SRR2014240), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/cal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555
https://github.com/broadinstitute/cromwell/issues/4555:1128,Availability,down,download,1128,"id workflow fails in the last stage in the latest development version of cromwell because of:; ```; Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types; ```; error.; I enclose both wdl and input. ; [quantification.zip](https://github.com/broadinstitute/cromwell/files/2761544/quantification.zip). The error is the following:. ```json; Workflow failed. WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:1:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2000), WomString(characteristics) -> WomString(number of donors -> 1;age -> 26 years old;tissue -> Kidney;vendor -> Biochain;isolate -> Lot no.: B106007;gender -> Male), WomString(series) -> WomString(GSE69360), WomString(organism) -> WomString(Homo sapiens), WomString(run) -> WomString(SRR2014240), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014240), WomString(name) -> WomString(Biochain_Adult_Kidney), WomString(gsm) -> WomString(GSM1698570), WomString(title) -> WomString(Biochain_Adult_Kidney))), WomString(run) -> WomString(SRR2014240), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555
https://github.com/broadinstitute/cromwell/issues/4555:2172,Availability,failure,failure,2172,"ng(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:0:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2000), WomString(characteristics) -> WomString(number of donors -> 1;age -> 64 years old;tissue -> Liver;vendor -> Biochain;isolate -> Lot no.: B510092;gender -> Male), WomString(series) -> WomString(GSE69360), WomString(organism) -> WomString(Homo sapiens), WomString(run) -> WomString(SRR2014238), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238), WomString(name) -> WomString(Biochain_Adult_Liver), WomString(gsm) -> WomString(GSM1698568), WomString(title) -> WomString(Biochain_Adult_Liver))), WomString(run) -> WomString(SRR2014238), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-0/ScatterAt40_16/8c83d187-db36-4dc0-a6c4-f7e91b3d80f3/call-salmon/shard-0/execution/quant_SRR2014238), WomStr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555
https://github.com/broadinstitute/cromwell/issues/4555:2890,Availability,down,download,2890,"_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:0:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2000), WomString(characteristics) -> WomString(number of donors -> 1;age -> 64 years old;tissue -> Liver;vendor -> Biochain;isolate -> Lot no.: B510092;gender -> Male), WomString(series) -> WomString(GSE69360), WomString(organism) -> WomString(Homo sapiens), WomString(run) -> WomString(SRR2014238), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238), WomString(name) -> WomString(Biochain_Adult_Liver), WomString(gsm) -> WomString(GSM1698568), WomString(title) -> WomString(Biochain_Adult_Liver))), WomString(run) -> WomString(SRR2014238), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-0/ScatterAt40_16/8c83d187-db36-4dc0-a6c4-f7e91b3d80f3/call-salmon/shard-0/execution/quant_SRR2014238), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-0/ScatterAt40_16/8c83d187-db36-4dc0-a6c4-f7e91b3d80f3/call-salmon/shard-0/execution/quant_SRR2014238/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555
https://github.com/broadinstitute/cromwell/issues/4555:3932,Availability,failure,failure,3932,"erAt40_16/shard-0/ScatterAt40_16/8c83d187-db36-4dc0-a6c4-f7e91b3d80f3/call-salmon/shard-0/execution/quant_SRR2014238), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-0/ScatterAt40_16/8c83d187-db36-4dc0-a6c4-f7e91b3d80f3/call-salmon/shard-0/execution/quant_SRR2014238/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-0/ScatterAt40_16/8c83d187-db36-4dc0-a6c4-f7e91b3d80f3/call-salmon/shard-0/execution/quant_SRR2014238/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:4:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2500), WomString(characteristics) -> WomString(strain -> indigenous;location -> Chengdu, Sichuan province, China;tissue -> liver;age -> ~4 years old), WomString(series) -> WomString(GSE77020), WomString(organism) -> WomString(Bos taurus), WomString(run) -> WomString(SRR3109705), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra37/SRR/003036/SRR3109705), WomString(name) -> WomString(GSM2042593), WomString(gsm) -> WomString(GSM2042593), WomString(title) -> WomString(cattle_liver_1))), WomString(run) -> WomString(SRR3109705), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-4/ScatterAt40_16/c24f9f18-1429-473b-a2a6-bd92e5975d30/call-salmon/shard-0/execution/quant_SRR3109705), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-4/ScatterAt40_16/c24f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555
https://github.com/broadinstitute/cromwell/issues/4555:4630,Availability,down,download,4630,"erAt40_16/shard-0/ScatterAt40_16/8c83d187-db36-4dc0-a6c4-f7e91b3d80f3/call-salmon/shard-0/execution/quant_SRR2014238), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-0/ScatterAt40_16/8c83d187-db36-4dc0-a6c4-f7e91b3d80f3/call-salmon/shard-0/execution/quant_SRR2014238/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-0/ScatterAt40_16/8c83d187-db36-4dc0-a6c4-f7e91b3d80f3/call-salmon/shard-0/execution/quant_SRR2014238/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:4:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2500), WomString(characteristics) -> WomString(strain -> indigenous;location -> Chengdu, Sichuan province, China;tissue -> liver;age -> ~4 years old), WomString(series) -> WomString(GSE77020), WomString(organism) -> WomString(Bos taurus), WomString(run) -> WomString(SRR3109705), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra37/SRR/003036/SRR3109705), WomString(name) -> WomString(GSM2042593), WomString(gsm) -> WomString(GSM2042593), WomString(title) -> WomString(cattle_liver_1))), WomString(run) -> WomString(SRR3109705), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-4/ScatterAt40_16/c24f9f18-1429-473b-a2a6-bd92e5975d30/call-salmon/shard-0/execution/quant_SRR3109705), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-4/ScatterAt40_16/c24f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555
https://github.com/broadinstitute/cromwell/issues/4555:5656,Availability,failure,failure,5656,"erAt40_16/shard-4/ScatterAt40_16/c24f9f18-1429-473b-a2a6-bd92e5975d30/call-salmon/shard-0/execution/quant_SRR3109705), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-4/ScatterAt40_16/c24f9f18-1429-473b-a2a6-bd92e5975d30/call-salmon/shard-0/execution/quant_SRR3109705/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-4/ScatterAt40_16/c24f9f18-1429-473b-a2a6-bd92e5975d30/call-salmon/shard-0/execution/quant_SRR3109705/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:5:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2500), WomString(characteristics) -> WomString(strain -> indigenous;location -> Chengdu, Sichuan province, China;tissue -> kidney;age -> ~4 years old), WomString(series) -> WomString(GSE77020), WomString(organism) -> WomString(Bos taurus), WomString(run) -> WomString(SRR3109708), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra38/SRR/003036/SRR3109708), WomString(name) -> WomString(GSM2042596), WomString(gsm) -> WomString(GSM2042596), WomString(title) -> WomString(cattle_kidney_1))), WomString(run) -> WomString(SRR3109708), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-5/ScatterAt40_16/897d0635-6fdf-4b22-b98f-36d49683ce08/call-salmon/shard-0/execution/quant_SRR3109708), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-5/ScatterAt40_16/897",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555
https://github.com/broadinstitute/cromwell/issues/4555:6355,Availability,down,download,6355,"erAt40_16/shard-4/ScatterAt40_16/c24f9f18-1429-473b-a2a6-bd92e5975d30/call-salmon/shard-0/execution/quant_SRR3109705), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-4/ScatterAt40_16/c24f9f18-1429-473b-a2a6-bd92e5975d30/call-salmon/shard-0/execution/quant_SRR3109705/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-4/ScatterAt40_16/c24f9f18-1429-473b-a2a6-bd92e5975d30/call-salmon/shard-0/execution/quant_SRR3109705/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:5:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2500), WomString(characteristics) -> WomString(strain -> indigenous;location -> Chengdu, Sichuan province, China;tissue -> kidney;age -> ~4 years old), WomString(series) -> WomString(GSE77020), WomString(organism) -> WomString(Bos taurus), WomString(run) -> WomString(SRR3109708), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra38/SRR/003036/SRR3109708), WomString(name) -> WomString(GSM2042596), WomString(gsm) -> WomString(GSM2042596), WomString(title) -> WomString(cattle_kidney_1))), WomString(run) -> WomString(SRR3109708), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-5/ScatterAt40_16/897d0635-6fdf-4b22-b98f-36d49683ce08/call-salmon/shard-0/execution/quant_SRR3109708), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-5/ScatterAt40_16/897",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555
https://github.com/broadinstitute/cromwell/issues/4555:7382,Availability,failure,failure,7382,"all-ScatterAt40_16/shard-5/ScatterAt40_16/897d0635-6fdf-4b22-b98f-36d49683ce08/call-salmon/shard-0/execution/quant_SRR3109708), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-5/ScatterAt40_16/897d0635-6fdf-4b22-b98f-36d49683ce08/call-salmon/shard-0/execution/quant_SRR3109708/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-5/ScatterAt40_16/897d0635-6fdf-4b22-b98f-36d49683ce08/call-salmon/shard-0/execution/quant_SRR3109708/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:3:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2500), WomString(characteristics) -> WomString(strain -> CAST/EiJ;genotype -> Wild-type;treatment -> Clean-air;tissue -> kidney), WomString(series) -> WomString(GSE108990), WomString(organism) -> WomString(Mus musculus), WomString(run) -> WomString(SRR6456754), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra57/SRR/006305/SRR6456754), WomString(name) -> WomString(GSM2927750), WomString(gsm) -> WomString(GSM2927750), WomString(title) -> WomString(RNA_105_kidney_Control))), WomString(run) -> WomString(SRR6456754), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-3/ScatterAt40_16/fae142d3-7b38-418e-82cb-a1a437458c72/call-salmon/shard-0/execution/quant_SRR6456754), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-3/ScatterAt40_16/fae14",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555
https://github.com/broadinstitute/cromwell/issues/4555:8062,Availability,down,download,8062,"all-ScatterAt40_16/shard-5/ScatterAt40_16/897d0635-6fdf-4b22-b98f-36d49683ce08/call-salmon/shard-0/execution/quant_SRR3109708), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-5/ScatterAt40_16/897d0635-6fdf-4b22-b98f-36d49683ce08/call-salmon/shard-0/execution/quant_SRR3109708/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-5/ScatterAt40_16/897d0635-6fdf-4b22-b98f-36d49683ce08/call-salmon/shard-0/execution/quant_SRR3109708/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:3:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2500), WomString(characteristics) -> WomString(strain -> CAST/EiJ;genotype -> Wild-type;treatment -> Clean-air;tissue -> kidney), WomString(series) -> WomString(GSE108990), WomString(organism) -> WomString(Mus musculus), WomString(run) -> WomString(SRR6456754), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra57/SRR/006305/SRR6456754), WomString(name) -> WomString(GSM2927750), WomString(gsm) -> WomString(GSM2927750), WomString(title) -> WomString(RNA_105_kidney_Control))), WomString(run) -> WomString(SRR6456754), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-3/ScatterAt40_16/fae142d3-7b38-418e-82cb-a1a437458c72/call-salmon/shard-0/execution/quant_SRR6456754), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-3/ScatterAt40_16/fae14",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555
https://github.com/broadinstitute/cromwell/issues/4555:9096,Availability,failure,failure,9096,"call-ScatterAt40_16/shard-3/ScatterAt40_16/fae142d3-7b38-418e-82cb-a1a437458c72/call-salmon/shard-0/execution/quant_SRR6456754), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-3/ScatterAt40_16/fae142d3-7b38-418e-82cb-a1a437458c72/call-salmon/shard-0/execution/quant_SRR6456754/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-3/ScatterAt40_16/fae142d3-7b38-418e-82cb-a1a437458c72/call-salmon/shard-0/execution/quant_SRR6456754/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:2:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2500), WomString(characteristics) -> WomString(strain -> CAST/EiJ;genotype -> Wild-type;treatment -> Clean-air;tissue -> liver), WomString(series) -> WomString(GSE108990), WomString(organism) -> WomString(Mus musculus), WomString(run) -> WomString(SRR6456687), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra57/SRR/006305/SRR6456687), WomString(name) -> WomString(GSM2927683), WomString(gsm) -> WomString(GSM2927683), WomString(title) -> WomString(RNA_105_liver_Control))), WomString(run) -> WomString(SRR6456687), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-2/ScatterAt40_16/dc76f95e-2040-4e38-a0d7-0b82c48bbca6/call-salmon/shard-0/execution/quant_SRR6456687), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-2/ScatterAt40_16/dc76f95",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555
https://github.com/broadinstitute/cromwell/issues/4555:9775,Availability,down,download,9775,"call-ScatterAt40_16/shard-3/ScatterAt40_16/fae142d3-7b38-418e-82cb-a1a437458c72/call-salmon/shard-0/execution/quant_SRR6456754), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-3/ScatterAt40_16/fae142d3-7b38-418e-82cb-a1a437458c72/call-salmon/shard-0/execution/quant_SRR6456754/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-3/ScatterAt40_16/fae142d3-7b38-418e-82cb-a1a437458c72/call-salmon/shard-0/execution/quant_SRR6456754/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:2:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2500), WomString(characteristics) -> WomString(strain -> CAST/EiJ;genotype -> Wild-type;treatment -> Clean-air;tissue -> liver), WomString(series) -> WomString(GSE108990), WomString(organism) -> WomString(Mus musculus), WomString(run) -> WomString(SRR6456687), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra57/SRR/006305/SRR6456687), WomString(name) -> WomString(GSM2927683), WomString(gsm) -> WomString(GSM2927683), WomString(title) -> WomString(RNA_105_liver_Control))), WomString(run) -> WomString(SRR6456687), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-2/ScatterAt40_16/dc76f95e-2040-4e38-a0d7-0b82c48bbca6/call-salmon/shard-0/execution/quant_SRR6456687), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-2/ScatterAt40_16/dc76f95",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555
https://github.com/broadinstitute/cromwell/issues/4555:445,Energy Efficiency,monitor,monitoring,445,"Totally valid workflow fails in the last stage in the latest development version of cromwell because of:; ```; Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types; ```; error.; I enclose both wdl and input. ; [quantification.zip](https://github.com/broadinstitute/cromwell/files/2761544/quantification.zip). The error is the following:. ```json; Workflow failed. WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:1:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2000), WomString(characteristics) -> WomString(number of donors -> 1;age -> 26 years old;tissue -> Kidney;vendor -> Biochain;isolate -> Lot no.: B106007;gender -> Male), WomString(series) -> WomString(GSE69360), WomString(organism) -> WomString(Homo sapiens), WomString(run) -> WomString(SRR2014240), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014240), WomString(name) -> WomString(Biochain_Adult_Kidney), WomString(gsm) -> WomString(GSM1698570), WomString(title) -> WomString(Biochain_Adult_Kidney))), WomString(run) -> WomString(SRR2014240), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/cal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555
https://github.com/broadinstitute/cromwell/issues/4555:2208,Energy Efficiency,monitor,monitoring,2208,"ng(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:0:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2000), WomString(characteristics) -> WomString(number of donors -> 1;age -> 64 years old;tissue -> Liver;vendor -> Biochain;isolate -> Lot no.: B510092;gender -> Male), WomString(series) -> WomString(GSE69360), WomString(organism) -> WomString(Homo sapiens), WomString(run) -> WomString(SRR2014238), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238), WomString(name) -> WomString(Biochain_Adult_Liver), WomString(gsm) -> WomString(GSM1698568), WomString(title) -> WomString(Biochain_Adult_Liver))), WomString(run) -> WomString(SRR2014238), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-0/ScatterAt40_16/8c83d187-db36-4dc0-a6c4-f7e91b3d80f3/call-salmon/shard-0/execution/quant_SRR2014238), WomStr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555
https://github.com/broadinstitute/cromwell/issues/4555:3968,Energy Efficiency,monitor,monitoring,3968,"erAt40_16/shard-0/ScatterAt40_16/8c83d187-db36-4dc0-a6c4-f7e91b3d80f3/call-salmon/shard-0/execution/quant_SRR2014238), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-0/ScatterAt40_16/8c83d187-db36-4dc0-a6c4-f7e91b3d80f3/call-salmon/shard-0/execution/quant_SRR2014238/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-0/ScatterAt40_16/8c83d187-db36-4dc0-a6c4-f7e91b3d80f3/call-salmon/shard-0/execution/quant_SRR2014238/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:4:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2500), WomString(characteristics) -> WomString(strain -> indigenous;location -> Chengdu, Sichuan province, China;tissue -> liver;age -> ~4 years old), WomString(series) -> WomString(GSE77020), WomString(organism) -> WomString(Bos taurus), WomString(run) -> WomString(SRR3109705), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra37/SRR/003036/SRR3109705), WomString(name) -> WomString(GSM2042593), WomString(gsm) -> WomString(GSM2042593), WomString(title) -> WomString(cattle_liver_1))), WomString(run) -> WomString(SRR3109705), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-4/ScatterAt40_16/c24f9f18-1429-473b-a2a6-bd92e5975d30/call-salmon/shard-0/execution/quant_SRR3109705), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-4/ScatterAt40_16/c24f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555
https://github.com/broadinstitute/cromwell/issues/4555:5692,Energy Efficiency,monitor,monitoring,5692,"erAt40_16/shard-4/ScatterAt40_16/c24f9f18-1429-473b-a2a6-bd92e5975d30/call-salmon/shard-0/execution/quant_SRR3109705), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-4/ScatterAt40_16/c24f9f18-1429-473b-a2a6-bd92e5975d30/call-salmon/shard-0/execution/quant_SRR3109705/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-4/ScatterAt40_16/c24f9f18-1429-473b-a2a6-bd92e5975d30/call-salmon/shard-0/execution/quant_SRR3109705/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:5:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2500), WomString(characteristics) -> WomString(strain -> indigenous;location -> Chengdu, Sichuan province, China;tissue -> kidney;age -> ~4 years old), WomString(series) -> WomString(GSE77020), WomString(organism) -> WomString(Bos taurus), WomString(run) -> WomString(SRR3109708), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra38/SRR/003036/SRR3109708), WomString(name) -> WomString(GSM2042596), WomString(gsm) -> WomString(GSM2042596), WomString(title) -> WomString(cattle_kidney_1))), WomString(run) -> WomString(SRR3109708), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-5/ScatterAt40_16/897d0635-6fdf-4b22-b98f-36d49683ce08/call-salmon/shard-0/execution/quant_SRR3109708), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-5/ScatterAt40_16/897",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555
https://github.com/broadinstitute/cromwell/issues/4555:7418,Energy Efficiency,monitor,monitoring,7418,"all-ScatterAt40_16/shard-5/ScatterAt40_16/897d0635-6fdf-4b22-b98f-36d49683ce08/call-salmon/shard-0/execution/quant_SRR3109708), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-5/ScatterAt40_16/897d0635-6fdf-4b22-b98f-36d49683ce08/call-salmon/shard-0/execution/quant_SRR3109708/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-5/ScatterAt40_16/897d0635-6fdf-4b22-b98f-36d49683ce08/call-salmon/shard-0/execution/quant_SRR3109708/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:3:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2500), WomString(characteristics) -> WomString(strain -> CAST/EiJ;genotype -> Wild-type;treatment -> Clean-air;tissue -> kidney), WomString(series) -> WomString(GSE108990), WomString(organism) -> WomString(Mus musculus), WomString(run) -> WomString(SRR6456754), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra57/SRR/006305/SRR6456754), WomString(name) -> WomString(GSM2927750), WomString(gsm) -> WomString(GSM2927750), WomString(title) -> WomString(RNA_105_kidney_Control))), WomString(run) -> WomString(SRR6456754), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-3/ScatterAt40_16/fae142d3-7b38-418e-82cb-a1a437458c72/call-salmon/shard-0/execution/quant_SRR6456754), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-3/ScatterAt40_16/fae14",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555
https://github.com/broadinstitute/cromwell/issues/4555:9132,Energy Efficiency,monitor,monitoring,9132,"call-ScatterAt40_16/shard-3/ScatterAt40_16/fae142d3-7b38-418e-82cb-a1a437458c72/call-salmon/shard-0/execution/quant_SRR6456754), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-3/ScatterAt40_16/fae142d3-7b38-418e-82cb-a1a437458c72/call-salmon/shard-0/execution/quant_SRR6456754/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-3/ScatterAt40_16/fae142d3-7b38-418e-82cb-a1a437458c72/call-salmon/shard-0/execution/quant_SRR6456754/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:2:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2500), WomString(characteristics) -> WomString(strain -> CAST/EiJ;genotype -> Wild-type;treatment -> Clean-air;tissue -> liver), WomString(series) -> WomString(GSE108990), WomString(organism) -> WomString(Mus musculus), WomString(run) -> WomString(SRR6456687), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra57/SRR/006305/SRR6456687), WomString(name) -> WomString(GSM2927683), WomString(gsm) -> WomString(GSM2927683), WomString(title) -> WomString(RNA_105_liver_Control))), WomString(run) -> WomString(SRR6456687), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-2/ScatterAt40_16/dc76f95e-2040-4e38-a0d7-0b82c48bbca6/call-salmon/shard-0/execution/quant_SRR6456687), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-2/ScatterAt40_16/dc76f95",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555
https://github.com/broadinstitute/cromwell/issues/4557:23,Integrability,message,message,23,"Refined as:. - One log message when a hog group begins to be limited; - One log message when a hog group is no longer being limited; - One log message at a configurable interval, listing *all* hog groups being limited (and their total jobs)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4557
https://github.com/broadinstitute/cromwell/issues/4557:80,Integrability,message,message,80,"Refined as:. - One log message when a hog group begins to be limited; - One log message when a hog group is no longer being limited; - One log message at a configurable interval, listing *all* hog groups being limited (and their total jobs)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4557
https://github.com/broadinstitute/cromwell/issues/4557:143,Integrability,message,message,143,"Refined as:. - One log message when a hog group begins to be limited; - One log message when a hog group is no longer being limited; - One log message at a configurable interval, listing *all* hog groups being limited (and their total jobs)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4557
https://github.com/broadinstitute/cromwell/issues/4557:156,Modifiability,config,configurable,156,"Refined as:. - One log message when a hog group begins to be limited; - One log message when a hog group is no longer being limited; - One log message at a configurable interval, listing *all* hog groups being limited (and their total jobs)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4557
https://github.com/broadinstitute/cromwell/issues/4557:19,Testability,log,log,19,"Refined as:. - One log message when a hog group begins to be limited; - One log message when a hog group is no longer being limited; - One log message at a configurable interval, listing *all* hog groups being limited (and their total jobs)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4557
https://github.com/broadinstitute/cromwell/issues/4557:76,Testability,log,log,76,"Refined as:. - One log message when a hog group begins to be limited; - One log message when a hog group is no longer being limited; - One log message at a configurable interval, listing *all* hog groups being limited (and their total jobs)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4557
https://github.com/broadinstitute/cromwell/issues/4557:139,Testability,log,log,139,"Refined as:. - One log message when a hog group begins to be limited; - One log message when a hog group is no longer being limited; - One log message at a configurable interval, listing *all* hog groups being limited (and their total jobs)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4557
https://github.com/broadinstitute/cromwell/issues/4560:2238,Availability,alive,alive,2238,"rectory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). # exit-code-timeout-seconds = 120. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560
https://github.com/broadinstitute/cromwell/issues/4560:2402,Availability,alive,alive,2402,"rectory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). # exit-code-timeout-seconds = 120. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560
https://github.com/broadinstitute/cromwell/issues/4560:2756,Availability,alive,alive,2756,"rectory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). # exit-code-timeout-seconds = 120. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560
https://github.com/broadinstitute/cromwell/issues/4560:938,Deployability,configurat,configuration,938,"It works with the standard backend. <!-- Which backend are you running? -->; SLURM. <!-- Paste/Attach your workflow if possible: -->; cwlVersion: v1.0; class: Workflow. requirements:; SubworkflowFeatureRequirement: {}. inputs:; fastqc_output_dir:; type: string; fastqc_input_files:; type: string[]; fastqc_thread_count:; type: int; multiqc_output_dir:; type: string; outputs: []. steps:; fastqc_mkdir:; run: mkdir-cmd.cwl; in:; directory: fastqc_output_dir; out: [created_directory]; fastqc_execute:; run: fastqc-step.cwl; in:; input_files: fastqc_input_files; output_dir: fastqc_mkdir/created_directory; thread_count: fastqc_thread_count; out: [output_directory]; multiqc_mkdir:; run: mkdir-cmd.cwl; in:; directory: multiqc_output_dir; out: [created_directory]; multiqc_execute:; run: multiqc-cmd.cwl; in:; output_dir: multiqc_mkdir/created_directory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560
https://github.com/broadinstitute/cromwell/issues/4560:1808,Deployability,configurat,configuration,1808,"kdir/created_directory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). # exit-code-timeout-seconds = 120. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560
https://github.com/broadinstitute/cromwell/issues/4560:2681,Integrability,wrap,wrap,2681,"rectory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). # exit-code-timeout-seconds = 120. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560
https://github.com/broadinstitute/cromwell/issues/4560:938,Modifiability,config,configuration,938,"It works with the standard backend. <!-- Which backend are you running? -->; SLURM. <!-- Paste/Attach your workflow if possible: -->; cwlVersion: v1.0; class: Workflow. requirements:; SubworkflowFeatureRequirement: {}. inputs:; fastqc_output_dir:; type: string; fastqc_input_files:; type: string[]; fastqc_thread_count:; type: int; multiqc_output_dir:; type: string; outputs: []. steps:; fastqc_mkdir:; run: mkdir-cmd.cwl; in:; directory: fastqc_output_dir; out: [created_directory]; fastqc_execute:; run: fastqc-step.cwl; in:; input_files: fastqc_input_files; output_dir: fastqc_mkdir/created_directory; thread_count: fastqc_thread_count; out: [output_directory]; multiqc_mkdir:; run: mkdir-cmd.cwl; in:; directory: multiqc_output_dir; out: [created_directory]; multiqc_execute:; run: multiqc-cmd.cwl; in:; output_dir: multiqc_mkdir/created_directory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560
https://github.com/broadinstitute/cromwell/issues/4560:1386,Modifiability,config,config,1386,"astqc_mkdir:; run: mkdir-cmd.cwl; in:; directory: fastqc_output_dir; out: [created_directory]; fastqc_execute:; run: fastqc-step.cwl; in:; input_files: fastqc_input_files; output_dir: fastqc_mkdir/created_directory; thread_count: fastqc_thread_count; out: [output_directory]; multiqc_mkdir:; run: mkdir-cmd.cwl; in:; directory: multiqc_output_dir; out: [created_directory]; multiqc_execute:; run: multiqc-cmd.cwl; in:; output_dir: multiqc_mkdir/created_directory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560
https://github.com/broadinstitute/cromwell/issues/4560:1443,Modifiability,Config,ConfigAsyncJobExecutionActor,1443,"_dir; out: [created_directory]; fastqc_execute:; run: fastqc-step.cwl; in:; input_files: fastqc_input_files; output_dir: fastqc_mkdir/created_directory; thread_count: fastqc_thread_count; out: [output_directory]; multiqc_mkdir:; run: mkdir-cmd.cwl; in:; directory: multiqc_output_dir; out: [created_directory]; multiqc_execute:; run: multiqc-cmd.cwl; in:; output_dir: multiqc_mkdir/created_directory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560
https://github.com/broadinstitute/cromwell/issues/4560:1513,Modifiability,config,config,1513,"p.cwl; in:; input_files: fastqc_input_files; output_dir: fastqc_mkdir/created_directory; thread_count: fastqc_thread_count; out: [output_directory]; multiqc_mkdir:; run: mkdir-cmd.cwl; in:; directory: multiqc_output_dir; out: [created_directory]; multiqc_execute:; run: multiqc-cmd.cwl; in:; output_dir: multiqc_mkdir/created_directory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). # exit-code-timeout-seconds = 120. submit = ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560
https://github.com/broadinstitute/cromwell/issues/4560:1570,Modifiability,Config,ConfigAsyncJobExecutionActor,1570,"_mkdir/created_directory; thread_count: fastqc_thread_count; out: [output_directory]; multiqc_mkdir:; run: mkdir-cmd.cwl; in:; directory: multiqc_output_dir; out: [created_directory]; multiqc_execute:; run: multiqc-cmd.cwl; in:; output_dir: multiqc_mkdir/created_directory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). # exit-code-timeout-seconds = 120. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560
https://github.com/broadinstitute/cromwell/issues/4560:1808,Modifiability,config,configuration,1808,"kdir/created_directory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). # exit-code-timeout-seconds = 120. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560
https://github.com/broadinstitute/cromwell/issues/4560:1964,Modifiability,config,config,1964,"rectory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). # exit-code-timeout-seconds = 120. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560
https://github.com/broadinstitute/cromwell/issues/4560:1971,Modifiability,Config,ConfigBackendLifecycleActorFactory,1971,"rectory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). # exit-code-timeout-seconds = 120. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560
https://github.com/broadinstitute/cromwell/issues/4560:2008,Modifiability,config,config,2008,"rectory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). # exit-code-timeout-seconds = 120. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560
https://github.com/broadinstitute/cromwell/issues/4560:2133,Performance,queue,queue,2133,"rectory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). # exit-code-timeout-seconds = 120. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560
https://github.com/broadinstitute/cromwell/issues/4560:2601,Performance,queue,queue,2601,"rectory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). # exit-code-timeout-seconds = 120. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560
https://github.com/broadinstitute/cromwell/issues/4560:2172,Safety,timeout,timeout-seconds,2172,"rectory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). # exit-code-timeout-seconds = 120. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560
https://github.com/broadinstitute/cromwell/issues/4560:2265,Safety,timeout,timeout,2265,"rectory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). # exit-code-timeout-seconds = 120. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560
https://github.com/broadinstitute/cromwell/issues/4560:2463,Safety,timeout,timeout,2463,"rectory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). # exit-code-timeout-seconds = 120. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560
https://github.com/broadinstitute/cromwell/issues/4560:2485,Safety,timeout,timeout-seconds,2485,"rectory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). # exit-code-timeout-seconds = 120. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560
https://github.com/broadinstitute/cromwell/issues/4560:983,Security,PASSWORD,PASSWORDS,983,"It works with the standard backend. <!-- Which backend are you running? -->; SLURM. <!-- Paste/Attach your workflow if possible: -->; cwlVersion: v1.0; class: Workflow. requirements:; SubworkflowFeatureRequirement: {}. inputs:; fastqc_output_dir:; type: string; fastqc_input_files:; type: string[]; fastqc_thread_count:; type: int; multiqc_output_dir:; type: string; outputs: []. steps:; fastqc_mkdir:; run: mkdir-cmd.cwl; in:; directory: fastqc_output_dir; out: [created_directory]; fastqc_execute:; run: fastqc-step.cwl; in:; input_files: fastqc_input_files; output_dir: fastqc_mkdir/created_directory; thread_count: fastqc_thread_count; out: [output_directory]; multiqc_mkdir:; run: mkdir-cmd.cwl; in:; directory: multiqc_output_dir; out: [created_directory]; multiqc_execute:; run: multiqc-cmd.cwl; in:; output_dir: multiqc_mkdir/created_directory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560
https://github.com/broadinstitute/cromwell/issues/4563:366,Availability,error,error,366,"Backend: AWS Batch. Workflow: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/frankenstein.wdl. Input file: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/map-variantcall-hg38.json. Possibly related to #4412 but not sure as I don't see the same error message. When submitting a workflow via the cromwell server we **consistently** see a failure to hash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-anothe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:458,Availability,failure,failure,458,"Backend: AWS Batch. Workflow: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/frankenstein.wdl. Input file: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/map-variantcall-hg38.json. Possibly related to #4412 but not sure as I don't see the same error message. When submitting a workflow via the cromwell server we **consistently** see a failure to hash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-anothe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:695,Availability,avail,available,695,"Backend: AWS Batch. Workflow: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/frankenstein.wdl. Input file: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/map-variantcall-hg38.json. Possibly related to #4412 but not sure as I don't see the same error message. When submitting a workflow via the cromwell server we **consistently** see a failure to hash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-anothe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:940,Availability,avail,available,940,"Backend: AWS Batch. Workflow: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/frankenstein.wdl. Input file: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/map-variantcall-hg38.json. Possibly related to #4412 but not sure as I don't see the same error message. When submitting a workflow via the cromwell server we **consistently** see a failure to hash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-anothe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:989,Availability,down,download,989,"Backend: AWS Batch. Workflow: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/frankenstein.wdl. Input file: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/map-variantcall-hg38.json. Possibly related to #4412 but not sure as I don't see the same error message. When submitting a workflow via the cromwell server we **consistently** see a failure to hash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-anothe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:1019,Availability,Error,Error,1019,"b.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/frankenstein.wdl. Input file: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/map-variantcall-hg38.json. Possibly related to #4412 but not sure as I don't see the same error message. When submitting a workflow via the cromwell server we **consistently** see a failure to hash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-another""; scheme = ""assume_role""; base-auth = ""d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:372,Integrability,message,message,372,"Backend: AWS Batch. Workflow: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/frankenstein.wdl. Input file: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/map-variantcall-hg38.json. Possibly related to #4412 but not sure as I don't see the same error message. When submitting a workflow via the cromwell server we **consistently** see a failure to hash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-anothe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:1025,Integrability,message,message,1025,"b.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/frankenstein.wdl. Input file: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/map-variantcall-hg38.json. Possibly related to #4412 but not sure as I don't see the same error message. When submitting a workflow via the cromwell server we **consistently** see a failure to hash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-another""; scheme = ""assume_role""; base-auth = ""d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:1132,Integrability,message,message,1132,"utch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/map-variantcall-hg38.json. Possibly related to #4412 but not sure as I don't see the same error message. When submitting a workflow via the cromwell server we **consistently** see a failure to hash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-another""; scheme = ""assume_role""; base-auth = ""default""; role-arn = ""arn:aws:iam::xxx:role/fbucketname""; }; ]; // diff 1:; # region = ""us-west-2"" // uses region from ~/.aws/config set by aws configu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:1464,Modifiability,Config,Config,1464,"ash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-another""; scheme = ""assume_role""; base-auth = ""default""; role-arn = ""arn:aws:iam::xxx:role/fbucketname""; }; ]; // diff 1:; # region = ""us-west-2"" // uses region from ~/.aws/config set by aws configure command,; # // or us-east-1 by default; }; engine {; filesystems {; s3 {; auth = ""assume-role-based-on-another""; }; }; }; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; //",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:2168,Modifiability,config,config,2168,"rom `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-another""; scheme = ""assume_role""; base-auth = ""default""; role-arn = ""arn:aws:iam::xxx:role/fbucketname""; }; ]; // diff 1:; # region = ""us-west-2"" // uses region from ~/.aws/config set by aws configure command,; # // or us-east-1 by default; }; engine {; filesystems {; s3 {; auth = ""assume-role-based-on-another""; }; }; }; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; // Base bucket for workflow executions; root = ""s3://bucketname/cromwell-tests""; // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default""; // diff 2:; numSubmitAttempts = 1; // diff 3:; numCreateDefinitionAttempts = 1; default-runtime-attributes {; queueArn: ""arn:aws:batch:us-west-2:xxx:job-queue/GenomicsHighPriorityQue-xxx""; }; filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:2186,Modifiability,config,configure,2186,"rom `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-another""; scheme = ""assume_role""; base-auth = ""default""; role-arn = ""arn:aws:iam::xxx:role/fbucketname""; }; ]; // diff 1:; # region = ""us-west-2"" // uses region from ~/.aws/config set by aws configure command,; # // or us-east-1 by default; }; engine {; filesystems {; s3 {; auth = ""assume-role-based-on-another""; }; }; }; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; // Base bucket for workflow executions; root = ""s3://bucketname/cromwell-tests""; // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default""; // diff 2:; numSubmitAttempts = 1; // diff 3:; numCreateDefinitionAttempts = 1; default-runtime-attributes {; queueArn: ""arn:aws:batch:us-west-2:xxx:job-queue/GenomicsHighPriorityQue-xxx""; }; filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:2458,Modifiability,config,config,2458,"rom `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-another""; scheme = ""assume_role""; base-auth = ""default""; role-arn = ""arn:aws:iam::xxx:role/fbucketname""; }; ]; // diff 1:; # region = ""us-west-2"" // uses region from ~/.aws/config set by aws configure command,; # // or us-east-1 by default; }; engine {; filesystems {; s3 {; auth = ""assume-role-based-on-another""; }; }; }; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; // Base bucket for workflow executions; root = ""s3://bucketname/cromwell-tests""; // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default""; // diff 2:; numSubmitAttempts = 1; // diff 3:; numCreateDefinitionAttempts = 1; default-runtime-attributes {; queueArn: ""arn:aws:batch:us-west-2:xxx:job-queue/GenomicsHighPriorityQue-xxx""; }; filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:1395,Performance,Cache,Cache,1395,"g a workflow via the cromwell server we **consistently** see a failure to hash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-another""; scheme = ""assume_role""; base-auth = ""default""; role-arn = ""arn:aws:iam::xxx:role/fbucketname""; }; ]; // diff 1:; # region = ""us-west-2"" // uses region from ~/.aws/config set by aws configure command,; # // or us-east-1 by default; }; engine {; filesystems {; s3 {; auth = ""assume-role-based-on-another""; }; }; }; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cro",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:1574,Performance,cache,cache-results,1574," for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-another""; scheme = ""assume_role""; base-auth = ""default""; role-arn = ""arn:aws:iam::xxx:role/fbucketname""; }; ]; // diff 1:; # region = ""us-west-2"" // uses region from ~/.aws/config set by aws configure command,; # // or us-east-1 by default; }; engine {; filesystems {; s3 {; auth = ""assume-role-based-on-another""; }; }; }; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; // Base bucket for workflow executions; root = ""s3://bucketname/cromwell-tests""; // A referenc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:2808,Performance,queue,queueArn,2808,"rom `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-another""; scheme = ""assume_role""; base-auth = ""default""; role-arn = ""arn:aws:iam::xxx:role/fbucketname""; }; ]; // diff 1:; # region = ""us-west-2"" // uses region from ~/.aws/config set by aws configure command,; # // or us-east-1 by default; }; engine {; filesystems {; s3 {; auth = ""assume-role-based-on-another""; }; }; }; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; // Base bucket for workflow executions; root = ""s3://bucketname/cromwell-tests""; // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default""; // diff 2:; numSubmitAttempts = 1; // diff 3:; numCreateDefinitionAttempts = 1; default-runtime-attributes {; queueArn: ""arn:aws:batch:us-west-2:xxx:job-queue/GenomicsHighPriorityQue-xxx""; }; filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:2851,Performance,queue,queue,2851,"rom `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-another""; scheme = ""assume_role""; base-auth = ""default""; role-arn = ""arn:aws:iam::xxx:role/fbucketname""; }; ]; // diff 1:; # region = ""us-west-2"" // uses region from ~/.aws/config set by aws configure command,; # // or us-east-1 by default; }; engine {; filesystems {; s3 {; auth = ""assume-role-based-on-another""; }; }; }; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; // Base bucket for workflow executions; root = ""s3://bucketname/cromwell-tests""; // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default""; // diff 2:; numSubmitAttempts = 1; // diff 3:; numCreateDefinitionAttempts = 1; default-runtime-attributes {; queueArn: ""arn:aws:batch:us-west-2:xxx:job-queue/GenomicsHighPriorityQue-xxx""; }; filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:893,Safety,timeout,timeouts,893,"Backend: AWS Batch. Workflow: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/frankenstein.wdl. Input file: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/map-variantcall-hg38.json. Possibly related to #4412 but not sure as I don't see the same error message. When submitting a workflow via the cromwell server we **consistently** see a failure to hash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-anothe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:469,Security,hash,hash,469,"Backend: AWS Batch. Workflow: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/frankenstein.wdl. Input file: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/map-variantcall-hg38.json. Possibly related to #4412 but not sure as I don't see the same error message. When submitting a workflow via the cromwell server we **consistently** see a failure to hash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-anothe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:830,Security,hash,hash,830,"Backend: AWS Batch. Workflow: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/frankenstein.wdl. Input file: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/map-variantcall-hg38.json. Possibly related to #4412 but not sure as I don't see the same error message. When submitting a workflow via the cromwell server we **consistently** see a failure to hash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-anothe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:921,Security,hash,hashes,921,"Backend: AWS Batch. Workflow: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/frankenstein.wdl. Input file: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/map-variantcall-hg38.json. Possibly related to #4412 but not sure as I don't see the same error message. When submitting a workflow via the cromwell server we **consistently** see a failure to hash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-anothe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:1093,Security,hash,hashFailures,1093,"utch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/map-variantcall-hg38.json. Possibly related to #4412 but not sure as I don't see the same error message. When submitting a workflow via the cromwell server we **consistently** see a failure to hash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-another""; scheme = ""assume_role""; base-auth = ""default""; role-arn = ""arn:aws:iam::xxx:role/fbucketname""; }; ]; // diff 1:; # region = ""us-west-2"" // uses region from ~/.aws/config set by aws configu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:1143,Security,Hash,Hashing,1143,"utch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/map-variantcall-hg38.json. Possibly related to #4412 but not sure as I don't see the same error message. When submitting a workflow via the cromwell server we **consistently** see a failure to hash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-another""; scheme = ""assume_role""; base-auth = ""default""; role-arn = ""arn:aws:iam::xxx:role/fbucketname""; }; ]; // diff 1:; # region = ""us-west-2"" // uses region from ~/.aws/config set by aws configu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:1199,Testability,test,tests,1199,"utch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/map-variantcall-hg38.json. Possibly related to #4412 but not sure as I don't see the same error message. When submitting a workflow via the cromwell server we **consistently** see a failure to hash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-another""; scheme = ""assume_role""; base-auth = ""default""; role-arn = ""arn:aws:iam::xxx:role/fbucketname""; }; ]; // diff 1:; # region = ""us-west-2"" // uses region from ~/.aws/config set by aws configu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/issues/4563:2541,Testability,test,tests,2541,"rom `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-another""; scheme = ""assume_role""; base-auth = ""default""; role-arn = ""arn:aws:iam::xxx:role/fbucketname""; }; ]; // diff 1:; # region = ""us-west-2"" // uses region from ~/.aws/config set by aws configure command,; # // or us-east-1 by default; }; engine {; filesystems {; s3 {; auth = ""assume-role-based-on-another""; }; }; }; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; // Base bucket for workflow executions; root = ""s3://bucketname/cromwell-tests""; // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default""; // diff 2:; numSubmitAttempts = 1; // diff 3:; numCreateDefinitionAttempts = 1; default-runtime-attributes {; queueArn: ""arn:aws:batch:us-west-2:xxx:job-queue/GenomicsHighPriorityQue-xxx""; }; filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563
https://github.com/broadinstitute/cromwell/pull/4567:898,Availability,avail,available,898,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567
https://github.com/broadinstitute/cromwell/pull/4567:962,Availability,avail,available,962,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567
https://github.com/broadinstitute/cromwell/pull/4567:461,Integrability,message,message,461,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567
https://github.com/broadinstitute/cromwell/pull/4567:24,Modifiability,config,configurable,24,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567
https://github.com/broadinstitute/cromwell/pull/4567:381,Performance,queue,queues,381,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567
https://github.com/broadinstitute/cromwell/pull/4567:579,Performance,queue,queue,579,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567
https://github.com/broadinstitute/cromwell/pull/4567:635,Performance,queue,queues,635,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567
https://github.com/broadinstitute/cromwell/pull/4567:713,Performance,queue,queue,713,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567
https://github.com/broadinstitute/cromwell/pull/4567:731,Performance,queue,queue,731,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567
https://github.com/broadinstitute/cromwell/pull/4567:765,Performance,queue,queue,765,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567
https://github.com/broadinstitute/cromwell/pull/4567:809,Performance,queue,queue,809,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567
https://github.com/broadinstitute/cromwell/pull/4567:84,Safety,safe,safety,84,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567
https://github.com/broadinstitute/cromwell/pull/4567:50,Testability,log,log-interval-seconds,50,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567
https://github.com/broadinstitute/cromwell/pull/4567:103,Testability,Log,Logs,103,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567
https://github.com/broadinstitute/cromwell/pull/4567:171,Testability,log,log-interval-seconds,171,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567
https://github.com/broadinstitute/cromwell/pull/4567:219,Testability,Log,Logs,219,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567
https://github.com/broadinstitute/cromwell/pull/4567:289,Testability,log,log-interval-seconds,289,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567
https://github.com/broadinstitute/cromwell/pull/4567:335,Testability,Log,Logs,335,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567
https://github.com/broadinstitute/cromwell/pull/4567:418,Testability,log,log-interval-seconds,418,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567
https://github.com/broadinstitute/cromwell/pull/4567:457,Testability,log,log,457,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567
https://github.com/broadinstitute/cromwell/issues/4570:510,Availability,error,error,510,"(Probably related to #4081). . Using cromwell v36. I had the following code block in my wdl:. ```; call MergePileupSummaries as MergeTumorPileups {; input:; input_tables = TumorPileups.pileups,; output_name = , // <--- forgot to specify this input; ref_dict = ref_dict,; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible_attempts = preemptible_attempts,; max_retries = max_retries,; disk_space = ceil(SumSubVcfs.total_size * large_input_to_output_multiplier) + disk_pad; }; ```. Cromwell error didn't give me anything, and `womtool validate` threw an exception:. ```; tsato@gsa5:fc-read-orientation: java -jar womtool-36.jar validate mutect2.wdl; Exception in thread ""main"" scala.MatchError: null; 	at wdl.draft2.model.WdlExpression$.toString(WdlExpression.scala:117); 	at wdl.draft2.model.WdlExpression.toString(WdlExpression.scala:200); 	at wdl.draft2.model.WdlExpression.toWomString(WdlExpression.scala:203); 	at wom.values.WomValue.valueString(WomValue.scala:50); 	at wom.values.WomValue.valueString$(WomValue.scala:50); 	at wdl.draft2.model.WdlExpression.valueString(WdlExpression.scala:185); 	at wdl.draft2.model.WdlWomExpression.sourceString(WdlExpression.scala:219); 	at wom.graph.expression.ExpressionNode$.$anonfun$buildFromConstructor$4(ExpressionNode.scala:66); 	at cats.data.NonEmptyList.map(NonEmptyList.scala:76); 	at wom.graph.expression.ExpressionNode$.$anonfun$buildFromConstructor$3(ExpressionNode.scala:66); 	at cats.data.Validated.leftMap(Validated.scala:203); 	at wom.graph.expression.ExpressionNode$.buildFromConstructor(ExpressionNode.scala:66); 	at wom.graph.expression.AnonymousExpressionNode$.fromInputMapping(AnonymousExpressionNode.scala:17); 	at wdl.draft2.model.WdlWomExpression$.$anonfun$toAnonymousExpressionNode$1(WdlExpression.scala:293); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flat; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4570
https://github.com/broadinstitute/cromwell/issues/4570:1818,Availability,Error,ErrorOr,1818,"(Probably related to #4081). . Using cromwell v36. I had the following code block in my wdl:. ```; call MergePileupSummaries as MergeTumorPileups {; input:; input_tables = TumorPileups.pileups,; output_name = , // <--- forgot to specify this input; ref_dict = ref_dict,; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible_attempts = preemptible_attempts,; max_retries = max_retries,; disk_space = ceil(SumSubVcfs.total_size * large_input_to_output_multiplier) + disk_pad; }; ```. Cromwell error didn't give me anything, and `womtool validate` threw an exception:. ```; tsato@gsa5:fc-read-orientation: java -jar womtool-36.jar validate mutect2.wdl; Exception in thread ""main"" scala.MatchError: null; 	at wdl.draft2.model.WdlExpression$.toString(WdlExpression.scala:117); 	at wdl.draft2.model.WdlExpression.toString(WdlExpression.scala:200); 	at wdl.draft2.model.WdlExpression.toWomString(WdlExpression.scala:203); 	at wom.values.WomValue.valueString(WomValue.scala:50); 	at wom.values.WomValue.valueString$(WomValue.scala:50); 	at wdl.draft2.model.WdlExpression.valueString(WdlExpression.scala:185); 	at wdl.draft2.model.WdlWomExpression.sourceString(WdlExpression.scala:219); 	at wom.graph.expression.ExpressionNode$.$anonfun$buildFromConstructor$4(ExpressionNode.scala:66); 	at cats.data.NonEmptyList.map(NonEmptyList.scala:76); 	at wom.graph.expression.ExpressionNode$.$anonfun$buildFromConstructor$3(ExpressionNode.scala:66); 	at cats.data.Validated.leftMap(Validated.scala:203); 	at wom.graph.expression.ExpressionNode$.buildFromConstructor(ExpressionNode.scala:66); 	at wom.graph.expression.AnonymousExpressionNode$.fromInputMapping(AnonymousExpressionNode.scala:17); 	at wdl.draft2.model.WdlWomExpression$.$anonfun$toAnonymousExpressionNode$1(WdlExpression.scala:293); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flat; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4570
https://github.com/broadinstitute/cromwell/issues/4570:554,Security,validat,validate,554,"(Probably related to #4081). . Using cromwell v36. I had the following code block in my wdl:. ```; call MergePileupSummaries as MergeTumorPileups {; input:; input_tables = TumorPileups.pileups,; output_name = , // <--- forgot to specify this input; ref_dict = ref_dict,; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible_attempts = preemptible_attempts,; max_retries = max_retries,; disk_space = ceil(SumSubVcfs.total_size * large_input_to_output_multiplier) + disk_pad; }; ```. Cromwell error didn't give me anything, and `womtool validate` threw an exception:. ```; tsato@gsa5:fc-read-orientation: java -jar womtool-36.jar validate mutect2.wdl; Exception in thread ""main"" scala.MatchError: null; 	at wdl.draft2.model.WdlExpression$.toString(WdlExpression.scala:117); 	at wdl.draft2.model.WdlExpression.toString(WdlExpression.scala:200); 	at wdl.draft2.model.WdlExpression.toWomString(WdlExpression.scala:203); 	at wom.values.WomValue.valueString(WomValue.scala:50); 	at wom.values.WomValue.valueString$(WomValue.scala:50); 	at wdl.draft2.model.WdlExpression.valueString(WdlExpression.scala:185); 	at wdl.draft2.model.WdlWomExpression.sourceString(WdlExpression.scala:219); 	at wom.graph.expression.ExpressionNode$.$anonfun$buildFromConstructor$4(ExpressionNode.scala:66); 	at cats.data.NonEmptyList.map(NonEmptyList.scala:76); 	at wom.graph.expression.ExpressionNode$.$anonfun$buildFromConstructor$3(ExpressionNode.scala:66); 	at cats.data.Validated.leftMap(Validated.scala:203); 	at wom.graph.expression.ExpressionNode$.buildFromConstructor(ExpressionNode.scala:66); 	at wom.graph.expression.AnonymousExpressionNode$.fromInputMapping(AnonymousExpressionNode.scala:17); 	at wdl.draft2.model.WdlWomExpression$.$anonfun$toAnonymousExpressionNode$1(WdlExpression.scala:293); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flat; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4570
https://github.com/broadinstitute/cromwell/issues/4570:647,Security,validat,validate,647,"(Probably related to #4081). . Using cromwell v36. I had the following code block in my wdl:. ```; call MergePileupSummaries as MergeTumorPileups {; input:; input_tables = TumorPileups.pileups,; output_name = , // <--- forgot to specify this input; ref_dict = ref_dict,; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible_attempts = preemptible_attempts,; max_retries = max_retries,; disk_space = ceil(SumSubVcfs.total_size * large_input_to_output_multiplier) + disk_pad; }; ```. Cromwell error didn't give me anything, and `womtool validate` threw an exception:. ```; tsato@gsa5:fc-read-orientation: java -jar womtool-36.jar validate mutect2.wdl; Exception in thread ""main"" scala.MatchError: null; 	at wdl.draft2.model.WdlExpression$.toString(WdlExpression.scala:117); 	at wdl.draft2.model.WdlExpression.toString(WdlExpression.scala:200); 	at wdl.draft2.model.WdlExpression.toWomString(WdlExpression.scala:203); 	at wom.values.WomValue.valueString(WomValue.scala:50); 	at wom.values.WomValue.valueString$(WomValue.scala:50); 	at wdl.draft2.model.WdlExpression.valueString(WdlExpression.scala:185); 	at wdl.draft2.model.WdlWomExpression.sourceString(WdlExpression.scala:219); 	at wom.graph.expression.ExpressionNode$.$anonfun$buildFromConstructor$4(ExpressionNode.scala:66); 	at cats.data.NonEmptyList.map(NonEmptyList.scala:76); 	at wom.graph.expression.ExpressionNode$.$anonfun$buildFromConstructor$3(ExpressionNode.scala:66); 	at cats.data.Validated.leftMap(Validated.scala:203); 	at wom.graph.expression.ExpressionNode$.buildFromConstructor(ExpressionNode.scala:66); 	at wom.graph.expression.AnonymousExpressionNode$.fromInputMapping(AnonymousExpressionNode.scala:17); 	at wdl.draft2.model.WdlWomExpression$.$anonfun$toAnonymousExpressionNode$1(WdlExpression.scala:293); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flat; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4570
https://github.com/broadinstitute/cromwell/issues/4570:1464,Security,Validat,Validated,1464,"(Probably related to #4081). . Using cromwell v36. I had the following code block in my wdl:. ```; call MergePileupSummaries as MergeTumorPileups {; input:; input_tables = TumorPileups.pileups,; output_name = , // <--- forgot to specify this input; ref_dict = ref_dict,; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible_attempts = preemptible_attempts,; max_retries = max_retries,; disk_space = ceil(SumSubVcfs.total_size * large_input_to_output_multiplier) + disk_pad; }; ```. Cromwell error didn't give me anything, and `womtool validate` threw an exception:. ```; tsato@gsa5:fc-read-orientation: java -jar womtool-36.jar validate mutect2.wdl; Exception in thread ""main"" scala.MatchError: null; 	at wdl.draft2.model.WdlExpression$.toString(WdlExpression.scala:117); 	at wdl.draft2.model.WdlExpression.toString(WdlExpression.scala:200); 	at wdl.draft2.model.WdlExpression.toWomString(WdlExpression.scala:203); 	at wom.values.WomValue.valueString(WomValue.scala:50); 	at wom.values.WomValue.valueString$(WomValue.scala:50); 	at wdl.draft2.model.WdlExpression.valueString(WdlExpression.scala:185); 	at wdl.draft2.model.WdlWomExpression.sourceString(WdlExpression.scala:219); 	at wom.graph.expression.ExpressionNode$.$anonfun$buildFromConstructor$4(ExpressionNode.scala:66); 	at cats.data.NonEmptyList.map(NonEmptyList.scala:76); 	at wom.graph.expression.ExpressionNode$.$anonfun$buildFromConstructor$3(ExpressionNode.scala:66); 	at cats.data.Validated.leftMap(Validated.scala:203); 	at wom.graph.expression.ExpressionNode$.buildFromConstructor(ExpressionNode.scala:66); 	at wom.graph.expression.AnonymousExpressionNode$.fromInputMapping(AnonymousExpressionNode.scala:17); 	at wdl.draft2.model.WdlWomExpression$.$anonfun$toAnonymousExpressionNode$1(WdlExpression.scala:293); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flat; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4570
https://github.com/broadinstitute/cromwell/issues/4570:1482,Security,Validat,Validated,1482,"(Probably related to #4081). . Using cromwell v36. I had the following code block in my wdl:. ```; call MergePileupSummaries as MergeTumorPileups {; input:; input_tables = TumorPileups.pileups,; output_name = , // <--- forgot to specify this input; ref_dict = ref_dict,; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible_attempts = preemptible_attempts,; max_retries = max_retries,; disk_space = ceil(SumSubVcfs.total_size * large_input_to_output_multiplier) + disk_pad; }; ```. Cromwell error didn't give me anything, and `womtool validate` threw an exception:. ```; tsato@gsa5:fc-read-orientation: java -jar womtool-36.jar validate mutect2.wdl; Exception in thread ""main"" scala.MatchError: null; 	at wdl.draft2.model.WdlExpression$.toString(WdlExpression.scala:117); 	at wdl.draft2.model.WdlExpression.toString(WdlExpression.scala:200); 	at wdl.draft2.model.WdlExpression.toWomString(WdlExpression.scala:203); 	at wom.values.WomValue.valueString(WomValue.scala:50); 	at wom.values.WomValue.valueString$(WomValue.scala:50); 	at wdl.draft2.model.WdlExpression.valueString(WdlExpression.scala:185); 	at wdl.draft2.model.WdlWomExpression.sourceString(WdlExpression.scala:219); 	at wom.graph.expression.ExpressionNode$.$anonfun$buildFromConstructor$4(ExpressionNode.scala:66); 	at cats.data.NonEmptyList.map(NonEmptyList.scala:76); 	at wom.graph.expression.ExpressionNode$.$anonfun$buildFromConstructor$3(ExpressionNode.scala:66); 	at cats.data.Validated.leftMap(Validated.scala:203); 	at wom.graph.expression.ExpressionNode$.buildFromConstructor(ExpressionNode.scala:66); 	at wom.graph.expression.AnonymousExpressionNode$.fromInputMapping(AnonymousExpressionNode.scala:17); 	at wdl.draft2.model.WdlWomExpression$.$anonfun$toAnonymousExpressionNode$1(WdlExpression.scala:293); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flat; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4570
https://github.com/broadinstitute/cromwell/issues/4570:1807,Security,validat,validation,1807,"(Probably related to #4081). . Using cromwell v36. I had the following code block in my wdl:. ```; call MergePileupSummaries as MergeTumorPileups {; input:; input_tables = TumorPileups.pileups,; output_name = , // <--- forgot to specify this input; ref_dict = ref_dict,; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible_attempts = preemptible_attempts,; max_retries = max_retries,; disk_space = ceil(SumSubVcfs.total_size * large_input_to_output_multiplier) + disk_pad; }; ```. Cromwell error didn't give me anything, and `womtool validate` threw an exception:. ```; tsato@gsa5:fc-read-orientation: java -jar womtool-36.jar validate mutect2.wdl; Exception in thread ""main"" scala.MatchError: null; 	at wdl.draft2.model.WdlExpression$.toString(WdlExpression.scala:117); 	at wdl.draft2.model.WdlExpression.toString(WdlExpression.scala:200); 	at wdl.draft2.model.WdlExpression.toWomString(WdlExpression.scala:203); 	at wom.values.WomValue.valueString(WomValue.scala:50); 	at wom.values.WomValue.valueString$(WomValue.scala:50); 	at wdl.draft2.model.WdlExpression.valueString(WdlExpression.scala:185); 	at wdl.draft2.model.WdlWomExpression.sourceString(WdlExpression.scala:219); 	at wom.graph.expression.ExpressionNode$.$anonfun$buildFromConstructor$4(ExpressionNode.scala:66); 	at cats.data.NonEmptyList.map(NonEmptyList.scala:76); 	at wom.graph.expression.ExpressionNode$.$anonfun$buildFromConstructor$3(ExpressionNode.scala:66); 	at cats.data.Validated.leftMap(Validated.scala:203); 	at wom.graph.expression.ExpressionNode$.buildFromConstructor(ExpressionNode.scala:66); 	at wom.graph.expression.AnonymousExpressionNode$.fromInputMapping(AnonymousExpressionNode.scala:17); 	at wdl.draft2.model.WdlWomExpression$.$anonfun$toAnonymousExpressionNode$1(WdlExpression.scala:293); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flat; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4570
https://github.com/broadinstitute/cromwell/pull/4571:231,Usability,undo,undocumented,231,"Closes #4452 ; Closes #4461 . The `images`, `importedDescriptorTypes`, `meta`, and `parameter_meta` fields are not yet implemented - I felt I should land this chunk before piling on even more code. Right now, those four fields are undocumented in Swagger and show up empty in the response. I understand that it is generally accepted that returning additional fields beyond those specified does not harm sensibly-programmed clients.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4571
https://github.com/broadinstitute/cromwell/issues/4573:841,Performance,throughput,throughput,841,"Use [Apache bench](https://httpd.apache.org/docs/2.4/programs/ab.html) or similar to measure how many requests per second we can handle. To submit the multi-part form, use the `-p` option for a post data file, example contents below. You can generate your own by capturing the network requests Swagger makes.; ```; ------WebKitFormBoundaryRwwHyBadF8e8Mwbp; Content-Disposition: form-data; name=""workflowUrl"". https://firecloud-orchestration.dsde-alpha.broadinstitute.org/ga4gh/v1/tools/anichols:cnv_somatic_pair_workflow/versions/1/plain-WDL/descriptor; ------WebKitFormBoundaryRwwHyBadF8e8Mwbp; Content-Disposition: form-data; name=""workflowType"". WDL; ------WebKitFormBoundaryRwwHyBadF8e8Mwbp; Content-Disposition: form-data; name=""workflowTypeVersion"". draft-2; ------WebKitFormBoundaryRwwHyBadF8e8Mwbp--; ```; In my preliminary testing, throughput was about 15 validations per second of the workflow above. I had `ab` make multiple requests simultaneously with the `-c` option (don't remember the concurrency level though). N.B. the above us a pretty tough scenario - workflow by URL with many HTTP imports. Our production use case uses files, not URLs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4573
https://github.com/broadinstitute/cromwell/issues/4573:1001,Performance,concurren,concurrency,1001,"Use [Apache bench](https://httpd.apache.org/docs/2.4/programs/ab.html) or similar to measure how many requests per second we can handle. To submit the multi-part form, use the `-p` option for a post data file, example contents below. You can generate your own by capturing the network requests Swagger makes.; ```; ------WebKitFormBoundaryRwwHyBadF8e8Mwbp; Content-Disposition: form-data; name=""workflowUrl"". https://firecloud-orchestration.dsde-alpha.broadinstitute.org/ga4gh/v1/tools/anichols:cnv_somatic_pair_workflow/versions/1/plain-WDL/descriptor; ------WebKitFormBoundaryRwwHyBadF8e8Mwbp; Content-Disposition: form-data; name=""workflowType"". WDL; ------WebKitFormBoundaryRwwHyBadF8e8Mwbp; Content-Disposition: form-data; name=""workflowTypeVersion"". draft-2; ------WebKitFormBoundaryRwwHyBadF8e8Mwbp--; ```; In my preliminary testing, throughput was about 15 validations per second of the workflow above. I had `ab` make multiple requests simultaneously with the `-c` option (don't remember the concurrency level though). N.B. the above us a pretty tough scenario - workflow by URL with many HTTP imports. Our production use case uses files, not URLs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4573
https://github.com/broadinstitute/cromwell/issues/4573:865,Security,validat,validations,865,"Use [Apache bench](https://httpd.apache.org/docs/2.4/programs/ab.html) or similar to measure how many requests per second we can handle. To submit the multi-part form, use the `-p` option for a post data file, example contents below. You can generate your own by capturing the network requests Swagger makes.; ```; ------WebKitFormBoundaryRwwHyBadF8e8Mwbp; Content-Disposition: form-data; name=""workflowUrl"". https://firecloud-orchestration.dsde-alpha.broadinstitute.org/ga4gh/v1/tools/anichols:cnv_somatic_pair_workflow/versions/1/plain-WDL/descriptor; ------WebKitFormBoundaryRwwHyBadF8e8Mwbp; Content-Disposition: form-data; name=""workflowType"". WDL; ------WebKitFormBoundaryRwwHyBadF8e8Mwbp; Content-Disposition: form-data; name=""workflowTypeVersion"". draft-2; ------WebKitFormBoundaryRwwHyBadF8e8Mwbp--; ```; In my preliminary testing, throughput was about 15 validations per second of the workflow above. I had `ab` make multiple requests simultaneously with the `-c` option (don't remember the concurrency level though). N.B. the above us a pretty tough scenario - workflow by URL with many HTTP imports. Our production use case uses files, not URLs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4573
https://github.com/broadinstitute/cromwell/issues/4573:832,Testability,test,testing,832,"Use [Apache bench](https://httpd.apache.org/docs/2.4/programs/ab.html) or similar to measure how many requests per second we can handle. To submit the multi-part form, use the `-p` option for a post data file, example contents below. You can generate your own by capturing the network requests Swagger makes.; ```; ------WebKitFormBoundaryRwwHyBadF8e8Mwbp; Content-Disposition: form-data; name=""workflowUrl"". https://firecloud-orchestration.dsde-alpha.broadinstitute.org/ga4gh/v1/tools/anichols:cnv_somatic_pair_workflow/versions/1/plain-WDL/descriptor; ------WebKitFormBoundaryRwwHyBadF8e8Mwbp; Content-Disposition: form-data; name=""workflowType"". WDL; ------WebKitFormBoundaryRwwHyBadF8e8Mwbp; Content-Disposition: form-data; name=""workflowTypeVersion"". draft-2; ------WebKitFormBoundaryRwwHyBadF8e8Mwbp--; ```; In my preliminary testing, throughput was about 15 validations per second of the workflow above. I had `ab` make multiple requests simultaneously with the `-c` option (don't remember the concurrency level though). N.B. the above us a pretty tough scenario - workflow by URL with many HTTP imports. Our production use case uses files, not URLs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4573
https://github.com/broadinstitute/cromwell/issues/4577:290,Availability,error,error,290,"Cromwell: Development (`37-3b2affa`); Backend: HPC (`ConfigBackendLifecycleActorFactory`). I wanted access to a recently merged pull request (#4437), so I built a development version of Cromwell. However, when I run it with the same configuration file as I used for Cromwell 36, I get this error:; ```; [ERROR] [01/24/2019 11:10:24.126] [cromwell-system-akka.actor.default-dispatcher-4] [akka://cromwell-system/user/SingleWorkflowRunnerActor] No configuration setting found for key 'services' ; akka.actor.ActorInitializationException: akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:304,Availability,ERROR,ERROR,304,"Cromwell: Development (`37-3b2affa`); Backend: HPC (`ConfigBackendLifecycleActorFactory`). I wanted access to a recently merged pull request (#4437), so I built a development version of Cromwell. However, when I run it with the same configuration file as I used for Cromwell 36, I get this error:; ```; [ERROR] [01/24/2019 11:10:24.126] [cromwell-system-akka.actor.default-dispatcher-4] [akka://cromwell-system/user/SingleWorkflowRunnerActor] No configuration setting found for key 'services' ; akka.actor.ActorInitializationException: akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:2697,Availability,ERROR,ERROR,2697,"com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleAc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:3039,Availability,ERROR,ERROR,3039,"5) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Sin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:4371,Availability,alive,alive,4371,"ld probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Singularity/2.5.0-intel-2017.u2 || true; singularity pull docker://${docker}; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec -B ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}"" ; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Here's what I added to my config for 37 that causes the missing class errors:; ```; services { ; MetadataService { ; class = ""cromwell.services.metadata.impl.MetadataServiceActor""; }; } ; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:4536,Availability,error,errors,4536,"ld probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Singularity/2.5.0-intel-2017.u2 || true; singularity pull docker://${docker}; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec -B ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}"" ; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Here's what I added to my config for 37 that causes the missing class errors:; ```; services { ; MetadataService { ; class = ""cromwell.services.metadata.impl.MetadataServiceActor""; }; } ; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:233,Deployability,configurat,configuration,233,"Cromwell: Development (`37-3b2affa`); Backend: HPC (`ConfigBackendLifecycleActorFactory`). I wanted access to a recently merged pull request (#4437), so I built a development version of Cromwell. However, when I run it with the same configuration file as I used for Cromwell 36, I get this error:; ```; [ERROR] [01/24/2019 11:10:24.126] [cromwell-system-akka.actor.default-dispatcher-4] [akka://cromwell-system/user/SingleWorkflowRunnerActor] No configuration setting found for key 'services' ; akka.actor.ActorInitializationException: akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:446,Deployability,configurat,configuration,446,"Cromwell: Development (`37-3b2affa`); Backend: HPC (`ConfigBackendLifecycleActorFactory`). I wanted access to a recently merged pull request (#4437), so I built a development version of Cromwell. However, when I run it with the same configuration file as I used for Cromwell 36, I get this error:; ```; [ERROR] [01/24/2019 11:10:24.126] [cromwell-system-akka.actor.default-dispatcher-4] [akka://cromwell-system/user/SingleWorkflowRunnerActor] No configuration setting found for key 'services' ; akka.actor.ActorInitializationException: akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:1426,Deployability,configurat,configuration,1426,r] No configuration setting found for key 'services' ; akka.actor.ActorInitializationException: akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:2970,Integrability,Message,Message,2970,"5) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Sin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:3313,Integrability,Message,Message,3313,".actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Singularity/2.5.0-intel-2017.u2 || true; singularity pull docker://${docker}; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec -B ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}"" ; """""". kill = ""scancel ${job_id}""; check-alive =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:3949,Integrability,wrap,wrap,3949,"ld probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Singularity/2.5.0-intel-2017.u2 || true; singularity pull docker://${docker}; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec -B ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}"" ; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Here's what I added to my config for 37 that causes the missing class errors:; ```; services { ; MetadataService { ; class = ""cromwell.services.metadata.impl.MetadataServiceActor""; }; } ; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:4240,Integrability,wrap,wrap,4240,"ld probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Singularity/2.5.0-intel-2017.u2 || true; singularity pull docker://${docker}; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec -B ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}"" ; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Here's what I added to my config for 37 that causes the missing class errors:; ```; services { ; MetadataService { ; class = ""cromwell.services.metadata.impl.MetadataServiceActor""; }; } ; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:53,Modifiability,Config,ConfigBackendLifecycleActorFactory,53,"Cromwell: Development (`37-3b2affa`); Backend: HPC (`ConfigBackendLifecycleActorFactory`). I wanted access to a recently merged pull request (#4437), so I built a development version of Cromwell. However, when I run it with the same configuration file as I used for Cromwell 36, I get this error:; ```; [ERROR] [01/24/2019 11:10:24.126] [cromwell-system-akka.actor.default-dispatcher-4] [akka://cromwell-system/user/SingleWorkflowRunnerActor] No configuration setting found for key 'services' ; akka.actor.ActorInitializationException: akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:233,Modifiability,config,configuration,233,"Cromwell: Development (`37-3b2affa`); Backend: HPC (`ConfigBackendLifecycleActorFactory`). I wanted access to a recently merged pull request (#4437), so I built a development version of Cromwell. However, when I run it with the same configuration file as I used for Cromwell 36, I get this error:; ```; [ERROR] [01/24/2019 11:10:24.126] [cromwell-system-akka.actor.default-dispatcher-4] [akka://cromwell-system/user/SingleWorkflowRunnerActor] No configuration setting found for key 'services' ; akka.actor.ActorInitializationException: akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:446,Modifiability,config,configuration,446,"Cromwell: Development (`37-3b2affa`); Backend: HPC (`ConfigBackendLifecycleActorFactory`). I wanted access to a recently merged pull request (#4437), so I built a development version of Cromwell. However, when I run it with the same configuration file as I used for Cromwell 36, I get this error:; ```; [ERROR] [01/24/2019 11:10:24.126] [cromwell-system-akka.actor.default-dispatcher-4] [akka://cromwell-system/user/SingleWorkflowRunnerActor] No configuration setting found for key 'services' ; akka.actor.ActorInitializationException: akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:1391,Modifiability,config,config,1391,/cromwell-system/user/SingleWorkflowRunnerActor] No configuration setting found for key 'services' ; akka.actor.ActorInitializationException: akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newA,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:1398,Modifiability,Config,ConfigException,1398,r] No configuration setting found for key 'services' ; akka.actor.ActorInitializationException: akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:1426,Modifiability,config,configuration,1426,r] No configuration setting found for key 'services' ; akka.actor.ActorInitializationException: akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:1491,Modifiability,config,config,1491, akka.actor.ActorInitializationException: akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorC,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:1571,Modifiability,config,config,1571,flowRunnerActor/ServiceRegistryActor: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` sta,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:1648,Modifiability,config,config,1648,"or.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:1719,Modifiability,config,config,1719,".ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:1790,Modifiability,config,config,1790,"All$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka:/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:1866,Modifiability,config,config,1866,"scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Receiv",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:2958,Modifiability,config,configured,2958,"config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:3301,Modifiability,config,configured,3301,"ices.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Singularity/2.5.0-intel-2017.u2 || true; singularity pull docker://${docker}; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:3460,Modifiability,config,config,3460,"rCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Singularity/2.5.0-intel-2017.u2 || true; singularity pull docker://${docker}; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec -B ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}"" ; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Here's what I added to my c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:3605,Modifiability,config,config,3605," the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Singularity/2.5.0-intel-2017.u2 || true; singularity pull docker://${docker}; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec -B ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}"" ; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Here's what I added to my config for 37 that causes the missing class errors:; ```; services { ; MetadataService { ; class = ""cromwell.services",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:3612,Modifiability,Config,ConfigBackendLifecycleActorFactory,3612,"ld probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Singularity/2.5.0-intel-2017.u2 || true; singularity pull docker://${docker}; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec -B ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}"" ; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Here's what I added to my config for 37 that causes the missing class errors:; ```; services { ; MetadataService { ; class = ""cromwell.services.metadata.impl.MetadataServiceActor""; }; } ; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:3649,Modifiability,config,config,3649,"ld probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Singularity/2.5.0-intel-2017.u2 || true; singularity pull docker://${docker}; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec -B ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}"" ; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Here's what I added to my config for 37 that causes the missing class errors:; ```; services { ; MetadataService { ; class = ""cromwell.services.metadata.impl.MetadataServiceActor""; }; } ; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:4492,Modifiability,config,config,4492,"ld probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Singularity/2.5.0-intel-2017.u2 || true; singularity pull docker://${docker}; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec -B ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}"" ; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Here's what I added to my config for 37 that causes the missing class errors:; ```; services { ; MetadataService { ; class = ""cromwell.services.metadata.impl.MetadataServiceActor""; }; } ; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:2918,Performance,Load,LoadController,2918,"config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:2979,Performance,Load,LoadMetric,2979,"5) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Sin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:3427,Performance,queue,queue,3427,".actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Singularity/2.5.0-intel-2017.u2 || true; singularity pull docker://${docker}; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec -B ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}"" ; """""". kill = ""scancel ${job_id}""; check-alive =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:4010,Performance,load,load,4010,"ld probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Singularity/2.5.0-intel-2017.u2 || true; singularity pull docker://${docker}; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec -B ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}"" ; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Here's what I added to my config for 37 that causes the missing class errors:; ```; services { ; MetadataService { ; class = ""cromwell.services.metadata.impl.MetadataServiceActor""; }; } ; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:100,Security,access,access,100,"Cromwell: Development (`37-3b2affa`); Backend: HPC (`ConfigBackendLifecycleActorFactory`). I wanted access to a recently merged pull request (#4437), so I built a development version of Cromwell. However, when I run it with the same configuration file as I used for Cromwell 36, I get this error:; ```; [ERROR] [01/24/2019 11:10:24.126] [cromwell-system-akka.actor.default-dispatcher-4] [akka://cromwell-system/user/SingleWorkflowRunnerActor] No configuration setting found for key 'services' ; akka.actor.ActorInitializationException: akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:1503,Usability,Simpl,SimpleConfig,1503,orInitializationException: akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(Acto,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:1530,Usability,Simpl,SimpleConfig,1530,ption: akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; .,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:1583,Usability,Simpl,SimpleConfig,1583,"/ServiceRegistryActor: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:1607,Usability,Simpl,SimpleConfig,1607,"or: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:1660,Usability,Simpl,SimpleConfig,1660,"izationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probabl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:1678,Usability,Simpl,SimpleConfig,1678,"$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:1731,Usability,Simpl,SimpleConfig,1731,"te(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwel",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:1749,Usability,Simpl,SimpleConfig,1749,"la:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:1802,Usability,Simpl,SimpleConfig,1802,".scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-syste",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:1825,Usability,Simpl,SimpleConfig,1825,"kka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:1878,Usability,Simpl,SimpleConfig,1878," akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegis",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4577:1901,Usability,Simpl,SimpleConfig,1901,"lbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577
https://github.com/broadinstitute/cromwell/issues/4579:312,Availability,down,downloading,312,"Hi, ; To use Cromwell in AWS, may I have my own mount point?; If I use the following in my WDL file, I will got two mount points. But the problem is I can't provide my own Source Path. Cromwell will generate one for test1. If we can provide own source path, then we can use our reference files in EFS instead of downloading them each time from S3.; runtime {; docker: ""ubuntu:latest""; disks: ""local-disk, test1""; }. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4579
https://github.com/broadinstitute/cromwell/pull/4582:116,Modifiability,variab,variable,116,....by using a Travis [encryption key](https://docs.travis-ci.com/user/encryption-keys/) rather than an environment variable. Results: https://travis-ci.com/broadinstitute/cromwell/builds/98563607,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4582
https://github.com/broadinstitute/cromwell/pull/4582:23,Security,encrypt,encryption,23,....by using a Travis [encryption key](https://docs.travis-ci.com/user/encryption-keys/) rather than an environment variable. Results: https://travis-ci.com/broadinstitute/cromwell/builds/98563607,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4582
https://github.com/broadinstitute/cromwell/pull/4582:71,Security,encrypt,encryption-keys,71,....by using a Travis [encryption key](https://docs.travis-ci.com/user/encryption-keys/) rather than an environment variable. Results: https://travis-ci.com/broadinstitute/cromwell/builds/98563607,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4582
https://github.com/broadinstitute/cromwell/issues/4586:1418,Availability,error,error,1418,"chols. The workflow is a small test with everything in an S3 bucket:; ; https://github.com/bcbio/test_bcbio_cwl/tree/master/aws. I'm happy to report that I made good progress and have bcbio-vm using CloudFormation templates to setup the Cromwell batch ready AMI and AWS Batch requirements. I can then generate the right Cromwell AWS configuration and launch jobs to AWS batch. I see them get submitted, EC2 resources get spun up and jobs get queued and run. Awesome. When they're all ready and prepped to run, the instances fail with not finding the `cwl.inputs.json` file staged into the working directory:; ```; [2019-01-25 13:53:43,03] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10prep_samples_to_rec:NA:1]: Status change from Initializing to Running; [2019-01-25 13:53:59,61] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10alignment_to_rec:NA:1]: Status change from Initializing to Running; [2019-01-25 13:58:25,58] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10alignment_to_rec:NA:1]: Status change from Running to Failed; [2019-01-25 13:58:39,11] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10prep_samples_to_rec:NA:1]: Status change from Running to Failed; [2019-01-25 13:58:40,06] [error] WorkflowManagerActor Workflow 2c2e5a10-8c57-4f9f-8d80-c2fccacbb452 failed (during ExecutingWorkflowState): Job alignment_to_rec:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-alignment_to_rec/alignment_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 48, in process; fnargs, parallel, out",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586
https://github.com/broadinstitute/cromwell/issues/4586:4512,Availability,error,error,4512,"ormation: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-prep_samples_to_rec/prep_samples_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 48, in process; fnargs, parallel, out_keys, input_files = _world_from_cwl(args.name, fnargs[1:], work_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 185, in _world_from_cwl; assert os.path.exists(os.path.join(work_dir, ""cwl.inputs.json"")); AssertionError. [2019-01-25 13:58:40,07] [info] WorkflowManagerActor WorkflowActor-2c2e5a10-8c57-4f9f-8d80-c2fccacbb452 is in a terminal state: WorkflowFailedState; [2019-01-25 13:58:59,66] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-01-25 13:59:03,42] [info] SingleWorkflowRunnerActor writing metadata to /home/chapmanb/drive/work/cwl/test_bcbio_cwl/aws/cromwell_work/somatic-metadata.json; ```; I can see this file in the s3 bucket, although it's not in the `execution` directory which is normally where things get staged (at least on local runs):; ```; $ aws s3 ls s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-prep_samples_to_rec/; PRE glob-b34dfc006a981a93d6da067cf50036fe/; 2019-01-25 13:51:55 0; 2019-01-25 13:51:59 6059 cwl.inputs.json; 2019-01-25 13:58:01 0 glob-b34dfc006a981a93d6da067cf50036fe.list; 2019-01-25 13:58:01 2 prep_samples_to_rec-rc.txt; 2019-01-25 13:58:40 571 prep_samples_to_rec-stderr.log; 2019-01-25 13:58:02 0 prep_samples_to_rec-stdout.log; ```; Does this error look familiar to anyone?. I'm excited to be making progress with this and will also work on writing up and documenting the setup and run process so far. Thanks for any suggestions or pointers.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586
https://github.com/broadinstitute/cromwell/issues/4586:529,Deployability,configurat,configuration,529,"I've been working on testing the AWS Batch Cromwell support, following documentation from @wleepang (https://docs.opendata.aws/genomics-workflows) in the CWL hackathon with @cjllanwarne and @aednichols. The workflow is a small test with everything in an S3 bucket:; ; https://github.com/bcbio/test_bcbio_cwl/tree/master/aws. I'm happy to report that I made good progress and have bcbio-vm using CloudFormation templates to setup the Cromwell batch ready AMI and AWS Batch requirements. I can then generate the right Cromwell AWS configuration and launch jobs to AWS batch. I see them get submitted, EC2 resources get spun up and jobs get queued and run. Awesome. When they're all ready and prepped to run, the instances fail with not finding the `cwl.inputs.json` file staged into the working directory:; ```; [2019-01-25 13:53:43,03] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10prep_samples_to_rec:NA:1]: Status change from Initializing to Running; [2019-01-25 13:53:59,61] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10alignment_to_rec:NA:1]: Status change from Initializing to Running; [2019-01-25 13:58:25,58] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10alignment_to_rec:NA:1]: Status change from Running to Failed; [2019-01-25 13:58:39,11] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10prep_samples_to_rec:NA:1]: Status change from Running to Failed; [2019-01-25 13:58:40,06] [error] WorkflowManagerActor Workflow 2c2e5a10-8c57-4f9f-8d80-c2fccacbb452 failed (during ExecutingWorkflowState): Job alignment_to_rec:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-alignment_to_rec/alignment_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", lin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586
https://github.com/broadinstitute/cromwell/issues/4586:529,Modifiability,config,configuration,529,"I've been working on testing the AWS Batch Cromwell support, following documentation from @wleepang (https://docs.opendata.aws/genomics-workflows) in the CWL hackathon with @cjllanwarne and @aednichols. The workflow is a small test with everything in an S3 bucket:; ; https://github.com/bcbio/test_bcbio_cwl/tree/master/aws. I'm happy to report that I made good progress and have bcbio-vm using CloudFormation templates to setup the Cromwell batch ready AMI and AWS Batch requirements. I can then generate the right Cromwell AWS configuration and launch jobs to AWS batch. I see them get submitted, EC2 resources get spun up and jobs get queued and run. Awesome. When they're all ready and prepped to run, the instances fail with not finding the `cwl.inputs.json` file staged into the working directory:; ```; [2019-01-25 13:53:43,03] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10prep_samples_to_rec:NA:1]: Status change from Initializing to Running; [2019-01-25 13:53:59,61] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10alignment_to_rec:NA:1]: Status change from Initializing to Running; [2019-01-25 13:58:25,58] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10alignment_to_rec:NA:1]: Status change from Running to Failed; [2019-01-25 13:58:39,11] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10prep_samples_to_rec:NA:1]: Status change from Running to Failed; [2019-01-25 13:58:40,06] [error] WorkflowManagerActor Workflow 2c2e5a10-8c57-4f9f-8d80-c2fccacbb452 failed (during ExecutingWorkflowState): Job alignment_to_rec:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-alignment_to_rec/alignment_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", lin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586
https://github.com/broadinstitute/cromwell/issues/4586:638,Performance,queue,queued,638,"I've been working on testing the AWS Batch Cromwell support, following documentation from @wleepang (https://docs.opendata.aws/genomics-workflows) in the CWL hackathon with @cjllanwarne and @aednichols. The workflow is a small test with everything in an S3 bucket:; ; https://github.com/bcbio/test_bcbio_cwl/tree/master/aws. I'm happy to report that I made good progress and have bcbio-vm using CloudFormation templates to setup the Cromwell batch ready AMI and AWS Batch requirements. I can then generate the right Cromwell AWS configuration and launch jobs to AWS batch. I see them get submitted, EC2 resources get spun up and jobs get queued and run. Awesome. When they're all ready and prepped to run, the instances fail with not finding the `cwl.inputs.json` file staged into the working directory:; ```; [2019-01-25 13:53:43,03] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10prep_samples_to_rec:NA:1]: Status change from Initializing to Running; [2019-01-25 13:53:59,61] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10alignment_to_rec:NA:1]: Status change from Initializing to Running; [2019-01-25 13:58:25,58] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10alignment_to_rec:NA:1]: Status change from Running to Failed; [2019-01-25 13:58:39,11] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10prep_samples_to_rec:NA:1]: Status change from Running to Failed; [2019-01-25 13:58:40,06] [error] WorkflowManagerActor Workflow 2c2e5a10-8c57-4f9f-8d80-c2fccacbb452 failed (during ExecutingWorkflowState): Job alignment_to_rec:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-alignment_to_rec/alignment_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", lin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586
https://github.com/broadinstitute/cromwell/issues/4586:21,Testability,test,testing,21,"I've been working on testing the AWS Batch Cromwell support, following documentation from @wleepang (https://docs.opendata.aws/genomics-workflows) in the CWL hackathon with @cjllanwarne and @aednichols. The workflow is a small test with everything in an S3 bucket:; ; https://github.com/bcbio/test_bcbio_cwl/tree/master/aws. I'm happy to report that I made good progress and have bcbio-vm using CloudFormation templates to setup the Cromwell batch ready AMI and AWS Batch requirements. I can then generate the right Cromwell AWS configuration and launch jobs to AWS batch. I see them get submitted, EC2 resources get spun up and jobs get queued and run. Awesome. When they're all ready and prepped to run, the instances fail with not finding the `cwl.inputs.json` file staged into the working directory:; ```; [2019-01-25 13:53:43,03] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10prep_samples_to_rec:NA:1]: Status change from Initializing to Running; [2019-01-25 13:53:59,61] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10alignment_to_rec:NA:1]: Status change from Initializing to Running; [2019-01-25 13:58:25,58] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10alignment_to_rec:NA:1]: Status change from Running to Failed; [2019-01-25 13:58:39,11] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10prep_samples_to_rec:NA:1]: Status change from Running to Failed; [2019-01-25 13:58:40,06] [error] WorkflowManagerActor Workflow 2c2e5a10-8c57-4f9f-8d80-c2fccacbb452 failed (during ExecutingWorkflowState): Job alignment_to_rec:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-alignment_to_rec/alignment_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", lin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586
https://github.com/broadinstitute/cromwell/issues/4586:227,Testability,test,test,227,"I've been working on testing the AWS Batch Cromwell support, following documentation from @wleepang (https://docs.opendata.aws/genomics-workflows) in the CWL hackathon with @cjllanwarne and @aednichols. The workflow is a small test with everything in an S3 bucket:; ; https://github.com/bcbio/test_bcbio_cwl/tree/master/aws. I'm happy to report that I made good progress and have bcbio-vm using CloudFormation templates to setup the Cromwell batch ready AMI and AWS Batch requirements. I can then generate the right Cromwell AWS configuration and launch jobs to AWS batch. I see them get submitted, EC2 resources get spun up and jobs get queued and run. Awesome. When they're all ready and prepped to run, the instances fail with not finding the `cwl.inputs.json` file staged into the working directory:; ```; [2019-01-25 13:53:43,03] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10prep_samples_to_rec:NA:1]: Status change from Initializing to Running; [2019-01-25 13:53:59,61] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10alignment_to_rec:NA:1]: Status change from Initializing to Running; [2019-01-25 13:58:25,58] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10alignment_to_rec:NA:1]: Status change from Running to Failed; [2019-01-25 13:58:39,11] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10prep_samples_to_rec:NA:1]: Status change from Running to Failed; [2019-01-25 13:58:40,06] [error] WorkflowManagerActor Workflow 2c2e5a10-8c57-4f9f-8d80-c2fccacbb452 failed (during ExecutingWorkflowState): Job alignment_to_rec:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-alignment_to_rec/alignment_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", lin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586
https://github.com/broadinstitute/cromwell/issues/4586:1792,Testability,test,test,1792," file staged into the working directory:; ```; [2019-01-25 13:53:43,03] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10prep_samples_to_rec:NA:1]: Status change from Initializing to Running; [2019-01-25 13:53:59,61] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10alignment_to_rec:NA:1]: Status change from Initializing to Running; [2019-01-25 13:58:25,58] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10alignment_to_rec:NA:1]: Status change from Running to Failed; [2019-01-25 13:58:39,11] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10prep_samples_to_rec:NA:1]: Status change from Running to Failed; [2019-01-25 13:58:40,06] [error] WorkflowManagerActor Workflow 2c2e5a10-8c57-4f9f-8d80-c2fccacbb452 failed (during ExecutingWorkflowState): Job alignment_to_rec:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-alignment_to_rec/alignment_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 48, in process; fnargs, parallel, out_keys, input_files = _world_from_cwl(args.name, fnargs[1:], work_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 185, in _world_from_cwl; assert os.path.exists(os.path.join(work_dir, ""cwl.inputs.json"")); AssertionError. Job prep_samples_to_rec:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: s3://bcbio-batch-cromwell-test/cromwell-exec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586
https://github.com/broadinstitute/cromwell/issues/4586:1916,Testability,log,log,1916,"atus change from Initializing to Running; [2019-01-25 13:53:59,61] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10alignment_to_rec:NA:1]: Status change from Initializing to Running; [2019-01-25 13:58:25,58] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10alignment_to_rec:NA:1]: Status change from Running to Failed; [2019-01-25 13:58:39,11] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10prep_samples_to_rec:NA:1]: Status change from Running to Failed; [2019-01-25 13:58:40,06] [error] WorkflowManagerActor Workflow 2c2e5a10-8c57-4f9f-8d80-c2fccacbb452 failed (during ExecutingWorkflowState): Job alignment_to_rec:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-alignment_to_rec/alignment_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 48, in process; fnargs, parallel, out_keys, input_files = _world_from_cwl(args.name, fnargs[1:], work_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 185, in _world_from_cwl; assert os.path.exists(os.path.join(work_dir, ""cwl.inputs.json"")); AssertionError. Job prep_samples_to_rec:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-prep_samples_to_rec/prep_samples_to_rec-stderr.log.; Traceback (most recent call last):; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586
https://github.com/broadinstitute/cromwell/issues/4586:2401,Testability,assert,assert,2401,"ed; [2019-01-25 13:58:40,06] [error] WorkflowManagerActor Workflow 2c2e5a10-8c57-4f9f-8d80-c2fccacbb452 failed (during ExecutingWorkflowState): Job alignment_to_rec:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-alignment_to_rec/alignment_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 48, in process; fnargs, parallel, out_keys, input_files = _world_from_cwl(args.name, fnargs[1:], work_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 185, in _world_from_cwl; assert os.path.exists(os.path.join(work_dir, ""cwl.inputs.json"")); AssertionError. Job prep_samples_to_rec:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-prep_samples_to_rec/prep_samples_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 48, in process; fnargs, parallel, out_keys, input_files = _world_from_cwl(args.name, fnargs[1:], work_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 185, in _world_from_cwl; assert os.path.exists(os.pat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586
https://github.com/broadinstitute/cromwell/issues/4586:2467,Testability,Assert,AssertionError,2467,"f9f-8d80-c2fccacbb452 failed (during ExecutingWorkflowState): Job alignment_to_rec:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-alignment_to_rec/alignment_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 48, in process; fnargs, parallel, out_keys, input_files = _world_from_cwl(args.name, fnargs[1:], work_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 185, in _world_from_cwl; assert os.path.exists(os.path.join(work_dir, ""cwl.inputs.json"")); AssertionError. Job prep_samples_to_rec:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-prep_samples_to_rec/prep_samples_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 48, in process; fnargs, parallel, out_keys, input_files = _world_from_cwl(args.name, fnargs[1:], work_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 185, in _world_from_cwl; assert os.path.exists(os.path.join(work_dir, ""cwl.inputs.json"")); AssertionError. [2019-01-25 13:58:40,07] [i",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586
https://github.com/broadinstitute/cromwell/issues/4586:2746,Testability,test,test,2746," of stderr for potential additional information: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-alignment_to_rec/alignment_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 48, in process; fnargs, parallel, out_keys, input_files = _world_from_cwl(args.name, fnargs[1:], work_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 185, in _world_from_cwl; assert os.path.exists(os.path.join(work_dir, ""cwl.inputs.json"")); AssertionError. Job prep_samples_to_rec:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-prep_samples_to_rec/prep_samples_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 48, in process; fnargs, parallel, out_keys, input_files = _world_from_cwl(args.name, fnargs[1:], work_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 185, in _world_from_cwl; assert os.path.exists(os.path.join(work_dir, ""cwl.inputs.json"")); AssertionError. [2019-01-25 13:58:40,07] [info] WorkflowManagerActor WorkflowActor-2c2e5a10-8c57-4f9f-8d80-c2fccacbb452 is in a terminal state: WorkflowFailedState; [2019-01-25 13:58:59,66] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-01-25 13:59:03,42] [i",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586
https://github.com/broadinstitute/cromwell/issues/4586:2876,Testability,log,log,2876,"gnment_to_rec/alignment_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 48, in process; fnargs, parallel, out_keys, input_files = _world_from_cwl(args.name, fnargs[1:], work_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 185, in _world_from_cwl; assert os.path.exists(os.path.join(work_dir, ""cwl.inputs.json"")); AssertionError. Job prep_samples_to_rec:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-prep_samples_to_rec/prep_samples_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 48, in process; fnargs, parallel, out_keys, input_files = _world_from_cwl(args.name, fnargs[1:], work_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 185, in _world_from_cwl; assert os.path.exists(os.path.join(work_dir, ""cwl.inputs.json"")); AssertionError. [2019-01-25 13:58:40,07] [info] WorkflowManagerActor WorkflowActor-2c2e5a10-8c57-4f9f-8d80-c2fccacbb452 is in a terminal state: WorkflowFailedState; [2019-01-25 13:58:59,66] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-01-25 13:59:03,42] [info] SingleWorkflowRunnerActor writing metadata to /home/chapmanb/drive/work/cwl/test_bcbio_cwl/aws/cromwell_work/somatic-metadata.json; ```; I can see this fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586
https://github.com/broadinstitute/cromwell/issues/4586:3361,Testability,assert,assert,3361,"distributed/runfn.py"", line 185, in _world_from_cwl; assert os.path.exists(os.path.join(work_dir, ""cwl.inputs.json"")); AssertionError. Job prep_samples_to_rec:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-prep_samples_to_rec/prep_samples_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 48, in process; fnargs, parallel, out_keys, input_files = _world_from_cwl(args.name, fnargs[1:], work_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 185, in _world_from_cwl; assert os.path.exists(os.path.join(work_dir, ""cwl.inputs.json"")); AssertionError. [2019-01-25 13:58:40,07] [info] WorkflowManagerActor WorkflowActor-2c2e5a10-8c57-4f9f-8d80-c2fccacbb452 is in a terminal state: WorkflowFailedState; [2019-01-25 13:58:59,66] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-01-25 13:59:03,42] [info] SingleWorkflowRunnerActor writing metadata to /home/chapmanb/drive/work/cwl/test_bcbio_cwl/aws/cromwell_work/somatic-metadata.json; ```; I can see this file in the s3 bucket, although it's not in the `execution` directory which is normally where things get staged (at least on local runs):; ```; $ aws s3 ls s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-prep_samples_to_rec/; PRE glob-b34dfc006a981a93d6da067cf50036fe/; 2019-01-25 13:51:55 0; 2019-01-25 13:51:59 6059 cwl.inputs.json; 2019-01-25 13:58:01 0 glob-b34dfc006a981a93d6da067cf50036fe.list; 2019-01-25 1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586
https://github.com/broadinstitute/cromwell/issues/4586:3427,Testability,Assert,AssertionError,3427,".join(work_dir, ""cwl.inputs.json"")); AssertionError. Job prep_samples_to_rec:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-prep_samples_to_rec/prep_samples_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 48, in process; fnargs, parallel, out_keys, input_files = _world_from_cwl(args.name, fnargs[1:], work_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 185, in _world_from_cwl; assert os.path.exists(os.path.join(work_dir, ""cwl.inputs.json"")); AssertionError. [2019-01-25 13:58:40,07] [info] WorkflowManagerActor WorkflowActor-2c2e5a10-8c57-4f9f-8d80-c2fccacbb452 is in a terminal state: WorkflowFailedState; [2019-01-25 13:58:59,66] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-01-25 13:59:03,42] [info] SingleWorkflowRunnerActor writing metadata to /home/chapmanb/drive/work/cwl/test_bcbio_cwl/aws/cromwell_work/somatic-metadata.json; ```; I can see this file in the s3 bucket, although it's not in the `execution` directory which is normally where things get staged (at least on local runs):; ```; $ aws s3 ls s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-prep_samples_to_rec/; PRE glob-b34dfc006a981a93d6da067cf50036fe/; 2019-01-25 13:51:55 0; 2019-01-25 13:51:59 6059 cwl.inputs.json; 2019-01-25 13:58:01 0 glob-b34dfc006a981a93d6da067cf50036fe.list; 2019-01-25 13:58:01 2 prep_samples_to_rec-rc.txt; 2019-01-25 13:58:40 571 prep_samples_to_rec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586
https://github.com/broadinstitute/cromwell/issues/4586:4057,Testability,test,test,4057,"ormation: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-prep_samples_to_rec/prep_samples_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 48, in process; fnargs, parallel, out_keys, input_files = _world_from_cwl(args.name, fnargs[1:], work_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 185, in _world_from_cwl; assert os.path.exists(os.path.join(work_dir, ""cwl.inputs.json"")); AssertionError. [2019-01-25 13:58:40,07] [info] WorkflowManagerActor WorkflowActor-2c2e5a10-8c57-4f9f-8d80-c2fccacbb452 is in a terminal state: WorkflowFailedState; [2019-01-25 13:58:59,66] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-01-25 13:59:03,42] [info] SingleWorkflowRunnerActor writing metadata to /home/chapmanb/drive/work/cwl/test_bcbio_cwl/aws/cromwell_work/somatic-metadata.json; ```; I can see this file in the s3 bucket, although it's not in the `execution` directory which is normally where things get staged (at least on local runs):; ```; $ aws s3 ls s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-prep_samples_to_rec/; PRE glob-b34dfc006a981a93d6da067cf50036fe/; 2019-01-25 13:51:55 0; 2019-01-25 13:51:59 6059 cwl.inputs.json; 2019-01-25 13:58:01 0 glob-b34dfc006a981a93d6da067cf50036fe.list; 2019-01-25 13:58:01 2 prep_samples_to_rec-rc.txt; 2019-01-25 13:58:40 571 prep_samples_to_rec-stderr.log; 2019-01-25 13:58:02 0 prep_samples_to_rec-stdout.log; ```; Does this error look familiar to anyone?. I'm excited to be making progress with this and will also work on writing up and documenting the setup and run process so far. Thanks for any suggestions or pointers.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586
https://github.com/broadinstitute/cromwell/issues/4586:4438,Testability,log,log,4438,"ormation: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-prep_samples_to_rec/prep_samples_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 48, in process; fnargs, parallel, out_keys, input_files = _world_from_cwl(args.name, fnargs[1:], work_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 185, in _world_from_cwl; assert os.path.exists(os.path.join(work_dir, ""cwl.inputs.json"")); AssertionError. [2019-01-25 13:58:40,07] [info] WorkflowManagerActor WorkflowActor-2c2e5a10-8c57-4f9f-8d80-c2fccacbb452 is in a terminal state: WorkflowFailedState; [2019-01-25 13:58:59,66] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-01-25 13:59:03,42] [info] SingleWorkflowRunnerActor writing metadata to /home/chapmanb/drive/work/cwl/test_bcbio_cwl/aws/cromwell_work/somatic-metadata.json; ```; I can see this file in the s3 bucket, although it's not in the `execution` directory which is normally where things get staged (at least on local runs):; ```; $ aws s3 ls s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-prep_samples_to_rec/; PRE glob-b34dfc006a981a93d6da067cf50036fe/; 2019-01-25 13:51:55 0; 2019-01-25 13:51:59 6059 cwl.inputs.json; 2019-01-25 13:58:01 0 glob-b34dfc006a981a93d6da067cf50036fe.list; 2019-01-25 13:58:01 2 prep_samples_to_rec-rc.txt; 2019-01-25 13:58:40 571 prep_samples_to_rec-stderr.log; 2019-01-25 13:58:02 0 prep_samples_to_rec-stdout.log; ```; Does this error look familiar to anyone?. I'm excited to be making progress with this and will also work on writing up and documenting the setup and run process so far. Thanks for any suggestions or pointers.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586
https://github.com/broadinstitute/cromwell/issues/4586:4492,Testability,log,log,4492,"ormation: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-prep_samples_to_rec/prep_samples_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 48, in process; fnargs, parallel, out_keys, input_files = _world_from_cwl(args.name, fnargs[1:], work_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 185, in _world_from_cwl; assert os.path.exists(os.path.join(work_dir, ""cwl.inputs.json"")); AssertionError. [2019-01-25 13:58:40,07] [info] WorkflowManagerActor WorkflowActor-2c2e5a10-8c57-4f9f-8d80-c2fccacbb452 is in a terminal state: WorkflowFailedState; [2019-01-25 13:58:59,66] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-01-25 13:59:03,42] [info] SingleWorkflowRunnerActor writing metadata to /home/chapmanb/drive/work/cwl/test_bcbio_cwl/aws/cromwell_work/somatic-metadata.json; ```; I can see this file in the s3 bucket, although it's not in the `execution` directory which is normally where things get staged (at least on local runs):; ```; $ aws s3 ls s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-prep_samples_to_rec/; PRE glob-b34dfc006a981a93d6da067cf50036fe/; 2019-01-25 13:51:55 0; 2019-01-25 13:51:59 6059 cwl.inputs.json; 2019-01-25 13:58:01 0 glob-b34dfc006a981a93d6da067cf50036fe.list; 2019-01-25 13:58:01 2 prep_samples_to_rec-rc.txt; 2019-01-25 13:58:40 571 prep_samples_to_rec-stderr.log; 2019-01-25 13:58:02 0 prep_samples_to_rec-stdout.log; ```; Does this error look familiar to anyone?. I'm excited to be making progress with this and will also work on writing up and documenting the setup and run process so far. Thanks for any suggestions or pointers.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586
https://github.com/broadinstitute/cromwell/issues/4587:1263,Availability,error,error,1263,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Backend:; Local, no conf file. <!-- Paste/Attach your workflow if possible: -->. Workflow: Files are here:; https://github.com/FredHutch/reproducible-workflows/tree/master/CWL/SingleStepWorkflow. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Details (see also [this post](https://gatkforums.broadinstitute.org/wdl/discussion/23265/cwl-workflow-fails-running-locally#latest)):. I can run this workflow just fine using cwltool/cwl-runner as follows:. ```; cwl-runner bwa-memWorkflow.cwl localInputs.yml; ```. When I try and run it with cromwell I get an error that ""The job was aborted from outside Cromwell"" but I definitely did not abort it myself. Here is the command I used to run this workflow in Cromwell:. ```; java -jar cromwell-36.jar run bwa-memWorkflow.cwl -i localInputs.yml -p bwa-pe.cwl.zip; ```. (`bwa-pe.cwl.zip` just contains the dependency `bwa-pe.cwl`). And here's the full output of it:. https://gist.github.com/dtenenba/61bcf60f129b817cd894ee222789369a. My ultimate goal is to switch over to the AWS Batch back end (in case you are wondering why I don't just stick with cwltool) but first I wanted to get the workflow running locally in cromwell. Any ideas about this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4587
https://github.com/broadinstitute/cromwell/issues/4587:855,Deployability,configurat,configuration,855,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Backend:; Local, no conf file. <!-- Paste/Attach your workflow if possible: -->. Workflow: Files are here:; https://github.com/FredHutch/reproducible-workflows/tree/master/CWL/SingleStepWorkflow. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Details (see also [this post](https://gatkforums.broadinstitute.org/wdl/discussion/23265/cwl-workflow-fails-running-locally#latest)):. I can run this workflow just fine using cwltool/cwl-runner as follows:. ```; cwl-runner bwa-memWorkflow.cwl localInputs.yml; ```. When I try and run it with cromwell I get an error that ""The job was aborted from outside Cromwell"" but I definitely did not abort it myself. Here is the command I used to run this workflow in Cromwell:. ```; java -jar cromwell-36.jar run bwa-memWorkflow.cwl -i localInputs.yml -p bwa-pe.cwl.zip; ```. (`bwa-pe.cwl.zip` just contains the dependency `bwa-pe.cwl`). And here's the full output of it:. https://gist.github.com/dtenenba/61bcf60f129b817cd894ee222789369a. My ultimate goal is to switch over to the AWS Batch back end (in case you are wondering why I don't just stick with cwltool) but first I wanted to get the workflow running locally in cromwell. Any ideas about this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4587
https://github.com/broadinstitute/cromwell/issues/4587:1556,Integrability,depend,dependency,1556,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Backend:; Local, no conf file. <!-- Paste/Attach your workflow if possible: -->. Workflow: Files are here:; https://github.com/FredHutch/reproducible-workflows/tree/master/CWL/SingleStepWorkflow. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Details (see also [this post](https://gatkforums.broadinstitute.org/wdl/discussion/23265/cwl-workflow-fails-running-locally#latest)):. I can run this workflow just fine using cwltool/cwl-runner as follows:. ```; cwl-runner bwa-memWorkflow.cwl localInputs.yml; ```. When I try and run it with cromwell I get an error that ""The job was aborted from outside Cromwell"" but I definitely did not abort it myself. Here is the command I used to run this workflow in Cromwell:. ```; java -jar cromwell-36.jar run bwa-memWorkflow.cwl -i localInputs.yml -p bwa-pe.cwl.zip; ```. (`bwa-pe.cwl.zip` just contains the dependency `bwa-pe.cwl`). And here's the full output of it:. https://gist.github.com/dtenenba/61bcf60f129b817cd894ee222789369a. My ultimate goal is to switch over to the AWS Batch back end (in case you are wondering why I don't just stick with cwltool) but first I wanted to get the workflow running locally in cromwell. Any ideas about this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4587
https://github.com/broadinstitute/cromwell/issues/4587:855,Modifiability,config,configuration,855,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Backend:; Local, no conf file. <!-- Paste/Attach your workflow if possible: -->. Workflow: Files are here:; https://github.com/FredHutch/reproducible-workflows/tree/master/CWL/SingleStepWorkflow. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Details (see also [this post](https://gatkforums.broadinstitute.org/wdl/discussion/23265/cwl-workflow-fails-running-locally#latest)):. I can run this workflow just fine using cwltool/cwl-runner as follows:. ```; cwl-runner bwa-memWorkflow.cwl localInputs.yml; ```. When I try and run it with cromwell I get an error that ""The job was aborted from outside Cromwell"" but I definitely did not abort it myself. Here is the command I used to run this workflow in Cromwell:. ```; java -jar cromwell-36.jar run bwa-memWorkflow.cwl -i localInputs.yml -p bwa-pe.cwl.zip; ```. (`bwa-pe.cwl.zip` just contains the dependency `bwa-pe.cwl`). And here's the full output of it:. https://gist.github.com/dtenenba/61bcf60f129b817cd894ee222789369a. My ultimate goal is to switch over to the AWS Batch back end (in case you are wondering why I don't just stick with cwltool) but first I wanted to get the workflow running locally in cromwell. Any ideas about this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4587
https://github.com/broadinstitute/cromwell/issues/4587:1287,Safety,abort,aborted,1287,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Backend:; Local, no conf file. <!-- Paste/Attach your workflow if possible: -->. Workflow: Files are here:; https://github.com/FredHutch/reproducible-workflows/tree/master/CWL/SingleStepWorkflow. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Details (see also [this post](https://gatkforums.broadinstitute.org/wdl/discussion/23265/cwl-workflow-fails-running-locally#latest)):. I can run this workflow just fine using cwltool/cwl-runner as follows:. ```; cwl-runner bwa-memWorkflow.cwl localInputs.yml; ```. When I try and run it with cromwell I get an error that ""The job was aborted from outside Cromwell"" but I definitely did not abort it myself. Here is the command I used to run this workflow in Cromwell:. ```; java -jar cromwell-36.jar run bwa-memWorkflow.cwl -i localInputs.yml -p bwa-pe.cwl.zip; ```. (`bwa-pe.cwl.zip` just contains the dependency `bwa-pe.cwl`). And here's the full output of it:. https://gist.github.com/dtenenba/61bcf60f129b817cd894ee222789369a. My ultimate goal is to switch over to the AWS Batch back end (in case you are wondering why I don't just stick with cwltool) but first I wanted to get the workflow running locally in cromwell. Any ideas about this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4587
https://github.com/broadinstitute/cromwell/issues/4587:1343,Safety,abort,abort,1343,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Backend:; Local, no conf file. <!-- Paste/Attach your workflow if possible: -->. Workflow: Files are here:; https://github.com/FredHutch/reproducible-workflows/tree/master/CWL/SingleStepWorkflow. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Details (see also [this post](https://gatkforums.broadinstitute.org/wdl/discussion/23265/cwl-workflow-fails-running-locally#latest)):. I can run this workflow just fine using cwltool/cwl-runner as follows:. ```; cwl-runner bwa-memWorkflow.cwl localInputs.yml; ```. When I try and run it with cromwell I get an error that ""The job was aborted from outside Cromwell"" but I definitely did not abort it myself. Here is the command I used to run this workflow in Cromwell:. ```; java -jar cromwell-36.jar run bwa-memWorkflow.cwl -i localInputs.yml -p bwa-pe.cwl.zip; ```. (`bwa-pe.cwl.zip` just contains the dependency `bwa-pe.cwl`). And here's the full output of it:. https://gist.github.com/dtenenba/61bcf60f129b817cd894ee222789369a. My ultimate goal is to switch over to the AWS Batch back end (in case you are wondering why I don't just stick with cwltool) but first I wanted to get the workflow running locally in cromwell. Any ideas about this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4587
https://github.com/broadinstitute/cromwell/issues/4587:900,Security,PASSWORD,PASSWORDS,900,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Backend:; Local, no conf file. <!-- Paste/Attach your workflow if possible: -->. Workflow: Files are here:; https://github.com/FredHutch/reproducible-workflows/tree/master/CWL/SingleStepWorkflow. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Details (see also [this post](https://gatkforums.broadinstitute.org/wdl/discussion/23265/cwl-workflow-fails-running-locally#latest)):. I can run this workflow just fine using cwltool/cwl-runner as follows:. ```; cwl-runner bwa-memWorkflow.cwl localInputs.yml; ```. When I try and run it with cromwell I get an error that ""The job was aborted from outside Cromwell"" but I definitely did not abort it myself. Here is the command I used to run this workflow in Cromwell:. ```; java -jar cromwell-36.jar run bwa-memWorkflow.cwl -i localInputs.yml -p bwa-pe.cwl.zip; ```. (`bwa-pe.cwl.zip` just contains the dependency `bwa-pe.cwl`). And here's the full output of it:. https://gist.github.com/dtenenba/61bcf60f129b817cd894ee222789369a. My ultimate goal is to switch over to the AWS Batch back end (in case you are wondering why I don't just stick with cwltool) but first I wanted to get the workflow running locally in cromwell. Any ideas about this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4587
https://github.com/broadinstitute/cromwell/issues/4587:47,Usability,feedback,feedback,47,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Backend:; Local, no conf file. <!-- Paste/Attach your workflow if possible: -->. Workflow: Files are here:; https://github.com/FredHutch/reproducible-workflows/tree/master/CWL/SingleStepWorkflow. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Details (see also [this post](https://gatkforums.broadinstitute.org/wdl/discussion/23265/cwl-workflow-fails-running-locally#latest)):. I can run this workflow just fine using cwltool/cwl-runner as follows:. ```; cwl-runner bwa-memWorkflow.cwl localInputs.yml; ```. When I try and run it with cromwell I get an error that ""The job was aborted from outside Cromwell"" but I definitely did not abort it myself. Here is the command I used to run this workflow in Cromwell:. ```; java -jar cromwell-36.jar run bwa-memWorkflow.cwl -i localInputs.yml -p bwa-pe.cwl.zip; ```. (`bwa-pe.cwl.zip` just contains the dependency `bwa-pe.cwl`). And here's the full output of it:. https://gist.github.com/dtenenba/61bcf60f129b817cd894ee222789369a. My ultimate goal is to switch over to the AWS Batch back end (in case you are wondering why I don't just stick with cwltool) but first I wanted to get the workflow running locally in cromwell. Any ideas about this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4587
https://github.com/broadinstitute/cromwell/pull/4588:22,Performance,throttle,throttle,22,"I confirmed this does throttle the token dispenser from the log output, though it may not get the AWS build passing until everyone rebases.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4588
https://github.com/broadinstitute/cromwell/pull/4588:60,Testability,log,log,60,"I confirmed this does throttle the token dispenser from the log output, though it may not get the AWS build passing until everyone rebases.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4588
https://github.com/broadinstitute/cromwell/issues/4590:235,Performance,cache,cache,235,"The call metadata key `commandLine` is written during the creation of the command script. Automated retries of the centaur test end up call caching the original-otherwise-successful call, and the key written during the caching. A call cache hit should probably publish a metadata entry for the command, even though the caching does not execute its own `commandLine`. It should be decided like in #3998 and #4001 if the key `commandLine` should be written for cache hits. https://github.com/broadinstitute/cromwell/blob/246b4fb9cecd02385cdd45a23a1d67fa24b27b8a/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L315-L320. Example log of a failed local centaur run. NOTE: Unlike some of the similar issues, centaur definitely did not restart cromwell during this run.; [commandLine_missing.txt](https://github.com/broadinstitute/cromwell/files/2798625/commandLine_missing.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4590
https://github.com/broadinstitute/cromwell/issues/4590:459,Performance,cache,cache,459,"The call metadata key `commandLine` is written during the creation of the command script. Automated retries of the centaur test end up call caching the original-otherwise-successful call, and the key written during the caching. A call cache hit should probably publish a metadata entry for the command, even though the caching does not execute its own `commandLine`. It should be decided like in #3998 and #4001 if the key `commandLine` should be written for cache hits. https://github.com/broadinstitute/cromwell/blob/246b4fb9cecd02385cdd45a23a1d67fa24b27b8a/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L315-L320. Example log of a failed local centaur run. NOTE: Unlike some of the similar issues, centaur definitely did not restart cromwell during this run.; [commandLine_missing.txt](https://github.com/broadinstitute/cromwell/files/2798625/commandLine_missing.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4590
https://github.com/broadinstitute/cromwell/issues/4590:123,Testability,test,test,123,"The call metadata key `commandLine` is written during the creation of the command script. Automated retries of the centaur test end up call caching the original-otherwise-successful call, and the key written during the caching. A call cache hit should probably publish a metadata entry for the command, even though the caching does not execute its own `commandLine`. It should be decided like in #3998 and #4001 if the key `commandLine` should be written for cache hits. https://github.com/broadinstitute/cromwell/blob/246b4fb9cecd02385cdd45a23a1d67fa24b27b8a/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L315-L320. Example log of a failed local centaur run. NOTE: Unlike some of the similar issues, centaur definitely did not restart cromwell during this run.; [commandLine_missing.txt](https://github.com/broadinstitute/cromwell/files/2798625/commandLine_missing.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4590
https://github.com/broadinstitute/cromwell/issues/4590:662,Testability,log,log,662,"The call metadata key `commandLine` is written during the creation of the command script. Automated retries of the centaur test end up call caching the original-otherwise-successful call, and the key written during the caching. A call cache hit should probably publish a metadata entry for the command, even though the caching does not execute its own `commandLine`. It should be decided like in #3998 and #4001 if the key `commandLine` should be written for cache hits. https://github.com/broadinstitute/cromwell/blob/246b4fb9cecd02385cdd45a23a1d67fa24b27b8a/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L315-L320. Example log of a failed local centaur run. NOTE: Unlike some of the similar issues, centaur definitely did not restart cromwell during this run.; [commandLine_missing.txt](https://github.com/broadinstitute/cromwell/files/2798625/commandLine_missing.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4590
https://github.com/broadinstitute/cromwell/issues/4591:493,Availability,error,error,493,"Hi,. What is the way to specify resource requirements (cores, ram etc) to AWSBatch? . I am executing a CWL workflow with AWS batch backend. ; Each task is submitted by cromwell and I can verify on AWS console that all jobs ended successfully.; However, for jobs that I have set coresMin and coresMax requirements I get the warning:. ```; [warn] AwsBatchAsyncBackendJobExecutionActor [6bd79e09fastqc_1:NA:1]: Unrecognized runtime attribute keys: cpuMax; ```. and at the end of the workflow the error:; ```; [error] Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:507,Availability,error,error,507,"Hi,. What is the way to specify resource requirements (cores, ram etc) to AWSBatch? . I am executing a CWL workflow with AWS batch backend. ; Each task is submitted by cromwell and I can verify on AWS console that all jobs ended successfully.; However, for jobs that I have set coresMin and coresMax requirements I get the warning:. ```; [warn] AwsBatchAsyncBackendJobExecutionActor [6bd79e09fastqc_1:NA:1]: Unrecognized runtime attribute keys: cpuMax; ```. and at the end of the workflow the error:; ```; [error] Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:2800,Availability,robust,robustExecuteOrRecover,2800,ctor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:3143,Availability,robust,robustExecuteOrRecover,3143,r.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.jav,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:4342,Availability,error,errors,4342,n.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. The job finally ends with errors:; ```; [error] WorkflowManagerActor Workflow 6bd79e09-cb56-480f-be46-0b2419591b3f failed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispat,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:4357,Availability,error,error,4357,n.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. The job finally ends with errors:; ```; [error] WorkflowManagerActor Workflow 6bd79e09-cb56-480f-be46-0b2419591b3f failed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispat,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:4899,Availability,Fault,FaultHandling,4899,t akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. The job finally ends with errors:; ```; [error] WorkflowManagerActor Workflow 6bd79e09-cb56-480f-be46-0b2419591b3f failed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute va,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:4948,Availability,Fault,FaultHandling,4948,.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. The job finally ends with errors:; ```; [error] WorkflowManagerActor Workflow 6bd79e09-cb56-480f-be46-0b2419591b3f failed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute v,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:4976,Availability,Fault,FaultHandling,4976,c(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. The job finally ends with errors:; ```; [error] WorkflowManagerActor Workflow 6bd79e09-cb56-480f-be46-0b2419591b3f failed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:5025,Availability,Fault,FaultHandling,5025,nTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. The job finally ends with errors:; ```; [error] WorkflowManagerActor Workflow 6bd79e09-cb56-480f-be46-0b2419591b3f failed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttribut,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:5054,Availability,Fault,FaultHandling,5054,Task.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. The job finally ends with errors:; ```; [error] WorkflowManagerActor Workflow 6bd79e09-cb56-480f-be46-0b2419591b3f failed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(Valida,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:7893,Availability,robust,robustExecuteOrRecover,7893,ctor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:8236,Availability,robust,robustExecuteOrRecover,8236,r.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	... 5 more; ```. The same workflow runs with local backend and local files both by `cwltool` and `cromwell` and on AWSBatch without specifying Resource requirements. That makes me assume that the,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:532,Security,validat,validation,532,"Hi,. What is the way to specify resource requirements (cores, ram etc) to AWSBatch? . I am executing a CWL workflow with AWS batch backend. ; Each task is submitted by cromwell and I can verify on AWS console that all jobs ended successfully.; However, for jobs that I have set coresMin and coresMax requirements I get the warning:. ```; [warn] AwsBatchAsyncBackendJobExecutionActor [6bd79e09fastqc_1:NA:1]: Unrecognized runtime attribute keys: cpuMax; ```. and at the end of the workflow the error:; ```; [error] Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:680,Security,validat,validation,680,"Hi,. What is the way to specify resource requirements (cores, ram etc) to AWSBatch? . I am executing a CWL workflow with AWS batch backend. ; Each task is submitted by cromwell and I can verify on AWS console that all jobs ended successfully.; However, for jobs that I have set coresMin and coresMax requirements I get the warning:. ```; [warn] AwsBatchAsyncBackendJobExecutionActor [6bd79e09fastqc_1:NA:1]: Unrecognized runtime attribute keys: cpuMax; ```. and at the end of the workflow the error:; ```; [error] Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:691,Security,Validat,ValidatedRuntimeAttributesBuilder,691,"Hi,. What is the way to specify resource requirements (cores, ram etc) to AWSBatch? . I am executing a CWL workflow with AWS batch backend. ; Each task is submitted by cromwell and I can verify on AWS console that all jobs ended successfully.; However, for jobs that I have set coresMin and coresMax requirements I get the warning:. ```; [warn] AwsBatchAsyncBackendJobExecutionActor [6bd79e09fastqc_1:NA:1]: Unrecognized runtime attribute keys: cpuMax; ```. and at the end of the workflow the error:; ```; [error] Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:752,Security,validat,validation,752,"Hi,. What is the way to specify resource requirements (cores, ram etc) to AWSBatch? . I am executing a CWL workflow with AWS batch backend. ; Each task is submitted by cromwell and I can verify on AWS console that all jobs ended successfully.; However, for jobs that I have set coresMin and coresMax requirements I get the warning:. ```; [warn] AwsBatchAsyncBackendJobExecutionActor [6bd79e09fastqc_1:NA:1]: Unrecognized runtime attribute keys: cpuMax; ```. and at the end of the workflow the error:; ```; [error] Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:904,Security,validat,validation,904,"Hi,. What is the way to specify resource requirements (cores, ram etc) to AWSBatch? . I am executing a CWL workflow with AWS batch backend. ; Each task is submitted by cromwell and I can verify on AWS console that all jobs ended successfully.; However, for jobs that I have set coresMin and coresMax requirements I get the warning:. ```; [warn] AwsBatchAsyncBackendJobExecutionActor [6bd79e09fastqc_1:NA:1]: Unrecognized runtime attribute keys: cpuMax; ```. and at the end of the workflow the error:; ```; [error] Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:915,Security,Validat,ValidatedRuntimeAttributesBuilder,915,"Hi,. What is the way to specify resource requirements (cores, ram etc) to AWSBatch? . I am executing a CWL workflow with AWS batch backend. ; Each task is submitted by cromwell and I can verify on AWS console that all jobs ended successfully.; However, for jobs that I have set coresMin and coresMax requirements I get the warning:. ```; [warn] AwsBatchAsyncBackendJobExecutionActor [6bd79e09fastqc_1:NA:1]: Unrecognized runtime attribute keys: cpuMax; ```. and at the end of the workflow the error:; ```; [error] Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:955,Security,Validat,ValidatedRuntimeAttributesBuilder,955,"Hi,. What is the way to specify resource requirements (cores, ram etc) to AWSBatch? . I am executing a CWL workflow with AWS batch backend. ; Each task is submitted by cromwell and I can verify on AWS console that all jobs ended successfully.; However, for jobs that I have set coresMin and coresMax requirements I get the warning:. ```; [warn] AwsBatchAsyncBackendJobExecutionActor [6bd79e09fastqc_1:NA:1]: Unrecognized runtime attribute keys: cpuMax; ```. and at the end of the workflow the error:; ```; [error] Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:1021,Security,validat,validation,1021,"ecify resource requirements (cores, ram etc) to AWSBatch? . I am executing a CWL workflow with AWS batch backend. ; Each task is submitted by cromwell and I can verify on AWS console that all jobs ended successfully.; However, for jobs that I have set coresMin and coresMax requirements I get the warning:. ```; [warn] AwsBatchAsyncBackendJobExecutionActor [6bd79e09fastqc_1:NA:1]: Unrecognized runtime attribute keys: cpuMax; ```. and at the end of the workflow the error:; ```; [error] Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadata",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:1032,Security,Validat,ValidatedRuntimeAttributesBuilder,1032,"ents (cores, ram etc) to AWSBatch? . I am executing a CWL workflow with AWS batch backend. ; Each task is submitted by cromwell and I can verify on AWS console that all jobs ended successfully.; However, for jobs that I have set coresMin and coresMax requirements I get the warning:. ```; [warn] AwsBatchAsyncBackendJobExecutionActor [6bd79e09fastqc_1:NA:1]: Unrecognized runtime attribute keys: cpuMax; ```. and at the end of the workflow the error:; ```; [error] Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:1073,Security,Validat,ValidatedRuntimeAttributesBuilder,1073,"I am executing a CWL workflow with AWS batch backend. ; Each task is submitted by cromwell and I can verify on AWS console that all jobs ended successfully.; However, for jobs that I have set coresMin and coresMax requirements I get the warning:. ```; [warn] AwsBatchAsyncBackendJobExecutionActor [6bd79e09fastqc_1:NA:1]: Unrecognized runtime attribute keys: cpuMax; ```. and at the end of the workflow the error:; ```; [error] Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwel",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:1352,Security,validat,validatedRuntimeAttributes,1352,tor [6bd79e09fastqc_1:NA:1]: Unrecognized runtime attribute keys: cpuMax; ```. and at the end of the workflow the error:; ```; [error] Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:1474,Security,validat,validatedRuntimeAttributes,1474,```; [error] Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); 	at cromwell.backe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:1608,Security,validat,validatedRuntimeAttributes,1608, greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); 	at cromwell.backend.impl.aws.AwsBatchAsyncBac,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:1763,Security,validat,validatedRuntimeAttributes,1763,ttribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anon,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:5773,Security,validat,validation,5773,n$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.sc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:5784,Security,Validat,ValidatedRuntimeAttributesBuilder,5784,andleFailure(FaultHandling.scala:298); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:5845,Security,validat,validation,5845,andleFailure(FaultHandling.scala:298); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:5997,Security,validat,validation,5997,	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataK,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:6008,Security,Validat,ValidatedRuntimeAttributesBuilder,6008,FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:6048,Security,Validat,ValidatedRuntimeAttributesBuilder,6048,dling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:6114,Security,validat,validation,6114,ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadata,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:6125,Security,Validat,ValidatedRuntimeAttributesBuilder,6125,at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCac,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:6166,Security,Validat,ValidatedRuntimeAttributesBuilder,6166,ctorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwel,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:6445,Security,validat,validatedRuntimeAttributes,6445,doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:6567,Security,validat,validatedRuntimeAttributes,6567,.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); 	at cromwell.backe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:6701,Security,validat,validatedRuntimeAttributes,6701,107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); 	at cromwell.backend.impl.aws.AwsBatchAsyncBac,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/issues/4591:6856,Security,validat,validatedRuntimeAttributes,6856,ttribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anon,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591
https://github.com/broadinstitute/cromwell/pull/4593:331,Deployability,Update,Update,331,"Succeeded at https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-papiv2/257/. This is an exact copy of Brad's work with one exception: I supersized the disk for `summarize_vc.cwl`. ~Simultaneously with submitting this PR, I am emailing Brad to request a permanent fix, and will incorporate it in a future PR.~. ~*Update*: Brad replied to the email already, having fixed the disk size. No more tweaks.~. Closes #4514",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4593
https://github.com/broadinstitute/cromwell/issues/4594:106,Integrability,depend,dependencies,106,Should aggregate a response from Cromwell & SAM and return those as a report of CromIAM's health (and its dependencies).,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4594
https://github.com/broadinstitute/cromwell/issues/4595:718,Availability,error,error,718,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; Hi all,. When running a workflow with `write_objects(Array[Struct])` in a task, the workflow fails with **runtime** error `Failed to evaluate input 'out' (reason 1 of 1): Failed to write_objects(...) (reason 1 of 1): Cannot TSV serialize a Array[WomCompositeType {\n a -> String \n}] (valid types are Array[Primitive], Array[Array[Primitive]], or Array[Object])` **if the array is empty**. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->; ```wdl; version 1.0. struct Input {; String a; }. workflow Test {; call test; }. task test {; input {; Array[Input] inputs = []; }. File out = write_objects(inputs). command {; cat '~{out}'; }. runtime {; docker: 'debian:stable-slim'; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Our Cromwell build is `37-a52c415-SNAP` (config available upon request)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4595
https://github.com/broadinstitute/cromwell/issues/4595:1491,Availability,avail,available,1491,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; Hi all,. When running a workflow with `write_objects(Array[Struct])` in a task, the workflow fails with **runtime** error `Failed to evaluate input 'out' (reason 1 of 1): Failed to write_objects(...) (reason 1 of 1): Cannot TSV serialize a Array[WomCompositeType {\n a -> String \n}] (valid types are Array[Primitive], Array[Array[Primitive]], or Array[Object])` **if the array is empty**. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->; ```wdl; version 1.0. struct Input {; String a; }. workflow Test {; call test; }. task test {; input {; Array[Input] inputs = []; }. File out = write_objects(inputs). command {; cat '~{out}'; }. runtime {; docker: 'debian:stable-slim'; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Our Cromwell build is `37-a52c415-SNAP` (config available upon request)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4595
https://github.com/broadinstitute/cromwell/issues/4595:1345,Deployability,configurat,configuration,1345,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; Hi all,. When running a workflow with `write_objects(Array[Struct])` in a task, the workflow fails with **runtime** error `Failed to evaluate input 'out' (reason 1 of 1): Failed to write_objects(...) (reason 1 of 1): Cannot TSV serialize a Array[WomCompositeType {\n a -> String \n}] (valid types are Array[Primitive], Array[Array[Primitive]], or Array[Object])` **if the array is empty**. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->; ```wdl; version 1.0. struct Input {; String a; }. workflow Test {; call test; }. task test {; input {; Array[Input] inputs = []; }. File out = write_objects(inputs). command {; cat '~{out}'; }. runtime {; docker: 'debian:stable-slim'; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Our Cromwell build is `37-a52c415-SNAP` (config available upon request)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4595
https://github.com/broadinstitute/cromwell/issues/4595:1345,Modifiability,config,configuration,1345,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; Hi all,. When running a workflow with `write_objects(Array[Struct])` in a task, the workflow fails with **runtime** error `Failed to evaluate input 'out' (reason 1 of 1): Failed to write_objects(...) (reason 1 of 1): Cannot TSV serialize a Array[WomCompositeType {\n a -> String \n}] (valid types are Array[Primitive], Array[Array[Primitive]], or Array[Object])` **if the array is empty**. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->; ```wdl; version 1.0. struct Input {; String a; }. workflow Test {; call test; }. task test {; input {; Array[Input] inputs = []; }. File out = write_objects(inputs). command {; cat '~{out}'; }. runtime {; docker: 'debian:stable-slim'; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Our Cromwell build is `37-a52c415-SNAP` (config available upon request)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4595
https://github.com/broadinstitute/cromwell/issues/4595:1484,Modifiability,config,config,1484,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; Hi all,. When running a workflow with `write_objects(Array[Struct])` in a task, the workflow fails with **runtime** error `Failed to evaluate input 'out' (reason 1 of 1): Failed to write_objects(...) (reason 1 of 1): Cannot TSV serialize a Array[WomCompositeType {\n a -> String \n}] (valid types are Array[Primitive], Array[Array[Primitive]], or Array[Object])` **if the array is empty**. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->; ```wdl; version 1.0. struct Input {; String a; }. workflow Test {; call test; }. task test {; input {; Array[Input] inputs = []; }. File out = write_objects(inputs). command {; cat '~{out}'; }. runtime {; docker: 'debian:stable-slim'; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Our Cromwell build is `37-a52c415-SNAP` (config available upon request)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4595
https://github.com/broadinstitute/cromwell/issues/4595:1390,Security,PASSWORD,PASSWORDS,1390,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; Hi all,. When running a workflow with `write_objects(Array[Struct])` in a task, the workflow fails with **runtime** error `Failed to evaluate input 'out' (reason 1 of 1): Failed to write_objects(...) (reason 1 of 1): Cannot TSV serialize a Array[WomCompositeType {\n a -> String \n}] (valid types are Array[Primitive], Array[Array[Primitive]], or Array[Object])` **if the array is empty**. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->; ```wdl; version 1.0. struct Input {; String a; }. workflow Test {; call test; }. task test {; input {; Array[Input] inputs = []; }. File out = write_objects(inputs). command {; cat '~{out}'; }. runtime {; docker: 'debian:stable-slim'; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Our Cromwell build is `37-a52c415-SNAP` (config available upon request)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4595
https://github.com/broadinstitute/cromwell/issues/4595:1142,Testability,Test,Test,1142,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; Hi all,. When running a workflow with `write_objects(Array[Struct])` in a task, the workflow fails with **runtime** error `Failed to evaluate input 'out' (reason 1 of 1): Failed to write_objects(...) (reason 1 of 1): Cannot TSV serialize a Array[WomCompositeType {\n a -> String \n}] (valid types are Array[Primitive], Array[Array[Primitive]], or Array[Object])` **if the array is empty**. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->; ```wdl; version 1.0. struct Input {; String a; }. workflow Test {; call test; }. task test {; input {; Array[Input] inputs = []; }. File out = write_objects(inputs). command {; cat '~{out}'; }. runtime {; docker: 'debian:stable-slim'; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Our Cromwell build is `37-a52c415-SNAP` (config available upon request)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4595
https://github.com/broadinstitute/cromwell/issues/4595:1155,Testability,test,test,1155,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; Hi all,. When running a workflow with `write_objects(Array[Struct])` in a task, the workflow fails with **runtime** error `Failed to evaluate input 'out' (reason 1 of 1): Failed to write_objects(...) (reason 1 of 1): Cannot TSV serialize a Array[WomCompositeType {\n a -> String \n}] (valid types are Array[Primitive], Array[Array[Primitive]], or Array[Object])` **if the array is empty**. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->; ```wdl; version 1.0. struct Input {; String a; }. workflow Test {; call test; }. task test {; input {; Array[Input] inputs = []; }. File out = write_objects(inputs). command {; cat '~{out}'; }. runtime {; docker: 'debian:stable-slim'; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Our Cromwell build is `37-a52c415-SNAP` (config available upon request)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4595
https://github.com/broadinstitute/cromwell/issues/4595:1169,Testability,test,test,1169,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; Hi all,. When running a workflow with `write_objects(Array[Struct])` in a task, the workflow fails with **runtime** error `Failed to evaluate input 'out' (reason 1 of 1): Failed to write_objects(...) (reason 1 of 1): Cannot TSV serialize a Array[WomCompositeType {\n a -> String \n}] (valid types are Array[Primitive], Array[Array[Primitive]], or Array[Object])` **if the array is empty**. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->; ```wdl; version 1.0. struct Input {; String a; }. workflow Test {; call test; }. task test {; input {; Array[Input] inputs = []; }. File out = write_objects(inputs). command {; cat '~{out}'; }. runtime {; docker: 'debian:stable-slim'; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Our Cromwell build is `37-a52c415-SNAP` (config available upon request)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4595
https://github.com/broadinstitute/cromwell/issues/4595:47,Usability,feedback,feedback,47,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; Hi all,. When running a workflow with `write_objects(Array[Struct])` in a task, the workflow fails with **runtime** error `Failed to evaluate input 'out' (reason 1 of 1): Failed to write_objects(...) (reason 1 of 1): Cannot TSV serialize a Array[WomCompositeType {\n a -> String \n}] (valid types are Array[Primitive], Array[Array[Primitive]], or Array[Object])` **if the array is empty**. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->; ```wdl; version 1.0. struct Input {; String a; }. workflow Test {; call test; }. task test {; input {; Array[Input] inputs = []; }. File out = write_objects(inputs). command {; cat '~{out}'; }. runtime {; docker: 'debian:stable-slim'; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Our Cromwell build is `37-a52c415-SNAP` (config available upon request)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4595
https://github.com/broadinstitute/cromwell/issues/4596:44,Deployability,deploy,deployments,44,Logically revert #4263 since multi-Cromwell deployments no longer need a specific abort server. A simple git revert has a ton of conflicts but hopefully this shouldn't be too tough.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4596
https://github.com/broadinstitute/cromwell/issues/4596:82,Safety,abort,abort,82,Logically revert #4263 since multi-Cromwell deployments no longer need a specific abort server. A simple git revert has a ton of conflicts but hopefully this shouldn't be too tough.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4596
https://github.com/broadinstitute/cromwell/issues/4596:0,Testability,Log,Logically,0,Logically revert #4263 since multi-Cromwell deployments no longer need a specific abort server. A simple git revert has a ton of conflicts but hopefully this shouldn't be too tough.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4596
https://github.com/broadinstitute/cromwell/issues/4596:98,Usability,simpl,simple,98,Logically revert #4263 since multi-Cromwell deployments no longer need a specific abort server. A simple git revert has a ton of conflicts but hopefully this shouldn't be too tough.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4596
https://github.com/broadinstitute/cromwell/issues/4598:1497,Integrability,DEPEND,DEPENDENT,1497,"MySQL instance.; 2. Submit a workflow to this CromIAM specifying a non-default collection name to associate the requesting auth with an additional collection besides the auth's email address.; 3. Capture the query actually issued by Cromwell for the JMUI request and analyze its performance on real-world-sized databases. The captured query appears at the end of this document since it takes up a ton of vertical space when linted. 🙂 This query takes about 0.7 seconds to execute on CaaS Prod and around 13 seconds on FC Prod. The `EXPLAIN` output on CaaS Prod looks like:. ```; mysql> EXPLAIN select...; +----+--------------------+--------------------+------+---------------------------------------------+-------------------------------+---------+-------------------------------------------+--------+------------------------------------+; | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |; +----+--------------------+--------------------+------+---------------------------------------------+-------------------------------+---------+-------------------------------------------+--------+------------------------------------+; | 1 | PRIMARY | x2 | ALL | NULL | NULL | NULL | NULL | 185900 | Using where |; | 4 | DEPENDENT SUBQUERY | CUSTOM_LABEL_ENTRY | ref | UC_CUSTOM_LABEL_ENTRY_CLK_WEU,SYS_IDX_11226 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 1070 | const,cromwell.x2.WORKFLOW_EXECUTION_UUID | 1 | Using index condition; Using where |; | 3 | DEPENDENT SUBQUERY | CUSTOM_LABEL_ENTRY | ref | UC_CUSTOM_LABEL_ENTRY_CLK_WEU,SYS_IDX_11226 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 1070 | const,cromwell.x2.WORKFLOW_EXECUTION_UUID | 1 | Using index condition; Using where |; | 2 | DEPENDENT SUBQUERY | CUSTOM_LABEL_ENTRY | ref | UC_CUSTOM_LABEL_ENTRY_CLK_WEU,SYS_IDX_11226 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 1070 | const,cromwell.x2.WORKFLOW_EXECUTION_UUID | 1 | Using index condition; Using where |; +----+--------------------+--------------------+------+---------------------------",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4598
https://github.com/broadinstitute/cromwell/issues/4598:1722,Integrability,DEPEND,DEPENDENT,1722,"ds on FC Prod. The `EXPLAIN` output on CaaS Prod looks like:. ```; mysql> EXPLAIN select...; +----+--------------------+--------------------+------+---------------------------------------------+-------------------------------+---------+-------------------------------------------+--------+------------------------------------+; | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |; +----+--------------------+--------------------+------+---------------------------------------------+-------------------------------+---------+-------------------------------------------+--------+------------------------------------+; | 1 | PRIMARY | x2 | ALL | NULL | NULL | NULL | NULL | 185900 | Using where |; | 4 | DEPENDENT SUBQUERY | CUSTOM_LABEL_ENTRY | ref | UC_CUSTOM_LABEL_ENTRY_CLK_WEU,SYS_IDX_11226 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 1070 | const,cromwell.x2.WORKFLOW_EXECUTION_UUID | 1 | Using index condition; Using where |; | 3 | DEPENDENT SUBQUERY | CUSTOM_LABEL_ENTRY | ref | UC_CUSTOM_LABEL_ENTRY_CLK_WEU,SYS_IDX_11226 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 1070 | const,cromwell.x2.WORKFLOW_EXECUTION_UUID | 1 | Using index condition; Using where |; | 2 | DEPENDENT SUBQUERY | CUSTOM_LABEL_ENTRY | ref | UC_CUSTOM_LABEL_ENTRY_CLK_WEU,SYS_IDX_11226 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 1070 | const,cromwell.x2.WORKFLOW_EXECUTION_UUID | 1 | Using index condition; Using where |; +----+--------------------+--------------------+------+---------------------------------------------+-------------------------------+---------+-------------------------------------------+--------+------------------------------------+; ```; The referenced index on `CUSTOM_LABEL_ENTRY` is `UC_CUSTOM_LABEL_ENTRY_CLK_WEU` which looks like:; ```; mysql> show index from CUSTOM_LABEL_ENTRY;; +--------------------+------------+-------------------------------+--------------+-------------------------+-----------+-------------+----------+--------+------+------------+---------+---------------+; | T",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4598
https://github.com/broadinstitute/cromwell/issues/4598:1947,Integrability,DEPEND,DEPENDENT,1947,"+---------+-------------------------------------------+--------+------------------------------------+; | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |; +----+--------------------+--------------------+------+---------------------------------------------+-------------------------------+---------+-------------------------------------------+--------+------------------------------------+; | 1 | PRIMARY | x2 | ALL | NULL | NULL | NULL | NULL | 185900 | Using where |; | 4 | DEPENDENT SUBQUERY | CUSTOM_LABEL_ENTRY | ref | UC_CUSTOM_LABEL_ENTRY_CLK_WEU,SYS_IDX_11226 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 1070 | const,cromwell.x2.WORKFLOW_EXECUTION_UUID | 1 | Using index condition; Using where |; | 3 | DEPENDENT SUBQUERY | CUSTOM_LABEL_ENTRY | ref | UC_CUSTOM_LABEL_ENTRY_CLK_WEU,SYS_IDX_11226 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 1070 | const,cromwell.x2.WORKFLOW_EXECUTION_UUID | 1 | Using index condition; Using where |; | 2 | DEPENDENT SUBQUERY | CUSTOM_LABEL_ENTRY | ref | UC_CUSTOM_LABEL_ENTRY_CLK_WEU,SYS_IDX_11226 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 1070 | const,cromwell.x2.WORKFLOW_EXECUTION_UUID | 1 | Using index condition; Using where |; +----+--------------------+--------------------+------+---------------------------------------------+-------------------------------+---------+-------------------------------------------+--------+------------------------------------+; ```; The referenced index on `CUSTOM_LABEL_ENTRY` is `UC_CUSTOM_LABEL_ENTRY_CLK_WEU` which looks like:; ```; mysql> show index from CUSTOM_LABEL_ENTRY;; +--------------------+------------+-------------------------------+--------------+-------------------------+-----------+-------------+----------+--------+------+------------+---------+---------------+; | Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |; +--------------------+------------+-------------------------------+-----",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4598
https://github.com/broadinstitute/cromwell/issues/4598:76,Performance,perform,perform,76,"Requests to Cromwell's `query` endpoint specifying label-related parameters perform poorly. JMUI will rely on this endpoint being performant with label parameters. Plan for investigation:. 1. Set up a local CromIAM and Cromwell writing to a local MySQL instance.; 2. Submit a workflow to this CromIAM specifying a non-default collection name to associate the requesting auth with an additional collection besides the auth's email address.; 3. Capture the query actually issued by Cromwell for the JMUI request and analyze its performance on real-world-sized databases. The captured query appears at the end of this document since it takes up a ton of vertical space when linted. 🙂 This query takes about 0.7 seconds to execute on CaaS Prod and around 13 seconds on FC Prod. The `EXPLAIN` output on CaaS Prod looks like:. ```; mysql> EXPLAIN select...; +----+--------------------+--------------------+------+---------------------------------------------+-------------------------------+---------+-------------------------------------------+--------+------------------------------------+; | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |; +----+--------------------+--------------------+------+---------------------------------------------+-------------------------------+---------+-------------------------------------------+--------+------------------------------------+; | 1 | PRIMARY | x2 | ALL | NULL | NULL | NULL | NULL | 185900 | Using where |; | 4 | DEPENDENT SUBQUERY | CUSTOM_LABEL_ENTRY | ref | UC_CUSTOM_LABEL_ENTRY_CLK_WEU,SYS_IDX_11226 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 1070 | const,cromwell.x2.WORKFLOW_EXECUTION_UUID | 1 | Using index condition; Using where |; | 3 | DEPENDENT SUBQUERY | CUSTOM_LABEL_ENTRY | ref | UC_CUSTOM_LABEL_ENTRY_CLK_WEU,SYS_IDX_11226 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 1070 | const,cromwell.x2.WORKFLOW_EXECUTION_UUID | 1 | Using index condition; Using where |; | 2 | DEPENDENT SUBQUERY | CUSTOM_LABEL_ENTRY | ref | UC_CUS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4598
https://github.com/broadinstitute/cromwell/issues/4598:130,Performance,perform,performant,130,"Requests to Cromwell's `query` endpoint specifying label-related parameters perform poorly. JMUI will rely on this endpoint being performant with label parameters. Plan for investigation:. 1. Set up a local CromIAM and Cromwell writing to a local MySQL instance.; 2. Submit a workflow to this CromIAM specifying a non-default collection name to associate the requesting auth with an additional collection besides the auth's email address.; 3. Capture the query actually issued by Cromwell for the JMUI request and analyze its performance on real-world-sized databases. The captured query appears at the end of this document since it takes up a ton of vertical space when linted. 🙂 This query takes about 0.7 seconds to execute on CaaS Prod and around 13 seconds on FC Prod. The `EXPLAIN` output on CaaS Prod looks like:. ```; mysql> EXPLAIN select...; +----+--------------------+--------------------+------+---------------------------------------------+-------------------------------+---------+-------------------------------------------+--------+------------------------------------+; | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |; +----+--------------------+--------------------+------+---------------------------------------------+-------------------------------+---------+-------------------------------------------+--------+------------------------------------+; | 1 | PRIMARY | x2 | ALL | NULL | NULL | NULL | NULL | 185900 | Using where |; | 4 | DEPENDENT SUBQUERY | CUSTOM_LABEL_ENTRY | ref | UC_CUSTOM_LABEL_ENTRY_CLK_WEU,SYS_IDX_11226 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 1070 | const,cromwell.x2.WORKFLOW_EXECUTION_UUID | 1 | Using index condition; Using where |; | 3 | DEPENDENT SUBQUERY | CUSTOM_LABEL_ENTRY | ref | UC_CUSTOM_LABEL_ENTRY_CLK_WEU,SYS_IDX_11226 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 1070 | const,cromwell.x2.WORKFLOW_EXECUTION_UUID | 1 | Using index condition; Using where |; | 2 | DEPENDENT SUBQUERY | CUSTOM_LABEL_ENTRY | ref | UC_CUS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4598
https://github.com/broadinstitute/cromwell/issues/4598:526,Performance,perform,performance,526,"Requests to Cromwell's `query` endpoint specifying label-related parameters perform poorly. JMUI will rely on this endpoint being performant with label parameters. Plan for investigation:. 1. Set up a local CromIAM and Cromwell writing to a local MySQL instance.; 2. Submit a workflow to this CromIAM specifying a non-default collection name to associate the requesting auth with an additional collection besides the auth's email address.; 3. Capture the query actually issued by Cromwell for the JMUI request and analyze its performance on real-world-sized databases. The captured query appears at the end of this document since it takes up a ton of vertical space when linted. 🙂 This query takes about 0.7 seconds to execute on CaaS Prod and around 13 seconds on FC Prod. The `EXPLAIN` output on CaaS Prod looks like:. ```; mysql> EXPLAIN select...; +----+--------------------+--------------------+------+---------------------------------------------+-------------------------------+---------+-------------------------------------------+--------+------------------------------------+; | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |; +----+--------------------+--------------------+------+---------------------------------------------+-------------------------------+---------+-------------------------------------------+--------+------------------------------------+; | 1 | PRIMARY | x2 | ALL | NULL | NULL | NULL | NULL | 185900 | Using where |; | 4 | DEPENDENT SUBQUERY | CUSTOM_LABEL_ENTRY | ref | UC_CUSTOM_LABEL_ENTRY_CLK_WEU,SYS_IDX_11226 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 1070 | const,cromwell.x2.WORKFLOW_EXECUTION_UUID | 1 | Using index condition; Using where |; | 3 | DEPENDENT SUBQUERY | CUSTOM_LABEL_ENTRY | ref | UC_CUSTOM_LABEL_ENTRY_CLK_WEU,SYS_IDX_11226 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 1070 | const,cromwell.x2.WORKFLOW_EXECUTION_UUID | 1 | Using index condition; Using where |; | 2 | DEPENDENT SUBQUERY | CUSTOM_LABEL_ENTRY | ref | UC_CUS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4598
https://github.com/broadinstitute/cromwell/issues/4598:4064,Security,access,access,4064,"-+---------+---------------+; | CUSTOM_LABEL_ENTRY | 0 | PRIMARY | 1 | CUSTOM_LABEL_ENTRY_ID | A | 531285 | NULL | NULL | | BTREE | | |; | CUSTOM_LABEL_ENTRY | 0 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 1 | CUSTOM_LABEL_KEY | A | 31 | NULL | NULL | YES | BTREE | | |; | CUSTOM_LABEL_ENTRY | 0 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 2 | WORKFLOW_EXECUTION_UUID | A | 531285 | NULL | NULL | | BTREE | | |; | CUSTOM_LABEL_ENTRY | 1 | SYS_IDX_11226 | 1 | WORKFLOW_EXECUTION_UUID | A | 132821 | NULL | NULL | | BTREE | | |; +--------------------+------------+-------------------------------+--------------+-------------------------+-----------+-------------+----------+--------+------+------------+---------+---------------+; ```; So MySQL appears to be table scanning `WORKFLOW_METADATA_SUMMARY_ENTRY` and then finding the the at-most-one matching rows in `CUSTOM_LABEL_ENTRY` for each label parameter using the unique index on `WORKFLOW_EXECUTION_UUID` + `CUSTOM_LABEL_KEY`. So the labels table access should be fast but the summary table is table scanning. I experimented with adding a non-unique index on `CUSTOM_LABEL_ENTRY` for `CUSTOM_LABEL_KEY` + `CUSTOM_LABEL_VALUE` in the hope that MySQL could use that first and then join back to the summary table on workflow ID. However I haven't had any luck getting MySQL to use this index for even the simplest possible queries:. ```; mysql> create index IDX_KEY_VALUE on CUSTOM_LABEL_ENTRY (CUSTOM_LABEL_KEY, CUSTOM_LABEL_VALUE); ; .; .; .; mysql> explain select WORKFLOW_EXECUTION_UUID from CUSTOM_LABEL_ENTRY where CUSTOM_LABEL_KEY = 'caas-collection-name' AND CUSTOM_LABEL_VALUE = 'miguel-collection';; +----+-------------+--------------------+------------+------+---------------------------------------------+-------------------------------+---------+-------+------+----------+-------------+; | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |; +----+-------------+--------------------+------------+--",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4598
https://github.com/broadinstitute/cromwell/issues/4598:4419,Usability,simpl,simplest,4419,"CLK_WEU | 2 | WORKFLOW_EXECUTION_UUID | A | 531285 | NULL | NULL | | BTREE | | |; | CUSTOM_LABEL_ENTRY | 1 | SYS_IDX_11226 | 1 | WORKFLOW_EXECUTION_UUID | A | 132821 | NULL | NULL | | BTREE | | |; +--------------------+------------+-------------------------------+--------------+-------------------------+-----------+-------------+----------+--------+------+------------+---------+---------------+; ```; So MySQL appears to be table scanning `WORKFLOW_METADATA_SUMMARY_ENTRY` and then finding the the at-most-one matching rows in `CUSTOM_LABEL_ENTRY` for each label parameter using the unique index on `WORKFLOW_EXECUTION_UUID` + `CUSTOM_LABEL_KEY`. So the labels table access should be fast but the summary table is table scanning. I experimented with adding a non-unique index on `CUSTOM_LABEL_ENTRY` for `CUSTOM_LABEL_KEY` + `CUSTOM_LABEL_VALUE` in the hope that MySQL could use that first and then join back to the summary table on workflow ID. However I haven't had any luck getting MySQL to use this index for even the simplest possible queries:. ```; mysql> create index IDX_KEY_VALUE on CUSTOM_LABEL_ENTRY (CUSTOM_LABEL_KEY, CUSTOM_LABEL_VALUE); ; .; .; .; mysql> explain select WORKFLOW_EXECUTION_UUID from CUSTOM_LABEL_ENTRY where CUSTOM_LABEL_KEY = 'caas-collection-name' AND CUSTOM_LABEL_VALUE = 'miguel-collection';; +----+-------------+--------------------+------------+------+---------------------------------------------+-------------------------------+---------+-------+------+----------+-------------+; | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |; +----+-------------+--------------------+------------+------+---------------------------------------------+-------------------------------+---------+-------+------+----------+-------------+; | 1 | SIMPLE | CUSTOM_LABEL_ENTRY | NULL | ref | UC_CUSTOM_LABEL_ENTRY_CLK_WEU,IDX_KEY_VALUE | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 1023 | const | 1 | 10.00 | Using where |; +----",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4598
https://github.com/broadinstitute/cromwell/issues/4598:5226,Usability,SIMPL,SIMPLE,5226,"ameter using the unique index on `WORKFLOW_EXECUTION_UUID` + `CUSTOM_LABEL_KEY`. So the labels table access should be fast but the summary table is table scanning. I experimented with adding a non-unique index on `CUSTOM_LABEL_ENTRY` for `CUSTOM_LABEL_KEY` + `CUSTOM_LABEL_VALUE` in the hope that MySQL could use that first and then join back to the summary table on workflow ID. However I haven't had any luck getting MySQL to use this index for even the simplest possible queries:. ```; mysql> create index IDX_KEY_VALUE on CUSTOM_LABEL_ENTRY (CUSTOM_LABEL_KEY, CUSTOM_LABEL_VALUE); ; .; .; .; mysql> explain select WORKFLOW_EXECUTION_UUID from CUSTOM_LABEL_ENTRY where CUSTOM_LABEL_KEY = 'caas-collection-name' AND CUSTOM_LABEL_VALUE = 'miguel-collection';; +----+-------------+--------------------+------------+------+---------------------------------------------+-------------------------------+---------+-------+------+----------+-------------+; | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |; +----+-------------+--------------------+------------+------+---------------------------------------------+-------------------------------+---------+-------+------+----------+-------------+; | 1 | SIMPLE | CUSTOM_LABEL_ENTRY | NULL | ref | UC_CUSTOM_LABEL_ENTRY_CLK_WEU,IDX_KEY_VALUE | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 1023 | const | 1 | 10.00 | Using where |; +----+-------------+--------------------+------------+------+---------------------------------------------+-------------------------------+---------+-------+------+----------+-------------+; ```. So MySQL sees the new index I created but then uses the unique `WORKFLOW_EXECUTION_UUID` + `CUSTOM_LABEL_KEY` index instead even though I don't see how that's applicable here. Actual CromIAM + Cromwell query issued against local MySQL:. ```; select; x2.`WORKFLOW_EXECUTION_UUID`,; x2.`WORKFLOW_NAME`,; x2.`WORKFLOW_STATUS`,; x2.`START_TIMESTAMP`,; x2.`END_TIMESTAMP`,; x2.`SUBMI",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4598
https://github.com/broadinstitute/cromwell/pull/4599:28,Testability,Test,Test,28,- Bump liquibase version; - Test generation of the rest api docs; - Start to use VAULT_TOKEN instead of JES_TOKEN,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4599
https://github.com/broadinstitute/cromwell/issues/4601:105,Modifiability,rewrite,rewrite,105,`excludeLabelAnd` and `excludeLabelOr` have their own performance issues for which I haven't found query rewrite optimizations like those for `label` and and `labelOr` in #4598. Breaking this out as a separate issue since it is not believed to be as important as positive label assertions in #4598.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4601
https://github.com/broadinstitute/cromwell/issues/4601:54,Performance,perform,performance,54,`excludeLabelAnd` and `excludeLabelOr` have their own performance issues for which I haven't found query rewrite optimizations like those for `label` and and `labelOr` in #4598. Breaking this out as a separate issue since it is not believed to be as important as positive label assertions in #4598.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4601
https://github.com/broadinstitute/cromwell/issues/4601:113,Performance,optimiz,optimizations,113,`excludeLabelAnd` and `excludeLabelOr` have their own performance issues for which I haven't found query rewrite optimizations like those for `label` and and `labelOr` in #4598. Breaking this out as a separate issue since it is not believed to be as important as positive label assertions in #4598.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4601
https://github.com/broadinstitute/cromwell/issues/4601:278,Testability,assert,assertions,278,`excludeLabelAnd` and `excludeLabelOr` have their own performance issues for which I haven't found query rewrite optimizations like those for `label` and and `labelOr` in #4598. Breaking this out as a separate issue since it is not believed to be as important as positive label assertions in #4598.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4601
https://github.com/broadinstitute/cromwell/issues/4602:932,Availability,error,error,932,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->; We are looking for a general guidance regarding how to use AWS EFS system with cromwell. ; Specifically, guidance of setup cromwell to recognize mounted EFS files in the backend and run jobs on AWS batch. . Details of what we have attempted to run the workflow using EFS on AWS:; - We tried specifying a aws EFS file system as one of the filesystems both within backend and engine constructs in addition to S3. But we get this error: ""Cannot find a filesystem with name efs in the configuration. Available filesystems: ftp, s3, demo-dos, gcs, oss, http"". If I just specify EFS, Cromwell does not start, errors out looking for S3. - Also, the EFS is mounted on my AWS batch computes. How do I specify the mount point to the container(I am asking this because, I don’t have control over creating AWS job definitions)?. We have checked lots of resources online but could not find any. And we have tried to ask questions on gatk forum with no luck: https://gatkforums.broadinstitute.org/wdl/discussion/23380/using-aws-backend-with-efs. Searched github issues, we found a similar issue opened here 7 days ago with no answer #4579 AWS backend with own source path to mount. Thanks for looking into this issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4602
https://github.com/broadinstitute/cromwell/issues/4602:1001,Availability,Avail,Available,1001,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->; We are looking for a general guidance regarding how to use AWS EFS system with cromwell. ; Specifically, guidance of setup cromwell to recognize mounted EFS files in the backend and run jobs on AWS batch. . Details of what we have attempted to run the workflow using EFS on AWS:; - We tried specifying a aws EFS file system as one of the filesystems both within backend and engine constructs in addition to S3. But we get this error: ""Cannot find a filesystem with name efs in the configuration. Available filesystems: ftp, s3, demo-dos, gcs, oss, http"". If I just specify EFS, Cromwell does not start, errors out looking for S3. - Also, the EFS is mounted on my AWS batch computes. How do I specify the mount point to the container(I am asking this because, I don’t have control over creating AWS job definitions)?. We have checked lots of resources online but could not find any. And we have tried to ask questions on gatk forum with no luck: https://gatkforums.broadinstitute.org/wdl/discussion/23380/using-aws-backend-with-efs. Searched github issues, we found a similar issue opened here 7 days ago with no answer #4579 AWS backend with own source path to mount. Thanks for looking into this issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4602
https://github.com/broadinstitute/cromwell/issues/4602:1108,Availability,error,errors,1108,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->; We are looking for a general guidance regarding how to use AWS EFS system with cromwell. ; Specifically, guidance of setup cromwell to recognize mounted EFS files in the backend and run jobs on AWS batch. . Details of what we have attempted to run the workflow using EFS on AWS:; - We tried specifying a aws EFS file system as one of the filesystems both within backend and engine constructs in addition to S3. But we get this error: ""Cannot find a filesystem with name efs in the configuration. Available filesystems: ftp, s3, demo-dos, gcs, oss, http"". If I just specify EFS, Cromwell does not start, errors out looking for S3. - Also, the EFS is mounted on my AWS batch computes. How do I specify the mount point to the container(I am asking this because, I don’t have control over creating AWS job definitions)?. We have checked lots of resources online but could not find any. And we have tried to ask questions on gatk forum with no luck: https://gatkforums.broadinstitute.org/wdl/discussion/23380/using-aws-backend-with-efs. Searched github issues, we found a similar issue opened here 7 days ago with no answer #4579 AWS backend with own source path to mount. Thanks for looking into this issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4602
https://github.com/broadinstitute/cromwell/issues/4602:986,Deployability,configurat,configuration,986,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->; We are looking for a general guidance regarding how to use AWS EFS system with cromwell. ; Specifically, guidance of setup cromwell to recognize mounted EFS files in the backend and run jobs on AWS batch. . Details of what we have attempted to run the workflow using EFS on AWS:; - We tried specifying a aws EFS file system as one of the filesystems both within backend and engine constructs in addition to S3. But we get this error: ""Cannot find a filesystem with name efs in the configuration. Available filesystems: ftp, s3, demo-dos, gcs, oss, http"". If I just specify EFS, Cromwell does not start, errors out looking for S3. - Also, the EFS is mounted on my AWS batch computes. How do I specify the mount point to the container(I am asking this because, I don’t have control over creating AWS job definitions)?. We have checked lots of resources online but could not find any. And we have tried to ask questions on gatk forum with no luck: https://gatkforums.broadinstitute.org/wdl/discussion/23380/using-aws-backend-with-efs. Searched github issues, we found a similar issue opened here 7 days ago with no answer #4579 AWS backend with own source path to mount. Thanks for looking into this issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4602
https://github.com/broadinstitute/cromwell/issues/4602:986,Modifiability,config,configuration,986,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->; We are looking for a general guidance regarding how to use AWS EFS system with cromwell. ; Specifically, guidance of setup cromwell to recognize mounted EFS files in the backend and run jobs on AWS batch. . Details of what we have attempted to run the workflow using EFS on AWS:; - We tried specifying a aws EFS file system as one of the filesystems both within backend and engine constructs in addition to S3. But we get this error: ""Cannot find a filesystem with name efs in the configuration. Available filesystems: ftp, s3, demo-dos, gcs, oss, http"". If I just specify EFS, Cromwell does not start, errors out looking for S3. - Also, the EFS is mounted on my AWS batch computes. How do I specify the mount point to the container(I am asking this because, I don’t have control over creating AWS job definitions)?. We have checked lots of resources online but could not find any. And we have tried to ask questions on gatk forum with no luck: https://gatkforums.broadinstitute.org/wdl/discussion/23380/using-aws-backend-with-efs. Searched github issues, we found a similar issue opened here 7 days ago with no answer #4579 AWS backend with own source path to mount. Thanks for looking into this issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4602
https://github.com/broadinstitute/cromwell/issues/4602:47,Usability,feedback,feedback,47,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->; We are looking for a general guidance regarding how to use AWS EFS system with cromwell. ; Specifically, guidance of setup cromwell to recognize mounted EFS files in the backend and run jobs on AWS batch. . Details of what we have attempted to run the workflow using EFS on AWS:; - We tried specifying a aws EFS file system as one of the filesystems both within backend and engine constructs in addition to S3. But we get this error: ""Cannot find a filesystem with name efs in the configuration. Available filesystems: ftp, s3, demo-dos, gcs, oss, http"". If I just specify EFS, Cromwell does not start, errors out looking for S3. - Also, the EFS is mounted on my AWS batch computes. How do I specify the mount point to the container(I am asking this because, I don’t have control over creating AWS job definitions)?. We have checked lots of resources online but could not find any. And we have tried to ask questions on gatk forum with no luck: https://gatkforums.broadinstitute.org/wdl/discussion/23380/using-aws-backend-with-efs. Searched github issues, we found a similar issue opened here 7 days ago with no answer #4579 AWS backend with own source path to mount. Thanks for looking into this issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4602
https://github.com/broadinstitute/cromwell/issues/4602:534,Usability,guid,guidance,534,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->; We are looking for a general guidance regarding how to use AWS EFS system with cromwell. ; Specifically, guidance of setup cromwell to recognize mounted EFS files in the backend and run jobs on AWS batch. . Details of what we have attempted to run the workflow using EFS on AWS:; - We tried specifying a aws EFS file system as one of the filesystems both within backend and engine constructs in addition to S3. But we get this error: ""Cannot find a filesystem with name efs in the configuration. Available filesystems: ftp, s3, demo-dos, gcs, oss, http"". If I just specify EFS, Cromwell does not start, errors out looking for S3. - Also, the EFS is mounted on my AWS batch computes. How do I specify the mount point to the container(I am asking this because, I don’t have control over creating AWS job definitions)?. We have checked lots of resources online but could not find any. And we have tried to ask questions on gatk forum with no luck: https://gatkforums.broadinstitute.org/wdl/discussion/23380/using-aws-backend-with-efs. Searched github issues, we found a similar issue opened here 7 days ago with no answer #4579 AWS backend with own source path to mount. Thanks for looking into this issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4602
https://github.com/broadinstitute/cromwell/issues/4602:610,Usability,guid,guidance,610,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->; We are looking for a general guidance regarding how to use AWS EFS system with cromwell. ; Specifically, guidance of setup cromwell to recognize mounted EFS files in the backend and run jobs on AWS batch. . Details of what we have attempted to run the workflow using EFS on AWS:; - We tried specifying a aws EFS file system as one of the filesystems both within backend and engine constructs in addition to S3. But we get this error: ""Cannot find a filesystem with name efs in the configuration. Available filesystems: ftp, s3, demo-dos, gcs, oss, http"". If I just specify EFS, Cromwell does not start, errors out looking for S3. - Also, the EFS is mounted on my AWS batch computes. How do I specify the mount point to the container(I am asking this because, I don’t have control over creating AWS job definitions)?. We have checked lots of resources online but could not find any. And we have tried to ask questions on gatk forum with no luck: https://gatkforums.broadinstitute.org/wdl/discussion/23380/using-aws-backend-with-efs. Searched github issues, we found a similar issue opened here 7 days ago with no answer #4579 AWS backend with own source path to mount. Thanks for looking into this issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4602
https://github.com/broadinstitute/cromwell/issues/4603:19,Availability,error,error,19,"Here is an example error for a task failing since there is less space than needed:. ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task exceed_disk_size.simple_localize_and_fetch_size:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: action 9: unexpected exit status 1 was not ignored\n[Localization] Input name: input_file - Unexpected exit status 1 while running \""/bin/sh -c python -c 'import base64; print(base64.b64decode(\\\""IyEvYmluL2Jhc2gKCmZvciBpIGluICQoc2VxIDMpOyBkbwogICgKICAgIGdzdXRpbCAgY3AgZ3M6Ly9zc19jcm9td2VsbF9idWNrZXQvcmFuZG9tX2ZpbGVfb3Zlcl8xX2diLnR4dCAvY3JvbXdlbGxfcm9vdC9zc19jcm9td2VsbF9idWNrZXQvcmFuZG9tX2ZpbGVfb3Zlcl8xX2diLnR4dCA+IGdzdXRpbF9vdXRwdXQudHh0IDI+JjEKIyBSZWNvcmQgdGhlIGV4aXQgY29kZSBvZiB0aGUgZ3N1dGlsIGNvbW1hbmQgd2l0aG91dCBwcm9qZWN0IGZsYWcKUkNfR1NVVElMPSQ/CmlmIFsgIiRSQ19HU1VUSUwiICE9ICIwIiBdOyB0aGVuCiAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgZ3N1dGlsXCBcIGNwXCBnczovL3NzX2Nyb213ZWxsX2J1Y2tldC9yYW5kb21fZmlsZV9vdmVyXzFfZ2IudHh0XCAvY3JvbXdlbGxfcm9vdC9zc19jcm9td2VsbF9idWNrZXQvcmFuZG9tX2ZpbGVfb3Zlcl8xX2diLnR4dFwgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CiAgCiAgIyBDaGVjayBpZiBpdCBtYXRjaGVzIHRoZSBCdWNrZXRJc1JlcXVlc3RlclBheXNFcnJvck1lc3NhZ2UKICBpZiBncmVwIC1xICJCdWNrZXQgaXMgcmVxdWVzdGVyIHBheXMgYnVja2V0IGJ1dCBubyB1c2VyIHByb2plY3QgcHJvdmlkZWQuIiBnc3V0aWxfb3V0cHV0LnR4dDsgdGhlbgogICAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgUmV0cnlpbmdcIHdpdGhcIHVzZXJcIHByb2plY3QKICAgIGdzdXRpbCAtdSBicm9hZC1kc2RlLWNyb213ZWxsLWRldiBjcCBnczovL3NzX2Nyb213ZWxsX2J1Y2tldC9yYW5kb21fZmlsZV9vdmVyXzFfZ2IudHh0IC9jcm9td2VsbF9yb290L3NzX2Nyb213ZWxsX2J1Y2tldC9yYW5kb21fZmlsZV9vdmVyXzFfZ2IudHh0CiAgZWxzZQogICAgZXhpdCAiJFJDX0dTVVRJTCIKICBmaQplbHNlCiAgZXhpdCAwCmZpCiAgKQogIFJDPSQ/CiAgaWYgWyAiJFJDIiA9ICIwIiBdOyB0aGVuCiAgICBicmVhawogIGZpCiAgaWYgWyAkaSAtbHQgMyBdOyB0aGVuCiAgICBwcmludGYgJyVzICVzXG4nICIkKGRhdGUgLXUgJyslWS8lbS8lZCAlSDolTTolUycpIiBXYWl0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4603
https://github.com/broadinstitute/cromwell/issues/4603:90,Availability,failure,failures,90,"Here is an example error for a task failing since there is less space than needed:. ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task exceed_disk_size.simple_localize_and_fetch_size:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: action 9: unexpected exit status 1 was not ignored\n[Localization] Input name: input_file - Unexpected exit status 1 while running \""/bin/sh -c python -c 'import base64; print(base64.b64decode(\\\""IyEvYmluL2Jhc2gKCmZvciBpIGluICQoc2VxIDMpOyBkbwogICgKICAgIGdzdXRpbCAgY3AgZ3M6Ly9zc19jcm9td2VsbF9idWNrZXQvcmFuZG9tX2ZpbGVfb3Zlcl8xX2diLnR4dCAvY3JvbXdlbGxfcm9vdC9zc19jcm9td2VsbF9idWNrZXQvcmFuZG9tX2ZpbGVfb3Zlcl8xX2diLnR4dCA+IGdzdXRpbF9vdXRwdXQudHh0IDI+JjEKIyBSZWNvcmQgdGhlIGV4aXQgY29kZSBvZiB0aGUgZ3N1dGlsIGNvbW1hbmQgd2l0aG91dCBwcm9qZWN0IGZsYWcKUkNfR1NVVElMPSQ/CmlmIFsgIiRSQ19HU1VUSUwiICE9ICIwIiBdOyB0aGVuCiAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgZ3N1dGlsXCBcIGNwXCBnczovL3NzX2Nyb213ZWxsX2J1Y2tldC9yYW5kb21fZmlsZV9vdmVyXzFfZ2IudHh0XCAvY3JvbXdlbGxfcm9vdC9zc19jcm9td2VsbF9idWNrZXQvcmFuZG9tX2ZpbGVfb3Zlcl8xX2diLnR4dFwgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CiAgCiAgIyBDaGVjayBpZiBpdCBtYXRjaGVzIHRoZSBCdWNrZXRJc1JlcXVlc3RlclBheXNFcnJvck1lc3NhZ2UKICBpZiBncmVwIC1xICJCdWNrZXQgaXMgcmVxdWVzdGVyIHBheXMgYnVja2V0IGJ1dCBubyB1c2VyIHByb2plY3QgcHJvdmlkZWQuIiBnc3V0aWxfb3V0cHV0LnR4dDsgdGhlbgogICAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgUmV0cnlpbmdcIHdpdGhcIHVzZXJcIHByb2plY3QKICAgIGdzdXRpbCAtdSBicm9hZC1kc2RlLWNyb213ZWxsLWRldiBjcCBnczovL3NzX2Nyb213ZWxsX2J1Y2tldC9yYW5kb21fZmlsZV9vdmVyXzFfZ2IudHh0IC9jcm9td2VsbF9yb290L3NzX2Nyb213ZWxsX2J1Y2tldC9yYW5kb21fZmlsZV9vdmVyXzFfZ2IudHh0CiAgZWxzZQogICAgZXhpdCAiJFJDX0dTVVRJTCIKICBmaQplbHNlCiAgZXhpdCAwCmZpCiAgKQogIFJDPSQ/CiAgaWYgWyAiJFJDIiA9ICIwIiBdOyB0aGVuCiAgICBicmVhawogIGZpCiAgaWYgWyAkaSAtbHQgMyBdOyB0aGVuCiAgICBwcmludGYgJyVzICVzXG4nICIkKGRhdGUgLXUgJyslWS8lbS8lZCAlSDolTTolUycpIiBXYWl0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4603
https://github.com/broadinstitute/cromwell/issues/4603:256,Availability,error,error,256,"Here is an example error for a task failing since there is less space than needed:. ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task exceed_disk_size.simple_localize_and_fetch_size:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: action 9: unexpected exit status 1 was not ignored\n[Localization] Input name: input_file - Unexpected exit status 1 while running \""/bin/sh -c python -c 'import base64; print(base64.b64decode(\\\""IyEvYmluL2Jhc2gKCmZvciBpIGluICQoc2VxIDMpOyBkbwogICgKICAgIGdzdXRpbCAgY3AgZ3M6Ly9zc19jcm9td2VsbF9idWNrZXQvcmFuZG9tX2ZpbGVfb3Zlcl8xX2diLnR4dCAvY3JvbXdlbGxfcm9vdC9zc19jcm9td2VsbF9idWNrZXQvcmFuZG9tX2ZpbGVfb3Zlcl8xX2diLnR4dCA+IGdzdXRpbF9vdXRwdXQudHh0IDI+JjEKIyBSZWNvcmQgdGhlIGV4aXQgY29kZSBvZiB0aGUgZ3N1dGlsIGNvbW1hbmQgd2l0aG91dCBwcm9qZWN0IGZsYWcKUkNfR1NVVElMPSQ/CmlmIFsgIiRSQ19HU1VUSUwiICE9ICIwIiBdOyB0aGVuCiAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgZ3N1dGlsXCBcIGNwXCBnczovL3NzX2Nyb213ZWxsX2J1Y2tldC9yYW5kb21fZmlsZV9vdmVyXzFfZ2IudHh0XCAvY3JvbXdlbGxfcm9vdC9zc19jcm9td2VsbF9idWNrZXQvcmFuZG9tX2ZpbGVfb3Zlcl8xX2diLnR4dFwgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CiAgCiAgIyBDaGVjayBpZiBpdCBtYXRjaGVzIHRoZSBCdWNrZXRJc1JlcXVlc3RlclBheXNFcnJvck1lc3NhZ2UKICBpZiBncmVwIC1xICJCdWNrZXQgaXMgcmVxdWVzdGVyIHBheXMgYnVja2V0IGJ1dCBubyB1c2VyIHByb2plY3QgcHJvdmlkZWQuIiBnc3V0aWxfb3V0cHV0LnR4dDsgdGhlbgogICAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgUmV0cnlpbmdcIHdpdGhcIHVzZXJcIHByb2plY3QKICAgIGdzdXRpbCAtdSBicm9hZC1kc2RlLWNyb213ZWxsLWRldiBjcCBnczovL3NzX2Nyb213ZWxsX2J1Y2tldC9yYW5kb21fZmlsZV9vdmVyXzFfZ2IudHh0IC9jcm9td2VsbF9yb290L3NzX2Nyb213ZWxsX2J1Y2tldC9yYW5kb21fZmlsZV9vdmVyXzFfZ2IudHh0CiAgZWxzZQogICAgZXhpdCAiJFJDX0dTVVRJTCIKICBmaQplbHNlCiAgZXhpdCAwCmZpCiAgKQogIFJDPSQ/CiAgaWYgWyAiJFJDIiA9ICIwIiBdOyB0aGVuCiAgICBicmVhawogIGZpCiAgaWYgWyAkaSAtbHQgMyBdOyB0aGVuCiAgICBwcmludGYgJyVzICVzXG4nICIkKGRhdGUgLXUgJyslWS8lbS8lZCAlSDolTTolUycpIiBXYWl0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4603
https://github.com/broadinstitute/cromwell/issues/4603:8889,Availability,error,error,8889,"JDX0dTVVRJTD0kPwppZiBbICIkUkNfR1NVVElMIiAhPSAiMCIgXTsgdGhlbgogIHByaW50ZiAnJXMgJXNcbicgIiQoZGF0ZSAtdSAnKyVZLyVtLyVkICVIOiVNOiVTJykiIGdzdXRpbFwgXCBcIGNwXCAvY3JvbXdlbGxfcm9vdC9yY1wgZ3M6Ly9zc19jcm9td2VsbF9idWNrZXQvY3JvbXdlbGwtZXhlY3V0aW9uL2V4Y2VlZF9kaXNrX3NpemUvYThlMzJjZDItZTMwNi00OGI2LTlkNWMtODQ3YjFlZDg1NzU0L2NhbGwtc2ltcGxlX2xvY2FsaXplX2FuZF9mZXRjaF9zaXplL1wgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CiAgCiAgIyBDaGVjayBpZiBpdCBtYXRjaGVzIHRoZSBCdWNrZXRJc1JlcXVlc3RlclBheXNFcnJvck1lc3NhZ2UKICBpZiBncmVwIC1xICJCdWNrZXQgaXMgcmVxdWVzdGVyIHBheXMgYnVja2V0IGJ1dCBubyB1c2VyIHByb2plY3QgcHJvdmlkZWQuIiBnc3V0aWxfb3V0cHV0LnR4dDsgdGhlbgogICAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgUmV0cnlpbmdcIHdpdGhcIHVzZXJcIHByb2plY3QKICAgIGdzdXRpbCAtdSBicm9hZC1kc2RlLWNyb213ZWxsLWRldiAgY3AgL2Nyb213ZWxsX3Jvb3QvcmMgZ3M6Ly9zc19jcm9td2VsbF9idWNrZXQvY3JvbXdlbGwtZXhlY3V0aW9uL2V4Y2VlZF9kaXNrX3NpemUvYThlMzJjZDItZTMwNi00OGI2LTlkNWMtODQ3YjFlZDg1NzU0L2NhbGwtc2ltcGxlX2xvY2FsaXplX2FuZF9mZXRjaF9zaXplLwogIGVsc2UKICAgIGV4aXQgIiRSQ19HU1VUSUwiCiAgZmkKZWxzZQogIGV4aXQgMApmaQogICkKICBSQz0kPwogIGlmIFsgIiRSQyIgPSAiMCIgXTsgdGhlbgogICAgYnJlYWsKICBmaQogIGlmIFsgJGkgLWx0IDMgXTsgdGhlbgogICAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgV2FpdGluZ1wgNVwgc2Vjb25kc1wgYW5kXCByZXRyeWluZwogICAgc2xlZXAgNQogIGZpCmRvbmUKZXhpdCAiJFJDIg==\\\""));' > /tmp/c7f23139-58e4-466e-b109-c06db3209f35.sh && chmod u+x /tmp/c7f23139-58e4-466e-b109-c06db3209f35.sh && sh /tmp/c7f23139-58e4-466e-b109-c06db3209f35.sh\"": ""; }; ]; ```. AC:; Improve this error message by...; 1. Removing all of the message starting from ` Unexpected exit status 1 while running ....`; 2. Replace:; `Execution failed: action 9: unexpected exit status 1 was not ignored\n[Localization] Input name: input_file `; with something similar to:; `Execution failed: action 9: Failed to copy input <input_name> <input_path ie, gs://.....>. Please check the log file for more details: <link to call log>`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4603
https://github.com/broadinstitute/cromwell/issues/4603:125,Integrability,message,message,125,"Here is an example error for a task failing since there is less space than needed:. ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task exceed_disk_size.simple_localize_and_fetch_size:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: action 9: unexpected exit status 1 was not ignored\n[Localization] Input name: input_file - Unexpected exit status 1 while running \""/bin/sh -c python -c 'import base64; print(base64.b64decode(\\\""IyEvYmluL2Jhc2gKCmZvciBpIGluICQoc2VxIDMpOyBkbwogICgKICAgIGdzdXRpbCAgY3AgZ3M6Ly9zc19jcm9td2VsbF9idWNrZXQvcmFuZG9tX2ZpbGVfb3Zlcl8xX2diLnR4dCAvY3JvbXdlbGxfcm9vdC9zc19jcm9td2VsbF9idWNrZXQvcmFuZG9tX2ZpbGVfb3Zlcl8xX2diLnR4dCA+IGdzdXRpbF9vdXRwdXQudHh0IDI+JjEKIyBSZWNvcmQgdGhlIGV4aXQgY29kZSBvZiB0aGUgZ3N1dGlsIGNvbW1hbmQgd2l0aG91dCBwcm9qZWN0IGZsYWcKUkNfR1NVVElMPSQ/CmlmIFsgIiRSQ19HU1VUSUwiICE9ICIwIiBdOyB0aGVuCiAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgZ3N1dGlsXCBcIGNwXCBnczovL3NzX2Nyb213ZWxsX2J1Y2tldC9yYW5kb21fZmlsZV9vdmVyXzFfZ2IudHh0XCAvY3JvbXdlbGxfcm9vdC9zc19jcm9td2VsbF9idWNrZXQvcmFuZG9tX2ZpbGVfb3Zlcl8xX2diLnR4dFwgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CiAgCiAgIyBDaGVjayBpZiBpdCBtYXRjaGVzIHRoZSBCdWNrZXRJc1JlcXVlc3RlclBheXNFcnJvck1lc3NhZ2UKICBpZiBncmVwIC1xICJCdWNrZXQgaXMgcmVxdWVzdGVyIHBheXMgYnVja2V0IGJ1dCBubyB1c2VyIHByb2plY3QgcHJvdmlkZWQuIiBnc3V0aWxfb3V0cHV0LnR4dDsgdGhlbgogICAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgUmV0cnlpbmdcIHdpdGhcIHVzZXJcIHByb2plY3QKICAgIGdzdXRpbCAtdSBicm9hZC1kc2RlLWNyb213ZWxsLWRldiBjcCBnczovL3NzX2Nyb213ZWxsX2J1Y2tldC9yYW5kb21fZmlsZV9vdmVyXzFfZ2IudHh0IC9jcm9td2VsbF9yb290L3NzX2Nyb213ZWxsX2J1Y2tldC9yYW5kb21fZmlsZV9vdmVyXzFfZ2IudHh0CiAgZWxzZQogICAgZXhpdCAiJFJDX0dTVVRJTCIKICBmaQplbHNlCiAgZXhpdCAwCmZpCiAgKQogIFJDPSQ/CiAgaWYgWyAiJFJDIiA9ICIwIiBdOyB0aGVuCiAgICBicmVhawogIGZpCiAgaWYgWyAkaSAtbHQgMyBdOyB0aGVuCiAgICBwcmludGYgJyVzICVzXG4nICIkKGRhdGUgLXUgJyslWS8lbS8lZCAlSDolTTolUycpIiBXYWl0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4603
https://github.com/broadinstitute/cromwell/issues/4603:8895,Integrability,message,message,8895,"JDX0dTVVRJTD0kPwppZiBbICIkUkNfR1NVVElMIiAhPSAiMCIgXTsgdGhlbgogIHByaW50ZiAnJXMgJXNcbicgIiQoZGF0ZSAtdSAnKyVZLyVtLyVkICVIOiVNOiVTJykiIGdzdXRpbFwgXCBcIGNwXCAvY3JvbXdlbGxfcm9vdC9yY1wgZ3M6Ly9zc19jcm9td2VsbF9idWNrZXQvY3JvbXdlbGwtZXhlY3V0aW9uL2V4Y2VlZF9kaXNrX3NpemUvYThlMzJjZDItZTMwNi00OGI2LTlkNWMtODQ3YjFlZDg1NzU0L2NhbGwtc2ltcGxlX2xvY2FsaXplX2FuZF9mZXRjaF9zaXplL1wgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CiAgCiAgIyBDaGVjayBpZiBpdCBtYXRjaGVzIHRoZSBCdWNrZXRJc1JlcXVlc3RlclBheXNFcnJvck1lc3NhZ2UKICBpZiBncmVwIC1xICJCdWNrZXQgaXMgcmVxdWVzdGVyIHBheXMgYnVja2V0IGJ1dCBubyB1c2VyIHByb2plY3QgcHJvdmlkZWQuIiBnc3V0aWxfb3V0cHV0LnR4dDsgdGhlbgogICAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgUmV0cnlpbmdcIHdpdGhcIHVzZXJcIHByb2plY3QKICAgIGdzdXRpbCAtdSBicm9hZC1kc2RlLWNyb213ZWxsLWRldiAgY3AgL2Nyb213ZWxsX3Jvb3QvcmMgZ3M6Ly9zc19jcm9td2VsbF9idWNrZXQvY3JvbXdlbGwtZXhlY3V0aW9uL2V4Y2VlZF9kaXNrX3NpemUvYThlMzJjZDItZTMwNi00OGI2LTlkNWMtODQ3YjFlZDg1NzU0L2NhbGwtc2ltcGxlX2xvY2FsaXplX2FuZF9mZXRjaF9zaXplLwogIGVsc2UKICAgIGV4aXQgIiRSQ19HU1VUSUwiCiAgZmkKZWxzZQogIGV4aXQgMApmaQogICkKICBSQz0kPwogIGlmIFsgIiRSQyIgPSAiMCIgXTsgdGhlbgogICAgYnJlYWsKICBmaQogIGlmIFsgJGkgLWx0IDMgXTsgdGhlbgogICAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgV2FpdGluZ1wgNVwgc2Vjb25kc1wgYW5kXCByZXRyeWluZwogICAgc2xlZXAgNQogIGZpCmRvbmUKZXhpdCAiJFJDIg==\\\""));' > /tmp/c7f23139-58e4-466e-b109-c06db3209f35.sh && chmod u+x /tmp/c7f23139-58e4-466e-b109-c06db3209f35.sh && sh /tmp/c7f23139-58e4-466e-b109-c06db3209f35.sh\"": ""; }; ]; ```. AC:; Improve this error message by...; 1. Removing all of the message starting from ` Unexpected exit status 1 while running ....`; 2. Replace:; `Execution failed: action 9: unexpected exit status 1 was not ignored\n[Localization] Input name: input_file `; with something similar to:; `Execution failed: action 9: Failed to copy input <input_name> <input_path ie, gs://.....>. Please check the log file for more details: <link to call log>`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4603
https://github.com/broadinstitute/cromwell/issues/4603:8933,Integrability,message,message,8933,"JDX0dTVVRJTD0kPwppZiBbICIkUkNfR1NVVElMIiAhPSAiMCIgXTsgdGhlbgogIHByaW50ZiAnJXMgJXNcbicgIiQoZGF0ZSAtdSAnKyVZLyVtLyVkICVIOiVNOiVTJykiIGdzdXRpbFwgXCBcIGNwXCAvY3JvbXdlbGxfcm9vdC9yY1wgZ3M6Ly9zc19jcm9td2VsbF9idWNrZXQvY3JvbXdlbGwtZXhlY3V0aW9uL2V4Y2VlZF9kaXNrX3NpemUvYThlMzJjZDItZTMwNi00OGI2LTlkNWMtODQ3YjFlZDg1NzU0L2NhbGwtc2ltcGxlX2xvY2FsaXplX2FuZF9mZXRjaF9zaXplL1wgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CiAgCiAgIyBDaGVjayBpZiBpdCBtYXRjaGVzIHRoZSBCdWNrZXRJc1JlcXVlc3RlclBheXNFcnJvck1lc3NhZ2UKICBpZiBncmVwIC1xICJCdWNrZXQgaXMgcmVxdWVzdGVyIHBheXMgYnVja2V0IGJ1dCBubyB1c2VyIHByb2plY3QgcHJvdmlkZWQuIiBnc3V0aWxfb3V0cHV0LnR4dDsgdGhlbgogICAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgUmV0cnlpbmdcIHdpdGhcIHVzZXJcIHByb2plY3QKICAgIGdzdXRpbCAtdSBicm9hZC1kc2RlLWNyb213ZWxsLWRldiAgY3AgL2Nyb213ZWxsX3Jvb3QvcmMgZ3M6Ly9zc19jcm9td2VsbF9idWNrZXQvY3JvbXdlbGwtZXhlY3V0aW9uL2V4Y2VlZF9kaXNrX3NpemUvYThlMzJjZDItZTMwNi00OGI2LTlkNWMtODQ3YjFlZDg1NzU0L2NhbGwtc2ltcGxlX2xvY2FsaXplX2FuZF9mZXRjaF9zaXplLwogIGVsc2UKICAgIGV4aXQgIiRSQ19HU1VUSUwiCiAgZmkKZWxzZQogIGV4aXQgMApmaQogICkKICBSQz0kPwogIGlmIFsgIiRSQyIgPSAiMCIgXTsgdGhlbgogICAgYnJlYWsKICBmaQogIGlmIFsgJGkgLWx0IDMgXTsgdGhlbgogICAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgV2FpdGluZ1wgNVwgc2Vjb25kc1wgYW5kXCByZXRyeWluZwogICAgc2xlZXAgNQogIGZpCmRvbmUKZXhpdCAiJFJDIg==\\\""));' > /tmp/c7f23139-58e4-466e-b109-c06db3209f35.sh && chmod u+x /tmp/c7f23139-58e4-466e-b109-c06db3209f35.sh && sh /tmp/c7f23139-58e4-466e-b109-c06db3209f35.sh\"": ""; }; ]; ```. AC:; Improve this error message by...; 1. Removing all of the message starting from ` Unexpected exit status 1 while running ....`; 2. Replace:; `Execution failed: action 9: unexpected exit status 1 was not ignored\n[Localization] Input name: input_file `; with something similar to:; `Execution failed: action 9: Failed to copy input <input_name> <input_path ie, gs://.....>. Please check the log file for more details: <link to call log>`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4603
https://github.com/broadinstitute/cromwell/issues/4603:9265,Testability,log,log,9265,"JDX0dTVVRJTD0kPwppZiBbICIkUkNfR1NVVElMIiAhPSAiMCIgXTsgdGhlbgogIHByaW50ZiAnJXMgJXNcbicgIiQoZGF0ZSAtdSAnKyVZLyVtLyVkICVIOiVNOiVTJykiIGdzdXRpbFwgXCBcIGNwXCAvY3JvbXdlbGxfcm9vdC9yY1wgZ3M6Ly9zc19jcm9td2VsbF9idWNrZXQvY3JvbXdlbGwtZXhlY3V0aW9uL2V4Y2VlZF9kaXNrX3NpemUvYThlMzJjZDItZTMwNi00OGI2LTlkNWMtODQ3YjFlZDg1NzU0L2NhbGwtc2ltcGxlX2xvY2FsaXplX2FuZF9mZXRjaF9zaXplL1wgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CiAgCiAgIyBDaGVjayBpZiBpdCBtYXRjaGVzIHRoZSBCdWNrZXRJc1JlcXVlc3RlclBheXNFcnJvck1lc3NhZ2UKICBpZiBncmVwIC1xICJCdWNrZXQgaXMgcmVxdWVzdGVyIHBheXMgYnVja2V0IGJ1dCBubyB1c2VyIHByb2plY3QgcHJvdmlkZWQuIiBnc3V0aWxfb3V0cHV0LnR4dDsgdGhlbgogICAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgUmV0cnlpbmdcIHdpdGhcIHVzZXJcIHByb2plY3QKICAgIGdzdXRpbCAtdSBicm9hZC1kc2RlLWNyb213ZWxsLWRldiAgY3AgL2Nyb213ZWxsX3Jvb3QvcmMgZ3M6Ly9zc19jcm9td2VsbF9idWNrZXQvY3JvbXdlbGwtZXhlY3V0aW9uL2V4Y2VlZF9kaXNrX3NpemUvYThlMzJjZDItZTMwNi00OGI2LTlkNWMtODQ3YjFlZDg1NzU0L2NhbGwtc2ltcGxlX2xvY2FsaXplX2FuZF9mZXRjaF9zaXplLwogIGVsc2UKICAgIGV4aXQgIiRSQ19HU1VUSUwiCiAgZmkKZWxzZQogIGV4aXQgMApmaQogICkKICBSQz0kPwogIGlmIFsgIiRSQyIgPSAiMCIgXTsgdGhlbgogICAgYnJlYWsKICBmaQogIGlmIFsgJGkgLWx0IDMgXTsgdGhlbgogICAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgV2FpdGluZ1wgNVwgc2Vjb25kc1wgYW5kXCByZXRyeWluZwogICAgc2xlZXAgNQogIGZpCmRvbmUKZXhpdCAiJFJDIg==\\\""));' > /tmp/c7f23139-58e4-466e-b109-c06db3209f35.sh && chmod u+x /tmp/c7f23139-58e4-466e-b109-c06db3209f35.sh && sh /tmp/c7f23139-58e4-466e-b109-c06db3209f35.sh\"": ""; }; ]; ```. AC:; Improve this error message by...; 1. Removing all of the message starting from ` Unexpected exit status 1 while running ....`; 2. Replace:; `Execution failed: action 9: unexpected exit status 1 was not ignored\n[Localization] Input name: input_file `; with something similar to:; `Execution failed: action 9: Failed to copy input <input_name> <input_path ie, gs://.....>. Please check the log file for more details: <link to call log>`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4603
https://github.com/broadinstitute/cromwell/issues/4603:9306,Testability,log,log,9306,"JDX0dTVVRJTD0kPwppZiBbICIkUkNfR1NVVElMIiAhPSAiMCIgXTsgdGhlbgogIHByaW50ZiAnJXMgJXNcbicgIiQoZGF0ZSAtdSAnKyVZLyVtLyVkICVIOiVNOiVTJykiIGdzdXRpbFwgXCBcIGNwXCAvY3JvbXdlbGxfcm9vdC9yY1wgZ3M6Ly9zc19jcm9td2VsbF9idWNrZXQvY3JvbXdlbGwtZXhlY3V0aW9uL2V4Y2VlZF9kaXNrX3NpemUvYThlMzJjZDItZTMwNi00OGI2LTlkNWMtODQ3YjFlZDg1NzU0L2NhbGwtc2ltcGxlX2xvY2FsaXplX2FuZF9mZXRjaF9zaXplL1wgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CiAgCiAgIyBDaGVjayBpZiBpdCBtYXRjaGVzIHRoZSBCdWNrZXRJc1JlcXVlc3RlclBheXNFcnJvck1lc3NhZ2UKICBpZiBncmVwIC1xICJCdWNrZXQgaXMgcmVxdWVzdGVyIHBheXMgYnVja2V0IGJ1dCBubyB1c2VyIHByb2plY3QgcHJvdmlkZWQuIiBnc3V0aWxfb3V0cHV0LnR4dDsgdGhlbgogICAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgUmV0cnlpbmdcIHdpdGhcIHVzZXJcIHByb2plY3QKICAgIGdzdXRpbCAtdSBicm9hZC1kc2RlLWNyb213ZWxsLWRldiAgY3AgL2Nyb213ZWxsX3Jvb3QvcmMgZ3M6Ly9zc19jcm9td2VsbF9idWNrZXQvY3JvbXdlbGwtZXhlY3V0aW9uL2V4Y2VlZF9kaXNrX3NpemUvYThlMzJjZDItZTMwNi00OGI2LTlkNWMtODQ3YjFlZDg1NzU0L2NhbGwtc2ltcGxlX2xvY2FsaXplX2FuZF9mZXRjaF9zaXplLwogIGVsc2UKICAgIGV4aXQgIiRSQ19HU1VUSUwiCiAgZmkKZWxzZQogIGV4aXQgMApmaQogICkKICBSQz0kPwogIGlmIFsgIiRSQyIgPSAiMCIgXTsgdGhlbgogICAgYnJlYWsKICBmaQogIGlmIFsgJGkgLWx0IDMgXTsgdGhlbgogICAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgV2FpdGluZ1wgNVwgc2Vjb25kc1wgYW5kXCByZXRyeWluZwogICAgc2xlZXAgNQogIGZpCmRvbmUKZXhpdCAiJFJDIg==\\\""));' > /tmp/c7f23139-58e4-466e-b109-c06db3209f35.sh && chmod u+x /tmp/c7f23139-58e4-466e-b109-c06db3209f35.sh && sh /tmp/c7f23139-58e4-466e-b109-c06db3209f35.sh\"": ""; }; ]; ```. AC:; Improve this error message by...; 1. Removing all of the message starting from ` Unexpected exit status 1 while running ....`; 2. Replace:; `Execution failed: action 9: unexpected exit status 1 was not ignored\n[Localization] Input name: input_file `; with something similar to:; `Execution failed: action 9: Failed to copy input <input_name> <input_path ie, gs://.....>. Please check the log file for more details: <link to call log>`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4603
https://github.com/broadinstitute/cromwell/issues/4604:157,Availability,error,error,157,"When I submit jobs to AWS Batch through cromwell, if my batch compute instance uses the latest ECS agent (ver 1.25) then all my AWS Batch jobs fail with the error: ""CannotStartContainerError: Error response from daemon: OCI runtime create failed: container_linux.go:348: starting container process caused ""exec: \""gzipdata\"": executable file not found in $PATH"": unknown"".; Jobs run fine if I directly submit them to batch instead of submitting thro cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4604
https://github.com/broadinstitute/cromwell/issues/4604:192,Availability,Error,Error,192,"When I submit jobs to AWS Batch through cromwell, if my batch compute instance uses the latest ECS agent (ver 1.25) then all my AWS Batch jobs fail with the error: ""CannotStartContainerError: Error response from daemon: OCI runtime create failed: container_linux.go:348: starting container process caused ""exec: \""gzipdata\"": executable file not found in $PATH"": unknown"".; Jobs run fine if I directly submit them to batch instead of submitting thro cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4604
https://github.com/broadinstitute/cromwell/issues/4605:42,Availability,error,error,42,"I get an `ArrayIndexOutOfBoundsException` error when starting the latest `develop` branch in server mode with a mysql (`mariadb-10.3.12-5.fc30.x86_64`) backend.; ```; [jeremiah@localhost cromwell]$ java -Dconfig.file=/home/jeremiah/cromwell.mysql.conf -jar server/target/scala-2.12/cromwell-37-56b0390-SNAP.jar server; 2019-01-31 18:29:28,169 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; 2019-01-31 18:29:32,763 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:32,786 INFO - CREATE TABLE cromwell.DATABASECHANGELOGLOCK (ID INT NOT NULL, `LOCKED` BIT(1) NOT NULL, LOCKGRANTED datetime NULL, LOCKEDBY VARCHAR(255) NULL, CONSTRAINT PK_DATABASECHANGELOGLOCK PRIMARY KEY (ID)); 2019-01-31 18:29:32,845 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:32,853 INFO - DELETE FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:32,854 INFO - INSERT INTO cromwell.DATABASECHANGELOGLOCK (ID, `LOCKED`) VALUES (1, 0); 2019-01-31 18:29:32,869 INFO - SELECT `LOCKED` FROM cromwell.DATABASECHANGELOGLOCK WHERE ID=1; 2019-01-31 18:29:32,888 INFO - Successfully acquired change log lock; 2019-01-31 18:29:35,077 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,078 INFO - CREATE TABLE cromwell.DATABASECHANGELOG (ID VARCHAR(255) NOT NULL, AUTHOR VARCHAR(255) NOT NULL, FILENAME VARCHAR(255) NOT NULL, DATEEXECUTED datetime NOT NULL, ORDEREXECUTED INT NOT NULL, EXECTYPE VARCHAR(10) NOT NULL, MD5SUM VARCHAR(35) NULL, `DESCRIPTION` VARCHAR(255) NULL, COMMENTS VARCHAR(255) NULL, TAG VARCHAR(255) NULL, LIQUIBASE VARCHAR(20) NULL, CONTEXTS VARCHAR(255) NULL, LABELS VARCHAR(255) NULL, DEPLOYMENT_ID VARCHAR(10) NULL); 2019-01-31 18:29:35,271 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,279 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,280 INFO - SELECT * FROM cromwell.DATABASECHANGELOG ORDER BY DATEEX",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:2210,Availability,ERROR,ERROR,2210," log lock; 2019-01-31 18:29:35,077 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,078 INFO - CREATE TABLE cromwell.DATABASECHANGELOG (ID VARCHAR(255) NOT NULL, AUTHOR VARCHAR(255) NOT NULL, FILENAME VARCHAR(255) NOT NULL, DATEEXECUTED datetime NOT NULL, ORDEREXECUTED INT NOT NULL, EXECTYPE VARCHAR(10) NOT NULL, MD5SUM VARCHAR(35) NULL, `DESCRIPTION` VARCHAR(255) NULL, COMMENTS VARCHAR(255) NULL, TAG VARCHAR(255) NULL, LIQUIBASE VARCHAR(20) NULL, CONTEXTS VARCHAR(255) NULL, LABELS VARCHAR(255) NULL, DEPLOYMENT_ID VARCHAR(10) NULL); 2019-01-31 18:29:35,271 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,279 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,280 INFO - SELECT * FROM cromwell.DATABASECHANGELOG ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-01-31 18:29:35,282 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:35,461 INFO - Successfully released change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(Liquibas",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:2266,Availability,down,down,2266,"DATABASECHANGELOG; 2019-01-31 18:29:35,078 INFO - CREATE TABLE cromwell.DATABASECHANGELOG (ID VARCHAR(255) NOT NULL, AUTHOR VARCHAR(255) NOT NULL, FILENAME VARCHAR(255) NOT NULL, DATEEXECUTED datetime NOT NULL, ORDEREXECUTED INT NOT NULL, EXECTYPE VARCHAR(10) NOT NULL, MD5SUM VARCHAR(35) NULL, `DESCRIPTION` VARCHAR(255) NULL, COMMENTS VARCHAR(255) NULL, TAG VARCHAR(255) NULL, LIQUIBASE VARCHAR(20) NULL, CONTEXTS VARCHAR(255) NULL, LABELS VARCHAR(255) NULL, DEPLOYMENT_ID VARCHAR(10) NULL); 2019-01-31 18:29:35,271 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,279 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,280 INFO - SELECT * FROM cromwell.DATABASECHANGELOG ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-01-31 18:29:35,282 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:35,461 INFO - Successfully released change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initializ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:2160,Deployability,release,released,2160," log lock; 2019-01-31 18:29:35,077 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,078 INFO - CREATE TABLE cromwell.DATABASECHANGELOG (ID VARCHAR(255) NOT NULL, AUTHOR VARCHAR(255) NOT NULL, FILENAME VARCHAR(255) NOT NULL, DATEEXECUTED datetime NOT NULL, ORDEREXECUTED INT NOT NULL, EXECTYPE VARCHAR(10) NOT NULL, MD5SUM VARCHAR(35) NULL, `DESCRIPTION` VARCHAR(255) NULL, COMMENTS VARCHAR(255) NULL, TAG VARCHAR(255) NULL, LIQUIBASE VARCHAR(20) NULL, CONTEXTS VARCHAR(255) NULL, LABELS VARCHAR(255) NULL, DEPLOYMENT_ID VARCHAR(10) NULL); 2019-01-31 18:29:35,271 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,279 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,280 INFO - SELECT * FROM cromwell.DATABASECHANGELOG ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-01-31 18:29:35,282 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:35,461 INFO - Successfully released change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(Liquibas",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:2919,Deployability,update,update,2919,":29:35,280 INFO - SELECT * FROM cromwell.DATABASECHANGELOG ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-01-31 18:29:35,282 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:35,461 INFO - Successfully released change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadP",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:2971,Deployability,update,update,2971,"NGELOG ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-01-31 18:29:35,282 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:35,461 INFO - Successfully released change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:3057,Deployability,update,updateSchema,3057,"OUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:35,461 INFO - Successfully released change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```; with the linked change; https://github.com",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:3154,Deployability,update,updateSchema,3154," change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```; with the linked change; https://github.com/jeremiahsavage/cromwell/commit/88f82a0d699184358149a17a5c1d957704cdced3; the database table crea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:4989,Deployability,configurat,configuration,4989,"hange; https://github.com/jeremiahsavage/cromwell/commit/88f82a0d699184358149a17a5c1d957704cdced3; the database table creation proceeds; as it undoes one of the changes in this commit; https://github.com/broadinstitute/cromwell/commit/775d2cb414734080978cc9c2533cab41b4acfad5; ```; [jeremiah@localhost cromwell]$ java -Dconfig.file=/home/jeremiah/cromwell.mysql.conf -jar server/target/scala-2.12/cromwell-37-88f82a0-SNAP.jar server; 2019-01-31 18:42:55,062 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; 2019-01-31 18:43:01,372 INFO - Successfully acquired change log lock; 2019-01-31 18:43:04,396 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,492 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,918 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:5164,Deployability,configurat,configuration,5164,"s commit; https://github.com/broadinstitute/cromwell/commit/775d2cb414734080978cc9c2533cab41b4acfad5; ```; [jeremiah@localhost cromwell]$ java -Dconfig.file=/home/jeremiah/cromwell.mysql.conf -jar server/target/scala-2.12/cromwell-37-88f82a0-SNAP.jar server; 2019-01-31 18:42:55,062 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; 2019-01-31 18:43:01,372 INFO - Successfully acquired change log lock; 2019-01-31 18:43:04,396 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,492 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,918 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:5339,Deployability,configurat,configuration,5339,"mwell.mysql.conf -jar server/target/scala-2.12/cromwell-37-88f82a0-SNAP.jar server; 2019-01-31 18:42:55,062 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; 2019-01-31 18:43:01,372 INFO - Successfully acquired change log lock; 2019-01-31 18:43:04,396 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,492 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,918 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:5514,Deployability,configurat,configuration,5514,"ll?rewriteBatchedStatements=true; 2019-01-31 18:43:01,372 INFO - Successfully acquired change log lock; 2019-01-31 18:43:04,396 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,492 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,918 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:5689,Deployability,configurat,configuration,5689,"e: cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,492 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,918 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:5864,Deployability,configurat,configuration,5864,"increment/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:6039,Deployability,configurat,configuration,6039,"increment/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:6214,Deployability,configurat,configuration,6214,"increment/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:05,044 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table WORKFLOW_EXECUTION created; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:6389,Deployability,configurat,configuration,6389,"increment/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:05,044 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table WORKFLOW_EXECUTION created; 2019-01-31 18:43:05,122 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table EXECUTION created; 2019-01-31 18:43:05,285 INFO - chang",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:6564,Deployability,configurat,configuration,6564,"increment/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:05,044 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table WORKFLOW_EXECUTION created; 2019-01-31 18:43:05,122 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table EXECUTION created; 2019-01-31 18:43:05,285 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table JES_JOB created; 2019-01-31 18:43:05,477 INFO - changelog.xml: changesets/db_schema.xml::db",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:6739,Deployability,configurat,configuration,6739,"increment/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:05,044 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table WORKFLOW_EXECUTION created; 2019-01-31 18:43:05,122 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table EXECUTION created; 2019-01-31 18:43:05,285 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table JES_JOB created; 2019-01-31 18:43:05,477 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table LOCAL_JOB created; 2019-01-31 18:43:05,477 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfraz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:6914,Deployability,configurat,configuration,6914,"increment/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:05,044 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table WORKFLOW_EXECUTION created; 2019-01-31 18:43:05,122 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table EXECUTION created; 2019-01-31 18:43:05,285 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table JES_JOB created; 2019-01-31 18:43:05,477 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table LOCAL_JOB created; 2019-01-31 18:43:05,477 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: ChangeSet changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer ran successfully in 538ms; 2019-01-31 18:43:05,650 INFO - changelog.xml: changesets/db_schema.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:7089,Deployability,configurat,configuration,7089,"uration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:05,044 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table WORKFLOW_EXECUTION created; 2019-01-31 18:43:05,122 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table EXECUTION created; 2019-01-31 18:43:05,285 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table JES_JOB created; 2019-01-31 18:43:05,477 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table LOCAL_JOB created; 2019-01-31 18:43:05,477 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: ChangeSet changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer ran successfully in 538ms; 2019-01-31 18:43:05,650 INFO - changelog.xml: changesets/db_schema.xml::db_schema_symbol_table_mysql::scottfrazer: Table SYMBOL created; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:3378,Energy Efficiency,adapt,adapted,3378,tion(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```; with the linked change; https://github.com/jeremiahsavage/cromwell/commit/88f82a0d699184358149a17a5c1d957704cdced3; the database table creation proceeds; as it undoes one of the changes in this commit; https://github.com/broadinstitute/cromwell/commit/775d2cb414734080978cc9c2533cab41b4acfad5; ```; [jeremiah@localhost cromwell]$ java -Dconfig.file=,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:413,Modifiability,rewrite,rewriteBatchedStatements,413,"I get an `ArrayIndexOutOfBoundsException` error when starting the latest `develop` branch in server mode with a mysql (`mariadb-10.3.12-5.fc30.x86_64`) backend.; ```; [jeremiah@localhost cromwell]$ java -Dconfig.file=/home/jeremiah/cromwell.mysql.conf -jar server/target/scala-2.12/cromwell-37-56b0390-SNAP.jar server; 2019-01-31 18:29:28,169 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; 2019-01-31 18:29:32,763 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:32,786 INFO - CREATE TABLE cromwell.DATABASECHANGELOGLOCK (ID INT NOT NULL, `LOCKED` BIT(1) NOT NULL, LOCKGRANTED datetime NULL, LOCKEDBY VARCHAR(255) NULL, CONSTRAINT PK_DATABASECHANGELOGLOCK PRIMARY KEY (ID)); 2019-01-31 18:29:32,845 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:32,853 INFO - DELETE FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:32,854 INFO - INSERT INTO cromwell.DATABASECHANGELOGLOCK (ID, `LOCKED`) VALUES (1, 0); 2019-01-31 18:29:32,869 INFO - SELECT `LOCKED` FROM cromwell.DATABASECHANGELOGLOCK WHERE ID=1; 2019-01-31 18:29:32,888 INFO - Successfully acquired change log lock; 2019-01-31 18:29:35,077 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,078 INFO - CREATE TABLE cromwell.DATABASECHANGELOG (ID VARCHAR(255) NOT NULL, AUTHOR VARCHAR(255) NOT NULL, FILENAME VARCHAR(255) NOT NULL, DATEEXECUTED datetime NOT NULL, ORDEREXECUTED INT NOT NULL, EXECTYPE VARCHAR(10) NOT NULL, MD5SUM VARCHAR(35) NULL, `DESCRIPTION` VARCHAR(255) NULL, COMMENTS VARCHAR(255) NULL, TAG VARCHAR(255) NULL, LIQUIBASE VARCHAR(20) NULL, CONTEXTS VARCHAR(255) NULL, LABELS VARCHAR(255) NULL, DEPLOYMENT_ID VARCHAR(10) NULL); 2019-01-31 18:29:35,271 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,279 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,280 INFO - SELECT * FROM cromwell.DATABASECHANGELOG ORDER BY DATEEX",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:3229,Modifiability,Enhance,EnhancedSqlDatabase,3229,antiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```; with the linked change; https://github.com/jeremiahsavage/cromwell/commit/88f82a0d699184358149a17a5c1d957704cdced3; the database table creation proceeds; as it undoes one of the changes in this commit; ht,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:3334,Modifiability,Enhance,EnhancedSqlDatabase,3334,ase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```; with the linked change; https://github.com/jeremiahsavage/cromwell/commit/88f82a0d699184358149a17a5c1d957704cdced3; the database table creation proceeds; as it undoes one of the changes in this commit; https://github.com/broadinstitute/cromwell/commit/775d2cb414734080978cc9c2533cab41b4acfad5; ```; [jeremiah@,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:3378,Modifiability,adapt,adapted,3378,tion(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```; with the linked change; https://github.com/jeremiahsavage/cromwell/commit/88f82a0d699184358149a17a5c1d957704cdced3; the database table creation proceeds; as it undoes one of the changes in this commit; https://github.com/broadinstitute/cromwell/commit/775d2cb414734080978cc9c2533cab41b4acfad5; ```; [jeremiah@localhost cromwell]$ java -Dconfig.file=,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:4574,Modifiability,rewrite,rewriteBatchedStatements,4574,"t slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```; with the linked change; https://github.com/jeremiahsavage/cromwell/commit/88f82a0d699184358149a17a5c1d957704cdced3; the database table creation proceeds; as it undoes one of the changes in this commit; https://github.com/broadinstitute/cromwell/commit/775d2cb414734080978cc9c2533cab41b4acfad5; ```; [jeremiah@localhost cromwell]$ java -Dconfig.file=/home/jeremiah/cromwell.mysql.conf -jar server/target/scala-2.12/cromwell-37-88f82a0-SNAP.jar server; 2019-01-31 18:42:55,062 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; 2019-01-31 18:43:01,372 INFO - Successfully acquired change log lock; 2019-01-31 18:43:04,396 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,492 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,918 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null setting",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:4989,Modifiability,config,configuration,4989,"hange; https://github.com/jeremiahsavage/cromwell/commit/88f82a0d699184358149a17a5c1d957704cdced3; the database table creation proceeds; as it undoes one of the changes in this commit; https://github.com/broadinstitute/cromwell/commit/775d2cb414734080978cc9c2533cab41b4acfad5; ```; [jeremiah@localhost cromwell]$ java -Dconfig.file=/home/jeremiah/cromwell.mysql.conf -jar server/target/scala-2.12/cromwell-37-88f82a0-SNAP.jar server; 2019-01-31 18:42:55,062 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; 2019-01-31 18:43:01,372 INFO - Successfully acquired change log lock; 2019-01-31 18:43:04,396 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,492 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,918 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:5164,Modifiability,config,configuration,5164,"s commit; https://github.com/broadinstitute/cromwell/commit/775d2cb414734080978cc9c2533cab41b4acfad5; ```; [jeremiah@localhost cromwell]$ java -Dconfig.file=/home/jeremiah/cromwell.mysql.conf -jar server/target/scala-2.12/cromwell-37-88f82a0-SNAP.jar server; 2019-01-31 18:42:55,062 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; 2019-01-31 18:43:01,372 INFO - Successfully acquired change log lock; 2019-01-31 18:43:04,396 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,492 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,918 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:5339,Modifiability,config,configuration,5339,"mwell.mysql.conf -jar server/target/scala-2.12/cromwell-37-88f82a0-SNAP.jar server; 2019-01-31 18:42:55,062 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; 2019-01-31 18:43:01,372 INFO - Successfully acquired change log lock; 2019-01-31 18:43:04,396 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,492 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,918 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:5514,Modifiability,config,configuration,5514,"ll?rewriteBatchedStatements=true; 2019-01-31 18:43:01,372 INFO - Successfully acquired change log lock; 2019-01-31 18:43:04,396 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,492 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,918 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:5689,Modifiability,config,configuration,5689,"e: cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,492 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,918 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:5864,Modifiability,config,configuration,5864,"increment/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:6039,Modifiability,config,configuration,6039,"increment/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:6214,Modifiability,config,configuration,6214,"increment/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:05,044 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table WORKFLOW_EXECUTION created; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:6389,Modifiability,config,configuration,6389,"increment/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:05,044 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table WORKFLOW_EXECUTION created; 2019-01-31 18:43:05,122 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table EXECUTION created; 2019-01-31 18:43:05,285 INFO - chang",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:6564,Modifiability,config,configuration,6564,"increment/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:05,044 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table WORKFLOW_EXECUTION created; 2019-01-31 18:43:05,122 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table EXECUTION created; 2019-01-31 18:43:05,285 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table JES_JOB created; 2019-01-31 18:43:05,477 INFO - changelog.xml: changesets/db_schema.xml::db",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:6739,Modifiability,config,configuration,6739,"increment/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:05,044 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table WORKFLOW_EXECUTION created; 2019-01-31 18:43:05,122 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table EXECUTION created; 2019-01-31 18:43:05,285 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table JES_JOB created; 2019-01-31 18:43:05,477 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table LOCAL_JOB created; 2019-01-31 18:43:05,477 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfraz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:6914,Modifiability,config,configuration,6914,"increment/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:05,044 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table WORKFLOW_EXECUTION created; 2019-01-31 18:43:05,122 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table EXECUTION created; 2019-01-31 18:43:05,285 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table JES_JOB created; 2019-01-31 18:43:05,477 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table LOCAL_JOB created; 2019-01-31 18:43:05,477 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: ChangeSet changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer ran successfully in 538ms; 2019-01-31 18:43:05,650 INFO - changelog.xml: changesets/db_schema.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:7089,Modifiability,config,configuration,7089,"uration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:05,044 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table WORKFLOW_EXECUTION created; 2019-01-31 18:43:05,122 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table EXECUTION created; 2019-01-31 18:43:05,285 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table JES_JOB created; 2019-01-31 18:43:05,477 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table LOCAL_JOB created; 2019-01-31 18:43:05,477 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: ChangeSet changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer ran successfully in 538ms; 2019-01-31 18:43:05,650 INFO - changelog.xml: changesets/db_schema.xml::db_schema_symbol_table_mysql::scottfrazer: Table SYMBOL created; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:3825,Performance,concurren,concurrent,3825,"hangelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```; with the linked change; https://github.com/jeremiahsavage/cromwell/commit/88f82a0d699184358149a17a5c1d957704cdced3; the database table creation proceeds; as it undoes one of the changes in this commit; https://github.com/broadinstitute/cromwell/commit/775d2cb414734080978cc9c2533cab41b4acfad5; ```; [jeremiah@localhost cromwell]$ java -Dconfig.file=/home/jeremiah/cromwell.mysql.conf -jar server/target/scala-2.12/cromwell-37-88f82a0-SNAP.jar server; 2019-01-31 18:42:55,062 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; 2019-01-31 18:43:01,372 INFO - Successfully acquired change log lock; 2019-01-31 18:43:04,396 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,492 INFO - Reading from cromwell.D",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:3910,Performance,concurren,concurrent,3910,"ase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```; with the linked change; https://github.com/jeremiahsavage/cromwell/commit/88f82a0d699184358149a17a5c1d957704cdced3; the database table creation proceeds; as it undoes one of the changes in this commit; https://github.com/broadinstitute/cromwell/commit/775d2cb414734080978cc9c2533cab41b4acfad5; ```; [jeremiah@localhost cromwell]$ java -Dconfig.file=/home/jeremiah/cromwell.mysql.conf -jar server/target/scala-2.12/cromwell-37-88f82a0-SNAP.jar server; 2019-01-31 18:42:55,062 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; 2019-01-31 18:43:01,372 INFO - Successfully acquired change log lock; 2019-01-31 18:43:04,396 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,492 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,918 WARN - modifyDataType will lose primary key",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:2688,Security,Validat,ValidatingVisitor,2688,"NULL, LABELS VARCHAR(255) NULL, DEPLOYMENT_ID VARCHAR(10) NULL); 2019-01-31 18:29:35,271 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,279 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,280 INFO - SELECT * FROM cromwell.DATABASECHANGELOG ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-01-31 18:29:35,282 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:35,461 INFO - Successfully released change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.lif",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:2712,Security,Validat,ValidatingVisitor,2712,"255) NULL, DEPLOYMENT_ID VARCHAR(10) NULL); 2019-01-31 18:29:35,271 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,279 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,280 INFO - SELECT * FROM cromwell.DATABASECHANGELOG ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-01-31 18:29:35,282 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:35,461 INFO - Successfully released change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBacke",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:2857,Security,validat,validate,2857," - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,280 INFO - SELECT * FROM cromwell.DATABASECHANGELOG ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-01-31 18:29:35,282 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:35,461 INFO - Successfully released change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(Thread",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:1175,Testability,log,log,1175,"ah@localhost cromwell]$ java -Dconfig.file=/home/jeremiah/cromwell.mysql.conf -jar server/target/scala-2.12/cromwell-37-56b0390-SNAP.jar server; 2019-01-31 18:29:28,169 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; 2019-01-31 18:29:32,763 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:32,786 INFO - CREATE TABLE cromwell.DATABASECHANGELOGLOCK (ID INT NOT NULL, `LOCKED` BIT(1) NOT NULL, LOCKGRANTED datetime NULL, LOCKEDBY VARCHAR(255) NULL, CONSTRAINT PK_DATABASECHANGELOGLOCK PRIMARY KEY (ID)); 2019-01-31 18:29:32,845 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:32,853 INFO - DELETE FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:32,854 INFO - INSERT INTO cromwell.DATABASECHANGELOGLOCK (ID, `LOCKED`) VALUES (1, 0); 2019-01-31 18:29:32,869 INFO - SELECT `LOCKED` FROM cromwell.DATABASECHANGELOGLOCK WHERE ID=1; 2019-01-31 18:29:32,888 INFO - Successfully acquired change log lock; 2019-01-31 18:29:35,077 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,078 INFO - CREATE TABLE cromwell.DATABASECHANGELOG (ID VARCHAR(255) NOT NULL, AUTHOR VARCHAR(255) NOT NULL, FILENAME VARCHAR(255) NOT NULL, DATEEXECUTED datetime NOT NULL, ORDEREXECUTED INT NOT NULL, EXECTYPE VARCHAR(10) NOT NULL, MD5SUM VARCHAR(35) NULL, `DESCRIPTION` VARCHAR(255) NULL, COMMENTS VARCHAR(255) NULL, TAG VARCHAR(255) NULL, LIQUIBASE VARCHAR(20) NULL, CONTEXTS VARCHAR(255) NULL, LABELS VARCHAR(255) NULL, DEPLOYMENT_ID VARCHAR(10) NULL); 2019-01-31 18:29:35,271 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,279 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,280 INFO - SELECT * FROM cromwell.DATABASECHANGELOG ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-01-31 18:29:35,282 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:35,461 INFO - Successfully released change",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:2176,Testability,log,log,2176," log lock; 2019-01-31 18:29:35,077 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,078 INFO - CREATE TABLE cromwell.DATABASECHANGELOG (ID VARCHAR(255) NOT NULL, AUTHOR VARCHAR(255) NOT NULL, FILENAME VARCHAR(255) NOT NULL, DATEEXECUTED datetime NOT NULL, ORDEREXECUTED INT NOT NULL, EXECTYPE VARCHAR(10) NOT NULL, MD5SUM VARCHAR(35) NULL, `DESCRIPTION` VARCHAR(255) NULL, COMMENTS VARCHAR(255) NULL, TAG VARCHAR(255) NULL, LIQUIBASE VARCHAR(20) NULL, CONTEXTS VARCHAR(255) NULL, LABELS VARCHAR(255) NULL, DEPLOYMENT_ID VARCHAR(10) NULL); 2019-01-31 18:29:35,271 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,279 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,280 INFO - SELECT * FROM cromwell.DATABASECHANGELOG ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-01-31 18:29:35,282 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:35,461 INFO - Successfully released change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(Liquibas",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:4665,Testability,log,log,4665,"t slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```; with the linked change; https://github.com/jeremiahsavage/cromwell/commit/88f82a0d699184358149a17a5c1d957704cdced3; the database table creation proceeds; as it undoes one of the changes in this commit; https://github.com/broadinstitute/cromwell/commit/775d2cb414734080978cc9c2533cab41b4acfad5; ```; [jeremiah@localhost cromwell]$ java -Dconfig.file=/home/jeremiah/cromwell.mysql.conf -jar server/target/scala-2.12/cromwell-37-88f82a0-SNAP.jar server; 2019-01-31 18:42:55,062 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; 2019-01-31 18:43:01,372 INFO - Successfully acquired change log lock; 2019-01-31 18:43:04,396 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,492 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,918 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null setting",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:3520,Usability,Simpl,SimpleJdbcAction,3520,"tChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```; with the linked change; https://github.com/jeremiahsavage/cromwell/commit/88f82a0d699184358149a17a5c1d957704cdced3; the database table creation proceeds; as it undoes one of the changes in this commit; https://github.com/broadinstitute/cromwell/commit/775d2cb414734080978cc9c2533cab41b4acfad5; ```; [jeremiah@localhost cromwell]$ java -Dconfig.file=/home/jeremiah/cromwell.mysql.conf -jar server/target/scala-2.12/cromwell-37-88f82a0-SNAP.jar server; 2019-01-31 18:42:55,062 INFO - Running with datab",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:3590,Usability,Simpl,SimpleJdbcAction,3590,"base.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```; with the linked change; https://github.com/jeremiahsavage/cromwell/commit/88f82a0d699184358149a17a5c1d957704cdced3; the database table creation proceeds; as it undoes one of the changes in this commit; https://github.com/broadinstitute/cromwell/commit/775d2cb414734080978cc9c2533cab41b4acfad5; ```; [jeremiah@localhost cromwell]$ java -Dconfig.file=/home/jeremiah/cromwell.mysql.conf -jar server/target/scala-2.12/cromwell-37-88f82a0-SNAP.jar server; 2019-01-31 18:42:55,062 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4605:4189,Usability,undo,undoes,4189,"ateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```; with the linked change; https://github.com/jeremiahsavage/cromwell/commit/88f82a0d699184358149a17a5c1d957704cdced3; the database table creation proceeds; as it undoes one of the changes in this commit; https://github.com/broadinstitute/cromwell/commit/775d2cb414734080978cc9c2533cab41b4acfad5; ```; [jeremiah@localhost cromwell]$ java -Dconfig.file=/home/jeremiah/cromwell.mysql.conf -jar server/target/scala-2.12/cromwell-37-88f82a0-SNAP.jar server; 2019-01-31 18:42:55,062 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; 2019-01-31 18:43:01,372 INFO - Successfully acquired change log lock; 2019-01-31 18:43:04,396 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,492 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,918 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-speci",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605
https://github.com/broadinstitute/cromwell/issues/4606:530,Availability,ERROR,ERROR,530,"When starting up the cromwell in server mode with a mysql (`mariadb-10.3.12-5.fc30.x86_64`) backend, I get a series of Exceptions, each of which is fixed by changing double quotes in a `.sql` file to single quotes. The first is; ```; 2019-01-31 19:14:34,340 INFO - changelog.xml: changesets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet: ChangeSet changesets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet ran successfully in 117ms; 2019-01-31 19:14:34,435 ERROR - changelog.xml: changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Change Set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi failed. Error: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 2019-01-31 19:14:34,471 INFO - changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.Services",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606
https://github.com/broadinstitute/cromwell/issues/4606:735,Availability,Error,Error,735,"When starting up the cromwell in server mode with a mysql (`mariadb-10.3.12-5.fc30.x86_64`) backend, I get a series of Exceptions, each of which is fixed by changing double quotes in a `.sql` file to single quotes. The first is; ```; 2019-01-31 19:14:34,340 INFO - changelog.xml: changesets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet: ChangeSet changesets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet ran successfully in 117ms; 2019-01-31 19:14:34,435 ERROR - changelog.xml: changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Change Set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi failed. Error: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 2019-01-31 19:14:34,471 INFO - changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.Services",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606
https://github.com/broadinstitute/cromwell/issues/4606:1049,Availability,ERROR,ERROR,1049,"e cromwell in server mode with a mysql (`mariadb-10.3.12-5.fc30.x86_64`) backend, I get a series of Exceptions, each of which is fixed by changing double quotes in a `.sql` file to single quotes. The first is; ```; 2019-01-31 19:14:34,340 INFO - changelog.xml: changesets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet: ChangeSet changesets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet ran successfully in 117ms; 2019-01-31 19:14:34,435 ERROR - changelog.xml: changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Change Set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi failed. Error: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 2019-01-31 19:14:34,471 INFO - changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlD",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606
https://github.com/broadinstitute/cromwell/issues/4606:1105,Availability,down,down,1105,"a series of Exceptions, each of which is fixed by changing double quotes in a `.sql` file to single quotes. The first is; ```; 2019-01-31 19:14:34,340 INFO - changelog.xml: changesets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet: ChangeSet changesets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet ran successfully in 117ms; 2019-01-31 19:14:34,435 ERROR - changelog.xml: changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Change Set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi failed. Error: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 2019-01-31 19:14:34,471 INFO - changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606
https://github.com/broadinstitute/cromwell/issues/4606:791,Deployability,UPDATE,UPDATE,791,"When starting up the cromwell in server mode with a mysql (`mariadb-10.3.12-5.fc30.x86_64`) backend, I get a series of Exceptions, each of which is fixed by changing double quotes in a `.sql` file to single quotes. The first is; ```; 2019-01-31 19:14:34,340 INFO - changelog.xml: changesets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet: ChangeSet changesets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet ran successfully in 117ms; 2019-01-31 19:14:34,435 ERROR - changelog.xml: changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Change Set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi failed. Error: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 2019-01-31 19:14:34,471 INFO - changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.Services",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606
https://github.com/broadinstitute/cromwell/issues/4606:999,Deployability,release,released,999,"e cromwell in server mode with a mysql (`mariadb-10.3.12-5.fc30.x86_64`) backend, I get a series of Exceptions, each of which is fixed by changing double quotes in a `.sql` file to single quotes. The first is; ```; 2019-01-31 19:14:34,340 INFO - changelog.xml: changesets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet: ChangeSet changesets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet ran successfully in 117ms; 2019-01-31 19:14:34,435 ERROR - changelog.xml: changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Change Set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi failed. Error: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 2019-01-31 19:14:34,471 INFO - changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlD",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606
https://github.com/broadinstitute/cromwell/issues/4606:1378,Deployability,UPDATE,UPDATE,1378,"ets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet ran successfully in 117ms; 2019-01-31 19:14:34,435 ERROR - changelog.xml: changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Change Set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi failed. Error: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 2019-01-31 19:14:34,471 INFO - changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(Str",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606
https://github.com/broadinstitute/cromwell/issues/4606:1555,Deployability,Update,UpdateVisitor,1555,"s/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Change Set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi failed. Error: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 2019-01-31 19:14:34,471 INFO - changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.ru",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606
https://github.com/broadinstitute/cromwell/issues/4606:1575,Deployability,Update,UpdateVisitor,1575,"ustom_labels.xml::replace_empty_custom_labels::rmunshi: Change Set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi failed. Error: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 2019-01-31 19:14:34,471 INFO - changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.sc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606
https://github.com/broadinstitute/cromwell/issues/4606:1697,Deployability,update,update,1697,"ustom_labels::rmunshi failed. Error: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 2019-01-31 19:14:34,471 INFO - changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadP",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606
https://github.com/broadinstitute/cromwell/issues/4606:1749,Deployability,update,update,1749,"'' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 2019-01-31 19:14:34,471 INFO - changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606
https://github.com/broadinstitute/cromwell/issues/4606:1835,Deployability,update,updateSchema,1835,"RE CUSTOM_LABELS = """"]; 2019-01-31 19:14:34,471 INFO - changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseExceptio",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606
https://github.com/broadinstitute/cromwell/issues/4606:1932,Deployability,update,updateSchema,1932,"::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABEL",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606
https://github.com/broadinstitute/cromwell/issues/4606:2901,Deployability,UPDATE,UPDATE,2901,".LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1277); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1259); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:582); 	... 16 common frames omitted; Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column '' in 'where clause'; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Cons",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606
https://github.com/broadinstitute/cromwell/issues/4606:2156,Energy Efficiency,adapt,adapted,2156,"dException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606
https://github.com/broadinstitute/cromwell/issues/4606:2007,Modifiability,Enhance,EnhancedSqlDatabase,2007,"ange log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.executor.jvm.J",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606
https://github.com/broadinstitute/cromwell/issues/4606:2112,Modifiability,Enhance,EnhancedSqlDatabase,2112,"ell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.Jdb",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606
https://github.com/broadinstitute/cromwell/issues/4606:2156,Modifiability,adapt,adapted,2156,"dException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606
https://github.com/broadinstitute/cromwell/issues/4606:2603,Performance,concurren,concurrent,2603,"base.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1277); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1259); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:582); 	... 16 common frames omitted; Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column '' in 'where clause';",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606
https://github.com/broadinstitute/cromwell/issues/4606:2688,Performance,concurren,concurrent,2688,"ase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1277); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1259); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:582); 	... 16 common frames omitted; Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column '' in 'where clause'; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.r",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606
https://github.com/broadinstitute/cromwell/issues/4606:1015,Testability,log,log,1015,"e cromwell in server mode with a mysql (`mariadb-10.3.12-5.fc30.x86_64`) backend, I get a series of Exceptions, each of which is fixed by changing double quotes in a `.sql` file to single quotes. The first is; ```; 2019-01-31 19:14:34,340 INFO - changelog.xml: changesets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet: ChangeSet changesets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet ran successfully in 117ms; 2019-01-31 19:14:34,435 ERROR - changelog.xml: changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Change Set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi failed. Error: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 2019-01-31 19:14:34,471 INFO - changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlD",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606
https://github.com/broadinstitute/cromwell/issues/4606:2298,Usability,Simpl,SimpleJdbcAction,2298,"ion.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606
https://github.com/broadinstitute/cromwell/issues/4606:2368,Usability,Simpl,SimpleJdbcAction,2368,": UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1277); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606
https://github.com/broadinstitute/cromwell/issues/4607:254,Availability,avail,available,254,"Hi there,. Biocontainers are incredibly popular tools in bioinformatics. Each time a [Bioconda recipe](https://bioconda.github.io/recipes.html) is made, a Biocontainer is automatically generated. This creates a collection of Dockerized tools immediately available to everybody. To reduce footprint, Biocontainers run on BusyBox, a Linux version tailored to embedded systems. Some tools therefore do not expose the full set of options of matching GNU tools on Ubuntu, CentOS, etc. Given the popularity of Biocontainers, it would be great if Cromwell could support them fully. There are a couple of small bugs related to `find` and `xargs` that should be easy to fix when one explores the file `stderr.background` for a task run from a Biocontainer:. ```; find: unrecognized: -empty; BusyBox v1.22.1 (2014-05-23 01:24:27 UTC) multi-call binary. Usage: find [-HL] [PATH]... [OPTIONS] [ACTIONS]. Search for files and perform actions on them.; First failed action stops processing of current file.; Defaults: PATH is current directory, action is '-print'. 	-L,-follow	Follow symlinks; 	-H		...on command line only; 	-xdev		Don't descend directories on other filesystems; 	-maxdepth N	Descend at most N levels. -maxdepth 0 applies; 			actions to command line arguments only; 	-mindepth N	Don't act on first N levels; 	-depth		Act on directory *after* traversing it. Actions:; 	( ACTIONS )	Group actions for -o / -a; 	! ACT		Invert ACT's success/failure; 	ACT1 [-a] ACT2	If ACT1 fails, stop, else do ACT2; 	ACT1 -o ACT2	If ACT1 succeeds, stop, else do ACT2; 			Note: -a has higher priority than -o; 	-name PATTERN	Match file name (w/o directory name) to PATTERN; 	-iname PATTERN	Case insensitive -name; 	-path PATTERN	Match path to PATTERN; 	-ipath PATTERN	Case insensitive -path; 	-regex PATTERN	Match path to regex PATTERN; 	-type X		File type is X (one of: f,d,l,b,c,...); 	-perm MASK	At least one mask bit (+MASK), all bits (-MASK),; 			or exactly MASK bits are set in file's mode; 	-mtime DAYS	mtime is ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4607
https://github.com/broadinstitute/cromwell/issues/4607:1439,Availability,failure,failure,1439,"ated to `find` and `xargs` that should be easy to fix when one explores the file `stderr.background` for a task run from a Biocontainer:. ```; find: unrecognized: -empty; BusyBox v1.22.1 (2014-05-23 01:24:27 UTC) multi-call binary. Usage: find [-HL] [PATH]... [OPTIONS] [ACTIONS]. Search for files and perform actions on them.; First failed action stops processing of current file.; Defaults: PATH is current directory, action is '-print'. 	-L,-follow	Follow symlinks; 	-H		...on command line only; 	-xdev		Don't descend directories on other filesystems; 	-maxdepth N	Descend at most N levels. -maxdepth 0 applies; 			actions to command line arguments only; 	-mindepth N	Don't act on first N levels; 	-depth		Act on directory *after* traversing it. Actions:; 	( ACTIONS )	Group actions for -o / -a; 	! ACT		Invert ACT's success/failure; 	ACT1 [-a] ACT2	If ACT1 fails, stop, else do ACT2; 	ACT1 -o ACT2	If ACT1 succeeds, stop, else do ACT2; 			Note: -a has higher priority than -o; 	-name PATTERN	Match file name (w/o directory name) to PATTERN; 	-iname PATTERN	Case insensitive -name; 	-path PATTERN	Match path to PATTERN; 	-ipath PATTERN	Case insensitive -path; 	-regex PATTERN	Match path to regex PATTERN; 	-type X		File type is X (one of: f,d,l,b,c,...); 	-perm MASK	At least one mask bit (+MASK), all bits (-MASK),; 			or exactly MASK bits are set in file's mode; 	-mtime DAYS	mtime is greater than (+N), less than (-N),; 			or exactly N days in the past; 	-mmin MINS	mtime is greater than (+N), less than (-N),; 			or exactly N minutes in the past; 	-newer FILE	mtime is more recent than FILE's; 	-user NAME/ID	File is owned by given user; 	-group NAME/ID	File is owned by given group; 	-size N[bck]	File size is N (c:bytes,k:kbytes,b:512 bytes(def.)); 			+/-N: file size is bigger/smaller than N; 	-prune		If current file is directory, don't descend into it; If none of the following actions is specified, -print is assumed; 	-print		Print file name; 	-print0		Print file name, NUL terminated; 	",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4607
https://github.com/broadinstitute/cromwell/issues/4607:1876,Availability,MASK,MASK,1876,"v		Don't descend directories on other filesystems; 	-maxdepth N	Descend at most N levels. -maxdepth 0 applies; 			actions to command line arguments only; 	-mindepth N	Don't act on first N levels; 	-depth		Act on directory *after* traversing it. Actions:; 	( ACTIONS )	Group actions for -o / -a; 	! ACT		Invert ACT's success/failure; 	ACT1 [-a] ACT2	If ACT1 fails, stop, else do ACT2; 	ACT1 -o ACT2	If ACT1 succeeds, stop, else do ACT2; 			Note: -a has higher priority than -o; 	-name PATTERN	Match file name (w/o directory name) to PATTERN; 	-iname PATTERN	Case insensitive -name; 	-path PATTERN	Match path to PATTERN; 	-ipath PATTERN	Case insensitive -path; 	-regex PATTERN	Match path to regex PATTERN; 	-type X		File type is X (one of: f,d,l,b,c,...); 	-perm MASK	At least one mask bit (+MASK), all bits (-MASK),; 			or exactly MASK bits are set in file's mode; 	-mtime DAYS	mtime is greater than (+N), less than (-N),; 			or exactly N days in the past; 	-mmin MINS	mtime is greater than (+N), less than (-N),; 			or exactly N minutes in the past; 	-newer FILE	mtime is more recent than FILE's; 	-user NAME/ID	File is owned by given user; 	-group NAME/ID	File is owned by given group; 	-size N[bck]	File size is N (c:bytes,k:kbytes,b:512 bytes(def.)); 			+/-N: file size is bigger/smaller than N; 	-prune		If current file is directory, don't descend into it; If none of the following actions is specified, -print is assumed; 	-print		Print file name; 	-print0		Print file name, NUL terminated; 	-exec CMD ARG ;	Run CMD with all instances of {} replaced by; 			file name. Fails if CMD exits with nonzero. xargs: invalid option -- 'I'; BusyBox v1.22.1 (2014-05-23 01:24:27 UTC) multi-call binary. Usage: xargs [OPTIONS] [PROG ARGS]. Run PROG on every item given by stdin. 	-r	Don't run command if input is empty; 	-0	Input is separated by NUL characters; 	-t	Print the command on stderr before execution; 	-e[STR]	STR stops input processing; 	-n N	Pass no more than N args to PROG; 	-s N	Pass command ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4607
https://github.com/broadinstitute/cromwell/issues/4607:1894,Availability,mask,mask,1894,"v		Don't descend directories on other filesystems; 	-maxdepth N	Descend at most N levels. -maxdepth 0 applies; 			actions to command line arguments only; 	-mindepth N	Don't act on first N levels; 	-depth		Act on directory *after* traversing it. Actions:; 	( ACTIONS )	Group actions for -o / -a; 	! ACT		Invert ACT's success/failure; 	ACT1 [-a] ACT2	If ACT1 fails, stop, else do ACT2; 	ACT1 -o ACT2	If ACT1 succeeds, stop, else do ACT2; 			Note: -a has higher priority than -o; 	-name PATTERN	Match file name (w/o directory name) to PATTERN; 	-iname PATTERN	Case insensitive -name; 	-path PATTERN	Match path to PATTERN; 	-ipath PATTERN	Case insensitive -path; 	-regex PATTERN	Match path to regex PATTERN; 	-type X		File type is X (one of: f,d,l,b,c,...); 	-perm MASK	At least one mask bit (+MASK), all bits (-MASK),; 			or exactly MASK bits are set in file's mode; 	-mtime DAYS	mtime is greater than (+N), less than (-N),; 			or exactly N days in the past; 	-mmin MINS	mtime is greater than (+N), less than (-N),; 			or exactly N minutes in the past; 	-newer FILE	mtime is more recent than FILE's; 	-user NAME/ID	File is owned by given user; 	-group NAME/ID	File is owned by given group; 	-size N[bck]	File size is N (c:bytes,k:kbytes,b:512 bytes(def.)); 			+/-N: file size is bigger/smaller than N; 	-prune		If current file is directory, don't descend into it; If none of the following actions is specified, -print is assumed; 	-print		Print file name; 	-print0		Print file name, NUL terminated; 	-exec CMD ARG ;	Run CMD with all instances of {} replaced by; 			file name. Fails if CMD exits with nonzero. xargs: invalid option -- 'I'; BusyBox v1.22.1 (2014-05-23 01:24:27 UTC) multi-call binary. Usage: xargs [OPTIONS] [PROG ARGS]. Run PROG on every item given by stdin. 	-r	Don't run command if input is empty; 	-0	Input is separated by NUL characters; 	-t	Print the command on stderr before execution; 	-e[STR]	STR stops input processing; 	-n N	Pass no more than N args to PROG; 	-s N	Pass command ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4607
https://github.com/broadinstitute/cromwell/issues/4607:1905,Availability,MASK,MASK,1905,"v		Don't descend directories on other filesystems; 	-maxdepth N	Descend at most N levels. -maxdepth 0 applies; 			actions to command line arguments only; 	-mindepth N	Don't act on first N levels; 	-depth		Act on directory *after* traversing it. Actions:; 	( ACTIONS )	Group actions for -o / -a; 	! ACT		Invert ACT's success/failure; 	ACT1 [-a] ACT2	If ACT1 fails, stop, else do ACT2; 	ACT1 -o ACT2	If ACT1 succeeds, stop, else do ACT2; 			Note: -a has higher priority than -o; 	-name PATTERN	Match file name (w/o directory name) to PATTERN; 	-iname PATTERN	Case insensitive -name; 	-path PATTERN	Match path to PATTERN; 	-ipath PATTERN	Case insensitive -path; 	-regex PATTERN	Match path to regex PATTERN; 	-type X		File type is X (one of: f,d,l,b,c,...); 	-perm MASK	At least one mask bit (+MASK), all bits (-MASK),; 			or exactly MASK bits are set in file's mode; 	-mtime DAYS	mtime is greater than (+N), less than (-N),; 			or exactly N days in the past; 	-mmin MINS	mtime is greater than (+N), less than (-N),; 			or exactly N minutes in the past; 	-newer FILE	mtime is more recent than FILE's; 	-user NAME/ID	File is owned by given user; 	-group NAME/ID	File is owned by given group; 	-size N[bck]	File size is N (c:bytes,k:kbytes,b:512 bytes(def.)); 			+/-N: file size is bigger/smaller than N; 	-prune		If current file is directory, don't descend into it; If none of the following actions is specified, -print is assumed; 	-print		Print file name; 	-print0		Print file name, NUL terminated; 	-exec CMD ARG ;	Run CMD with all instances of {} replaced by; 			file name. Fails if CMD exits with nonzero. xargs: invalid option -- 'I'; BusyBox v1.22.1 (2014-05-23 01:24:27 UTC) multi-call binary. Usage: xargs [OPTIONS] [PROG ARGS]. Run PROG on every item given by stdin. 	-r	Don't run command if input is empty; 	-0	Input is separated by NUL characters; 	-t	Print the command on stderr before execution; 	-e[STR]	STR stops input processing; 	-n N	Pass no more than N args to PROG; 	-s N	Pass command ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4607
https://github.com/broadinstitute/cromwell/issues/4607:1923,Availability,MASK,MASK,1923,"v		Don't descend directories on other filesystems; 	-maxdepth N	Descend at most N levels. -maxdepth 0 applies; 			actions to command line arguments only; 	-mindepth N	Don't act on first N levels; 	-depth		Act on directory *after* traversing it. Actions:; 	( ACTIONS )	Group actions for -o / -a; 	! ACT		Invert ACT's success/failure; 	ACT1 [-a] ACT2	If ACT1 fails, stop, else do ACT2; 	ACT1 -o ACT2	If ACT1 succeeds, stop, else do ACT2; 			Note: -a has higher priority than -o; 	-name PATTERN	Match file name (w/o directory name) to PATTERN; 	-iname PATTERN	Case insensitive -name; 	-path PATTERN	Match path to PATTERN; 	-ipath PATTERN	Case insensitive -path; 	-regex PATTERN	Match path to regex PATTERN; 	-type X		File type is X (one of: f,d,l,b,c,...); 	-perm MASK	At least one mask bit (+MASK), all bits (-MASK),; 			or exactly MASK bits are set in file's mode; 	-mtime DAYS	mtime is greater than (+N), less than (-N),; 			or exactly N days in the past; 	-mmin MINS	mtime is greater than (+N), less than (-N),; 			or exactly N minutes in the past; 	-newer FILE	mtime is more recent than FILE's; 	-user NAME/ID	File is owned by given user; 	-group NAME/ID	File is owned by given group; 	-size N[bck]	File size is N (c:bytes,k:kbytes,b:512 bytes(def.)); 			+/-N: file size is bigger/smaller than N; 	-prune		If current file is directory, don't descend into it; If none of the following actions is specified, -print is assumed; 	-print		Print file name; 	-print0		Print file name, NUL terminated; 	-exec CMD ARG ;	Run CMD with all instances of {} replaced by; 			file name. Fails if CMD exits with nonzero. xargs: invalid option -- 'I'; BusyBox v1.22.1 (2014-05-23 01:24:27 UTC) multi-call binary. Usage: xargs [OPTIONS] [PROG ARGS]. Run PROG on every item given by stdin. 	-r	Don't run command if input is empty; 	-0	Input is separated by NUL characters; 	-t	Print the command on stderr before execution; 	-e[STR]	STR stops input processing; 	-n N	Pass no more than N args to PROG; 	-s N	Pass command ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4607
https://github.com/broadinstitute/cromwell/issues/4607:1945,Availability,MASK,MASK,1945,"v		Don't descend directories on other filesystems; 	-maxdepth N	Descend at most N levels. -maxdepth 0 applies; 			actions to command line arguments only; 	-mindepth N	Don't act on first N levels; 	-depth		Act on directory *after* traversing it. Actions:; 	( ACTIONS )	Group actions for -o / -a; 	! ACT		Invert ACT's success/failure; 	ACT1 [-a] ACT2	If ACT1 fails, stop, else do ACT2; 	ACT1 -o ACT2	If ACT1 succeeds, stop, else do ACT2; 			Note: -a has higher priority than -o; 	-name PATTERN	Match file name (w/o directory name) to PATTERN; 	-iname PATTERN	Case insensitive -name; 	-path PATTERN	Match path to PATTERN; 	-ipath PATTERN	Case insensitive -path; 	-regex PATTERN	Match path to regex PATTERN; 	-type X		File type is X (one of: f,d,l,b,c,...); 	-perm MASK	At least one mask bit (+MASK), all bits (-MASK),; 			or exactly MASK bits are set in file's mode; 	-mtime DAYS	mtime is greater than (+N), less than (-N),; 			or exactly N days in the past; 	-mmin MINS	mtime is greater than (+N), less than (-N),; 			or exactly N minutes in the past; 	-newer FILE	mtime is more recent than FILE's; 	-user NAME/ID	File is owned by given user; 	-group NAME/ID	File is owned by given group; 	-size N[bck]	File size is N (c:bytes,k:kbytes,b:512 bytes(def.)); 			+/-N: file size is bigger/smaller than N; 	-prune		If current file is directory, don't descend into it; If none of the following actions is specified, -print is assumed; 	-print		Print file name; 	-print0		Print file name, NUL terminated; 	-exec CMD ARG ;	Run CMD with all instances of {} replaced by; 			file name. Fails if CMD exits with nonzero. xargs: invalid option -- 'I'; BusyBox v1.22.1 (2014-05-23 01:24:27 UTC) multi-call binary. Usage: xargs [OPTIONS] [PROG ARGS]. Run PROG on every item given by stdin. 	-r	Don't run command if input is empty; 	-0	Input is separated by NUL characters; 	-t	Print the command on stderr before execution; 	-e[STR]	STR stops input processing; 	-n N	Pass no more than N args to PROG; 	-s N	Pass command ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4607
https://github.com/broadinstitute/cromwell/issues/4607:3346,Deployability,integrat,integration,3346,"	! ACT		Invert ACT's success/failure; 	ACT1 [-a] ACT2	If ACT1 fails, stop, else do ACT2; 	ACT1 -o ACT2	If ACT1 succeeds, stop, else do ACT2; 			Note: -a has higher priority than -o; 	-name PATTERN	Match file name (w/o directory name) to PATTERN; 	-iname PATTERN	Case insensitive -name; 	-path PATTERN	Match path to PATTERN; 	-ipath PATTERN	Case insensitive -path; 	-regex PATTERN	Match path to regex PATTERN; 	-type X		File type is X (one of: f,d,l,b,c,...); 	-perm MASK	At least one mask bit (+MASK), all bits (-MASK),; 			or exactly MASK bits are set in file's mode; 	-mtime DAYS	mtime is greater than (+N), less than (-N),; 			or exactly N days in the past; 	-mmin MINS	mtime is greater than (+N), less than (-N),; 			or exactly N minutes in the past; 	-newer FILE	mtime is more recent than FILE's; 	-user NAME/ID	File is owned by given user; 	-group NAME/ID	File is owned by given group; 	-size N[bck]	File size is N (c:bytes,k:kbytes,b:512 bytes(def.)); 			+/-N: file size is bigger/smaller than N; 	-prune		If current file is directory, don't descend into it; If none of the following actions is specified, -print is assumed; 	-print		Print file name; 	-print0		Print file name, NUL terminated; 	-exec CMD ARG ;	Run CMD with all instances of {} replaced by; 			file name. Fails if CMD exits with nonzero. xargs: invalid option -- 'I'; BusyBox v1.22.1 (2014-05-23 01:24:27 UTC) multi-call binary. Usage: xargs [OPTIONS] [PROG ARGS]. Run PROG on every item given by stdin. 	-r	Don't run command if input is empty; 	-0	Input is separated by NUL characters; 	-t	Print the command on stderr before execution; 	-e[STR]	STR stops input processing; 	-n N	Pass no more than N args to PROG; 	-s N	Pass command line of no more than N bytes; 	-x	Exit if size is exceeded; ```. Notice that Nextflow had a [similar issue](https://github.com/nextflow-io/nextflow/issues/321) that I reported a few months ago, and is now fixed, allowing seamless integration of Biocontainers with Nextflow pipelines. Thank you!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4607
https://github.com/broadinstitute/cromwell/issues/4607:3389,Deployability,pipeline,pipelines,3389,"	! ACT		Invert ACT's success/failure; 	ACT1 [-a] ACT2	If ACT1 fails, stop, else do ACT2; 	ACT1 -o ACT2	If ACT1 succeeds, stop, else do ACT2; 			Note: -a has higher priority than -o; 	-name PATTERN	Match file name (w/o directory name) to PATTERN; 	-iname PATTERN	Case insensitive -name; 	-path PATTERN	Match path to PATTERN; 	-ipath PATTERN	Case insensitive -path; 	-regex PATTERN	Match path to regex PATTERN; 	-type X		File type is X (one of: f,d,l,b,c,...); 	-perm MASK	At least one mask bit (+MASK), all bits (-MASK),; 			or exactly MASK bits are set in file's mode; 	-mtime DAYS	mtime is greater than (+N), less than (-N),; 			or exactly N days in the past; 	-mmin MINS	mtime is greater than (+N), less than (-N),; 			or exactly N minutes in the past; 	-newer FILE	mtime is more recent than FILE's; 	-user NAME/ID	File is owned by given user; 	-group NAME/ID	File is owned by given group; 	-size N[bck]	File size is N (c:bytes,k:kbytes,b:512 bytes(def.)); 			+/-N: file size is bigger/smaller than N; 	-prune		If current file is directory, don't descend into it; If none of the following actions is specified, -print is assumed; 	-print		Print file name; 	-print0		Print file name, NUL terminated; 	-exec CMD ARG ;	Run CMD with all instances of {} replaced by; 			file name. Fails if CMD exits with nonzero. xargs: invalid option -- 'I'; BusyBox v1.22.1 (2014-05-23 01:24:27 UTC) multi-call binary. Usage: xargs [OPTIONS] [PROG ARGS]. Run PROG on every item given by stdin. 	-r	Don't run command if input is empty; 	-0	Input is separated by NUL characters; 	-t	Print the command on stderr before execution; 	-e[STR]	STR stops input processing; 	-n N	Pass no more than N args to PROG; 	-s N	Pass command line of no more than N bytes; 	-x	Exit if size is exceeded; ```. Notice that Nextflow had a [similar issue](https://github.com/nextflow-io/nextflow/issues/321) that I reported a few months ago, and is now fixed, allowing seamless integration of Biocontainers with Nextflow pipelines. Thank you!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4607
https://github.com/broadinstitute/cromwell/issues/4607:281,Energy Efficiency,reduce,reduce,281,"Hi there,. Biocontainers are incredibly popular tools in bioinformatics. Each time a [Bioconda recipe](https://bioconda.github.io/recipes.html) is made, a Biocontainer is automatically generated. This creates a collection of Dockerized tools immediately available to everybody. To reduce footprint, Biocontainers run on BusyBox, a Linux version tailored to embedded systems. Some tools therefore do not expose the full set of options of matching GNU tools on Ubuntu, CentOS, etc. Given the popularity of Biocontainers, it would be great if Cromwell could support them fully. There are a couple of small bugs related to `find` and `xargs` that should be easy to fix when one explores the file `stderr.background` for a task run from a Biocontainer:. ```; find: unrecognized: -empty; BusyBox v1.22.1 (2014-05-23 01:24:27 UTC) multi-call binary. Usage: find [-HL] [PATH]... [OPTIONS] [ACTIONS]. Search for files and perform actions on them.; First failed action stops processing of current file.; Defaults: PATH is current directory, action is '-print'. 	-L,-follow	Follow symlinks; 	-H		...on command line only; 	-xdev		Don't descend directories on other filesystems; 	-maxdepth N	Descend at most N levels. -maxdepth 0 applies; 			actions to command line arguments only; 	-mindepth N	Don't act on first N levels; 	-depth		Act on directory *after* traversing it. Actions:; 	( ACTIONS )	Group actions for -o / -a; 	! ACT		Invert ACT's success/failure; 	ACT1 [-a] ACT2	If ACT1 fails, stop, else do ACT2; 	ACT1 -o ACT2	If ACT1 succeeds, stop, else do ACT2; 			Note: -a has higher priority than -o; 	-name PATTERN	Match file name (w/o directory name) to PATTERN; 	-iname PATTERN	Case insensitive -name; 	-path PATTERN	Match path to PATTERN; 	-ipath PATTERN	Case insensitive -path; 	-regex PATTERN	Match path to regex PATTERN; 	-type X		File type is X (one of: f,d,l,b,c,...); 	-perm MASK	At least one mask bit (+MASK), all bits (-MASK),; 			or exactly MASK bits are set in file's mode; 	-mtime DAYS	mtime is ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4607
https://github.com/broadinstitute/cromwell/issues/4607:3346,Integrability,integrat,integration,3346,"	! ACT		Invert ACT's success/failure; 	ACT1 [-a] ACT2	If ACT1 fails, stop, else do ACT2; 	ACT1 -o ACT2	If ACT1 succeeds, stop, else do ACT2; 			Note: -a has higher priority than -o; 	-name PATTERN	Match file name (w/o directory name) to PATTERN; 	-iname PATTERN	Case insensitive -name; 	-path PATTERN	Match path to PATTERN; 	-ipath PATTERN	Case insensitive -path; 	-regex PATTERN	Match path to regex PATTERN; 	-type X		File type is X (one of: f,d,l,b,c,...); 	-perm MASK	At least one mask bit (+MASK), all bits (-MASK),; 			or exactly MASK bits are set in file's mode; 	-mtime DAYS	mtime is greater than (+N), less than (-N),; 			or exactly N days in the past; 	-mmin MINS	mtime is greater than (+N), less than (-N),; 			or exactly N minutes in the past; 	-newer FILE	mtime is more recent than FILE's; 	-user NAME/ID	File is owned by given user; 	-group NAME/ID	File is owned by given group; 	-size N[bck]	File size is N (c:bytes,k:kbytes,b:512 bytes(def.)); 			+/-N: file size is bigger/smaller than N; 	-prune		If current file is directory, don't descend into it; If none of the following actions is specified, -print is assumed; 	-print		Print file name; 	-print0		Print file name, NUL terminated; 	-exec CMD ARG ;	Run CMD with all instances of {} replaced by; 			file name. Fails if CMD exits with nonzero. xargs: invalid option -- 'I'; BusyBox v1.22.1 (2014-05-23 01:24:27 UTC) multi-call binary. Usage: xargs [OPTIONS] [PROG ARGS]. Run PROG on every item given by stdin. 	-r	Don't run command if input is empty; 	-0	Input is separated by NUL characters; 	-t	Print the command on stderr before execution; 	-e[STR]	STR stops input processing; 	-n N	Pass no more than N args to PROG; 	-s N	Pass command line of no more than N bytes; 	-x	Exit if size is exceeded; ```. Notice that Nextflow had a [similar issue](https://github.com/nextflow-io/nextflow/issues/321) that I reported a few months ago, and is now fixed, allowing seamless integration of Biocontainers with Nextflow pipelines. Thank you!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4607
https://github.com/broadinstitute/cromwell/issues/4607:913,Performance,perform,perform,913,"Hi there,. Biocontainers are incredibly popular tools in bioinformatics. Each time a [Bioconda recipe](https://bioconda.github.io/recipes.html) is made, a Biocontainer is automatically generated. This creates a collection of Dockerized tools immediately available to everybody. To reduce footprint, Biocontainers run on BusyBox, a Linux version tailored to embedded systems. Some tools therefore do not expose the full set of options of matching GNU tools on Ubuntu, CentOS, etc. Given the popularity of Biocontainers, it would be great if Cromwell could support them fully. There are a couple of small bugs related to `find` and `xargs` that should be easy to fix when one explores the file `stderr.background` for a task run from a Biocontainer:. ```; find: unrecognized: -empty; BusyBox v1.22.1 (2014-05-23 01:24:27 UTC) multi-call binary. Usage: find [-HL] [PATH]... [OPTIONS] [ACTIONS]. Search for files and perform actions on them.; First failed action stops processing of current file.; Defaults: PATH is current directory, action is '-print'. 	-L,-follow	Follow symlinks; 	-H		...on command line only; 	-xdev		Don't descend directories on other filesystems; 	-maxdepth N	Descend at most N levels. -maxdepth 0 applies; 			actions to command line arguments only; 	-mindepth N	Don't act on first N levels; 	-depth		Act on directory *after* traversing it. Actions:; 	( ACTIONS )	Group actions for -o / -a; 	! ACT		Invert ACT's success/failure; 	ACT1 [-a] ACT2	If ACT1 fails, stop, else do ACT2; 	ACT1 -o ACT2	If ACT1 succeeds, stop, else do ACT2; 			Note: -a has higher priority than -o; 	-name PATTERN	Match file name (w/o directory name) to PATTERN; 	-iname PATTERN	Case insensitive -name; 	-path PATTERN	Match path to PATTERN; 	-ipath PATTERN	Case insensitive -path; 	-regex PATTERN	Match path to regex PATTERN; 	-type X		File type is X (one of: f,d,l,b,c,...); 	-perm MASK	At least one mask bit (+MASK), all bits (-MASK),; 			or exactly MASK bits are set in file's mode; 	-mtime DAYS	mtime is ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4607
https://github.com/broadinstitute/cromwell/issues/4607:403,Security,expose,expose,403,"Hi there,. Biocontainers are incredibly popular tools in bioinformatics. Each time a [Bioconda recipe](https://bioconda.github.io/recipes.html) is made, a Biocontainer is automatically generated. This creates a collection of Dockerized tools immediately available to everybody. To reduce footprint, Biocontainers run on BusyBox, a Linux version tailored to embedded systems. Some tools therefore do not expose the full set of options of matching GNU tools on Ubuntu, CentOS, etc. Given the popularity of Biocontainers, it would be great if Cromwell could support them fully. There are a couple of small bugs related to `find` and `xargs` that should be easy to fix when one explores the file `stderr.background` for a task run from a Biocontainer:. ```; find: unrecognized: -empty; BusyBox v1.22.1 (2014-05-23 01:24:27 UTC) multi-call binary. Usage: find [-HL] [PATH]... [OPTIONS] [ACTIONS]. Search for files and perform actions on them.; First failed action stops processing of current file.; Defaults: PATH is current directory, action is '-print'. 	-L,-follow	Follow symlinks; 	-H		...on command line only; 	-xdev		Don't descend directories on other filesystems; 	-maxdepth N	Descend at most N levels. -maxdepth 0 applies; 			actions to command line arguments only; 	-mindepth N	Don't act on first N levels; 	-depth		Act on directory *after* traversing it. Actions:; 	( ACTIONS )	Group actions for -o / -a; 	! ACT		Invert ACT's success/failure; 	ACT1 [-a] ACT2	If ACT1 fails, stop, else do ACT2; 	ACT1 -o ACT2	If ACT1 succeeds, stop, else do ACT2; 			Note: -a has higher priority than -o; 	-name PATTERN	Match file name (w/o directory name) to PATTERN; 	-iname PATTERN	Case insensitive -name; 	-path PATTERN	Match path to PATTERN; 	-ipath PATTERN	Case insensitive -path; 	-regex PATTERN	Match path to regex PATTERN; 	-type X		File type is X (one of: f,d,l,b,c,...); 	-perm MASK	At least one mask bit (+MASK), all bits (-MASK),; 			or exactly MASK bits are set in file's mode; 	-mtime DAYS	mtime is ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4607
https://github.com/broadinstitute/cromwell/issues/4608:513,Deployability,pipeline,pipelines,513,"When testing our workflow, we are seeing variations in some calculations based on the cpu architecture chosen for a VM. In PAPI V1, we worked around this issue by restricting the test to only run in a single region. This caused the same cpu to be used for all of the affected tests, and we were able to easily validate the workflow output. Now that we are testing the workflow under PAPI V2, we are running into the same issue, and setting the region no longer fixes the issue. It looks like the `minCpuPlatform` pipelines API setting would be sufficient for this. See [Specifying a Minimum CPU Platform for VM Instances](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform). Does this sound like a reasonable feature to add to cromwell's PAPI V2 support?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4608
https://github.com/broadinstitute/cromwell/issues/4608:310,Security,validat,validate,310,"When testing our workflow, we are seeing variations in some calculations based on the cpu architecture chosen for a VM. In PAPI V1, we worked around this issue by restricting the test to only run in a single region. This caused the same cpu to be used for all of the affected tests, and we were able to easily validate the workflow output. Now that we are testing the workflow under PAPI V2, we are running into the same issue, and setting the region no longer fixes the issue. It looks like the `minCpuPlatform` pipelines API setting would be sufficient for this. See [Specifying a Minimum CPU Platform for VM Instances](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform). Does this sound like a reasonable feature to add to cromwell's PAPI V2 support?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4608
https://github.com/broadinstitute/cromwell/issues/4608:5,Testability,test,testing,5,"When testing our workflow, we are seeing variations in some calculations based on the cpu architecture chosen for a VM. In PAPI V1, we worked around this issue by restricting the test to only run in a single region. This caused the same cpu to be used for all of the affected tests, and we were able to easily validate the workflow output. Now that we are testing the workflow under PAPI V2, we are running into the same issue, and setting the region no longer fixes the issue. It looks like the `minCpuPlatform` pipelines API setting would be sufficient for this. See [Specifying a Minimum CPU Platform for VM Instances](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform). Does this sound like a reasonable feature to add to cromwell's PAPI V2 support?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4608
https://github.com/broadinstitute/cromwell/issues/4608:179,Testability,test,test,179,"When testing our workflow, we are seeing variations in some calculations based on the cpu architecture chosen for a VM. In PAPI V1, we worked around this issue by restricting the test to only run in a single region. This caused the same cpu to be used for all of the affected tests, and we were able to easily validate the workflow output. Now that we are testing the workflow under PAPI V2, we are running into the same issue, and setting the region no longer fixes the issue. It looks like the `minCpuPlatform` pipelines API setting would be sufficient for this. See [Specifying a Minimum CPU Platform for VM Instances](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform). Does this sound like a reasonable feature to add to cromwell's PAPI V2 support?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4608
https://github.com/broadinstitute/cromwell/issues/4608:276,Testability,test,tests,276,"When testing our workflow, we are seeing variations in some calculations based on the cpu architecture chosen for a VM. In PAPI V1, we worked around this issue by restricting the test to only run in a single region. This caused the same cpu to be used for all of the affected tests, and we were able to easily validate the workflow output. Now that we are testing the workflow under PAPI V2, we are running into the same issue, and setting the region no longer fixes the issue. It looks like the `minCpuPlatform` pipelines API setting would be sufficient for this. See [Specifying a Minimum CPU Platform for VM Instances](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform). Does this sound like a reasonable feature to add to cromwell's PAPI V2 support?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4608
https://github.com/broadinstitute/cromwell/issues/4608:356,Testability,test,testing,356,"When testing our workflow, we are seeing variations in some calculations based on the cpu architecture chosen for a VM. In PAPI V1, we worked around this issue by restricting the test to only run in a single region. This caused the same cpu to be used for all of the affected tests, and we were able to easily validate the workflow output. Now that we are testing the workflow under PAPI V2, we are running into the same issue, and setting the region no longer fixes the issue. It looks like the `minCpuPlatform` pipelines API setting would be sufficient for this. See [Specifying a Minimum CPU Platform for VM Instances](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform). Does this sound like a reasonable feature to add to cromwell's PAPI V2 support?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4608
https://github.com/broadinstitute/cromwell/pull/4609:114,Security,validat,validated,114,This is my attempt at addressing issue #4608 . - add `cpuPlatform` as a new runtime attribute; - contents are not validated; they're passed on directly to PAPI V2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4609
https://github.com/broadinstitute/cromwell/pull/4610:286,Availability,avail,available,286,"One representative pair of timings on a stone cold prod clone (restarted between queries) for a single submission ID label with two ORed collection labels:. develop: 31.20 sec; this branch: 0.08 sec. No index changes required. MySQL will use a k/v index on `CUSTOM_LABEL_ENTRY` if it's available but its presence doesn't seem to actually make much difference in execution time. Exclude labels have been folded into the new system, I still need to measure the performance impact of those changes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4610
https://github.com/broadinstitute/cromwell/pull/4610:459,Performance,perform,performance,459,"One representative pair of timings on a stone cold prod clone (restarted between queries) for a single submission ID label with two ORed collection labels:. develop: 31.20 sec; this branch: 0.08 sec. No index changes required. MySQL will use a k/v index on `CUSTOM_LABEL_ENTRY` if it's available but its presence doesn't seem to actually make much difference in execution time. Exclude labels have been folded into the new system, I still need to measure the performance impact of those changes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4610
https://github.com/broadinstitute/cromwell/issues/4611:871,Availability,error,error,871,"I am trying to run the GATK4 workflow on Cromwell using the Spark backend. The WDL is from the Broad's GitHub repo:; https://github.com/broadinstitute/gatk4-data-processing/blob/master/processing-for-variant-discovery-gatk4.wdl. The following is my cromwell conf file where cromwell is running on the namenode of the spark cluster:; ```json; include required(classpath(""application"")). backend {; default = ""Spark""; providers {; Spark {; actor-factory = ""cromwell.backend.impl.spark.SparkBackendFactory""; config {; root: ""/mnt/data/cromwell"". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; }; }; }; master: ""yarn""; deployMode: ""cilent""; }; }; }; }; ```. It looks like the shell script cromwell is generating to submit the job to spark using spark-submit has a syntax error in it, so the workflow fails immediately. ```bash; [ec2-user@ip-10-66-51-33 execution]$ pwd; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ ls -l; total 8; -rwxr--r-- 1 root root 1182 Feb 4 19:37 script; -rw-r--r-- 1 root root 296 Feb 4 19:37 stderr; -rw-r--r-- 1 root root 0 Feb 4 19:37 stdout; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ cat stderr; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: 3: /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: Syntax error: ""("" unexpected; ```. ```; [ec2-user@ip-10-66-51-33 execution]$ cat script ; #!/bin/sh; cd /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; spark-submit --master yarn --total-executor-cores 1 --deploy-mode cilent --class GATK4 --executor-memory 1gb InstantiatedCommand(# Not setti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4611
https://github.com/broadinstitute/cromwell/issues/4611:1639,Availability,error,error,1639,"rkBackendFactory""; config {; root: ""/mnt/data/cromwell"". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; }; }; }; master: ""yarn""; deployMode: ""cilent""; }; }; }; }; ```. It looks like the shell script cromwell is generating to submit the job to spark using spark-submit has a syntax error in it, so the workflow fails immediately. ```bash; [ec2-user@ip-10-66-51-33 execution]$ pwd; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ ls -l; total 8; -rwxr--r-- 1 root root 1182 Feb 4 19:37 script; -rw-r--r-- 1 root root 296 Feb 4 19:37 stderr; -rw-r--r-- 1 root root 0 Feb 4 19:37 stdout; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ cat stderr; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: 3: /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: Syntax error: ""("" unexpected; ```. ```; [ec2-user@ip-10-66-51-33 execution]$ cat script ; #!/bin/sh; cd /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; spark-submit --master yarn --total-executor-cores 1 --deploy-mode cilent --class GATK4 --executor-memory 1gb InstantiatedCommand(# Not setting ""set -o pipefail"" here because /bwa has a rc=1 and we don't want to allow rc=1 to succeed ; # because the sed may also fail with that error and that is something we actually want to fail on.; /usr/local/bin/bwa 2>&1 | \; grep -e '^Version' | \; sed 's/Version: //',Map(),List(),None,None,None,List((LocalName(mem_size),WomString(1 GB)), (LocalName(bwa_path),WomString(/usr/local/bin/)), (LocalName(preemptible_tries),WomInteger(3)), (LocalName(entry_point),WomString(GATK4)), (Local",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4611
https://github.com/broadinstitute/cromwell/issues/4611:2138,Availability,error,error,2138,"mediately. ```bash; [ec2-user@ip-10-66-51-33 execution]$ pwd; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ ls -l; total 8; -rwxr--r-- 1 root root 1182 Feb 4 19:37 script; -rw-r--r-- 1 root root 296 Feb 4 19:37 stderr; -rw-r--r-- 1 root root 0 Feb 4 19:37 stdout; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ cat stderr; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: 3: /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: Syntax error: ""("" unexpected; ```. ```; [ec2-user@ip-10-66-51-33 execution]$ cat script ; #!/bin/sh; cd /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; spark-submit --master yarn --total-executor-cores 1 --deploy-mode cilent --class GATK4 --executor-memory 1gb InstantiatedCommand(# Not setting ""set -o pipefail"" here because /bwa has a rc=1 and we don't want to allow rc=1 to succeed ; # because the sed may also fail with that error and that is something we actually want to fail on.; /usr/local/bin/bwa 2>&1 | \; grep -e '^Version' | \; sed 's/Version: //',Map(),List(),None,None,None,List((LocalName(mem_size),WomString(1 GB)), (LocalName(bwa_path),WomString(/usr/local/bin/)), (LocalName(preemptible_tries),WomInteger(3)), (LocalName(entry_point),WomString(GATK4)), (LocalName(docker_image),WomString(227114915345.dkr.ecr.us-east-1.amazonaws.com/genomes-in-the-cloud:20190115))),List((LocalName(mem_size),WomString(1 GB)), (LocalName(bwa_path),WomString(/usr/local/bin/)), (LocalName(preemptible_tries),WomInteger(3)), (LocalName(entry_point),WomString(GATK4)), (LocalName(docker_image),WomString(227114915345.dkr.ecr.us-east-1.amazonaws.com/genomes-in-the-cloud:20190115)))); echo $? > rc; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4611
https://github.com/broadinstitute/cromwell/issues/4611:2891,Availability,echo,echo,2891,"mediately. ```bash; [ec2-user@ip-10-66-51-33 execution]$ pwd; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ ls -l; total 8; -rwxr--r-- 1 root root 1182 Feb 4 19:37 script; -rw-r--r-- 1 root root 296 Feb 4 19:37 stderr; -rw-r--r-- 1 root root 0 Feb 4 19:37 stdout; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ cat stderr; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: 3: /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: Syntax error: ""("" unexpected; ```. ```; [ec2-user@ip-10-66-51-33 execution]$ cat script ; #!/bin/sh; cd /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; spark-submit --master yarn --total-executor-cores 1 --deploy-mode cilent --class GATK4 --executor-memory 1gb InstantiatedCommand(# Not setting ""set -o pipefail"" here because /bwa has a rc=1 and we don't want to allow rc=1 to succeed ; # because the sed may also fail with that error and that is something we actually want to fail on.; /usr/local/bin/bwa 2>&1 | \; grep -e '^Version' | \; sed 's/Version: //',Map(),List(),None,None,None,List((LocalName(mem_size),WomString(1 GB)), (LocalName(bwa_path),WomString(/usr/local/bin/)), (LocalName(preemptible_tries),WomInteger(3)), (LocalName(entry_point),WomString(GATK4)), (LocalName(docker_image),WomString(227114915345.dkr.ecr.us-east-1.amazonaws.com/genomes-in-the-cloud:20190115))),List((LocalName(mem_size),WomString(1 GB)), (LocalName(bwa_path),WomString(/usr/local/bin/)), (LocalName(preemptible_tries),WomInteger(3)), (LocalName(entry_point),WomString(GATK4)), (LocalName(docker_image),WomString(227114915345.dkr.ecr.us-east-1.amazonaws.com/genomes-in-the-cloud:20190115)))); echo $? > rc; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4611
https://github.com/broadinstitute/cromwell/issues/4611:719,Deployability,deploy,deployMode,719,"I am trying to run the GATK4 workflow on Cromwell using the Spark backend. The WDL is from the Broad's GitHub repo:; https://github.com/broadinstitute/gatk4-data-processing/blob/master/processing-for-variant-discovery-gatk4.wdl. The following is my cromwell conf file where cromwell is running on the namenode of the spark cluster:; ```json; include required(classpath(""application"")). backend {; default = ""Spark""; providers {; Spark {; actor-factory = ""cromwell.backend.impl.spark.SparkBackendFactory""; config {; root: ""/mnt/data/cromwell"". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; }; }; }; master: ""yarn""; deployMode: ""cilent""; }; }; }; }; ```. It looks like the shell script cromwell is generating to submit the job to spark using spark-submit has a syntax error in it, so the workflow fails immediately. ```bash; [ec2-user@ip-10-66-51-33 execution]$ pwd; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ ls -l; total 8; -rwxr--r-- 1 root root 1182 Feb 4 19:37 script; -rw-r--r-- 1 root root 296 Feb 4 19:37 stderr; -rw-r--r-- 1 root root 0 Feb 4 19:37 stdout; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ cat stderr; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: 3: /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: Syntax error: ""("" unexpected; ```. ```; [ec2-user@ip-10-66-51-33 execution]$ cat script ; #!/bin/sh; cd /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; spark-submit --master yarn --total-executor-cores 1 --deploy-mode cilent --class GATK4 --executor-memory 1gb InstantiatedCommand(# Not setti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4611
https://github.com/broadinstitute/cromwell/issues/4611:1915,Deployability,deploy,deploy-mode,1915,"mediately. ```bash; [ec2-user@ip-10-66-51-33 execution]$ pwd; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ ls -l; total 8; -rwxr--r-- 1 root root 1182 Feb 4 19:37 script; -rw-r--r-- 1 root root 296 Feb 4 19:37 stderr; -rw-r--r-- 1 root root 0 Feb 4 19:37 stdout; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ cat stderr; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: 3: /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: Syntax error: ""("" unexpected; ```. ```; [ec2-user@ip-10-66-51-33 execution]$ cat script ; #!/bin/sh; cd /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; spark-submit --master yarn --total-executor-cores 1 --deploy-mode cilent --class GATK4 --executor-memory 1gb InstantiatedCommand(# Not setting ""set -o pipefail"" here because /bwa has a rc=1 and we don't want to allow rc=1 to succeed ; # because the sed may also fail with that error and that is something we actually want to fail on.; /usr/local/bin/bwa 2>&1 | \; grep -e '^Version' | \; sed 's/Version: //',Map(),List(),None,None,None,List((LocalName(mem_size),WomString(1 GB)), (LocalName(bwa_path),WomString(/usr/local/bin/)), (LocalName(preemptible_tries),WomInteger(3)), (LocalName(entry_point),WomString(GATK4)), (LocalName(docker_image),WomString(227114915345.dkr.ecr.us-east-1.amazonaws.com/genomes-in-the-cloud:20190115))),List((LocalName(mem_size),WomString(1 GB)), (LocalName(bwa_path),WomString(/usr/local/bin/)), (LocalName(preemptible_tries),WomInteger(3)), (LocalName(entry_point),WomString(GATK4)), (LocalName(docker_image),WomString(227114915345.dkr.ecr.us-east-1.amazonaws.com/genomes-in-the-cloud:20190115)))); echo $? > rc; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4611
https://github.com/broadinstitute/cromwell/issues/4611:505,Modifiability,config,config,505,"I am trying to run the GATK4 workflow on Cromwell using the Spark backend. The WDL is from the Broad's GitHub repo:; https://github.com/broadinstitute/gatk4-data-processing/blob/master/processing-for-variant-discovery-gatk4.wdl. The following is my cromwell conf file where cromwell is running on the namenode of the spark cluster:; ```json; include required(classpath(""application"")). backend {; default = ""Spark""; providers {; Spark {; actor-factory = ""cromwell.backend.impl.spark.SparkBackendFactory""; config {; root: ""/mnt/data/cromwell"". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; }; }; }; master: ""yarn""; deployMode: ""cilent""; }; }; }; }; ```. It looks like the shell script cromwell is generating to submit the job to spark using spark-submit has a syntax error in it, so the workflow fails immediately. ```bash; [ec2-user@ip-10-66-51-33 execution]$ pwd; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ ls -l; total 8; -rwxr--r-- 1 root root 1182 Feb 4 19:37 script; -rw-r--r-- 1 root root 296 Feb 4 19:37 stderr; -rw-r--r-- 1 root root 0 Feb 4 19:37 stdout; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ cat stderr; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: 3: /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: Syntax error: ""("" unexpected; ```. ```; [ec2-user@ip-10-66-51-33 execution]$ cat script ; #!/bin/sh; cd /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; spark-submit --master yarn --total-executor-cores 1 --deploy-mode cilent --class GATK4 --executor-memory 1gb InstantiatedCommand(# Not setti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4611
https://github.com/broadinstitute/cromwell/issues/4613:268,Deployability,update,update,268,"Discussed at standup 2019-02-04. Right now, bcbio uses:; - pinned CWL (checked in); - floating docker; - floating data (not controlled by us). As such, it breaks pretty frequently due to external factors, negating its usefulness as a regression test. For this ticket, update our bcbio configuration to be two separate jobs - one fully floating to test compatibility at the cutting edge, one fully pinned to serve as a stable regression test. Floating job:; - pull CWL from Brad's repo; - floating docker [same as now]; - floating data [same as now]. Pinned job:; - pinned CWL [same as now]; - our own stable copy of the data; - our own stable copy of the dockers",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4613
https://github.com/broadinstitute/cromwell/issues/4613:285,Deployability,configurat,configuration,285,"Discussed at standup 2019-02-04. Right now, bcbio uses:; - pinned CWL (checked in); - floating docker; - floating data (not controlled by us). As such, it breaks pretty frequently due to external factors, negating its usefulness as a regression test. For this ticket, update our bcbio configuration to be two separate jobs - one fully floating to test compatibility at the cutting edge, one fully pinned to serve as a stable regression test. Floating job:; - pull CWL from Brad's repo; - floating docker [same as now]; - floating data [same as now]. Pinned job:; - pinned CWL [same as now]; - our own stable copy of the data; - our own stable copy of the dockers",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4613
https://github.com/broadinstitute/cromwell/issues/4613:285,Modifiability,config,configuration,285,"Discussed at standup 2019-02-04. Right now, bcbio uses:; - pinned CWL (checked in); - floating docker; - floating data (not controlled by us). As such, it breaks pretty frequently due to external factors, negating its usefulness as a regression test. For this ticket, update our bcbio configuration to be two separate jobs - one fully floating to test compatibility at the cutting edge, one fully pinned to serve as a stable regression test. Floating job:; - pull CWL from Brad's repo; - floating docker [same as now]; - floating data [same as now]. Pinned job:; - pinned CWL [same as now]; - our own stable copy of the data; - our own stable copy of the dockers",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4613
https://github.com/broadinstitute/cromwell/issues/4613:245,Testability,test,test,245,"Discussed at standup 2019-02-04. Right now, bcbio uses:; - pinned CWL (checked in); - floating docker; - floating data (not controlled by us). As such, it breaks pretty frequently due to external factors, negating its usefulness as a regression test. For this ticket, update our bcbio configuration to be two separate jobs - one fully floating to test compatibility at the cutting edge, one fully pinned to serve as a stable regression test. Floating job:; - pull CWL from Brad's repo; - floating docker [same as now]; - floating data [same as now]. Pinned job:; - pinned CWL [same as now]; - our own stable copy of the data; - our own stable copy of the dockers",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4613
https://github.com/broadinstitute/cromwell/issues/4613:347,Testability,test,test,347,"Discussed at standup 2019-02-04. Right now, bcbio uses:; - pinned CWL (checked in); - floating docker; - floating data (not controlled by us). As such, it breaks pretty frequently due to external factors, negating its usefulness as a regression test. For this ticket, update our bcbio configuration to be two separate jobs - one fully floating to test compatibility at the cutting edge, one fully pinned to serve as a stable regression test. Floating job:; - pull CWL from Brad's repo; - floating docker [same as now]; - floating data [same as now]. Pinned job:; - pinned CWL [same as now]; - our own stable copy of the data; - our own stable copy of the dockers",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4613
https://github.com/broadinstitute/cromwell/issues/4613:436,Testability,test,test,436,"Discussed at standup 2019-02-04. Right now, bcbio uses:; - pinned CWL (checked in); - floating docker; - floating data (not controlled by us). As such, it breaks pretty frequently due to external factors, negating its usefulness as a regression test. For this ticket, update our bcbio configuration to be two separate jobs - one fully floating to test compatibility at the cutting edge, one fully pinned to serve as a stable regression test. Floating job:; - pull CWL from Brad's repo; - floating docker [same as now]; - floating data [same as now]. Pinned job:; - pinned CWL [same as now]; - our own stable copy of the data; - our own stable copy of the dockers",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4613
https://github.com/broadinstitute/cromwell/issues/4614:279,Performance,perform,performant,279,"An audit of Cromwell traffic shows >96% of traffic hits the status endpoint for polling. A sample of this traffic shows an average of 40/s. As a perf benchmark we'd like to ensure that CromIAM can handle 200 reqs/s to this endpoint, ideally assuming SAM & Cromwell are perfectly performant.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4614
https://github.com/broadinstitute/cromwell/issues/4614:3,Security,audit,audit,3,"An audit of Cromwell traffic shows >96% of traffic hits the status endpoint for polling. A sample of this traffic shows an average of 40/s. As a perf benchmark we'd like to ensure that CromIAM can handle 200 reqs/s to this endpoint, ideally assuming SAM & Cromwell are perfectly performant.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4614
https://github.com/broadinstitute/cromwell/issues/4614:150,Testability,benchmark,benchmark,150,"An audit of Cromwell traffic shows >96% of traffic hits the status endpoint for polling. A sample of this traffic shows an average of 40/s. As a perf benchmark we'd like to ensure that CromIAM can handle 200 reqs/s to this endpoint, ideally assuming SAM & Cromwell are perfectly performant.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4614
https://github.com/broadinstitute/cromwell/pull/4615:126,Modifiability,config,config,126,"Searched the codebase for `request-timeout`, found that we seem to use 40 seconds not the 55 previously discussed. Copied the config stanza from `cromwell/server/src/main/resources/application.conf` to CromIAM.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4615
https://github.com/broadinstitute/cromwell/pull/4615:35,Safety,timeout,timeout,35,"Searched the codebase for `request-timeout`, found that we seem to use 40 seconds not the 55 previously discussed. Copied the config stanza from `cromwell/server/src/main/resources/application.conf` to CromIAM.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4615
https://github.com/broadinstitute/cromwell/issues/4616:160,Security,hash,hash,160,"Feature request: allow call caching to work across different engines. Currently, only calls run on the same engine can be reused. But if task inputs and docker hash are the same, results should be reusable across engines. This might require copying files across filesystems; but if there is a Local filesystem, then any file can be copied first to that and then to the target filesystem.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4616
https://github.com/broadinstitute/cromwell/issues/4618:30,Deployability,update,update,30,"At some point we will have to update our liquibase library, either for vulnerability patches or other bug fixes. . However there is a report that the current liquibase version (3.6.3) plus the way our changelog differentiates databases (ex: using strings matching for things like `mysql`) causes an issue with MariaDB. This ticket is not about updating liquibase. Before that can occur we need a CI regression test to ensure that MariaDB is supported. A/C:; - At least centaur-local running against mariadb 10.3+. Links:; - https://github.com/broadinstitute/cromwell/issues/4605; - https://www.sourceclear.com/vulnerability-database/security/cross-site-scripting-xss-/java/sid-6098; - https://liquibase.jira.com/browse/CORE-3203; - https://liquibase.jira.com/browse/CORE-3263 (may not be related) ; - https://docs.travis-ci.com/user/database-setup/#mariadb; - https://docs.travis-ci.com/user/build-matrix/#explicitly-including-jobs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4618
https://github.com/broadinstitute/cromwell/issues/4618:85,Deployability,patch,patches,85,"At some point we will have to update our liquibase library, either for vulnerability patches or other bug fixes. . However there is a report that the current liquibase version (3.6.3) plus the way our changelog differentiates databases (ex: using strings matching for things like `mysql`) causes an issue with MariaDB. This ticket is not about updating liquibase. Before that can occur we need a CI regression test to ensure that MariaDB is supported. A/C:; - At least centaur-local running against mariadb 10.3+. Links:; - https://github.com/broadinstitute/cromwell/issues/4605; - https://www.sourceclear.com/vulnerability-database/security/cross-site-scripting-xss-/java/sid-6098; - https://liquibase.jira.com/browse/CORE-3203; - https://liquibase.jira.com/browse/CORE-3263 (may not be related) ; - https://docs.travis-ci.com/user/database-setup/#mariadb; - https://docs.travis-ci.com/user/build-matrix/#explicitly-including-jobs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4618
https://github.com/broadinstitute/cromwell/issues/4618:633,Security,secur,security,633,"At some point we will have to update our liquibase library, either for vulnerability patches or other bug fixes. . However there is a report that the current liquibase version (3.6.3) plus the way our changelog differentiates databases (ex: using strings matching for things like `mysql`) causes an issue with MariaDB. This ticket is not about updating liquibase. Before that can occur we need a CI regression test to ensure that MariaDB is supported. A/C:; - At least centaur-local running against mariadb 10.3+. Links:; - https://github.com/broadinstitute/cromwell/issues/4605; - https://www.sourceclear.com/vulnerability-database/security/cross-site-scripting-xss-/java/sid-6098; - https://liquibase.jira.com/browse/CORE-3203; - https://liquibase.jira.com/browse/CORE-3263 (may not be related) ; - https://docs.travis-ci.com/user/database-setup/#mariadb; - https://docs.travis-ci.com/user/build-matrix/#explicitly-including-jobs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4618
https://github.com/broadinstitute/cromwell/issues/4618:663,Security,xss,xss,663,"At some point we will have to update our liquibase library, either for vulnerability patches or other bug fixes. . However there is a report that the current liquibase version (3.6.3) plus the way our changelog differentiates databases (ex: using strings matching for things like `mysql`) causes an issue with MariaDB. This ticket is not about updating liquibase. Before that can occur we need a CI regression test to ensure that MariaDB is supported. A/C:; - At least centaur-local running against mariadb 10.3+. Links:; - https://github.com/broadinstitute/cromwell/issues/4605; - https://www.sourceclear.com/vulnerability-database/security/cross-site-scripting-xss-/java/sid-6098; - https://liquibase.jira.com/browse/CORE-3203; - https://liquibase.jira.com/browse/CORE-3263 (may not be related) ; - https://docs.travis-ci.com/user/database-setup/#mariadb; - https://docs.travis-ci.com/user/build-matrix/#explicitly-including-jobs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4618
https://github.com/broadinstitute/cromwell/issues/4618:410,Testability,test,test,410,"At some point we will have to update our liquibase library, either for vulnerability patches or other bug fixes. . However there is a report that the current liquibase version (3.6.3) plus the way our changelog differentiates databases (ex: using strings matching for things like `mysql`) causes an issue with MariaDB. This ticket is not about updating liquibase. Before that can occur we need a CI regression test to ensure that MariaDB is supported. A/C:; - At least centaur-local running against mariadb 10.3+. Links:; - https://github.com/broadinstitute/cromwell/issues/4605; - https://www.sourceclear.com/vulnerability-database/security/cross-site-scripting-xss-/java/sid-6098; - https://liquibase.jira.com/browse/CORE-3203; - https://liquibase.jira.com/browse/CORE-3263 (may not be related) ; - https://docs.travis-ci.com/user/database-setup/#mariadb; - https://docs.travis-ci.com/user/build-matrix/#explicitly-including-jobs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4618
https://github.com/broadinstitute/cromwell/issues/4621:41,Security,validat,validate,41,See example of a bad WDL which shouldn't validate (since corrected in the WDL so it has to be this hash...) in [ENCODE](https://github.com/ENCODE-DCC/chip-seq-pipeline2/blob/a5cfe39835f9322f770a19f1ff1b777e6111cdb6/chip.wdl#L929-L938),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4621
https://github.com/broadinstitute/cromwell/issues/4621:99,Security,hash,hash,99,See example of a bad WDL which shouldn't validate (since corrected in the WDL so it has to be this hash...) in [ENCODE](https://github.com/ENCODE-DCC/chip-seq-pipeline2/blob/a5cfe39835f9322f770a19f1ff1b777e6111cdb6/chip.wdl#L929-L938),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4621
https://github.com/broadinstitute/cromwell/issues/4622:28,Performance,cache,cache,28,"It appears that when a call cache hits on PAPIv1, the `stdout` and `stderr` links in the metadata are incorrect.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4622
https://github.com/broadinstitute/cromwell/pull/4624:2,Modifiability,Refactor,Refactored,2,- Refactored more `Future` to `IO`.; - Hard-coded pretty-printed json now returns correct content-type.; - Check content-type of responses in tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4624
https://github.com/broadinstitute/cromwell/pull/4624:142,Testability,test,tests,142,- Refactored more `Future` to `IO`.; - Hard-coded pretty-printed json now returns correct content-type.; - Check content-type of responses in tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4624
https://github.com/broadinstitute/cromwell/issues/4625:415,Availability,echo,echo,415,"(First, I've registered at; https://gatkforums.broadinstitute.org/wdl/categories/wdl#; but I cannot figure out how to post a question there. If you help me with that, I would be happy to re-ask in the ""user forum"" there, but so far I do not see how.). My question is on `write_json` versus `write_lines`. The following works fine (using Cromwell-36):; ```; task foo {; input {; Array[File] results; }; command <<<; echo ~{sep=',' results}; files_fn=""~{write_lines(results)}""; echo ${files_fn}; >>>; }; ```; But this fails:; ```; task foo {; input {; Array[File] results; }; command <<<; echo ~{sep=',' results}; files_fn=""~{write_json(results)}""; echo ${files_fn}; >>>; }; ```; It says `write_json` needs an ""Object"". Am I doing something wrong?; ```; Failed to process task definition 'foo' (reason 1 of 1): Failed to process expression 'write_js; on(results)' (reason 1 of 1): Invalid parameter 'IdentifierLookup(results)'. Expected 'Object' but got 'Array[File]'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4625
https://github.com/broadinstitute/cromwell/issues/4625:476,Availability,echo,echo,476,"(First, I've registered at; https://gatkforums.broadinstitute.org/wdl/categories/wdl#; but I cannot figure out how to post a question there. If you help me with that, I would be happy to re-ask in the ""user forum"" there, but so far I do not see how.). My question is on `write_json` versus `write_lines`. The following works fine (using Cromwell-36):; ```; task foo {; input {; Array[File] results; }; command <<<; echo ~{sep=',' results}; files_fn=""~{write_lines(results)}""; echo ${files_fn}; >>>; }; ```; But this fails:; ```; task foo {; input {; Array[File] results; }; command <<<; echo ~{sep=',' results}; files_fn=""~{write_json(results)}""; echo ${files_fn}; >>>; }; ```; It says `write_json` needs an ""Object"". Am I doing something wrong?; ```; Failed to process task definition 'foo' (reason 1 of 1): Failed to process expression 'write_js; on(results)' (reason 1 of 1): Invalid parameter 'IdentifierLookup(results)'. Expected 'Object' but got 'Array[File]'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4625
https://github.com/broadinstitute/cromwell/issues/4625:587,Availability,echo,echo,587,"(First, I've registered at; https://gatkforums.broadinstitute.org/wdl/categories/wdl#; but I cannot figure out how to post a question there. If you help me with that, I would be happy to re-ask in the ""user forum"" there, but so far I do not see how.). My question is on `write_json` versus `write_lines`. The following works fine (using Cromwell-36):; ```; task foo {; input {; Array[File] results; }; command <<<; echo ~{sep=',' results}; files_fn=""~{write_lines(results)}""; echo ${files_fn}; >>>; }; ```; But this fails:; ```; task foo {; input {; Array[File] results; }; command <<<; echo ~{sep=',' results}; files_fn=""~{write_json(results)}""; echo ${files_fn}; >>>; }; ```; It says `write_json` needs an ""Object"". Am I doing something wrong?; ```; Failed to process task definition 'foo' (reason 1 of 1): Failed to process expression 'write_js; on(results)' (reason 1 of 1): Invalid parameter 'IdentifierLookup(results)'. Expected 'Object' but got 'Array[File]'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4625
https://github.com/broadinstitute/cromwell/issues/4625:647,Availability,echo,echo,647,"(First, I've registered at; https://gatkforums.broadinstitute.org/wdl/categories/wdl#; but I cannot figure out how to post a question there. If you help me with that, I would be happy to re-ask in the ""user forum"" there, but so far I do not see how.). My question is on `write_json` versus `write_lines`. The following works fine (using Cromwell-36):; ```; task foo {; input {; Array[File] results; }; command <<<; echo ~{sep=',' results}; files_fn=""~{write_lines(results)}""; echo ${files_fn}; >>>; }; ```; But this fails:; ```; task foo {; input {; Array[File] results; }; command <<<; echo ~{sep=',' results}; files_fn=""~{write_json(results)}""; echo ${files_fn}; >>>; }; ```; It says `write_json` needs an ""Object"". Am I doing something wrong?; ```; Failed to process task definition 'foo' (reason 1 of 1): Failed to process expression 'write_js; on(results)' (reason 1 of 1): Invalid parameter 'IdentifierLookup(results)'. Expected 'Object' but got 'Array[File]'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4625
https://github.com/broadinstitute/cromwell/issues/4626:842,Availability,echo,echo,842,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. Running on a Local backend with `java -jar $CROMWELL_JAR run -i input.json wf.wdl`. <!-- Paste/Attach your workflow if possible: -->; ```; task hello {; String outfilename; String ? name. command {; echo ""Hello ${default='world' name}"" > ${outfilename}; }; output {; File out = ""${outfilename}""; }; }. workflow test1 {; String name. call hello {; input: outfilename=""${name}.txt"", name = ""${name}""; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Using default configuration. Output:; ```; [2020-02-11 10:13:03,33] [info] Running with database db.url = jdbc:hsqldb:mem:89c116e0-5bca-4467-aaff-ae492c2ebbaf;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:14,71] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [201",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:1819,Availability,heartbeat,heartbeat,1819,"}; }; output {; File out = ""${outfilename}""; }; }. workflow test1 {; String name. call hello {; input: outfilename=""${name}.txt"", name = ""${name}""; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Using default configuration. Output:; ```; [2020-02-11 10:13:03,33] [info] Running with database db.url = jdbc:hsqldb:mem:89c116e0-5bca-4467-aaff-ae492c2ebbaf;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:14,71] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13:16,20] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,23] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,25] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-02-11 10:13:16,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-02-11 10:13:16,33] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-02-11 10:13:17,45] [info] SingleWorkflowRunnerActor: Version 37; [2019-02-11 10:13:17,46] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-02-11 10:13:17,59] [info] Unspecifi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:1883,Availability,heartbeat,heartbeatInterval,1883,"}; }; output {; File out = ""${outfilename}""; }; }. workflow test1 {; String name. call hello {; input: outfilename=""${name}.txt"", name = ""${name}""; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Using default configuration. Output:; ```; [2020-02-11 10:13:03,33] [info] Running with database db.url = jdbc:hsqldb:mem:89c116e0-5bca-4467-aaff-ae492c2ebbaf;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:14,71] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13:16,20] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,23] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,25] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-02-11 10:13:16,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-02-11 10:13:16,33] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-02-11 10:13:17,45] [info] SingleWorkflowRunnerActor: Version 37; [2019-02-11 10:13:17,46] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-02-11 10:13:17,59] [info] Unspecifi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:10911,Availability,echo,echo,10911,"scala:81); at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:21,51] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Starting test1.hello; [2019-02-11 10:13:22,36] [info] Assigned new job execution tokens to the following groups: 52999e15: 1; [2019-02-11 10:13:22,66] [info] BackgroundConfigAsyncJobExecutionActor [52999e15test1.hello:NA:1]: echo ""Hello World"" > World.txt; [2019-02-11 10:13:22,75] [info] BackgroundConfigAsyncJobExecutionActor [52999e15test1.hello:NA:1]: executing: /usr/bin/env bash /spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/script; [2019-02-11 10:13:26,29] [info] BackgroundConfigAsyncJobExecutionActor [52999e15test1.hello:NA:1]: job id: 12910; [2019-02-11 10:13:26,30] [info] BackgroundConfigAsyncJobExecutionActor [52999e15test1.hello:NA:1]: Status change from - to Done; [2019-02-11 10:13:27,61] [info] Request method=GET uri=https://auth.docker.io/token?service=registry.docker.io&scope=repository%3Alibrary/ubuntu%3Apull headers= threw an exception on attempt #4. Giving up.; java.nio.channels.UnresolvedAddressException: null; at sun.nio.ch.Net.checkAddress(Net.java:101); at sun.nio.ch.UnixAsynchronousSocketChannelImpl.implConnect(UnixAsynchronousSocketChannelImpl.java:301); at sun.nio.ch.AsynchronousSocketChannelImpl.connect(Async",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:15261,Availability,down,down,15261,"ion settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2019-02-11 10:13:27,65] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Workflow test1 complete. Final Outputs:; {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; }; [2019-02-11 10:13:27,69] [info] WorkflowManagerActor WorkflowActor-52999e15-953f-44d6-aaae-1774c74d2910 is in a terminal state: WorkflowSucceededState; [2019-02-11 10:13:35,97] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; },; ""id"": ""52999e15-953f-44d6-aaae-1774c74d2910""; }; [2019-02-11 10:13:36,30] [info] Workflow polling stopped; [2019-02-11 10:13:36,32] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-11 10:13:36,33] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stoppe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:15349,Availability,down,down,15349,"ion settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2019-02-11 10:13:27,65] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Workflow test1 complete. Final Outputs:; {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; }; [2019-02-11 10:13:27,69] [info] WorkflowManagerActor WorkflowActor-52999e15-953f-44d6-aaae-1774c74d2910 is in a terminal state: WorkflowSucceededState; [2019-02-11 10:13:35,97] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; },; ""id"": ""52999e15-953f-44d6-aaae-1774c74d2910""; }; [2019-02-11 10:13:36,30] [info] Workflow polling stopped; [2019-02-11 10:13:36,32] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-11 10:13:36,33] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stoppe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:15565,Availability,down,down,15565,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:15792,Availability,down,down,15792,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:16037,Availability,down,down,16037,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:16084,Availability,down,down,16084,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:16178,Availability,down,down,16178,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:16382,Availability,down,down,16382,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:16474,Availability,down,down,16474,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:16567,Availability,down,down,16567,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:16655,Availability,down,down,16655,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:16755,Availability,down,down,16755,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:16961,Availability,down,down,16961,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:17108,Availability,down,down,17108,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:17247,Availability,down,down,17247,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:17396,Availability,down,down,17396,"0 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,81] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,84] [info] Database closed; [2019-02-11 10:13:36,84] [info] Stream materializer shut down; [2019-02-11 10:13:36,84] [info] WDL HTTP import resolver closed. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:17545,Availability,down,down,17545,"0 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,81] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,84] [info] Database closed; [2019-02-11 10:13:36,84] [info] Stream materializer shut down; [2019-02-11 10:13:36,84] [info] WDL HTTP import resolver closed. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:17759,Availability,down,down,17759,"0 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,81] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,84] [info] Database closed; [2019-02-11 10:13:36,84] [info] Stream materializer shut down; [2019-02-11 10:13:36,84] [info] WDL HTTP import resolver closed. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:1069,Deployability,configurat,configuration,1069,"e check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. Running on a Local backend with `java -jar $CROMWELL_JAR run -i input.json wf.wdl`. <!-- Paste/Attach your workflow if possible: -->; ```; task hello {; String outfilename; String ? name. command {; echo ""Hello ${default='world' name}"" > ${outfilename}; }; output {; File out = ""${outfilename}""; }; }. workflow test1 {; String name. call hello {; input: outfilename=""${name}.txt"", name = ""${name}""; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Using default configuration. Output:; ```; [2020-02-11 10:13:03,33] [info] Running with database db.url = jdbc:hsqldb:mem:89c116e0-5bca-4467-aaff-ae492c2ebbaf;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:14,71] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:1181,Deployability,configurat,configuration,1181,"dinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. Running on a Local backend with `java -jar $CROMWELL_JAR run -i input.json wf.wdl`. <!-- Paste/Attach your workflow if possible: -->; ```; task hello {; String outfilename; String ? name. command {; echo ""Hello ${default='world' name}"" > ${outfilename}; }; output {; File out = ""${outfilename}""; }; }. workflow test1 {; String name. call hello {; input: outfilename=""${name}.txt"", name = ""${name}""; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Using default configuration. Output:; ```; [2020-02-11 10:13:03,33] [info] Running with database db.url = jdbc:hsqldb:mem:89c116e0-5bca-4467-aaff-ae492c2ebbaf;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:14,71] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13:16,20] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 se",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:1829,Deployability,configurat,configuration,1829,"}; }; output {; File out = ""${outfilename}""; }; }. workflow test1 {; String name. call hello {; input: outfilename=""${name}.txt"", name = ""${name}""; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Using default configuration. Output:; ```; [2020-02-11 10:13:03,33] [info] Running with database db.url = jdbc:hsqldb:mem:89c116e0-5bca-4467-aaff-ae492c2ebbaf;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:14,71] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13:16,20] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,23] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,25] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-02-11 10:13:16,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-02-11 10:13:16,33] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-02-11 10:13:17,45] [info] SingleWorkflowRunnerActor: Version 37; [2019-02-11 10:13:17,46] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-02-11 10:13:17,59] [info] Unspecifi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:14272,Deployability,configurat,configuration,14272,"(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:27,63] [info] Message [cromwell.docker.DockerInfoActor$DockerInfoFailedResponse] from Actor[akka://cromwell-system/user/HealthMonitorDockerHashActor#-638598959] to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered, no more dead letters will be logged. If this is not an expected behavior, then [Actor[akka://cromwell-system/deadLetters]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2019-02-11 10:13:27,65] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Workflow test1 complete. Final Outputs:; {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; }; [2019-02-11 10:13:27,69] [info] WorkflowManagerActor WorkflowActor-52999e15-953f-44d6-aaae-1774c74d2910 is in a terminal state: WorkflowSucceededState; [2019-02-11 10:13:35,97] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; },; ""id"": ""52999e15-953f-44d6-aaae-1774c74d2910""; }; [2019-02-11 10:13:36,30] [info] Workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:13826,Integrability,Message,Message,13826,"ect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:27,63] [info] Message [cromwell.docker.DockerInfoActor$DockerInfoFailedResponse] from Actor[akka://cromwell-system/user/HealthMonitorDockerHashActor#-638598959] to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered, no more dead letters will be logged. If this is not an expected behavior, then [Actor[akka://cromwell-system/deadLetters]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2019-02-11 10:13:27,65] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Workflow test1 complete. Final Outputs:; {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; }; [2019-02-11 10:13:27,69] [info] WorkflowManagerActor WorkflowActor-52999e15-953f-44d6-aaae-1774c74d2910 is in a terminal state: WorkflowS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:16770,Integrability,message,messages,16770,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:16976,Integrability,message,messages,16976,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:17123,Integrability,message,messages,17123,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:1069,Modifiability,config,configuration,1069,"e check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. Running on a Local backend with `java -jar $CROMWELL_JAR run -i input.json wf.wdl`. <!-- Paste/Attach your workflow if possible: -->; ```; task hello {; String outfilename; String ? name. command {; echo ""Hello ${default='world' name}"" > ${outfilename}; }; output {; File out = ""${outfilename}""; }; }. workflow test1 {; String name. call hello {; input: outfilename=""${name}.txt"", name = ""${name}""; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Using default configuration. Output:; ```; [2020-02-11 10:13:03,33] [info] Running with database db.url = jdbc:hsqldb:mem:89c116e0-5bca-4467-aaff-ae492c2ebbaf;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:14,71] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:1181,Modifiability,config,configuration,1181,"dinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. Running on a Local backend with `java -jar $CROMWELL_JAR run -i input.json wf.wdl`. <!-- Paste/Attach your workflow if possible: -->; ```; task hello {; String outfilename; String ? name. command {; echo ""Hello ${default='world' name}"" > ${outfilename}; }; output {; File out = ""${outfilename}""; }; }. workflow test1 {; String name. call hello {; input: outfilename=""${name}.txt"", name = ""${name}""; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Using default configuration. Output:; ```; [2020-02-11 10:13:03,33] [info] Running with database db.url = jdbc:hsqldb:mem:89c116e0-5bca-4467-aaff-ae492c2ebbaf;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:14,71] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13:16,20] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 se",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:1829,Modifiability,config,configuration,1829,"}; }; output {; File out = ""${outfilename}""; }; }. workflow test1 {; String name. call hello {; input: outfilename=""${name}.txt"", name = ""${name}""; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Using default configuration. Output:; ```; [2020-02-11 10:13:03,33] [info] Running with database db.url = jdbc:hsqldb:mem:89c116e0-5bca-4467-aaff-ae492c2ebbaf;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:14,71] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13:16,20] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,23] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,25] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-02-11 10:13:16,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-02-11 10:13:16,33] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-02-11 10:13:17,45] [info] SingleWorkflowRunnerActor: Version 37; [2019-02-11 10:13:17,46] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-02-11 10:13:17,59] [info] Unspecifi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:2120,Modifiability,config,configured,2120,"S AND OTHER SENSITIVE MATERIAL: -->. Using default configuration. Output:; ```; [2020-02-11 10:13:03,33] [info] Running with database db.url = jdbc:hsqldb:mem:89c116e0-5bca-4467-aaff-ae492c2ebbaf;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:14,71] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13:16,20] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,23] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,25] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-02-11 10:13:16,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-02-11 10:13:16,33] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-02-11 10:13:17,45] [info] SingleWorkflowRunnerActor: Version 37; [2019-02-11 10:13:17,46] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-02-11 10:13:17,59] [info] Unspecified type (Unspecified version) workflow 52999e15-953f-44d6-aaae-1774c74d2910 submitted; [2019-02-11 10:13:17,65] [info] SingleWorkflowRunnerActor: Workflow submitted 52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,65] [info] ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:2240,Modifiability,config,configured,2240,"ng with database db.url = jdbc:hsqldb:mem:89c116e0-5bca-4467-aaff-ae492c2ebbaf;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:14,71] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13:16,20] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,23] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,25] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-02-11 10:13:16,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-02-11 10:13:16,33] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-02-11 10:13:17,45] [info] SingleWorkflowRunnerActor: Version 37; [2019-02-11 10:13:17,46] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-02-11 10:13:17,59] [info] Unspecified type (Unspecified version) workflow 52999e15-953f-44d6-aaae-1774c74d2910 submitted; [2019-02-11 10:13:17,65] [info] SingleWorkflowRunnerActor: Workflow submitted 52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,65] [info] 1 new workflows fetched; [2019-02-11 10:13:17,66] [info] WorkflowManagerActor Starting workflow 52999e15-953f-44d6-aa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:2535,Modifiability,config,configured,2535,"RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13:16,20] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,23] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,25] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-02-11 10:13:16,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-02-11 10:13:16,33] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-02-11 10:13:17,45] [info] SingleWorkflowRunnerActor: Version 37; [2019-02-11 10:13:17,46] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-02-11 10:13:17,59] [info] Unspecified type (Unspecified version) workflow 52999e15-953f-44d6-aaae-1774c74d2910 submitted; [2019-02-11 10:13:17,65] [info] SingleWorkflowRunnerActor: Workflow submitted 52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,65] [info] 1 new workflows fetched; [2019-02-11 10:13:17,66] [info] WorkflowManagerActor Starting workflow 52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,67] [info] WorkflowManagerActor Successfully started WorkflowActor-52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,67] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2019-02-11 10:13:17,70] [info] WorkflowStoreHeartbeatWriteActor c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:3538,Modifiability,config,configured,3538,"e 200 and process rate 5 seconds.; [2019-02-11 10:13:16,23] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,25] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-02-11 10:13:16,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-02-11 10:13:16,33] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-02-11 10:13:17,45] [info] SingleWorkflowRunnerActor: Version 37; [2019-02-11 10:13:17,46] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-02-11 10:13:17,59] [info] Unspecified type (Unspecified version) workflow 52999e15-953f-44d6-aaae-1774c74d2910 submitted; [2019-02-11 10:13:17,65] [info] SingleWorkflowRunnerActor: Workflow submitted 52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,65] [info] 1 new workflows fetched; [2019-02-11 10:13:17,66] [info] WorkflowManagerActor Starting workflow 52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,67] [info] WorkflowManagerActor Successfully started WorkflowActor-52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,67] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2019-02-11 10:13:17,70] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2019-02-11 10:13:17,91] [info] MaterializeWorkflowDescriptorActor [52999e15]: Parsing workflow as WDL draft-2; [2019-02-11 10:13:19,06] [info] MaterializeWorkflowDescriptorActor [52999e15]: Call-to-Backend assignments: test1.hello -> Local; [2019-02-11 10:13:19,22] [info] Request threw an exception on attempt #1. Retrying after 269 milliseconds; java.nio.channels.UnresolvedAddressException: null; at sun.nio.ch.Net.checkAddress(Net.java:101); at sun.nio.ch.UnixAsynchronousSocketChannelImpl.implConnect(UnixAsynchronousSocketChannelImpl.java",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:14272,Modifiability,config,configuration,14272,"(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:27,63] [info] Message [cromwell.docker.DockerInfoActor$DockerInfoFailedResponse] from Actor[akka://cromwell-system/user/HealthMonitorDockerHashActor#-638598959] to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered, no more dead letters will be logged. If this is not an expected behavior, then [Actor[akka://cromwell-system/deadLetters]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2019-02-11 10:13:27,65] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Workflow test1 complete. Final Outputs:; {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; }; [2019-02-11 10:13:27,69] [info] WorkflowManagerActor WorkflowActor-52999e15-953f-44d6-aaae-1774c74d2910 is in a terminal state: WorkflowSucceededState; [2019-02-11 10:13:35,97] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; },; ""id"": ""52999e15-953f-44d6-aaae-1774c74d2910""; }; [2019-02-11 10:13:36,30] [info] Workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:2443,Performance,throttle,throttle,2443,"ze of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13:16,20] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,23] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,25] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-02-11 10:13:16,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-02-11 10:13:16,33] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-02-11 10:13:17,45] [info] SingleWorkflowRunnerActor: Version 37; [2019-02-11 10:13:17,46] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-02-11 10:13:17,59] [info] Unspecified type (Unspecified version) workflow 52999e15-953f-44d6-aaae-1774c74d2910 submitted; [2019-02-11 10:13:17,65] [info] SingleWorkflowRunnerActor: Workflow submitted 52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,65] [info] 1 new workflows fetched; [2019-02-11 10:13:17,66] [info] WorkflowManagerActor Starting workflow 52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,67] [info] WorkflowManagerActor Successfully started WorkflowActor-52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,67] [info] Retrieved 1 workflows from the Workflo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:5375,Performance,concurren,concurrent,5375,"Support.buildPipeline(Http1Support.scala:53); at org.http4s.client.blaze.Http1Support.$anonfun$makeClient$1(Http1Support.scala:45); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:87); at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:372); at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:312); at cats.effect.internals.Callback$AsyncIdempotentCallback.run(Callback.scala:127); at cats.effect.internals.Trampoline.cats$effect$internals$Trampoline$$immediateLoop(Trampoline.scala:70); at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:19,49] [info] Request threw an exception on attempt #2. Retrying after 1364 milliseconds; java.nio.channels.UnresolvedAddressException: null; at sun.nio.ch.Net.checkAddress(Net.java:101); at sun.nio.ch.UnixAsynchronousSocketChannelImpl.implConnect(UnixAsynchronousSocke",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:5897,Performance,concurren,concurrent,5897,".run(Callback.scala:127); at cats.effect.internals.Trampoline.cats$effect$internals$Trampoline$$immediateLoop(Trampoline.scala:70); at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:19,49] [info] Request threw an exception on attempt #2. Retrying after 1364 milliseconds; java.nio.channels.UnresolvedAddressException: null; at sun.nio.ch.Net.checkAddress(Net.java:101); at sun.nio.ch.UnixAsynchronousSocketChannelImpl.implConnect(UnixAsynchronousSocketChannelImpl.java:301); at sun.nio.ch.AsynchronousSocketChannelImpl.connect(AsynchronousSocketChannelImpl.java:209); at org.http4s.blaze.channel.nio2.ClientChannelFactory.connect(ClientChannelFactory.scala:36); at org.http4s.client.blaze.Http1Support.buildPipeline(Http1Support.scala:53); at org.http4s.client.blaze.Http1Support.$anonfun$makeClient$1(Http1Support.scala:45); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:87); at cats.effect.internals.IORunLoop$RestartCallback.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:5981,Performance,concurren,concurrent,5981,"Trampoline$$immediateLoop(Trampoline.scala:70); at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:19,49] [info] Request threw an exception on attempt #2. Retrying after 1364 milliseconds; java.nio.channels.UnresolvedAddressException: null; at sun.nio.ch.Net.checkAddress(Net.java:101); at sun.nio.ch.UnixAsynchronousSocketChannelImpl.implConnect(UnixAsynchronousSocketChannelImpl.java:301); at sun.nio.ch.AsynchronousSocketChannelImpl.connect(AsynchronousSocketChannelImpl.java:209); at org.http4s.blaze.channel.nio2.ClientChannelFactory.connect(ClientChannelFactory.scala:36); at org.http4s.client.blaze.Http1Support.buildPipeline(Http1Support.scala:53); at org.http4s.client.blaze.Http1Support.$anonfun$makeClient$1(Http1Support.scala:45); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:87); at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); at cats.effect.internals.IORunLoop$RestartCallback.apply",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:7619,Performance,concurren,concurrent,7619,"Support.buildPipeline(Http1Support.scala:53); at org.http4s.client.blaze.Http1Support.$anonfun$makeClient$1(Http1Support.scala:45); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:87); at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:372); at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:312); at cats.effect.internals.Callback$AsyncIdempotentCallback.run(Callback.scala:127); at cats.effect.internals.Trampoline.cats$effect$internals$Trampoline$$immediateLoop(Trampoline.scala:70); at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:20,86] [info] Request threw an exception on attempt #3. Retrying after 6737 milliseconds; java.nio.channels.UnresolvedAddressException: null; at sun.nio.ch.Net.checkAddress(Net.java:101); at sun.nio.ch.UnixAsynchronousSocketChannelImpl.implConnect(UnixAsynchronousSocke",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:8141,Performance,concurren,concurrent,8141,".run(Callback.scala:127); at cats.effect.internals.Trampoline.cats$effect$internals$Trampoline$$immediateLoop(Trampoline.scala:70); at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:20,86] [info] Request threw an exception on attempt #3. Retrying after 6737 milliseconds; java.nio.channels.UnresolvedAddressException: null; at sun.nio.ch.Net.checkAddress(Net.java:101); at sun.nio.ch.UnixAsynchronousSocketChannelImpl.implConnect(UnixAsynchronousSocketChannelImpl.java:301); at sun.nio.ch.AsynchronousSocketChannelImpl.connect(AsynchronousSocketChannelImpl.java:209); at org.http4s.blaze.channel.nio2.ClientChannelFactory.connect(ClientChannelFactory.scala:36); at org.http4s.client.blaze.Http1Support.buildPipeline(Http1Support.scala:53); at org.http4s.client.blaze.Http1Support.$anonfun$makeClient$1(Http1Support.scala:45); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:87); at cats.effect.internals.IORunLoop$RestartCallback.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:8225,Performance,concurren,concurrent,8225,"Trampoline$$immediateLoop(Trampoline.scala:70); at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:20,86] [info] Request threw an exception on attempt #3. Retrying after 6737 milliseconds; java.nio.channels.UnresolvedAddressException: null; at sun.nio.ch.Net.checkAddress(Net.java:101); at sun.nio.ch.UnixAsynchronousSocketChannelImpl.implConnect(UnixAsynchronousSocketChannelImpl.java:301); at sun.nio.ch.AsynchronousSocketChannelImpl.connect(AsynchronousSocketChannelImpl.java:209); at org.http4s.blaze.channel.nio2.ClientChannelFactory.connect(ClientChannelFactory.scala:36); at org.http4s.client.blaze.Http1Support.buildPipeline(Http1Support.scala:53); at org.http4s.client.blaze.Http1Support.$anonfun$makeClient$1(Http1Support.scala:45); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:87); at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); at cats.effect.internals.IORunLoop$RestartCallback.apply",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:9863,Performance,concurren,concurrent,9863,"Support.buildPipeline(Http1Support.scala:53); at org.http4s.client.blaze.Http1Support.$anonfun$makeClient$1(Http1Support.scala:45); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:87); at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:372); at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:312); at cats.effect.internals.Callback$AsyncIdempotentCallback.run(Callback.scala:127); at cats.effect.internals.Trampoline.cats$effect$internals$Trampoline$$immediateLoop(Trampoline.scala:70); at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:21,51] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Starting test1.hello; [2019-02-11 10:13:22,36] [info] Assigned new job execution tokens to the following groups: 52999e15: 1; [2019-02-11 10:13:22,66] [info] BackgroundConfigAsyncJobE",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:10385,Performance,concurren,concurrent,10385,".run(Callback.scala:127); at cats.effect.internals.Trampoline.cats$effect$internals$Trampoline$$immediateLoop(Trampoline.scala:70); at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:21,51] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Starting test1.hello; [2019-02-11 10:13:22,36] [info] Assigned new job execution tokens to the following groups: 52999e15: 1; [2019-02-11 10:13:22,66] [info] BackgroundConfigAsyncJobExecutionActor [52999e15test1.hello:NA:1]: echo ""Hello World"" > World.txt; [2019-02-11 10:13:22,75] [info] BackgroundConfigAsyncJobExecutionActor [52999e15test1.hello:NA:1]: executing: /usr/bin/env bash /spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/script; [2019-02-11 10:13:26,29] [info] BackgroundConfigAsyncJobExecutionActor [52999e15test1.hello:NA:1]: job id: 12910; [2019-02-11 10:13:26,30] [info] BackgroundConfigAsyncJobExecutionActo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:10469,Performance,concurren,concurrent,10469,"Trampoline$$immediateLoop(Trampoline.scala:70); at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:21,51] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Starting test1.hello; [2019-02-11 10:13:22,36] [info] Assigned new job execution tokens to the following groups: 52999e15: 1; [2019-02-11 10:13:22,66] [info] BackgroundConfigAsyncJobExecutionActor [52999e15test1.hello:NA:1]: echo ""Hello World"" > World.txt; [2019-02-11 10:13:22,75] [info] BackgroundConfigAsyncJobExecutionActor [52999e15test1.hello:NA:1]: executing: /usr/bin/env bash /spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/script; [2019-02-11 10:13:26,29] [info] BackgroundConfigAsyncJobExecutionActor [52999e15test1.hello:NA:1]: job id: 12910; [2019-02-11 10:13:26,30] [info] BackgroundConfigAsyncJobExecutionActor [52999e15test1.hello:NA:1]: Status change from - to Done; [2019-02-11 10:13:27,61]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:13075,Performance,concurren,concurrent,13075,"Support.buildPipeline(Http1Support.scala:53); at org.http4s.client.blaze.Http1Support.$anonfun$makeClient$1(Http1Support.scala:45); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:87); at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:372); at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:312); at cats.effect.internals.Callback$AsyncIdempotentCallback.run(Callback.scala:127); at cats.effect.internals.Trampoline.cats$effect$internals$Trampoline$$immediateLoop(Trampoline.scala:70); at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:27,63] [info] Message [cromwell.docker.DockerInfoActor$DockerInfoFailedResponse] from Actor[akka://cromwell-system/user/HealthMonitorDockerHashActor#-638598959] to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered, no more dead l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:13597,Performance,concurren,concurrent,13597,".run(Callback.scala:127); at cats.effect.internals.Trampoline.cats$effect$internals$Trampoline$$immediateLoop(Trampoline.scala:70); at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:27,63] [info] Message [cromwell.docker.DockerInfoActor$DockerInfoFailedResponse] from Actor[akka://cromwell-system/user/HealthMonitorDockerHashActor#-638598959] to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered, no more dead letters will be logged. If this is not an expected behavior, then [Actor[akka://cromwell-system/deadLetters]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2019-02-11 10:13:27,65] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Workflow test1 complete. Final Outputs:; {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:13681,Performance,concurren,concurrent,13681,"Trampoline$$immediateLoop(Trampoline.scala:70); at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:27,63] [info] Message [cromwell.docker.DockerInfoActor$DockerInfoFailedResponse] from Actor[akka://cromwell-system/user/HealthMonitorDockerHashActor#-638598959] to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered, no more dead letters will be logged. If this is not an expected behavior, then [Actor[akka://cromwell-system/deadLetters]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2019-02-11 10:13:27,65] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Workflow test1 complete. Final Outputs:; {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; }; [2019-02-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:16763,Performance,queue,queued,16763,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:16969,Performance,queue,queued,16969,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:17116,Performance,queue,queued,17116,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:15287,Safety,Timeout,Timeout,15287,"ion settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2019-02-11 10:13:27,65] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Workflow test1 complete. Final Outputs:; {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; }; [2019-02-11 10:13:27,69] [info] WorkflowManagerActor WorkflowActor-52999e15-953f-44d6-aaae-1774c74d2910 is in a terminal state: WorkflowSucceededState; [2019-02-11 10:13:35,97] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; },; ""id"": ""52999e15-953f-44d6-aaae-1774c74d2910""; }; [2019-02-11 10:13:36,30] [info] Workflow polling stopped; [2019-02-11 10:13:36,32] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-11 10:13:36,33] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stoppe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:15378,Safety,Timeout,Timeout,15378,"ion settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2019-02-11 10:13:27,65] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Workflow test1 complete. Final Outputs:; {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; }; [2019-02-11 10:13:27,69] [info] WorkflowManagerActor WorkflowActor-52999e15-953f-44d6-aaae-1774c74d2910 is in a terminal state: WorkflowSucceededState; [2019-02-11 10:13:35,97] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; },; ""id"": ""52999e15-953f-44d6-aaae-1774c74d2910""; }; [2019-02-11 10:13:36,30] [info] Workflow polling stopped; [2019-02-11 10:13:36,32] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-11 10:13:36,33] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stoppe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:15431,Safety,Abort,Aborting,15431,"ion settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2019-02-11 10:13:27,65] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Workflow test1 complete. Final Outputs:; {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; }; [2019-02-11 10:13:27,69] [info] WorkflowManagerActor WorkflowActor-52999e15-953f-44d6-aaae-1774c74d2910 is in a terminal state: WorkflowSucceededState; [2019-02-11 10:13:35,97] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; },; ""id"": ""52999e15-953f-44d6-aaae-1774c74d2910""; }; [2019-02-11 10:13:36,30] [info] Workflow polling stopped; [2019-02-11 10:13:36,32] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-11 10:13:36,33] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stoppe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:15599,Safety,Timeout,Timeout,15599,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:15820,Safety,Timeout,Timeout,15820,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:16113,Safety,Timeout,Timeout,16113,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:16199,Safety,Timeout,Timeout,16199,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:16409,Safety,Timeout,Timeout,16409,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:16502,Safety,Timeout,Timeout,16502,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:16590,Safety,Timeout,Timeout,16590,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:16670,Safety,Timeout,Timeout,16670,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:1114,Security,PASSWORD,PASSWORDS,1114,"e check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. Running on a Local backend with `java -jar $CROMWELL_JAR run -i input.json wf.wdl`. <!-- Paste/Attach your workflow if possible: -->; ```; task hello {; String outfilename; String ? name. command {; echo ""Hello ${default='world' name}"" > ${outfilename}; }; output {; File out = ""${outfilename}""; }; }. workflow test1 {; String name. call hello {; input: outfilename=""${name}.txt"", name = ""${name}""; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Using default configuration. Output:; ```; [2020-02-11 10:13:03,33] [info] Running with database db.url = jdbc:hsqldb:mem:89c116e0-5bca-4467-aaff-ae492c2ebbaf;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:14,71] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:2349,Security,hash,hash-lookup,2349,"; [2019-02-11 10:13:14,71] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13:16,20] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,23] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,25] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-02-11 10:13:16,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-02-11 10:13:16,33] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-02-11 10:13:17,45] [info] SingleWorkflowRunnerActor: Version 37; [2019-02-11 10:13:17,46] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-02-11 10:13:17,59] [info] Unspecified type (Unspecified version) workflow 52999e15-953f-44d6-aaae-1774c74d2910 submitted; [2019-02-11 10:13:17,65] [info] SingleWorkflowRunnerActor: Workflow submitted 52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,65] [info] 1 new workflows fetched; [2019-02-11 10:13:17,66] [info] WorkflowManagerActor Starting workflow 52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,67] [info] WorkflowManagerActor Successfully started WorkflowActor-529",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:2427,Security,hash,hash-lookup,2427,"adata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13:16,20] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,23] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,25] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-02-11 10:13:16,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-02-11 10:13:16,33] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-02-11 10:13:17,45] [info] SingleWorkflowRunnerActor: Version 37; [2019-02-11 10:13:17,46] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-02-11 10:13:17,59] [info] Unspecified type (Unspecified version) workflow 52999e15-953f-44d6-aaae-1774c74d2910 submitted; [2019-02-11 10:13:17,65] [info] SingleWorkflowRunnerActor: Workflow submitted 52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,65] [info] 1 new workflows fetched; [2019-02-11 10:13:17,66] [info] WorkflowManagerActor Starting workflow 52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,67] [info] WorkflowManagerActor Successfully started WorkflowActor-52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,67] [info] Retrieved 1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:14096,Testability,log,logged,14096,"scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:27,63] [info] Message [cromwell.docker.DockerInfoActor$DockerInfoFailedResponse] from Actor[akka://cromwell-system/user/HealthMonitorDockerHashActor#-638598959] to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered, no more dead letters will be logged. If this is not an expected behavior, then [Actor[akka://cromwell-system/deadLetters]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2019-02-11 10:13:27,65] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Workflow test1 complete. Final Outputs:; {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; }; [2019-02-11 10:13:27,69] [info] WorkflowManagerActor WorkflowActor-52999e15-953f-44d6-aaae-1774c74d2910 is in a terminal state: WorkflowSucceededState; [2019-02-11 10:13:35,97] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:14229,Testability,log,logging,14229,"(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:27,63] [info] Message [cromwell.docker.DockerInfoActor$DockerInfoFailedResponse] from Actor[akka://cromwell-system/user/HealthMonitorDockerHashActor#-638598959] to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered, no more dead letters will be logged. If this is not an expected behavior, then [Actor[akka://cromwell-system/deadLetters]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2019-02-11 10:13:27,65] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Workflow test1 complete. Final Outputs:; {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; }; [2019-02-11 10:13:27,69] [info] WorkflowManagerActor WorkflowActor-52999e15-953f-44d6-aaae-1774c74d2910 is in a terminal state: WorkflowSucceededState; [2019-02-11 10:13:35,97] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; },; ""id"": ""52999e15-953f-44d6-aaae-1774c74d2910""; }; [2019-02-11 10:13:36,30] [info] Workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:14301,Testability,log,log-dead-letters,14301,"ls.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:27,63] [info] Message [cromwell.docker.DockerInfoActor$DockerInfoFailedResponse] from Actor[akka://cromwell-system/user/HealthMonitorDockerHashActor#-638598959] to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered, no more dead letters will be logged. If this is not an expected behavior, then [Actor[akka://cromwell-system/deadLetters]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2019-02-11 10:13:27,65] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Workflow test1 complete. Final Outputs:; {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; }; [2019-02-11 10:13:27,69] [info] WorkflowManagerActor WorkflowActor-52999e15-953f-44d6-aaae-1774c74d2910 is in a terminal state: WorkflowSucceededState; [2019-02-11 10:13:35,97] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; },; ""id"": ""52999e15-953f-44d6-aaae-1774c74d2910""; }; [2019-02-11 10:13:36,30] [info] Workflow polling stopped; [2019-02-11 10:13:36,32] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:14329,Testability,log,log-dead-letters-during-shutdown,14329,"ineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:27,63] [info] Message [cromwell.docker.DockerInfoActor$DockerInfoFailedResponse] from Actor[akka://cromwell-system/user/HealthMonitorDockerHashActor#-638598959] to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered, no more dead letters will be logged. If this is not an expected behavior, then [Actor[akka://cromwell-system/deadLetters]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2019-02-11 10:13:27,65] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Workflow test1 complete. Final Outputs:; {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; }; [2019-02-11 10:13:27,69] [info] WorkflowManagerActor WorkflowActor-52999e15-953f-44d6-aaae-1774c74d2910 is in a terminal state: WorkflowSucceededState; [2019-02-11 10:13:35,97] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; },; ""id"": ""52999e15-953f-44d6-aaae-1774c74d2910""; }; [2019-02-11 10:13:36,30] [info] Workflow polling stopped; [2019-02-11 10:13:36,32] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-11 10:13:36,33] [info] Shutti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/issues/4626:47,Usability,feedback,feedback,47,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. Running on a Local backend with `java -jar $CROMWELL_JAR run -i input.json wf.wdl`. <!-- Paste/Attach your workflow if possible: -->; ```; task hello {; String outfilename; String ? name. command {; echo ""Hello ${default='world' name}"" > ${outfilename}; }; output {; File out = ""${outfilename}""; }; }. workflow test1 {; String name. call hello {; input: outfilename=""${name}.txt"", name = ""${name}""; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Using default configuration. Output:; ```; [2020-02-11 10:13:03,33] [info] Running with database db.url = jdbc:hsqldb:mem:89c116e0-5bca-4467-aaff-ae492c2ebbaf;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:14,71] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [201",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626
https://github.com/broadinstitute/cromwell/pull/4632:129,Deployability,deploy,deployment,129,For discussion. The current default `StandardHealthMonitorServiceActor` checks both Docker and the Engine database. Depending on deployment the first check may be completely unnecessary or undesirable (see #4626) and the second check is only really useful if there are queries to Cromwell's status endpoint to read the status. If the basic idea is accepted there would need to be additional documentation announcing the change in default and how to restore the previous default behavior.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4632
https://github.com/broadinstitute/cromwell/pull/4632:116,Integrability,Depend,Depending,116,For discussion. The current default `StandardHealthMonitorServiceActor` checks both Docker and the Engine database. Depending on deployment the first check may be completely unnecessary or undesirable (see #4626) and the second check is only really useful if there are queries to Cromwell's status endpoint to read the status. If the basic idea is accepted there would need to be additional documentation announcing the change in default and how to restore the previous default behavior.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4632
https://github.com/broadinstitute/cromwell/pull/4635:684,Deployability,configurat,configuration,684,"Following on from the conversation at #4039, I've started a ""Getting started with Containers"" on the Cromwell docs. My thoughts are it might be a good place to put thoughts about containers, how they can be used and any caveats about them. This tutorial follows the [user goal](https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-330341279) (from #2177):; > As a **user with images in Singularity**, I want **Cromwell to support using Singularity images (either via Singularity Hub and the command line, or connecting via API)**, so that I can **use Singularity images and not have to duplicate them in Docker**. However, without any code changes, just purely through configuration. . I think it's worth touching on using these container technologies with job schedulers, so I've included the ConfigurationFile and the HPCIntro tutorials as prerequisites.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635
https://github.com/broadinstitute/cromwell/pull/4635:809,Deployability,Configurat,ConfigurationFile,809,"Following on from the conversation at #4039, I've started a ""Getting started with Containers"" on the Cromwell docs. My thoughts are it might be a good place to put thoughts about containers, how they can be used and any caveats about them. This tutorial follows the [user goal](https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-330341279) (from #2177):; > As a **user with images in Singularity**, I want **Cromwell to support using Singularity images (either via Singularity Hub and the command line, or connecting via API)**, so that I can **use Singularity images and not have to duplicate them in Docker**. However, without any code changes, just purely through configuration. . I think it's worth touching on using these container technologies with job schedulers, so I've included the ConfigurationFile and the HPCIntro tutorials as prerequisites.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635
https://github.com/broadinstitute/cromwell/pull/4635:776,Energy Efficiency,schedul,schedulers,776,"Following on from the conversation at #4039, I've started a ""Getting started with Containers"" on the Cromwell docs. My thoughts are it might be a good place to put thoughts about containers, how they can be used and any caveats about them. This tutorial follows the [user goal](https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-330341279) (from #2177):; > As a **user with images in Singularity**, I want **Cromwell to support using Singularity images (either via Singularity Hub and the command line, or connecting via API)**, so that I can **use Singularity images and not have to duplicate them in Docker**. However, without any code changes, just purely through configuration. . I think it's worth touching on using these container technologies with job schedulers, so I've included the ConfigurationFile and the HPCIntro tutorials as prerequisites.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635
https://github.com/broadinstitute/cromwell/pull/4635:684,Modifiability,config,configuration,684,"Following on from the conversation at #4039, I've started a ""Getting started with Containers"" on the Cromwell docs. My thoughts are it might be a good place to put thoughts about containers, how they can be used and any caveats about them. This tutorial follows the [user goal](https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-330341279) (from #2177):; > As a **user with images in Singularity**, I want **Cromwell to support using Singularity images (either via Singularity Hub and the command line, or connecting via API)**, so that I can **use Singularity images and not have to duplicate them in Docker**. However, without any code changes, just purely through configuration. . I think it's worth touching on using these container technologies with job schedulers, so I've included the ConfigurationFile and the HPCIntro tutorials as prerequisites.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635
https://github.com/broadinstitute/cromwell/pull/4635:809,Modifiability,Config,ConfigurationFile,809,"Following on from the conversation at #4039, I've started a ""Getting started with Containers"" on the Cromwell docs. My thoughts are it might be a good place to put thoughts about containers, how they can be used and any caveats about them. This tutorial follows the [user goal](https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-330341279) (from #2177):; > As a **user with images in Singularity**, I want **Cromwell to support using Singularity images (either via Singularity Hub and the command line, or connecting via API)**, so that I can **use Singularity images and not have to duplicate them in Docker**. However, without any code changes, just purely through configuration. . I think it's worth touching on using these container technologies with job schedulers, so I've included the ConfigurationFile and the HPCIntro tutorials as prerequisites.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635
https://github.com/broadinstitute/cromwell/issues/4638:177,Modifiability,config,config,177,"Some FireCloud users want to communicate to various resources in Google via Cromwell - the current use case being interacting with BigQuery. . Ideally this can be set both by a config and optionally overwritten by a workflow option. The minimum scopes that are needed should always be included, so the scopes provided should just add to that.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4638
https://github.com/broadinstitute/cromwell/pull/4639:66,Deployability,update,update,66,"This PR is:; - 90% examples; - 9% Cromwell code; - 1% WDL grammar update. Once merged, OpenWDL can merge as ""implemented"":; - [x] https://github.com/openwdl/wdl/pull/263; - [x] https://github.com/openwdl/wdl/pull/284",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4639
https://github.com/broadinstitute/cromwell/issues/4640:499,Availability,echo,echo,499,"from Aaron Kemp:. > Yes - I think you should. You would need to switch to using 'gcr.io/cloud-genomics-pipelines/io' instead of 'google/cloud-sdk:slim' and replace your existing script with something like:. ```; gsutil -h ""Content-Type: text/plain; charset=UTF-8"" -m rsync -r /google/logs gs://broad-pharma5-execution2/.../pipelines-logs 2> gsutil_output.txt; RC_GSUTIL=$?; ; if [ ""$RC_GSUTIL"" = ""1"" ]; then; grep ""Bucket is requester pays bucket but no user project provided."" gsutil_output.txt && echo ""Retrying with user project""; ; gsutil -u broad-pharma5-compute3 -h ""Content-Type: text/plain; charset=UTF-8"" -m rsync -r /google/logs gs://broad-pharma5-execution2/.../pipelines-logs;; fi; ```. since now `gsutil` will actually be running our magic retry script.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4640
https://github.com/broadinstitute/cromwell/issues/4640:103,Deployability,pipeline,pipelines,103,"from Aaron Kemp:. > Yes - I think you should. You would need to switch to using 'gcr.io/cloud-genomics-pipelines/io' instead of 'google/cloud-sdk:slim' and replace your existing script with something like:. ```; gsutil -h ""Content-Type: text/plain; charset=UTF-8"" -m rsync -r /google/logs gs://broad-pharma5-execution2/.../pipelines-logs 2> gsutil_output.txt; RC_GSUTIL=$?; ; if [ ""$RC_GSUTIL"" = ""1"" ]; then; grep ""Bucket is requester pays bucket but no user project provided."" gsutil_output.txt && echo ""Retrying with user project""; ; gsutil -u broad-pharma5-compute3 -h ""Content-Type: text/plain; charset=UTF-8"" -m rsync -r /google/logs gs://broad-pharma5-execution2/.../pipelines-logs;; fi; ```. since now `gsutil` will actually be running our magic retry script.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4640
https://github.com/broadinstitute/cromwell/issues/4640:323,Deployability,pipeline,pipelines-logs,323,"from Aaron Kemp:. > Yes - I think you should. You would need to switch to using 'gcr.io/cloud-genomics-pipelines/io' instead of 'google/cloud-sdk:slim' and replace your existing script with something like:. ```; gsutil -h ""Content-Type: text/plain; charset=UTF-8"" -m rsync -r /google/logs gs://broad-pharma5-execution2/.../pipelines-logs 2> gsutil_output.txt; RC_GSUTIL=$?; ; if [ ""$RC_GSUTIL"" = ""1"" ]; then; grep ""Bucket is requester pays bucket but no user project provided."" gsutil_output.txt && echo ""Retrying with user project""; ; gsutil -u broad-pharma5-compute3 -h ""Content-Type: text/plain; charset=UTF-8"" -m rsync -r /google/logs gs://broad-pharma5-execution2/.../pipelines-logs;; fi; ```. since now `gsutil` will actually be running our magic retry script.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4640
https://github.com/broadinstitute/cromwell/issues/4640:673,Deployability,pipeline,pipelines-logs,673,"from Aaron Kemp:. > Yes - I think you should. You would need to switch to using 'gcr.io/cloud-genomics-pipelines/io' instead of 'google/cloud-sdk:slim' and replace your existing script with something like:. ```; gsutil -h ""Content-Type: text/plain; charset=UTF-8"" -m rsync -r /google/logs gs://broad-pharma5-execution2/.../pipelines-logs 2> gsutil_output.txt; RC_GSUTIL=$?; ; if [ ""$RC_GSUTIL"" = ""1"" ]; then; grep ""Bucket is requester pays bucket but no user project provided."" gsutil_output.txt && echo ""Retrying with user project""; ; gsutil -u broad-pharma5-compute3 -h ""Content-Type: text/plain; charset=UTF-8"" -m rsync -r /google/logs gs://broad-pharma5-execution2/.../pipelines-logs;; fi; ```. since now `gsutil` will actually be running our magic retry script.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4640
https://github.com/broadinstitute/cromwell/issues/4640:284,Testability,log,logs,284,"from Aaron Kemp:. > Yes - I think you should. You would need to switch to using 'gcr.io/cloud-genomics-pipelines/io' instead of 'google/cloud-sdk:slim' and replace your existing script with something like:. ```; gsutil -h ""Content-Type: text/plain; charset=UTF-8"" -m rsync -r /google/logs gs://broad-pharma5-execution2/.../pipelines-logs 2> gsutil_output.txt; RC_GSUTIL=$?; ; if [ ""$RC_GSUTIL"" = ""1"" ]; then; grep ""Bucket is requester pays bucket but no user project provided."" gsutil_output.txt && echo ""Retrying with user project""; ; gsutil -u broad-pharma5-compute3 -h ""Content-Type: text/plain; charset=UTF-8"" -m rsync -r /google/logs gs://broad-pharma5-execution2/.../pipelines-logs;; fi; ```. since now `gsutil` will actually be running our magic retry script.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4640
https://github.com/broadinstitute/cromwell/issues/4640:333,Testability,log,logs,333,"from Aaron Kemp:. > Yes - I think you should. You would need to switch to using 'gcr.io/cloud-genomics-pipelines/io' instead of 'google/cloud-sdk:slim' and replace your existing script with something like:. ```; gsutil -h ""Content-Type: text/plain; charset=UTF-8"" -m rsync -r /google/logs gs://broad-pharma5-execution2/.../pipelines-logs 2> gsutil_output.txt; RC_GSUTIL=$?; ; if [ ""$RC_GSUTIL"" = ""1"" ]; then; grep ""Bucket is requester pays bucket but no user project provided."" gsutil_output.txt && echo ""Retrying with user project""; ; gsutil -u broad-pharma5-compute3 -h ""Content-Type: text/plain; charset=UTF-8"" -m rsync -r /google/logs gs://broad-pharma5-execution2/.../pipelines-logs;; fi; ```. since now `gsutil` will actually be running our magic retry script.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4640
https://github.com/broadinstitute/cromwell/issues/4640:634,Testability,log,logs,634,"from Aaron Kemp:. > Yes - I think you should. You would need to switch to using 'gcr.io/cloud-genomics-pipelines/io' instead of 'google/cloud-sdk:slim' and replace your existing script with something like:. ```; gsutil -h ""Content-Type: text/plain; charset=UTF-8"" -m rsync -r /google/logs gs://broad-pharma5-execution2/.../pipelines-logs 2> gsutil_output.txt; RC_GSUTIL=$?; ; if [ ""$RC_GSUTIL"" = ""1"" ]; then; grep ""Bucket is requester pays bucket but no user project provided."" gsutil_output.txt && echo ""Retrying with user project""; ; gsutil -u broad-pharma5-compute3 -h ""Content-Type: text/plain; charset=UTF-8"" -m rsync -r /google/logs gs://broad-pharma5-execution2/.../pipelines-logs;; fi; ```. since now `gsutil` will actually be running our magic retry script.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4640
https://github.com/broadinstitute/cromwell/issues/4640:683,Testability,log,logs,683,"from Aaron Kemp:. > Yes - I think you should. You would need to switch to using 'gcr.io/cloud-genomics-pipelines/io' instead of 'google/cloud-sdk:slim' and replace your existing script with something like:. ```; gsutil -h ""Content-Type: text/plain; charset=UTF-8"" -m rsync -r /google/logs gs://broad-pharma5-execution2/.../pipelines-logs 2> gsutil_output.txt; RC_GSUTIL=$?; ; if [ ""$RC_GSUTIL"" = ""1"" ]; then; grep ""Bucket is requester pays bucket but no user project provided."" gsutil_output.txt && echo ""Retrying with user project""; ; gsutil -u broad-pharma5-compute3 -h ""Content-Type: text/plain; charset=UTF-8"" -m rsync -r /google/logs gs://broad-pharma5-execution2/.../pipelines-logs;; fi; ```. since now `gsutil` will actually be running our magic retry script.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4640
https://github.com/broadinstitute/cromwell/issues/4641:26,Availability,error,errors,26,"Green team is seeing many errors detailed [here](https://partnerissuetracker.corp.google.com/issues/122571609):. The workaround is:. >... cromwell should migrate to pipelines-io, but in the meantime, if you want to make a quick fix for this you can do:. ` rm -f $HOME/.config/gcloud/gce`. > immediately before invoking gsutil inside your retry loop. This would save approximately 10% of the failures being seen in Green Team efforts per @tbl3rd and the bug",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4641
https://github.com/broadinstitute/cromwell/issues/4641:391,Availability,failure,failures,391,"Green team is seeing many errors detailed [here](https://partnerissuetracker.corp.google.com/issues/122571609):. The workaround is:. >... cromwell should migrate to pipelines-io, but in the meantime, if you want to make a quick fix for this you can do:. ` rm -f $HOME/.config/gcloud/gce`. > immediately before invoking gsutil inside your retry loop. This would save approximately 10% of the failures being seen in Green Team efforts per @tbl3rd and the bug",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4641
https://github.com/broadinstitute/cromwell/issues/4641:165,Deployability,pipeline,pipelines-io,165,"Green team is seeing many errors detailed [here](https://partnerissuetracker.corp.google.com/issues/122571609):. The workaround is:. >... cromwell should migrate to pipelines-io, but in the meantime, if you want to make a quick fix for this you can do:. ` rm -f $HOME/.config/gcloud/gce`. > immediately before invoking gsutil inside your retry loop. This would save approximately 10% of the failures being seen in Green Team efforts per @tbl3rd and the bug",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4641
https://github.com/broadinstitute/cromwell/issues/4641:0,Energy Efficiency,Green,Green,0,"Green team is seeing many errors detailed [here](https://partnerissuetracker.corp.google.com/issues/122571609):. The workaround is:. >... cromwell should migrate to pipelines-io, but in the meantime, if you want to make a quick fix for this you can do:. ` rm -f $HOME/.config/gcloud/gce`. > immediately before invoking gsutil inside your retry loop. This would save approximately 10% of the failures being seen in Green Team efforts per @tbl3rd and the bug",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4641
https://github.com/broadinstitute/cromwell/issues/4641:414,Energy Efficiency,Green,Green,414,"Green team is seeing many errors detailed [here](https://partnerissuetracker.corp.google.com/issues/122571609):. The workaround is:. >... cromwell should migrate to pipelines-io, but in the meantime, if you want to make a quick fix for this you can do:. ` rm -f $HOME/.config/gcloud/gce`. > immediately before invoking gsutil inside your retry loop. This would save approximately 10% of the failures being seen in Green Team efforts per @tbl3rd and the bug",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4641
https://github.com/broadinstitute/cromwell/issues/4641:269,Modifiability,config,config,269,"Green team is seeing many errors detailed [here](https://partnerissuetracker.corp.google.com/issues/122571609):. The workaround is:. >... cromwell should migrate to pipelines-io, but in the meantime, if you want to make a quick fix for this you can do:. ` rm -f $HOME/.config/gcloud/gce`. > immediately before invoking gsutil inside your retry loop. This would save approximately 10% of the failures being seen in Green Team efforts per @tbl3rd and the bug",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4641
https://github.com/broadinstitute/cromwell/pull/4644:12,Testability,log,logging,12,"* Makes the logging start after 5 rather than 10 seconds; * Logs at least once, even if just to say that there will be no further logging; * Runs in centaur too (so that we can see the logs, and the code gets exercised more)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4644
https://github.com/broadinstitute/cromwell/pull/4644:60,Testability,Log,Logs,60,"* Makes the logging start after 5 rather than 10 seconds; * Logs at least once, even if just to say that there will be no further logging; * Runs in centaur too (so that we can see the logs, and the code gets exercised more)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4644
https://github.com/broadinstitute/cromwell/pull/4644:130,Testability,log,logging,130,"* Makes the logging start after 5 rather than 10 seconds; * Logs at least once, even if just to say that there will be no further logging; * Runs in centaur too (so that we can see the logs, and the code gets exercised more)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4644
https://github.com/broadinstitute/cromwell/pull/4644:185,Testability,log,logs,185,"* Makes the logging start after 5 rather than 10 seconds; * Logs at least once, even if just to say that there will be no further logging; * Runs in centaur too (so that we can see the logs, and the code gets exercised more)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4644
https://github.com/broadinstitute/cromwell/pull/4645:183,Integrability,message,message,183,"A useful refactor of the summarizer FSM, and a log of where the summarizer has reached; - Don't be afraid to vote 👎 on the logging if you think it's going to be too noisy. Sample log message: `2019-02-13 12:52:19,609 cromwell-system-akka.dispatchers.service-dispatcher-97 INFO - Metadata summarizer has now reached: 52837`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4645
https://github.com/broadinstitute/cromwell/pull/4645:9,Modifiability,refactor,refactor,9,"A useful refactor of the summarizer FSM, and a log of where the summarizer has reached; - Don't be afraid to vote 👎 on the logging if you think it's going to be too noisy. Sample log message: `2019-02-13 12:52:19,609 cromwell-system-akka.dispatchers.service-dispatcher-97 INFO - Metadata summarizer has now reached: 52837`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4645
https://github.com/broadinstitute/cromwell/pull/4645:47,Testability,log,log,47,"A useful refactor of the summarizer FSM, and a log of where the summarizer has reached; - Don't be afraid to vote 👎 on the logging if you think it's going to be too noisy. Sample log message: `2019-02-13 12:52:19,609 cromwell-system-akka.dispatchers.service-dispatcher-97 INFO - Metadata summarizer has now reached: 52837`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4645
https://github.com/broadinstitute/cromwell/pull/4645:123,Testability,log,logging,123,"A useful refactor of the summarizer FSM, and a log of where the summarizer has reached; - Don't be afraid to vote 👎 on the logging if you think it's going to be too noisy. Sample log message: `2019-02-13 12:52:19,609 cromwell-system-akka.dispatchers.service-dispatcher-97 INFO - Metadata summarizer has now reached: 52837`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4645
https://github.com/broadinstitute/cromwell/pull/4645:179,Testability,log,log,179,"A useful refactor of the summarizer FSM, and a log of where the summarizer has reached; - Don't be afraid to vote 👎 on the logging if you think it's going to be too noisy. Sample log message: `2019-02-13 12:52:19,609 cromwell-system-akka.dispatchers.service-dispatcher-97 INFO - Metadata summarizer has now reached: 52837`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4645
https://github.com/broadinstitute/cromwell/issues/4648:422,Deployability,deploy,deployment,422,"In #4406, we added a cleanup routine that deletes unzipped imports when we're done with them. It would be nice to handle imports without touching the disk at all - see [this branch](https://github.com/broadinstitute/cromwell/tree/aen_4406_zipfs) for a mostly-working implementation. That implementation is 99% complete but seems to suffer from a [Scala bug](https://github.com/scala/bug/issues/10247) in certain packaging/deployment configurations - such as the one used in Travis!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4648
https://github.com/broadinstitute/cromwell/issues/4648:433,Deployability,configurat,configurations,433,"In #4406, we added a cleanup routine that deletes unzipped imports when we're done with them. It would be nice to handle imports without touching the disk at all - see [this branch](https://github.com/broadinstitute/cromwell/tree/aen_4406_zipfs) for a mostly-working implementation. That implementation is 99% complete but seems to suffer from a [Scala bug](https://github.com/scala/bug/issues/10247) in certain packaging/deployment configurations - such as the one used in Travis!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4648
https://github.com/broadinstitute/cromwell/issues/4648:29,Integrability,rout,routine,29,"In #4406, we added a cleanup routine that deletes unzipped imports when we're done with them. It would be nice to handle imports without touching the disk at all - see [this branch](https://github.com/broadinstitute/cromwell/tree/aen_4406_zipfs) for a mostly-working implementation. That implementation is 99% complete but seems to suffer from a [Scala bug](https://github.com/scala/bug/issues/10247) in certain packaging/deployment configurations - such as the one used in Travis!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4648
https://github.com/broadinstitute/cromwell/issues/4648:433,Modifiability,config,configurations,433,"In #4406, we added a cleanup routine that deletes unzipped imports when we're done with them. It would be nice to handle imports without touching the disk at all - see [this branch](https://github.com/broadinstitute/cromwell/tree/aen_4406_zipfs) for a mostly-working implementation. That implementation is 99% complete but seems to suffer from a [Scala bug](https://github.com/scala/bug/issues/10247) in certain packaging/deployment configurations - such as the one used in Travis!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4648
https://github.com/broadinstitute/cromwell/issues/4650:709,Deployability,configurat,configuration,709,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4650
https://github.com/broadinstitute/cromwell/issues/4650:709,Modifiability,config,configuration,709,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4650
https://github.com/broadinstitute/cromwell/issues/4650:754,Security,PASSWORD,PASSWORDS,754,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4650
https://github.com/broadinstitute/cromwell/issues/4650:47,Usability,feedback,feedback,47,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4650
https://github.com/broadinstitute/cromwell/issues/4651:1036,Availability,alive,alive,1036,"I think I am experiencing a bug in cromwell version 37. The problem occurs when I submit a job to SLURM. The job gets submitted but cromwell crashes without waiting for the job to finish. Cromwell works fine when run locally or when I use version 36. . ## my command ; java is v1.8; ```; java -Dconfig.file=cori.conf -jar cromwell-37.jar run test.wdl ; ```. ## wdl ; ```; workflow jgi_dap_leo {. call doSomething { }. }. task doSomething {; runtime {; mem: ""8G""; cpu: 1; time: ""0:60:0""; backend: ""SLURM""; }; command {; free; }; }; ```. ## config ; ```; include required(classpath(""application"")). system {; job-rate-control {; jobs = 1; per = 1 second; }; }. backend {; default=""Local""; providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String time; Int cpu; String mem; """""". submit = """"""; sbatch -J leo_dap -t ${time} -c ${cpu} --mem=${mem} -C haswell -q regular -A m342 --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. ## system I'm running on . NERSC's cori machines:; Cray XC40, comprised of Intel Xeon ""Haswell"" processor nodes. ## cromwell logs; [cromwellError.txt](https://github.com/broadinstitute/cromwell/files/2866540/cromwellError.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4651
https://github.com/broadinstitute/cromwell/issues/4651:969,Integrability,wrap,wrap,969,"I think I am experiencing a bug in cromwell version 37. The problem occurs when I submit a job to SLURM. The job gets submitted but cromwell crashes without waiting for the job to finish. Cromwell works fine when run locally or when I use version 36. . ## my command ; java is v1.8; ```; java -Dconfig.file=cori.conf -jar cromwell-37.jar run test.wdl ; ```. ## wdl ; ```; workflow jgi_dap_leo {. call doSomething { }. }. task doSomething {; runtime {; mem: ""8G""; cpu: 1; time: ""0:60:0""; backend: ""SLURM""; }; command {; free; }; }; ```. ## config ; ```; include required(classpath(""application"")). system {; job-rate-control {; jobs = 1; per = 1 second; }; }. backend {; default=""Local""; providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String time; Int cpu; String mem; """""". submit = """"""; sbatch -J leo_dap -t ${time} -c ${cpu} --mem=${mem} -C haswell -q regular -A m342 --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. ## system I'm running on . NERSC's cori machines:; Cray XC40, comprised of Intel Xeon ""Haswell"" processor nodes. ## cromwell logs; [cromwellError.txt](https://github.com/broadinstitute/cromwell/files/2866540/cromwellError.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4651
https://github.com/broadinstitute/cromwell/issues/4651:539,Modifiability,config,config,539,"I think I am experiencing a bug in cromwell version 37. The problem occurs when I submit a job to SLURM. The job gets submitted but cromwell crashes without waiting for the job to finish. Cromwell works fine when run locally or when I use version 36. . ## my command ; java is v1.8; ```; java -Dconfig.file=cori.conf -jar cromwell-37.jar run test.wdl ; ```. ## wdl ; ```; workflow jgi_dap_leo {. call doSomething { }. }. task doSomething {; runtime {; mem: ""8G""; cpu: 1; time: ""0:60:0""; backend: ""SLURM""; }; command {; free; }; }; ```. ## config ; ```; include required(classpath(""application"")). system {; job-rate-control {; jobs = 1; per = 1 second; }; }. backend {; default=""Local""; providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String time; Int cpu; String mem; """""". submit = """"""; sbatch -J leo_dap -t ${time} -c ${cpu} --mem=${mem} -C haswell -q regular -A m342 --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. ## system I'm running on . NERSC's cori machines:; Cray XC40, comprised of Intel Xeon ""Haswell"" processor nodes. ## cromwell logs; [cromwellError.txt](https://github.com/broadinstitute/cromwell/files/2866540/cromwellError.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4651
https://github.com/broadinstitute/cromwell/issues/4651:752,Modifiability,config,config,752,"I think I am experiencing a bug in cromwell version 37. The problem occurs when I submit a job to SLURM. The job gets submitted but cromwell crashes without waiting for the job to finish. Cromwell works fine when run locally or when I use version 36. . ## my command ; java is v1.8; ```; java -Dconfig.file=cori.conf -jar cromwell-37.jar run test.wdl ; ```. ## wdl ; ```; workflow jgi_dap_leo {. call doSomething { }. }. task doSomething {; runtime {; mem: ""8G""; cpu: 1; time: ""0:60:0""; backend: ""SLURM""; }; command {; free; }; }; ```. ## config ; ```; include required(classpath(""application"")). system {; job-rate-control {; jobs = 1; per = 1 second; }; }. backend {; default=""Local""; providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String time; Int cpu; String mem; """""". submit = """"""; sbatch -J leo_dap -t ${time} -c ${cpu} --mem=${mem} -C haswell -q regular -A m342 --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. ## system I'm running on . NERSC's cori machines:; Cray XC40, comprised of Intel Xeon ""Haswell"" processor nodes. ## cromwell logs; [cromwellError.txt](https://github.com/broadinstitute/cromwell/files/2866540/cromwellError.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4651
https://github.com/broadinstitute/cromwell/issues/4651:759,Modifiability,Config,ConfigBackendLifecycleActorFactory,759,"I think I am experiencing a bug in cromwell version 37. The problem occurs when I submit a job to SLURM. The job gets submitted but cromwell crashes without waiting for the job to finish. Cromwell works fine when run locally or when I use version 36. . ## my command ; java is v1.8; ```; java -Dconfig.file=cori.conf -jar cromwell-37.jar run test.wdl ; ```. ## wdl ; ```; workflow jgi_dap_leo {. call doSomething { }. }. task doSomething {; runtime {; mem: ""8G""; cpu: 1; time: ""0:60:0""; backend: ""SLURM""; }; command {; free; }; }; ```. ## config ; ```; include required(classpath(""application"")). system {; job-rate-control {; jobs = 1; per = 1 second; }; }. backend {; default=""Local""; providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String time; Int cpu; String mem; """""". submit = """"""; sbatch -J leo_dap -t ${time} -c ${cpu} --mem=${mem} -C haswell -q regular -A m342 --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. ## system I'm running on . NERSC's cori machines:; Cray XC40, comprised of Intel Xeon ""Haswell"" processor nodes. ## cromwell logs; [cromwellError.txt](https://github.com/broadinstitute/cromwell/files/2866540/cromwellError.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4651
https://github.com/broadinstitute/cromwell/issues/4651:796,Modifiability,config,config,796,"I think I am experiencing a bug in cromwell version 37. The problem occurs when I submit a job to SLURM. The job gets submitted but cromwell crashes without waiting for the job to finish. Cromwell works fine when run locally or when I use version 36. . ## my command ; java is v1.8; ```; java -Dconfig.file=cori.conf -jar cromwell-37.jar run test.wdl ; ```. ## wdl ; ```; workflow jgi_dap_leo {. call doSomething { }. }. task doSomething {; runtime {; mem: ""8G""; cpu: 1; time: ""0:60:0""; backend: ""SLURM""; }; command {; free; }; }; ```. ## config ; ```; include required(classpath(""application"")). system {; job-rate-control {; jobs = 1; per = 1 second; }; }. backend {; default=""Local""; providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String time; Int cpu; String mem; """""". submit = """"""; sbatch -J leo_dap -t ${time} -c ${cpu} --mem=${mem} -C haswell -q regular -A m342 --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. ## system I'm running on . NERSC's cori machines:; Cray XC40, comprised of Intel Xeon ""Haswell"" processor nodes. ## cromwell logs; [cromwellError.txt](https://github.com/broadinstitute/cromwell/files/2866540/cromwellError.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4651
https://github.com/broadinstitute/cromwell/issues/4651:342,Testability,test,test,342,"I think I am experiencing a bug in cromwell version 37. The problem occurs when I submit a job to SLURM. The job gets submitted but cromwell crashes without waiting for the job to finish. Cromwell works fine when run locally or when I use version 36. . ## my command ; java is v1.8; ```; java -Dconfig.file=cori.conf -jar cromwell-37.jar run test.wdl ; ```. ## wdl ; ```; workflow jgi_dap_leo {. call doSomething { }. }. task doSomething {; runtime {; mem: ""8G""; cpu: 1; time: ""0:60:0""; backend: ""SLURM""; }; command {; free; }; }; ```. ## config ; ```; include required(classpath(""application"")). system {; job-rate-control {; jobs = 1; per = 1 second; }; }. backend {; default=""Local""; providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String time; Int cpu; String mem; """""". submit = """"""; sbatch -J leo_dap -t ${time} -c ${cpu} --mem=${mem} -C haswell -q regular -A m342 --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. ## system I'm running on . NERSC's cori machines:; Cray XC40, comprised of Intel Xeon ""Haswell"" processor nodes. ## cromwell logs; [cromwellError.txt](https://github.com/broadinstitute/cromwell/files/2866540/cromwellError.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4651
https://github.com/broadinstitute/cromwell/issues/4651:1256,Testability,log,logs,1256,"I think I am experiencing a bug in cromwell version 37. The problem occurs when I submit a job to SLURM. The job gets submitted but cromwell crashes without waiting for the job to finish. Cromwell works fine when run locally or when I use version 36. . ## my command ; java is v1.8; ```; java -Dconfig.file=cori.conf -jar cromwell-37.jar run test.wdl ; ```. ## wdl ; ```; workflow jgi_dap_leo {. call doSomething { }. }. task doSomething {; runtime {; mem: ""8G""; cpu: 1; time: ""0:60:0""; backend: ""SLURM""; }; command {; free; }; }; ```. ## config ; ```; include required(classpath(""application"")). system {; job-rate-control {; jobs = 1; per = 1 second; }; }. backend {; default=""Local""; providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String time; Int cpu; String mem; """""". submit = """"""; sbatch -J leo_dap -t ${time} -c ${cpu} --mem=${mem} -C haswell -q regular -A m342 --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. ## system I'm running on . NERSC's cori machines:; Cray XC40, comprised of Intel Xeon ""Haswell"" processor nodes. ## cromwell logs; [cromwellError.txt](https://github.com/broadinstitute/cromwell/files/2866540/cromwellError.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4651
https://github.com/broadinstitute/cromwell/issues/4652:0,Deployability,Upgrade,Upgrade,0,"Upgrade `getWomBundle` from `Checked` to `IOChecked` and get rid of the `Await.result` in `HttpResolver`. Now, update `WomtoolServiceInCromwellActor` to get rid of the `Future` we create and return the `IO` directly (at least I think that's generally the right idea). This way we won't tie up threads waiting on HTTP import requests and Akka will work the way it's designed. Note that `validateNamespace` looks like it's async already because it's a `IOChecked[ValidatedWomNamespace]`, but it calls `getWomBundle` under the covers.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4652
https://github.com/broadinstitute/cromwell/issues/4652:111,Deployability,update,update,111,"Upgrade `getWomBundle` from `Checked` to `IOChecked` and get rid of the `Await.result` in `HttpResolver`. Now, update `WomtoolServiceInCromwellActor` to get rid of the `Future` we create and return the `IO` directly (at least I think that's generally the right idea). This way we won't tie up threads waiting on HTTP import requests and Akka will work the way it's designed. Note that `validateNamespace` looks like it's async already because it's a `IOChecked[ValidatedWomNamespace]`, but it calls `getWomBundle` under the covers.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4652
https://github.com/broadinstitute/cromwell/issues/4652:386,Security,validat,validateNamespace,386,"Upgrade `getWomBundle` from `Checked` to `IOChecked` and get rid of the `Await.result` in `HttpResolver`. Now, update `WomtoolServiceInCromwellActor` to get rid of the `Future` we create and return the `IO` directly (at least I think that's generally the right idea). This way we won't tie up threads waiting on HTTP import requests and Akka will work the way it's designed. Note that `validateNamespace` looks like it's async already because it's a `IOChecked[ValidatedWomNamespace]`, but it calls `getWomBundle` under the covers.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4652
https://github.com/broadinstitute/cromwell/issues/4652:461,Security,Validat,ValidatedWomNamespace,461,"Upgrade `getWomBundle` from `Checked` to `IOChecked` and get rid of the `Await.result` in `HttpResolver`. Now, update `WomtoolServiceInCromwellActor` to get rid of the `Future` we create and return the `IO` directly (at least I think that's generally the right idea). This way we won't tie up threads waiting on HTTP import requests and Akka will work the way it's designed. Note that `validateNamespace` looks like it's async already because it's a `IOChecked[ValidatedWomNamespace]`, but it calls `getWomBundle` under the covers.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4652
https://github.com/broadinstitute/cromwell/pull/4654:28,Availability,error,error,28,"To fix the ""unmatched case"" error in #4651 :. * Make states a strict ADT rather than string-based; * Hopefully made the `pollStatus` logic a little easier to follow. Closes #4651",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4654
https://github.com/broadinstitute/cromwell/pull/4654:133,Testability,log,logic,133,"To fix the ""unmatched case"" error in #4651 :. * Make states a strict ADT rather than string-based; * Hopefully made the `pollStatus` logic a little easier to follow. Closes #4651",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4654
https://github.com/broadinstitute/cromwell/pull/4658:50,Testability,test,test,50,"Also:; - Added ""(beta)"" label to AWS docs.; - Add test for mkdocs.; - Fix bad mkdocs links.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4658
https://github.com/broadinstitute/cromwell/issues/4659:12,Availability,error,errors,12,"Cromwell 37 errors when the backend submit configuration contains an expression like:; `${""-l h_vmem="" + memory + ""G""}`: ; <details>; <summary> error message </summary>; <pre><code>; cromwell.core.CromwellFatalException: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:144,Availability,error,error,144,"Cromwell 37 errors when the backend submit configuration contains an expression like:; `${""-l h_vmem="" + memory + ""G""}`: ; <details>; <summary> error message </summary>; <pre><code>; cromwell.core.CromwellFatalException: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:266,Availability,Error,Error,266,"Cromwell 37 errors when the backend submit configuration contains an expression like:; `${""-l h_vmem="" + memory + ""G""}`: ; <details>; <summary> error message </summary>; <pre><code>; cromwell.core.CromwellFatalException: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:582,Availability,recover,recoverWith,582,"Cromwell 37 errors when the backend submit configuration contains an expression like:; `${""-l h_vmem="" + memory + ""G""}`: ; <details>; <summary> error message </summary>; <pre><code>; cromwell.core.CromwellFatalException: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:1708,Availability,Error,Error,1708,"akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromw",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:4765,Availability,robust,robustExecuteOrRecover,4765,"figAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:637); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more. </code></pre>; </details>. This is a minimal example of a config which gets such an error:; `Could not evaluate expression: ""echo "" + memory: Cannot perform operation: echo + WomLong(4)`; ```; include required(classpath(""application"")); webservice {; port = 8000; }; backend {; default=""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int? memory; String? docker; String? docker_user; """"""; submit = """"""; bash ${script}; ${""echo "" + memory}; """"""; }; }; }; } ; ```. Th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:5301,Availability,error,error,5301,"uteAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more. </code></pre>; </details>. This is a minimal example of a config which gets such an error:; `Could not evaluate expression: ""echo "" + memory: Cannot perform operation: echo + WomLong(4)`; ```; include required(classpath(""application"")); webservice {; port = 8000; }; backend {; default=""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int? memory; String? docker; String? docker_user; """"""; submit = """"""; bash ${script}; ${""echo "" + memory}; """"""; }; }; }; } ; ```. This means that the launch command given in the cromwell docs [here](https://cromwell.readthedocs.io/en/stable/backends/SGE/) will not work. A current workaround would be to use an expression like this instead:; `${true=""echo"" false="""" defined(memory)} ${memory}`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:5342,Availability,echo,echo,5342,"uteAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more. </code></pre>; </details>. This is a minimal example of a config which gets such an error:; `Could not evaluate expression: ""echo "" + memory: Cannot perform operation: echo + WomLong(4)`; ```; include required(classpath(""application"")); webservice {; port = 8000; }; backend {; default=""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int? memory; String? docker; String? docker_user; """"""; submit = """"""; bash ${script}; ${""echo "" + memory}; """"""; }; }; }; } ; ```. This means that the launch command given in the cromwell docs [here](https://cromwell.readthedocs.io/en/stable/backends/SGE/) will not work. A current workaround would be to use an expression like this instead:; `${true=""echo"" false="""" defined(memory)} ${memory}`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:5385,Availability,echo,echo,5385,"uteAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more. </code></pre>; </details>. This is a minimal example of a config which gets such an error:; `Could not evaluate expression: ""echo "" + memory: Cannot perform operation: echo + WomLong(4)`; ```; include required(classpath(""application"")); webservice {; port = 8000; }; backend {; default=""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int? memory; String? docker; String? docker_user; """"""; submit = """"""; bash ${script}; ${""echo "" + memory}; """"""; }; }; }; } ; ```. This means that the launch command given in the cromwell docs [here](https://cromwell.readthedocs.io/en/stable/backends/SGE/) will not work. A current workaround would be to use an expression like this instead:; `${true=""echo"" false="""" defined(memory)} ${memory}`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:5745,Availability,echo,echo,5745,"uteAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more. </code></pre>; </details>. This is a minimal example of a config which gets such an error:; `Could not evaluate expression: ""echo "" + memory: Cannot perform operation: echo + WomLong(4)`; ```; include required(classpath(""application"")); webservice {; port = 8000; }; backend {; default=""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int? memory; String? docker; String? docker_user; """"""; submit = """"""; bash ${script}; ${""echo "" + memory}; """"""; }; }; }; } ; ```. This means that the launch command given in the cromwell docs [here](https://cromwell.readthedocs.io/en/stable/backends/SGE/) will not work. A current workaround would be to use an expression like this instead:; `${true=""echo"" false="""" defined(memory)} ${memory}`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:6007,Availability,echo,echo,6007,"uteAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more. </code></pre>; </details>. This is a minimal example of a config which gets such an error:; `Could not evaluate expression: ""echo "" + memory: Cannot perform operation: echo + WomLong(4)`; ```; include required(classpath(""application"")); webservice {; port = 8000; }; backend {; default=""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int? memory; String? docker; String? docker_user; """"""; submit = """"""; bash ${script}; ${""echo "" + memory}; """"""; }; }; }; } ; ```. This means that the launch command given in the cromwell docs [here](https://cromwell.readthedocs.io/en/stable/backends/SGE/) will not work. A current workaround would be to use an expression like this instead:; `${true=""echo"" false="""" defined(memory)} ${memory}`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:43,Deployability,configurat,configuration,43,"Cromwell 37 errors when the backend submit configuration contains an expression like:; `${""-l h_vmem="" + memory + ""G""}`: ; <details>; <summary> error message </summary>; <pre><code>; cromwell.core.CromwellFatalException: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:5161,Energy Efficiency,Schedul,Scheduler,5161,"uteAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more. </code></pre>; </details>. This is a minimal example of a config which gets such an error:; `Could not evaluate expression: ""echo "" + memory: Cannot perform operation: echo + WomLong(4)`; ```; include required(classpath(""application"")); webservice {; port = 8000; }; backend {; default=""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int? memory; String? docker; String? docker_user; """"""; submit = """"""; bash ${script}; ${""echo "" + memory}; """"""; }; }; }; } ; ```. This means that the launch command given in the cromwell docs [here](https://cromwell.readthedocs.io/en/stable/backends/SGE/) will not work. A current workaround would be to use an expression like this instead:; `${true=""echo"" false="""" defined(memory)} ${memory}`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:5183,Energy Efficiency,Schedul,Scheduler,5183,"uteAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more. </code></pre>; </details>. This is a minimal example of a config which gets such an error:; `Could not evaluate expression: ""echo "" + memory: Cannot perform operation: echo + WomLong(4)`; ```; include required(classpath(""application"")); webservice {; port = 8000; }; backend {; default=""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int? memory; String? docker; String? docker_user; """"""; submit = """"""; bash ${script}; ${""echo "" + memory}; """"""; }; }; }; } ; ```. This means that the launch command given in the cromwell docs [here](https://cromwell.readthedocs.io/en/stable/backends/SGE/) will not work. A current workaround would be to use an expression like this instead:; `${true=""echo"" false="""" defined(memory)} ${memory}`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:150,Integrability,message,message,150,"Cromwell 37 errors when the backend submit configuration contains an expression like:; `${""-l h_vmem="" + memory + ""G""}`: ; <details>; <summary> error message </summary>; <pre><code>; cromwell.core.CromwellFatalException: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:43,Modifiability,config,configuration,43,"Cromwell 37 errors when the backend submit configuration contains an expression like:; `${""-l h_vmem="" + memory + ""G""}`: ; <details>; <summary> error message </summary>; <pre><code>; cromwell.core.CromwellFatalException: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:2031,Modifiability,config,config,2031,"lockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:2038,Modifiability,Config,ConfigAsyncJobExecutionActor,2038,"lockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemA",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:2083,Modifiability,Config,ConfigAsyncJobExecutionActor,2083,"t akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunn",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:2153,Modifiability,config,config,2153,"r.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:2160,Modifiability,Config,ConfigAsyncJobExecutionActor,2160,"ka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:2206,Modifiability,Config,ConfigAsyncJobExecutionActor,2206,"tDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecuti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:2275,Modifiability,config,config,2275,"or$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.makeProcessRunner(ConfigAsyncJobExecutionActor.scala:211); ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:2337,Modifiability,Config,ConfigAsyncJobExecutionActor,2337,"at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.makeProcessRunner(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:2407,Modifiability,config,config,2407,"); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.makeProcessRunner(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$execute$2(SharedFileSystemAsyncJobExecutionActor.scala:145);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:2414,Modifiability,Config,ConfigAsyncJobExecutionActor,2414,"h.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.makeProcessRunner(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$execute$2(SharedFileSystemAsyncJobExecutionActor.scala:145); at scala.util.Eit",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:2455,Modifiability,Config,ConfigAsyncJobExecutionActor,2455,"unTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.makeProcessRunner(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$execute$2(SharedFileSystemAsyncJobExecutionActor.scala:145); at scala.util.Either.fold(Either.scala:188); at crom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:2524,Modifiability,config,config,2524,"inPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.makeProcessRunner(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$execute$2(SharedFileSystemAsyncJobExecutionActor.scala:145); at scala.util.Either.fold(Either.scala:188); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:2531,Modifiability,Config,ConfigAsyncJobExecutionActor,2531,"orkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.makeProcessRunner(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$execute$2(SharedFileSystemAsyncJobExecutionActor.scala:145); at scala.util.Either.fold(Either.scala:188); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:2573,Modifiability,Config,ConfigAsyncJobExecutionActor,2573,"atch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.makeProcessRunner(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$execute$2(SharedFileSystemAsyncJobExecutionActor.scala:145); at scala.util.Either.fold(Either.scala:188); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:144); at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:2642,Modifiability,config,config,2642,":107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.makeProcessRunner(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$execute$2(SharedFileSystemAsyncJobExecutionActor.scala:145); at scala.util.Either.fold(Either.scala:188); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:144); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.exec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:2711,Modifiability,Config,ConfigAsyncJobExecutionActor,2711,"s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.makeProcessRunner(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$execute$2(SharedFileSystemAsyncJobExecutionActor.scala:145); at scala.util.Either.fold(Either.scala:188); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:144); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:139); at cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:2781,Modifiability,config,config,2781,t perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.makeProcessRunner(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$execute$2(SharedFileSystemAsyncJobExecutionActor.scala:145); at scala.util.Either.fold(Either.scala:188); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:144); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:139); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:2839,Modifiability,Config,ConfigAsyncJobExecutionActor,2839,ion.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.makeProcessRunner(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$execute$2(SharedFileSystemAsyncJobExecutionActor.scala:145); at scala.util.Either.fold(Either.scala:188); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:144); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:139); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.st,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:3174,Modifiability,config,config,3174,cutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.makeProcessRunner(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$execute$2(SharedFileSystemAsyncJobExecutionActor.scala:145); at scala.util.Either.fold(Either.scala:188); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:144); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:139); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:637); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardA,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:3238,Modifiability,Config,ConfigAsyncJobExecutionActor,3238,); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.makeProcessRunner(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$execute$2(SharedFileSystemAsyncJobExecutionActor.scala:145); at scala.util.Either.fold(Either.scala:188); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:144); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:139); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:637); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:3731,Modifiability,config,config,3731,Actor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.makeProcessRunner(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$execute$2(SharedFileSystemAsyncJobExecutionActor.scala:145); at scala.util.Either.fold(Either.scala:188); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:144); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:139); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:637); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBack,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:3785,Modifiability,Config,ConfigAsyncJobExecutionActor,3785,hedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.makeProcessRunner(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$execute$2(SharedFileSystemAsyncJobExecutionActor.scala:145); at scala.util.Either.fold(Either.scala:188); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:144); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:139); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:637); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:4238,Modifiability,config,config,4238,figAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$execute$2(SharedFileSystemAsyncJobExecutionActor.scala:145); at scala.util.Either.fold(Either.scala:188); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:144); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:139); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:637); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more. </code></pre>; </details>,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:4297,Modifiability,Config,ConfigAsyncJobExecutionActor,4297,aredFileSystemAsyncJobExecutionActor.$anonfun$execute$2(SharedFileSystemAsyncJobExecutionActor.scala:145); at scala.util.Either.fold(Either.scala:188); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:144); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:139); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:637); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more. </code></pre>; </details>. This is a minimal example of a config which gets such an erro,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:4596,Modifiability,config,config,4596,"sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:139); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:637); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more. </code></pre>; </details>. This is a minimal example of a config which gets such an error:; `Could not evaluate expression: ""echo "" + memory: Cannot perform operation: echo + WomLong(4)`; ```; include required(classpath(""application"")); webservice {; port = 8000; }; backend {; default=""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLif",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:4659,Modifiability,Config,ConfigAsyncJobExecutionActor,4659,"emAsyncJobExecutionActor.scala:139); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:637); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more. </code></pre>; </details>. This is a minimal example of a config which gets such an error:; `Could not evaluate expression: ""echo "" + memory: Cannot perform operation: echo + WomLong(4)`; ```; include required(classpath(""application"")); webservice {; port = 8000; }; backend {; default=""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int? mem",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:5275,Modifiability,config,config,5275,"uteAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more. </code></pre>; </details>. This is a minimal example of a config which gets such an error:; `Could not evaluate expression: ""echo "" + memory: Cannot perform operation: echo + WomLong(4)`; ```; include required(classpath(""application"")); webservice {; port = 8000; }; backend {; default=""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int? memory; String? docker; String? docker_user; """"""; submit = """"""; bash ${script}; ${""echo "" + memory}; """"""; }; }; }; } ; ```. This means that the launch command given in the cromwell docs [here](https://cromwell.readthedocs.io/en/stable/backends/SGE/) will not work. A current workaround would be to use an expression like this instead:; `${true=""echo"" false="""" defined(memory)} ${memory}`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:5577,Modifiability,config,config,5577,"uteAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more. </code></pre>; </details>. This is a minimal example of a config which gets such an error:; `Could not evaluate expression: ""echo "" + memory: Cannot perform operation: echo + WomLong(4)`; ```; include required(classpath(""application"")); webservice {; port = 8000; }; backend {; default=""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int? memory; String? docker; String? docker_user; """"""; submit = """"""; bash ${script}; ${""echo "" + memory}; """"""; }; }; }; } ; ```. This means that the launch command given in the cromwell docs [here](https://cromwell.readthedocs.io/en/stable/backends/SGE/) will not work. A current workaround would be to use an expression like this instead:; `${true=""echo"" false="""" defined(memory)} ${memory}`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:5584,Modifiability,Config,ConfigBackendLifecycleActorFactory,5584,"uteAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more. </code></pre>; </details>. This is a minimal example of a config which gets such an error:; `Could not evaluate expression: ""echo "" + memory: Cannot perform operation: echo + WomLong(4)`; ```; include required(classpath(""application"")); webservice {; port = 8000; }; backend {; default=""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int? memory; String? docker; String? docker_user; """"""; submit = """"""; bash ${script}; ${""echo "" + memory}; """"""; }; }; }; } ; ```. This means that the launch command given in the cromwell docs [here](https://cromwell.readthedocs.io/en/stable/backends/SGE/) will not work. A current workaround would be to use an expression like this instead:; `${true=""echo"" false="""" defined(memory)} ${memory}`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:5621,Modifiability,config,config,5621,"uteAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more. </code></pre>; </details>. This is a minimal example of a config which gets such an error:; `Could not evaluate expression: ""echo "" + memory: Cannot perform operation: echo + WomLong(4)`; ```; include required(classpath(""application"")); webservice {; port = 8000; }; backend {; default=""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int? memory; String? docker; String? docker_user; """"""; submit = """"""; bash ${script}; ${""echo "" + memory}; """"""; }; }; }; } ; ```. This means that the launch command given in the cromwell docs [here](https://cromwell.readthedocs.io/en/stable/backends/SGE/) will not work. A current workaround would be to use an expression like this instead:; `${true=""echo"" false="""" defined(memory)} ${memory}`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:344,Performance,perform,perform,344,"Cromwell 37 errors when the backend submit configuration contains an expression like:; `${""-l h_vmem="" + memory + ""G""}`: ; <details>; <summary> error message </summary>; <pre><code>; cromwell.core.CromwellFatalException: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:555,Performance,concurren,concurrent,555,"Cromwell 37 errors when the backend submit configuration contains an expression like:; `${""-l h_vmem="" + memory + ""G""}`: ; <details>; <summary> error message </summary>; <pre><code>; cromwell.core.CromwellFatalException: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:624,Performance,concurren,concurrent,624,"Cromwell 37 errors when the backend submit configuration contains an expression like:; `${""-l h_vmem="" + memory + ""G""}`: ; <details>; <summary> error message </summary>; <pre><code>; cromwell.core.CromwellFatalException: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:701,Performance,concurren,concurrent,701,"Cromwell 37 errors when the backend submit configuration contains an expression like:; `${""-l h_vmem="" + memory + ""G""}`: ; <details>; <summary> error message </summary>; <pre><code>; cromwell.core.CromwellFatalException: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:1022,Performance,concurren,concurrent,1022," backend submit configuration contains an expression like:; `${""-l h_vmem="" + memory + ""G""}`: ; <details>; <summary> error message </summary>; <pre><code>; cromwell.core.CromwellFatalException: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:1786,Performance,perform,perform,1786,"akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromw",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:5366,Performance,perform,perform,5366,"uteAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more. </code></pre>; </details>. This is a minimal example of a config which gets such an error:; `Could not evaluate expression: ""echo "" + memory: Cannot perform operation: echo + WomLong(4)`; ```; include required(classpath(""application"")); webservice {; port = 8000; }; backend {; default=""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int? memory; String? docker; String? docker_user; """"""; submit = """"""; bash ${script}; ${""echo "" + memory}; """"""; }; }; }; } ; ```. This means that the launch command given in the cromwell docs [here](https://cromwell.readthedocs.io/en/stable/backends/SGE/) will not work. A current workaround would be to use an expression like this instead:; `${true=""echo"" false="""" defined(memory)} ${memory}`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:582,Safety,recover,recoverWith,582,"Cromwell 37 errors when the backend submit configuration contains an expression like:; `${""-l h_vmem="" + memory + ""G""}`: ; <details>; <summary> error message </summary>; <pre><code>; cromwell.core.CromwellFatalException: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:1840,Security,validat,validation,1840," at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigA",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:1851,Security,Validat,Validation,1851,"atchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionAc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:1862,Security,Validat,ValidationTry,1862,"atchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionAc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:1894,Security,Validat,Validation,1894,"tch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:1926,Security,validat,validation,1926,"a:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:1937,Security,Validat,Validation,1937,"time.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makePro",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:1948,Security,Validat,ValidationTry,1948,"time.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makePro",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/issues/4659:1980,Security,Validat,Validation,1980,"p.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystem",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659
https://github.com/broadinstitute/cromwell/pull/4660:49,Testability,test,tests,49,Adds an expectation before all `pollUntilStatus` tests that we see at least *some* progress within a minute,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4660
https://github.com/broadinstitute/cromwell/pull/4661:228,Availability,error,error,228,"In response to:; 1. the difficulty of debugging https://github.com/broadinstitute/cromwell/issues/4555 and https://github.com/broadinstitute/cromwell/issues/4512 due to excessive, unformatted output; 2. the lack of tests on the error message generation",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4661
https://github.com/broadinstitute/cromwell/pull/4661:234,Integrability,message,message,234,"In response to:; 1. the difficulty of debugging https://github.com/broadinstitute/cromwell/issues/4555 and https://github.com/broadinstitute/cromwell/issues/4512 due to excessive, unformatted output; 2. the lack of tests on the error message generation",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4661
https://github.com/broadinstitute/cromwell/pull/4661:215,Testability,test,tests,215,"In response to:; 1. the difficulty of debugging https://github.com/broadinstitute/cromwell/issues/4555 and https://github.com/broadinstitute/cromwell/issues/4512 due to excessive, unformatted output; 2. the lack of tests on the error message generation",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4661
https://github.com/broadinstitute/cromwell/issues/4663:175,Deployability,release,releases,175,"Cromwell cannot handle any output that is struct base. I enclose a workflow where it is very clear to see that even though both latest development versions (as well as latest releases) of Cromwell and Womtool validated and executed the workflow for some strange reason at runtime Cromwell consider that it is a Map and not a struct and crashes in the very end of execution; ```; QuantifiedRun quantified_run = {""run"": srr, ""folder"": quant_folder, ""quant"": quant, ""lib"": quant_lib}; ```; ![screenshot_2019-02-15 screenshot](https://user-images.githubusercontent.com/842436/52889800-4dade280-318a-11e9-87b9-8b364e3408dd.png); [crashes_at_runtime.zip](https://github.com/broadinstitute/cromwell/files/2871310/crashes_at_runtime.zip)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4663
https://github.com/broadinstitute/cromwell/issues/4663:209,Security,validat,validated,209,"Cromwell cannot handle any output that is struct base. I enclose a workflow where it is very clear to see that even though both latest development versions (as well as latest releases) of Cromwell and Womtool validated and executed the workflow for some strange reason at runtime Cromwell consider that it is a Map and not a struct and crashes in the very end of execution; ```; QuantifiedRun quantified_run = {""run"": srr, ""folder"": quant_folder, ""quant"": quant, ""lib"": quant_lib}; ```; ![screenshot_2019-02-15 screenshot](https://user-images.githubusercontent.com/842436/52889800-4dade280-318a-11e9-87b9-8b364e3408dd.png); [crashes_at_runtime.zip](https://github.com/broadinstitute/cromwell/files/2871310/crashes_at_runtime.zip)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4663
https://github.com/broadinstitute/cromwell/issues/4663:93,Usability,clear,clear,93,"Cromwell cannot handle any output that is struct base. I enclose a workflow where it is very clear to see that even though both latest development versions (as well as latest releases) of Cromwell and Womtool validated and executed the workflow for some strange reason at runtime Cromwell consider that it is a Map and not a struct and crashes in the very end of execution; ```; QuantifiedRun quantified_run = {""run"": srr, ""folder"": quant_folder, ""quant"": quant, ""lib"": quant_lib}; ```; ![screenshot_2019-02-15 screenshot](https://user-images.githubusercontent.com/842436/52889800-4dade280-318a-11e9-87b9-8b364e3408dd.png); [crashes_at_runtime.zip](https://github.com/broadinstitute/cromwell/files/2871310/crashes_at_runtime.zip)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4663
https://github.com/broadinstitute/cromwell/issues/4664:360,Availability,error,error,360,"When I was debugging my issue with maps ( https://github.com/broadinstitute/cromwell/issues/4663 ) I noticed (multiple times); ![screenshot_2019-02-15 screenshot](https://user-images.githubusercontent.com/842436/52890479-9e730a80-318d-11e9-9858-e6961d935d62.png); that even though all the tasks succeeded, the latest task (salmon) did not cache when a runtime error was thrown with workflow output type. I think as the task itelf succeeded it should be cached, so users will not loose several hours when only a small adjustment in the output type is needed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4664
https://github.com/broadinstitute/cromwell/issues/4664:339,Performance,cache,cache,339,"When I was debugging my issue with maps ( https://github.com/broadinstitute/cromwell/issues/4663 ) I noticed (multiple times); ![screenshot_2019-02-15 screenshot](https://user-images.githubusercontent.com/842436/52890479-9e730a80-318d-11e9-9858-e6961d935d62.png); that even though all the tasks succeeded, the latest task (salmon) did not cache when a runtime error was thrown with workflow output type. I think as the task itelf succeeded it should be cached, so users will not loose several hours when only a small adjustment in the output type is needed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4664
https://github.com/broadinstitute/cromwell/issues/4664:453,Performance,cache,cached,453,"When I was debugging my issue with maps ( https://github.com/broadinstitute/cromwell/issues/4663 ) I noticed (multiple times); ![screenshot_2019-02-15 screenshot](https://user-images.githubusercontent.com/842436/52890479-9e730a80-318d-11e9-9858-e6961d935d62.png); that even though all the tasks succeeded, the latest task (salmon) did not cache when a runtime error was thrown with workflow output type. I think as the task itelf succeeded it should be cached, so users will not loose several hours when only a small adjustment in the output type is needed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4664
https://github.com/broadinstitute/cromwell/issues/4665:22,Availability,error,error,22,"Womtool does not give error if the file does not exist, so; ```; java -jar ~/Soft/womtool-37.jar validate I_do_not_exist.wdl; ```; Returns me ; ```; /pipelines/sources/rna-seq/pipelines/quantification/I_do_not_exist.wdl; ```; so I think that everything is ok as no erorrs is given while in fact I have a typo in the filename somewhere",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4665
https://github.com/broadinstitute/cromwell/issues/4665:150,Deployability,pipeline,pipelines,150,"Womtool does not give error if the file does not exist, so; ```; java -jar ~/Soft/womtool-37.jar validate I_do_not_exist.wdl; ```; Returns me ; ```; /pipelines/sources/rna-seq/pipelines/quantification/I_do_not_exist.wdl; ```; so I think that everything is ok as no erorrs is given while in fact I have a typo in the filename somewhere",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4665
https://github.com/broadinstitute/cromwell/issues/4665:176,Deployability,pipeline,pipelines,176,"Womtool does not give error if the file does not exist, so; ```; java -jar ~/Soft/womtool-37.jar validate I_do_not_exist.wdl; ```; Returns me ; ```; /pipelines/sources/rna-seq/pipelines/quantification/I_do_not_exist.wdl; ```; so I think that everything is ok as no erorrs is given while in fact I have a typo in the filename somewhere",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4665
https://github.com/broadinstitute/cromwell/issues/4665:97,Security,validat,validate,97,"Womtool does not give error if the file does not exist, so; ```; java -jar ~/Soft/womtool-37.jar validate I_do_not_exist.wdl; ```; Returns me ; ```; /pipelines/sources/rna-seq/pipelines/quantification/I_do_not_exist.wdl; ```; so I think that everything is ok as no erorrs is given while in fact I have a typo in the filename somewhere",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4665
https://github.com/broadinstitute/cromwell/issues/4668:344,Availability,avail,available,344,"Hi,. I recently ran a workflow on our dev server which was running cromwell 37. Right around that time dev-ops reverted it to 36. I wanted to confirm that it ran on 37, but couldn't find anything in the metadata to indicate the version of cromwell that was running when the workflow executed. I think it would be great to have that information available. Thanks. FYI - here's the metadata for the workflow in question:; https://cromwell.gotc-dev.broadinstitute.org/api/workflows/v1/0ac19869-db74-45ba-a785-9e3eebe4103d/metadata",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4668
https://github.com/broadinstitute/cromwell/pull/4669:62,Testability,test,tests,62,Was failing 70-80% of the time only during the PapiV2 centaur tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4669
https://github.com/broadinstitute/cromwell/issues/4670:116,Availability,error,error,116,"Hi!. I am attempting to run a tool that requires a directory structure as an input. One issue is that the tool will error out if there are files in the directory that are not the expected type. In our infrastructure we occasionally will have `.md5` files next to the important files which leads to the error in running the tool. To avoid this issue when launching Cromwell workflows I am attempting to list the good files in the listing attribute of the Directory input data type. The files I list are being staged in the inputs folder for the step but they are not being staged in the input directory folder path. Cromwell uses the empty input directory folder path as input to the tool which causes it to fail. . Example.cwl; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""ls""]; arguments: [""$(inputs.dir)""]. requirements:; - class: DockerRequirement; dockerPull: ""ubuntu:xenial"". inputs:; dir:; type: Directory. outputs:; example_out:; type: stdout. stdout: output.txt; ```; Input.yaml:; ```; dir: ; class: Directory; listing:; - class: File; path: ./data/1.txt; - class: File; path: ./data/2.txt; ```. staged files:; ```; => find inputs/; inputs/; inputs/1465754395; inputs/1465754395/2.txt; inputs/1465754395/1.txt; inputs/-143808698; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d/.file; ```. And the generated Cromwell command; ```; [d14c14d1example.cwl:NA:1]: 'ls' '/cromwell-executions/example.cwl/d14c14d1-ce96-44fc-9315-d1c431011f83/call-example.cwl/inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d'; ```. Tested using Cromwell 35 and 37. Cwltool works as expected after adding a basename to the input directory in the yaml. . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4670
https://github.com/broadinstitute/cromwell/issues/4670:302,Availability,error,error,302,"Hi!. I am attempting to run a tool that requires a directory structure as an input. One issue is that the tool will error out if there are files in the directory that are not the expected type. In our infrastructure we occasionally will have `.md5` files next to the important files which leads to the error in running the tool. To avoid this issue when launching Cromwell workflows I am attempting to list the good files in the listing attribute of the Directory input data type. The files I list are being staged in the inputs folder for the step but they are not being staged in the input directory folder path. Cromwell uses the empty input directory folder path as input to the tool which causes it to fail. . Example.cwl; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""ls""]; arguments: [""$(inputs.dir)""]. requirements:; - class: DockerRequirement; dockerPull: ""ubuntu:xenial"". inputs:; dir:; type: Directory. outputs:; example_out:; type: stdout. stdout: output.txt; ```; Input.yaml:; ```; dir: ; class: Directory; listing:; - class: File; path: ./data/1.txt; - class: File; path: ./data/2.txt; ```. staged files:; ```; => find inputs/; inputs/; inputs/1465754395; inputs/1465754395/2.txt; inputs/1465754395/1.txt; inputs/-143808698; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d/.file; ```. And the generated Cromwell command; ```; [d14c14d1example.cwl:NA:1]: 'ls' '/cromwell-executions/example.cwl/d14c14d1-ce96-44fc-9315-d1c431011f83/call-example.cwl/inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d'; ```. Tested using Cromwell 35 and 37. Cwltool works as expected after adding a basename to the input directory in the yaml. . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4670
https://github.com/broadinstitute/cromwell/issues/4670:2459,Deployability,configurat,configuration,2459,"ey are not being staged in the input directory folder path. Cromwell uses the empty input directory folder path as input to the tool which causes it to fail. . Example.cwl; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""ls""]; arguments: [""$(inputs.dir)""]. requirements:; - class: DockerRequirement; dockerPull: ""ubuntu:xenial"". inputs:; dir:; type: Directory. outputs:; example_out:; type: stdout. stdout: output.txt; ```; Input.yaml:; ```; dir: ; class: Directory; listing:; - class: File; path: ./data/1.txt; - class: File; path: ./data/2.txt; ```. staged files:; ```; => find inputs/; inputs/; inputs/1465754395; inputs/1465754395/2.txt; inputs/1465754395/1.txt; inputs/-143808698; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d/.file; ```. And the generated Cromwell command; ```; [d14c14d1example.cwl:NA:1]: 'ls' '/cromwell-executions/example.cwl/d14c14d1-ce96-44fc-9315-d1c431011f83/call-example.cwl/inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d'; ```. Tested using Cromwell 35 and 37. Cwltool works as expected after adding a basename to the input directory in the yaml. . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4670
https://github.com/broadinstitute/cromwell/issues/4670:2459,Modifiability,config,configuration,2459,"ey are not being staged in the input directory folder path. Cromwell uses the empty input directory folder path as input to the tool which causes it to fail. . Example.cwl; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""ls""]; arguments: [""$(inputs.dir)""]. requirements:; - class: DockerRequirement; dockerPull: ""ubuntu:xenial"". inputs:; dir:; type: Directory. outputs:; example_out:; type: stdout. stdout: output.txt; ```; Input.yaml:; ```; dir: ; class: Directory; listing:; - class: File; path: ./data/1.txt; - class: File; path: ./data/2.txt; ```. staged files:; ```; => find inputs/; inputs/; inputs/1465754395; inputs/1465754395/2.txt; inputs/1465754395/1.txt; inputs/-143808698; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d/.file; ```. And the generated Cromwell command; ```; [d14c14d1example.cwl:NA:1]: 'ls' '/cromwell-executions/example.cwl/d14c14d1-ce96-44fc-9315-d1c431011f83/call-example.cwl/inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d'; ```. Tested using Cromwell 35 and 37. Cwltool works as expected after adding a basename to the input directory in the yaml. . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4670
https://github.com/broadinstitute/cromwell/issues/4670:332,Safety,avoid,avoid,332,"Hi!. I am attempting to run a tool that requires a directory structure as an input. One issue is that the tool will error out if there are files in the directory that are not the expected type. In our infrastructure we occasionally will have `.md5` files next to the important files which leads to the error in running the tool. To avoid this issue when launching Cromwell workflows I am attempting to list the good files in the listing attribute of the Directory input data type. The files I list are being staged in the inputs folder for the step but they are not being staged in the input directory folder path. Cromwell uses the empty input directory folder path as input to the tool which causes it to fail. . Example.cwl; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""ls""]; arguments: [""$(inputs.dir)""]. requirements:; - class: DockerRequirement; dockerPull: ""ubuntu:xenial"". inputs:; dir:; type: Directory. outputs:; example_out:; type: stdout. stdout: output.txt; ```; Input.yaml:; ```; dir: ; class: Directory; listing:; - class: File; path: ./data/1.txt; - class: File; path: ./data/2.txt; ```. staged files:; ```; => find inputs/; inputs/; inputs/1465754395; inputs/1465754395/2.txt; inputs/1465754395/1.txt; inputs/-143808698; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d/.file; ```. And the generated Cromwell command; ```; [d14c14d1example.cwl:NA:1]: 'ls' '/cromwell-executions/example.cwl/d14c14d1-ce96-44fc-9315-d1c431011f83/call-example.cwl/inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d'; ```. Tested using Cromwell 35 and 37. Cwltool works as expected after adding a basename to the input directory in the yaml. . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4670
https://github.com/broadinstitute/cromwell/issues/4670:2504,Security,PASSWORD,PASSWORDS,2504,"ey are not being staged in the input directory folder path. Cromwell uses the empty input directory folder path as input to the tool which causes it to fail. . Example.cwl; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""ls""]; arguments: [""$(inputs.dir)""]. requirements:; - class: DockerRequirement; dockerPull: ""ubuntu:xenial"". inputs:; dir:; type: Directory. outputs:; example_out:; type: stdout. stdout: output.txt; ```; Input.yaml:; ```; dir: ; class: Directory; listing:; - class: File; path: ./data/1.txt; - class: File; path: ./data/2.txt; ```. staged files:; ```; => find inputs/; inputs/; inputs/1465754395; inputs/1465754395/2.txt; inputs/1465754395/1.txt; inputs/-143808698; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d/.file; ```. And the generated Cromwell command; ```; [d14c14d1example.cwl:NA:1]: 'ls' '/cromwell-executions/example.cwl/d14c14d1-ce96-44fc-9315-d1c431011f83/call-example.cwl/inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d'; ```. Tested using Cromwell 35 and 37. Cwltool works as expected after adding a basename to the input directory in the yaml. . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4670
https://github.com/broadinstitute/cromwell/issues/4670:1629,Testability,Test,Tested,1629,"ey are not being staged in the input directory folder path. Cromwell uses the empty input directory folder path as input to the tool which causes it to fail. . Example.cwl; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""ls""]; arguments: [""$(inputs.dir)""]. requirements:; - class: DockerRequirement; dockerPull: ""ubuntu:xenial"". inputs:; dir:; type: Directory. outputs:; example_out:; type: stdout. stdout: output.txt; ```; Input.yaml:; ```; dir: ; class: Directory; listing:; - class: File; path: ./data/1.txt; - class: File; path: ./data/2.txt; ```. staged files:; ```; => find inputs/; inputs/; inputs/1465754395; inputs/1465754395/2.txt; inputs/1465754395/1.txt; inputs/-143808698; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d/.file; ```. And the generated Cromwell command; ```; [d14c14d1example.cwl:NA:1]: 'ls' '/cromwell-executions/example.cwl/d14c14d1-ce96-44fc-9315-d1c431011f83/call-example.cwl/inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d'; ```. Tested using Cromwell 35 and 37. Cwltool works as expected after adding a basename to the input directory in the yaml. . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4670
https://github.com/broadinstitute/cromwell/issues/4670:1797,Usability,feedback,feedback,1797,"ey are not being staged in the input directory folder path. Cromwell uses the empty input directory folder path as input to the tool which causes it to fail. . Example.cwl; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""ls""]; arguments: [""$(inputs.dir)""]. requirements:; - class: DockerRequirement; dockerPull: ""ubuntu:xenial"". inputs:; dir:; type: Directory. outputs:; example_out:; type: stdout. stdout: output.txt; ```; Input.yaml:; ```; dir: ; class: Directory; listing:; - class: File; path: ./data/1.txt; - class: File; path: ./data/2.txt; ```. staged files:; ```; => find inputs/; inputs/; inputs/1465754395; inputs/1465754395/2.txt; inputs/1465754395/1.txt; inputs/-143808698; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d/.file; ```. And the generated Cromwell command; ```; [d14c14d1example.cwl:NA:1]: 'ls' '/cromwell-executions/example.cwl/d14c14d1-ce96-44fc-9315-d1c431011f83/call-example.cwl/inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d'; ```. Tested using Cromwell 35 and 37. Cwltool works as expected after adding a basename to the input directory in the yaml. . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4670
https://github.com/broadinstitute/cromwell/pull/4671:28,Availability,error,error,28,"* Addresses the ""programmer error"" in JES API manager; * Makes the logging in these classes more usable and traceable; * Replaces strings like `""JES API polling worker""` with strings like `""PAPI request worker""`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4671
https://github.com/broadinstitute/cromwell/pull/4671:67,Testability,log,logging,67,"* Addresses the ""programmer error"" in JES API manager; * Makes the logging in these classes more usable and traceable; * Replaces strings like `""JES API polling worker""` with strings like `""PAPI request worker""`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4671
https://github.com/broadinstitute/cromwell/pull/4671:97,Usability,usab,usable,97,"* Addresses the ""programmer error"" in JES API manager; * Makes the logging in these classes more usable and traceable; * Replaces strings like `""JES API polling worker""` with strings like `""PAPI request worker""`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4671
https://github.com/broadinstitute/cromwell/issues/4673:349,Availability,down,download,349,"There is a fairly large need to support custom container engines in Cromwell, for those HPC systems that cannot run docker. This is discussed in the PR here: #4635. . Currently the only hook we have for custom container engines is the `submit-docker` field, which is a script that runs when a task is run that specifies a docker image. This lets us download the image from docker, and convert it to a custom format (probably the Singularity SIF format) before submitting a job to the queue that runs the image. However, in the case of a scatter job, this means downloading the same Docker image N times, converting it to SIF format N times, and using up N times as much storage as we would like. This issue is discussed in my comment [here](https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463890367), and a number of preceding comments. If we had another hook for the `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`, called `pull-docker` that was run each time a Docker image needs to be pulled, then we could resolve this issue. The hook could run each time a new image is encountered in a given workflow, but only once each time, so that a scatter job would result in only one call to the hook. We would then have to find some way for the image built in the `pull-docker` hook to be communicated to the `submit-docker` hook. The default value of `pull-docker` would be some kind of no-op, because this is not needed using Docker itself. However, it would be invaluable for custom engines.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4673
https://github.com/broadinstitute/cromwell/issues/4673:561,Availability,down,downloading,561,"There is a fairly large need to support custom container engines in Cromwell, for those HPC systems that cannot run docker. This is discussed in the PR here: #4635. . Currently the only hook we have for custom container engines is the `submit-docker` field, which is a script that runs when a task is run that specifies a docker image. This lets us download the image from docker, and convert it to a custom format (probably the Singularity SIF format) before submitting a job to the queue that runs the image. However, in the case of a scatter job, this means downloading the same Docker image N times, converting it to SIF format N times, and using up N times as much storage as we would like. This issue is discussed in my comment [here](https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463890367), and a number of preceding comments. If we had another hook for the `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`, called `pull-docker` that was run each time a Docker image needs to be pulled, then we could resolve this issue. The hook could run each time a new image is encountered in a given workflow, but only once each time, so that a scatter job would result in only one call to the hook. We would then have to find some way for the image built in the `pull-docker` hook to be communicated to the `submit-docker` hook. The default value of `pull-docker` would be some kind of no-op, because this is not needed using Docker itself. However, it would be invaluable for custom engines.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4673
https://github.com/broadinstitute/cromwell/issues/4673:913,Modifiability,config,config,913,"There is a fairly large need to support custom container engines in Cromwell, for those HPC systems that cannot run docker. This is discussed in the PR here: #4635. . Currently the only hook we have for custom container engines is the `submit-docker` field, which is a script that runs when a task is run that specifies a docker image. This lets us download the image from docker, and convert it to a custom format (probably the Singularity SIF format) before submitting a job to the queue that runs the image. However, in the case of a scatter job, this means downloading the same Docker image N times, converting it to SIF format N times, and using up N times as much storage as we would like. This issue is discussed in my comment [here](https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463890367), and a number of preceding comments. If we had another hook for the `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`, called `pull-docker` that was run each time a Docker image needs to be pulled, then we could resolve this issue. The hook could run each time a new image is encountered in a given workflow, but only once each time, so that a scatter job would result in only one call to the hook. We would then have to find some way for the image built in the `pull-docker` hook to be communicated to the `submit-docker` hook. The default value of `pull-docker` would be some kind of no-op, because this is not needed using Docker itself. However, it would be invaluable for custom engines.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4673
https://github.com/broadinstitute/cromwell/issues/4673:920,Modifiability,Config,ConfigBackendLifecycleActorFactory,920,"There is a fairly large need to support custom container engines in Cromwell, for those HPC systems that cannot run docker. This is discussed in the PR here: #4635. . Currently the only hook we have for custom container engines is the `submit-docker` field, which is a script that runs when a task is run that specifies a docker image. This lets us download the image from docker, and convert it to a custom format (probably the Singularity SIF format) before submitting a job to the queue that runs the image. However, in the case of a scatter job, this means downloading the same Docker image N times, converting it to SIF format N times, and using up N times as much storage as we would like. This issue is discussed in my comment [here](https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463890367), and a number of preceding comments. If we had another hook for the `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`, called `pull-docker` that was run each time a Docker image needs to be pulled, then we could resolve this issue. The hook could run each time a new image is encountered in a given workflow, but only once each time, so that a scatter job would result in only one call to the hook. We would then have to find some way for the image built in the `pull-docker` hook to be communicated to the `submit-docker` hook. The default value of `pull-docker` would be some kind of no-op, because this is not needed using Docker itself. However, it would be invaluable for custom engines.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4673
https://github.com/broadinstitute/cromwell/issues/4673:484,Performance,queue,queue,484,"There is a fairly large need to support custom container engines in Cromwell, for those HPC systems that cannot run docker. This is discussed in the PR here: #4635. . Currently the only hook we have for custom container engines is the `submit-docker` field, which is a script that runs when a task is run that specifies a docker image. This lets us download the image from docker, and convert it to a custom format (probably the Singularity SIF format) before submitting a job to the queue that runs the image. However, in the case of a scatter job, this means downloading the same Docker image N times, converting it to SIF format N times, and using up N times as much storage as we would like. This issue is discussed in my comment [here](https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463890367), and a number of preceding comments. If we had another hook for the `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`, called `pull-docker` that was run each time a Docker image needs to be pulled, then we could resolve this issue. The hook could run each time a new image is encountered in a given workflow, but only once each time, so that a scatter job would result in only one call to the hook. We would then have to find some way for the image built in the `pull-docker` hook to be communicated to the `submit-docker` hook. The default value of `pull-docker` would be some kind of no-op, because this is not needed using Docker itself. However, it would be invaluable for custom engines.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4673
https://github.com/broadinstitute/cromwell/issues/4674:1054,Availability,Error,Error,1054,"termittently fails and succeeds, I'm not sure why it sometimes does and sometimes doesn't work. When it fails, it's because the EC2 instance fails to initialize. The most promising section I can find from the EC2 logs shows the following:; ```; Traceback (most recent call last):; File ""/usr/lib64/python2.7/logging/__init__.py"", line 891, in emit; stream.write(fs % msg.encode(""UTF-8"")); UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 49: ordinal not in range(128); Logged from file util.py, line 476; Traceback (most recent call last):; File ""/usr/lib64/python2.7/logging/__init__.py"", line 891, in emit; stream.write(fs % msg.encode(""UTF-8"")); UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 71: ordinal not in range(128); Logged from file util.py, line 476; [ 72.975338] EXT4-fs (dm-3): mounted filesystem with ordered data mode. Opts: (null); Error occurred during build: Command 04InstallECSAdditions failed; ```; The line `EXT4-fs (dm-3): mounted filesystem with ordered data mode` is repeated about 100 times in the real logs (below), I've just abridged it here for clarity. Anyway, the main thing this tells us that it's failing during step 04 of the EC2 startup script, which does the following:; ```yaml; 04InstallECSAdditions:; command:; Fn::If:; - UseCromwell; - !Join ["" "", [""sh"", ""/opt/ecs-additions/ecs-additions-cromwell.sh""]]; - echo ""OK""; env:; PATH: ""/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin""; ```. My best guess as to what is happening, is [this blog post](http://www.codeandcompost.com/post/cfn,-utf8-and-two-days-i%E2%80%99ll-never-get-back), which suggests:; > Apparently cfn-init has a limit on the amount of output it can process from a command, and I was pushing that limit. > I suspect the reason for the UTF8 error is that the output was truncated between two bytes or something, and when the parser underneath cfn-init tried to parse it, it encountered what appeared to be an invalid UTF8 character. . So per",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4674
https://github.com/broadinstitute/cromwell/issues/4674:1553,Availability,echo,echo,1553,"code byte 0xe2 in position 49: ordinal not in range(128); Logged from file util.py, line 476; Traceback (most recent call last):; File ""/usr/lib64/python2.7/logging/__init__.py"", line 891, in emit; stream.write(fs % msg.encode(""UTF-8"")); UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 71: ordinal not in range(128); Logged from file util.py, line 476; [ 72.975338] EXT4-fs (dm-3): mounted filesystem with ordered data mode. Opts: (null); Error occurred during build: Command 04InstallECSAdditions failed; ```; The line `EXT4-fs (dm-3): mounted filesystem with ordered data mode` is repeated about 100 times in the real logs (below), I've just abridged it here for clarity. Anyway, the main thing this tells us that it's failing during step 04 of the EC2 startup script, which does the following:; ```yaml; 04InstallECSAdditions:; command:; Fn::If:; - UseCromwell; - !Join ["" "", [""sh"", ""/opt/ecs-additions/ecs-additions-cromwell.sh""]]; - echo ""OK""; env:; PATH: ""/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin""; ```. My best guess as to what is happening, is [this blog post](http://www.codeandcompost.com/post/cfn,-utf8-and-two-days-i%E2%80%99ll-never-get-back), which suggests:; > Apparently cfn-init has a limit on the amount of output it can process from a command, and I was pushing that limit. > I suspect the reason for the UTF8 error is that the output was truncated between two bytes or something, and when the parser underneath cfn-init tried to parse it, it encountered what appeared to be an invalid UTF8 character. . So perhaps the reason this issue is intermittent is because the length of the logs from this command are occasionally too long for the `cfn-init` script? Or this might be a red herring. To aid with debugging, here are some useful logs; * [ec2_log.txt](https://github.com/broadinstitute/cromwell/files/2887960/ec2_log.txt): This is the console output from the EC2 instance that failed. I've censored out some of the key data, just in cas",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4674
https://github.com/broadinstitute/cromwell/issues/4674:1964,Availability,error,error,1964,"le util.py, line 476; [ 72.975338] EXT4-fs (dm-3): mounted filesystem with ordered data mode. Opts: (null); Error occurred during build: Command 04InstallECSAdditions failed; ```; The line `EXT4-fs (dm-3): mounted filesystem with ordered data mode` is repeated about 100 times in the real logs (below), I've just abridged it here for clarity. Anyway, the main thing this tells us that it's failing during step 04 of the EC2 startup script, which does the following:; ```yaml; 04InstallECSAdditions:; command:; Fn::If:; - UseCromwell; - !Join ["" "", [""sh"", ""/opt/ecs-additions/ecs-additions-cromwell.sh""]]; - echo ""OK""; env:; PATH: ""/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin""; ```. My best guess as to what is happening, is [this blog post](http://www.codeandcompost.com/post/cfn,-utf8-and-two-days-i%E2%80%99ll-never-get-back), which suggests:; > Apparently cfn-init has a limit on the amount of output it can process from a command, and I was pushing that limit. > I suspect the reason for the UTF8 error is that the output was truncated between two bytes or something, and when the parser underneath cfn-init tried to parse it, it encountered what appeared to be an invalid UTF8 character. . So perhaps the reason this issue is intermittent is because the length of the logs from this command are occasionally too long for the `cfn-init` script? Or this might be a red herring. To aid with debugging, here are some useful logs; * [ec2_log.txt](https://github.com/broadinstitute/cromwell/files/2887960/ec2_log.txt): This is the console output from the EC2 instance that failed. I've censored out some of the key data, just in case any of it involves my own public key (probably unnecessary, but I doubt it's related); * [stack_description.json.txt](https://github.com/broadinstitute/cromwell/files/2887962/stack_description.json.txt): The output from `aws cloudformation describe-stacks` on the stack I spun up. This should give you the exact parameters I used when it last failed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4674
https://github.com/broadinstitute/cromwell/issues/4674:376,Testability,log,logs,376,"I have been attempting to spin up the CloudFormation stack provided [here](https://docs.opendata.aws/genomics-workflows/cromwell/cromwell-aws-batch/#tldr). This intermittently fails and succeeds, I'm not sure why it sometimes does and sometimes doesn't work. When it fails, it's because the EC2 instance fails to initialize. The most promising section I can find from the EC2 logs shows the following:; ```; Traceback (most recent call last):; File ""/usr/lib64/python2.7/logging/__init__.py"", line 891, in emit; stream.write(fs % msg.encode(""UTF-8"")); UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 49: ordinal not in range(128); Logged from file util.py, line 476; Traceback (most recent call last):; File ""/usr/lib64/python2.7/logging/__init__.py"", line 891, in emit; stream.write(fs % msg.encode(""UTF-8"")); UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 71: ordinal not in range(128); Logged from file util.py, line 476; [ 72.975338] EXT4-fs (dm-3): mounted filesystem with ordered data mode. Opts: (null); Error occurred during build: Command 04InstallECSAdditions failed; ```; The line `EXT4-fs (dm-3): mounted filesystem with ordered data mode` is repeated about 100 times in the real logs (below), I've just abridged it here for clarity. Anyway, the main thing this tells us that it's failing during step 04 of the EC2 startup script, which does the following:; ```yaml; 04InstallECSAdditions:; command:; Fn::If:; - UseCromwell; - !Join ["" "", [""sh"", ""/opt/ecs-additions/ecs-additions-cromwell.sh""]]; - echo ""OK""; env:; PATH: ""/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin""; ```. My best guess as to what is happening, is [this blog post](http://www.codeandcompost.com/post/cfn,-utf8-and-two-days-i%E2%80%99ll-never-get-back), which suggests:; > Apparently cfn-init has a limit on the amount of output it can process from a command, and I was pushing that limit. > I suspect the reason for the UTF8 error is that the output was truncate",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4674
https://github.com/broadinstitute/cromwell/issues/4674:471,Testability,log,logging,471,"I have been attempting to spin up the CloudFormation stack provided [here](https://docs.opendata.aws/genomics-workflows/cromwell/cromwell-aws-batch/#tldr). This intermittently fails and succeeds, I'm not sure why it sometimes does and sometimes doesn't work. When it fails, it's because the EC2 instance fails to initialize. The most promising section I can find from the EC2 logs shows the following:; ```; Traceback (most recent call last):; File ""/usr/lib64/python2.7/logging/__init__.py"", line 891, in emit; stream.write(fs % msg.encode(""UTF-8"")); UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 49: ordinal not in range(128); Logged from file util.py, line 476; Traceback (most recent call last):; File ""/usr/lib64/python2.7/logging/__init__.py"", line 891, in emit; stream.write(fs % msg.encode(""UTF-8"")); UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 71: ordinal not in range(128); Logged from file util.py, line 476; [ 72.975338] EXT4-fs (dm-3): mounted filesystem with ordered data mode. Opts: (null); Error occurred during build: Command 04InstallECSAdditions failed; ```; The line `EXT4-fs (dm-3): mounted filesystem with ordered data mode` is repeated about 100 times in the real logs (below), I've just abridged it here for clarity. Anyway, the main thing this tells us that it's failing during step 04 of the EC2 startup script, which does the following:; ```yaml; 04InstallECSAdditions:; command:; Fn::If:; - UseCromwell; - !Join ["" "", [""sh"", ""/opt/ecs-additions/ecs-additions-cromwell.sh""]]; - echo ""OK""; env:; PATH: ""/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin""; ```. My best guess as to what is happening, is [this blog post](http://www.codeandcompost.com/post/cfn,-utf8-and-two-days-i%E2%80%99ll-never-get-back), which suggests:; > Apparently cfn-init has a limit on the amount of output it can process from a command, and I was pushing that limit. > I suspect the reason for the UTF8 error is that the output was truncate",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4674
https://github.com/broadinstitute/cromwell/issues/4674:652,Testability,Log,Logged,652,"I have been attempting to spin up the CloudFormation stack provided [here](https://docs.opendata.aws/genomics-workflows/cromwell/cromwell-aws-batch/#tldr). This intermittently fails and succeeds, I'm not sure why it sometimes does and sometimes doesn't work. When it fails, it's because the EC2 instance fails to initialize. The most promising section I can find from the EC2 logs shows the following:; ```; Traceback (most recent call last):; File ""/usr/lib64/python2.7/logging/__init__.py"", line 891, in emit; stream.write(fs % msg.encode(""UTF-8"")); UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 49: ordinal not in range(128); Logged from file util.py, line 476; Traceback (most recent call last):; File ""/usr/lib64/python2.7/logging/__init__.py"", line 891, in emit; stream.write(fs % msg.encode(""UTF-8"")); UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 71: ordinal not in range(128); Logged from file util.py, line 476; [ 72.975338] EXT4-fs (dm-3): mounted filesystem with ordered data mode. Opts: (null); Error occurred during build: Command 04InstallECSAdditions failed; ```; The line `EXT4-fs (dm-3): mounted filesystem with ordered data mode` is repeated about 100 times in the real logs (below), I've just abridged it here for clarity. Anyway, the main thing this tells us that it's failing during step 04 of the EC2 startup script, which does the following:; ```yaml; 04InstallECSAdditions:; command:; Fn::If:; - UseCromwell; - !Join ["" "", [""sh"", ""/opt/ecs-additions/ecs-additions-cromwell.sh""]]; - echo ""OK""; env:; PATH: ""/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin""; ```. My best guess as to what is happening, is [this blog post](http://www.codeandcompost.com/post/cfn,-utf8-and-two-days-i%E2%80%99ll-never-get-back), which suggests:; > Apparently cfn-init has a limit on the amount of output it can process from a command, and I was pushing that limit. > I suspect the reason for the UTF8 error is that the output was truncate",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4674
https://github.com/broadinstitute/cromwell/issues/4674:751,Testability,log,logging,751,"I have been attempting to spin up the CloudFormation stack provided [here](https://docs.opendata.aws/genomics-workflows/cromwell/cromwell-aws-batch/#tldr). This intermittently fails and succeeds, I'm not sure why it sometimes does and sometimes doesn't work. When it fails, it's because the EC2 instance fails to initialize. The most promising section I can find from the EC2 logs shows the following:; ```; Traceback (most recent call last):; File ""/usr/lib64/python2.7/logging/__init__.py"", line 891, in emit; stream.write(fs % msg.encode(""UTF-8"")); UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 49: ordinal not in range(128); Logged from file util.py, line 476; Traceback (most recent call last):; File ""/usr/lib64/python2.7/logging/__init__.py"", line 891, in emit; stream.write(fs % msg.encode(""UTF-8"")); UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 71: ordinal not in range(128); Logged from file util.py, line 476; [ 72.975338] EXT4-fs (dm-3): mounted filesystem with ordered data mode. Opts: (null); Error occurred during build: Command 04InstallECSAdditions failed; ```; The line `EXT4-fs (dm-3): mounted filesystem with ordered data mode` is repeated about 100 times in the real logs (below), I've just abridged it here for clarity. Anyway, the main thing this tells us that it's failing during step 04 of the EC2 startup script, which does the following:; ```yaml; 04InstallECSAdditions:; command:; Fn::If:; - UseCromwell; - !Join ["" "", [""sh"", ""/opt/ecs-additions/ecs-additions-cromwell.sh""]]; - echo ""OK""; env:; PATH: ""/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin""; ```. My best guess as to what is happening, is [this blog post](http://www.codeandcompost.com/post/cfn,-utf8-and-two-days-i%E2%80%99ll-never-get-back), which suggests:; > Apparently cfn-init has a limit on the amount of output it can process from a command, and I was pushing that limit. > I suspect the reason for the UTF8 error is that the output was truncate",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4674
https://github.com/broadinstitute/cromwell/issues/4674:932,Testability,Log,Logged,932,"I have been attempting to spin up the CloudFormation stack provided [here](https://docs.opendata.aws/genomics-workflows/cromwell/cromwell-aws-batch/#tldr). This intermittently fails and succeeds, I'm not sure why it sometimes does and sometimes doesn't work. When it fails, it's because the EC2 instance fails to initialize. The most promising section I can find from the EC2 logs shows the following:; ```; Traceback (most recent call last):; File ""/usr/lib64/python2.7/logging/__init__.py"", line 891, in emit; stream.write(fs % msg.encode(""UTF-8"")); UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 49: ordinal not in range(128); Logged from file util.py, line 476; Traceback (most recent call last):; File ""/usr/lib64/python2.7/logging/__init__.py"", line 891, in emit; stream.write(fs % msg.encode(""UTF-8"")); UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 71: ordinal not in range(128); Logged from file util.py, line 476; [ 72.975338] EXT4-fs (dm-3): mounted filesystem with ordered data mode. Opts: (null); Error occurred during build: Command 04InstallECSAdditions failed; ```; The line `EXT4-fs (dm-3): mounted filesystem with ordered data mode` is repeated about 100 times in the real logs (below), I've just abridged it here for clarity. Anyway, the main thing this tells us that it's failing during step 04 of the EC2 startup script, which does the following:; ```yaml; 04InstallECSAdditions:; command:; Fn::If:; - UseCromwell; - !Join ["" "", [""sh"", ""/opt/ecs-additions/ecs-additions-cromwell.sh""]]; - echo ""OK""; env:; PATH: ""/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin""; ```. My best guess as to what is happening, is [this blog post](http://www.codeandcompost.com/post/cfn,-utf8-and-two-days-i%E2%80%99ll-never-get-back), which suggests:; > Apparently cfn-init has a limit on the amount of output it can process from a command, and I was pushing that limit. > I suspect the reason for the UTF8 error is that the output was truncate",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4674
https://github.com/broadinstitute/cromwell/issues/4674:1235,Testability,log,logs,1235,"termittently fails and succeeds, I'm not sure why it sometimes does and sometimes doesn't work. When it fails, it's because the EC2 instance fails to initialize. The most promising section I can find from the EC2 logs shows the following:; ```; Traceback (most recent call last):; File ""/usr/lib64/python2.7/logging/__init__.py"", line 891, in emit; stream.write(fs % msg.encode(""UTF-8"")); UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 49: ordinal not in range(128); Logged from file util.py, line 476; Traceback (most recent call last):; File ""/usr/lib64/python2.7/logging/__init__.py"", line 891, in emit; stream.write(fs % msg.encode(""UTF-8"")); UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 71: ordinal not in range(128); Logged from file util.py, line 476; [ 72.975338] EXT4-fs (dm-3): mounted filesystem with ordered data mode. Opts: (null); Error occurred during build: Command 04InstallECSAdditions failed; ```; The line `EXT4-fs (dm-3): mounted filesystem with ordered data mode` is repeated about 100 times in the real logs (below), I've just abridged it here for clarity. Anyway, the main thing this tells us that it's failing during step 04 of the EC2 startup script, which does the following:; ```yaml; 04InstallECSAdditions:; command:; Fn::If:; - UseCromwell; - !Join ["" "", [""sh"", ""/opt/ecs-additions/ecs-additions-cromwell.sh""]]; - echo ""OK""; env:; PATH: ""/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin""; ```. My best guess as to what is happening, is [this blog post](http://www.codeandcompost.com/post/cfn,-utf8-and-two-days-i%E2%80%99ll-never-get-back), which suggests:; > Apparently cfn-init has a limit on the amount of output it can process from a command, and I was pushing that limit. > I suspect the reason for the UTF8 error is that the output was truncated between two bytes or something, and when the parser underneath cfn-init tried to parse it, it encountered what appeared to be an invalid UTF8 character. . So per",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4674
https://github.com/broadinstitute/cromwell/issues/4674:2236,Testability,log,logs,2236,"le util.py, line 476; [ 72.975338] EXT4-fs (dm-3): mounted filesystem with ordered data mode. Opts: (null); Error occurred during build: Command 04InstallECSAdditions failed; ```; The line `EXT4-fs (dm-3): mounted filesystem with ordered data mode` is repeated about 100 times in the real logs (below), I've just abridged it here for clarity. Anyway, the main thing this tells us that it's failing during step 04 of the EC2 startup script, which does the following:; ```yaml; 04InstallECSAdditions:; command:; Fn::If:; - UseCromwell; - !Join ["" "", [""sh"", ""/opt/ecs-additions/ecs-additions-cromwell.sh""]]; - echo ""OK""; env:; PATH: ""/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin""; ```. My best guess as to what is happening, is [this blog post](http://www.codeandcompost.com/post/cfn,-utf8-and-two-days-i%E2%80%99ll-never-get-back), which suggests:; > Apparently cfn-init has a limit on the amount of output it can process from a command, and I was pushing that limit. > I suspect the reason for the UTF8 error is that the output was truncated between two bytes or something, and when the parser underneath cfn-init tried to parse it, it encountered what appeared to be an invalid UTF8 character. . So perhaps the reason this issue is intermittent is because the length of the logs from this command are occasionally too long for the `cfn-init` script? Or this might be a red herring. To aid with debugging, here are some useful logs; * [ec2_log.txt](https://github.com/broadinstitute/cromwell/files/2887960/ec2_log.txt): This is the console output from the EC2 instance that failed. I've censored out some of the key data, just in case any of it involves my own public key (probably unnecessary, but I doubt it's related); * [stack_description.json.txt](https://github.com/broadinstitute/cromwell/files/2887962/stack_description.json.txt): The output from `aws cloudformation describe-stacks` on the stack I spun up. This should give you the exact parameters I used when it last failed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4674
https://github.com/broadinstitute/cromwell/issues/4674:2388,Testability,log,logs,2388,"le util.py, line 476; [ 72.975338] EXT4-fs (dm-3): mounted filesystem with ordered data mode. Opts: (null); Error occurred during build: Command 04InstallECSAdditions failed; ```; The line `EXT4-fs (dm-3): mounted filesystem with ordered data mode` is repeated about 100 times in the real logs (below), I've just abridged it here for clarity. Anyway, the main thing this tells us that it's failing during step 04 of the EC2 startup script, which does the following:; ```yaml; 04InstallECSAdditions:; command:; Fn::If:; - UseCromwell; - !Join ["" "", [""sh"", ""/opt/ecs-additions/ecs-additions-cromwell.sh""]]; - echo ""OK""; env:; PATH: ""/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin""; ```. My best guess as to what is happening, is [this blog post](http://www.codeandcompost.com/post/cfn,-utf8-and-two-days-i%E2%80%99ll-never-get-back), which suggests:; > Apparently cfn-init has a limit on the amount of output it can process from a command, and I was pushing that limit. > I suspect the reason for the UTF8 error is that the output was truncated between two bytes or something, and when the parser underneath cfn-init tried to parse it, it encountered what appeared to be an invalid UTF8 character. . So perhaps the reason this issue is intermittent is because the length of the logs from this command are occasionally too long for the `cfn-init` script? Or this might be a red herring. To aid with debugging, here are some useful logs; * [ec2_log.txt](https://github.com/broadinstitute/cromwell/files/2887960/ec2_log.txt): This is the console output from the EC2 instance that failed. I've censored out some of the key data, just in case any of it involves my own public key (probably unnecessary, but I doubt it's related); * [stack_description.json.txt](https://github.com/broadinstitute/cromwell/files/2887962/stack_description.json.txt): The output from `aws cloudformation describe-stacks` on the stack I spun up. This should give you the exact parameters I used when it last failed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4674
https://github.com/broadinstitute/cromwell/pull/4675:290,Availability,failure,failures,290,This PR cherry picks the below features/functionalities from Cromwell 37 onto 36_hotfix:. - Statsd logging in CromIAM [link](https://github.com/broadinstitute/cromwell/pull/4293); - Call cache copy fail blacklisting [link](https://github.com/broadinstitute/cromwell/pull/4359); - Forbidden failures are recognized/treated as IO Failures [link](https://github.com/broadinstitute/cromwell/pull/4376); - Auto-sizing boot disk in PAPI v2 [link](https://github.com/broadinstitute/cromwell/pull/4472); - Allow for a 'name-for-call-caching-purposes' to override the backend name [link](https://github.com/broadinstitute/cromwell/pull/4490); - User-service-account auth for Pipelines API v2 [link](https://github.com/broadinstitute/cromwell/pull/4566); - Add the Token Queue info to Cromwell log [link](https://github.com/broadinstitute/cromwell/pull/4567); - Labels query performance update [link](https://github.com/broadinstitute/cromwell/pull/4610); - Workflow validation for labels PATCH goes to summary not metadata [link](https://github.com/broadinstitute/cromwell/pull/4617); - CI Updates [link](https://github.com/broadinstitute/cromwell/commit/1a739fc7aabb1c557d96b57944c50ddc2006236a),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4675
https://github.com/broadinstitute/cromwell/pull/4675:328,Availability,Failure,Failures,328,This PR cherry picks the below features/functionalities from Cromwell 37 onto 36_hotfix:. - Statsd logging in CromIAM [link](https://github.com/broadinstitute/cromwell/pull/4293); - Call cache copy fail blacklisting [link](https://github.com/broadinstitute/cromwell/pull/4359); - Forbidden failures are recognized/treated as IO Failures [link](https://github.com/broadinstitute/cromwell/pull/4376); - Auto-sizing boot disk in PAPI v2 [link](https://github.com/broadinstitute/cromwell/pull/4472); - Allow for a 'name-for-call-caching-purposes' to override the backend name [link](https://github.com/broadinstitute/cromwell/pull/4490); - User-service-account auth for Pipelines API v2 [link](https://github.com/broadinstitute/cromwell/pull/4566); - Add the Token Queue info to Cromwell log [link](https://github.com/broadinstitute/cromwell/pull/4567); - Labels query performance update [link](https://github.com/broadinstitute/cromwell/pull/4610); - Workflow validation for labels PATCH goes to summary not metadata [link](https://github.com/broadinstitute/cromwell/pull/4617); - CI Updates [link](https://github.com/broadinstitute/cromwell/commit/1a739fc7aabb1c557d96b57944c50ddc2006236a),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4675
https://github.com/broadinstitute/cromwell/pull/4675:666,Deployability,Pipeline,Pipelines,666,This PR cherry picks the below features/functionalities from Cromwell 37 onto 36_hotfix:. - Statsd logging in CromIAM [link](https://github.com/broadinstitute/cromwell/pull/4293); - Call cache copy fail blacklisting [link](https://github.com/broadinstitute/cromwell/pull/4359); - Forbidden failures are recognized/treated as IO Failures [link](https://github.com/broadinstitute/cromwell/pull/4376); - Auto-sizing boot disk in PAPI v2 [link](https://github.com/broadinstitute/cromwell/pull/4472); - Allow for a 'name-for-call-caching-purposes' to override the backend name [link](https://github.com/broadinstitute/cromwell/pull/4490); - User-service-account auth for Pipelines API v2 [link](https://github.com/broadinstitute/cromwell/pull/4566); - Add the Token Queue info to Cromwell log [link](https://github.com/broadinstitute/cromwell/pull/4567); - Labels query performance update [link](https://github.com/broadinstitute/cromwell/pull/4610); - Workflow validation for labels PATCH goes to summary not metadata [link](https://github.com/broadinstitute/cromwell/pull/4617); - CI Updates [link](https://github.com/broadinstitute/cromwell/commit/1a739fc7aabb1c557d96b57944c50ddc2006236a),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4675
https://github.com/broadinstitute/cromwell/pull/4675:877,Deployability,update,update,877,This PR cherry picks the below features/functionalities from Cromwell 37 onto 36_hotfix:. - Statsd logging in CromIAM [link](https://github.com/broadinstitute/cromwell/pull/4293); - Call cache copy fail blacklisting [link](https://github.com/broadinstitute/cromwell/pull/4359); - Forbidden failures are recognized/treated as IO Failures [link](https://github.com/broadinstitute/cromwell/pull/4376); - Auto-sizing boot disk in PAPI v2 [link](https://github.com/broadinstitute/cromwell/pull/4472); - Allow for a 'name-for-call-caching-purposes' to override the backend name [link](https://github.com/broadinstitute/cromwell/pull/4490); - User-service-account auth for Pipelines API v2 [link](https://github.com/broadinstitute/cromwell/pull/4566); - Add the Token Queue info to Cromwell log [link](https://github.com/broadinstitute/cromwell/pull/4567); - Labels query performance update [link](https://github.com/broadinstitute/cromwell/pull/4610); - Workflow validation for labels PATCH goes to summary not metadata [link](https://github.com/broadinstitute/cromwell/pull/4617); - CI Updates [link](https://github.com/broadinstitute/cromwell/commit/1a739fc7aabb1c557d96b57944c50ddc2006236a),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4675
https://github.com/broadinstitute/cromwell/pull/4675:979,Deployability,PATCH,PATCH,979,This PR cherry picks the below features/functionalities from Cromwell 37 onto 36_hotfix:. - Statsd logging in CromIAM [link](https://github.com/broadinstitute/cromwell/pull/4293); - Call cache copy fail blacklisting [link](https://github.com/broadinstitute/cromwell/pull/4359); - Forbidden failures are recognized/treated as IO Failures [link](https://github.com/broadinstitute/cromwell/pull/4376); - Auto-sizing boot disk in PAPI v2 [link](https://github.com/broadinstitute/cromwell/pull/4472); - Allow for a 'name-for-call-caching-purposes' to override the backend name [link](https://github.com/broadinstitute/cromwell/pull/4490); - User-service-account auth for Pipelines API v2 [link](https://github.com/broadinstitute/cromwell/pull/4566); - Add the Token Queue info to Cromwell log [link](https://github.com/broadinstitute/cromwell/pull/4567); - Labels query performance update [link](https://github.com/broadinstitute/cromwell/pull/4610); - Workflow validation for labels PATCH goes to summary not metadata [link](https://github.com/broadinstitute/cromwell/pull/4617); - CI Updates [link](https://github.com/broadinstitute/cromwell/commit/1a739fc7aabb1c557d96b57944c50ddc2006236a),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4675
https://github.com/broadinstitute/cromwell/pull/4675:1081,Deployability,Update,Updates,1081,This PR cherry picks the below features/functionalities from Cromwell 37 onto 36_hotfix:. - Statsd logging in CromIAM [link](https://github.com/broadinstitute/cromwell/pull/4293); - Call cache copy fail blacklisting [link](https://github.com/broadinstitute/cromwell/pull/4359); - Forbidden failures are recognized/treated as IO Failures [link](https://github.com/broadinstitute/cromwell/pull/4376); - Auto-sizing boot disk in PAPI v2 [link](https://github.com/broadinstitute/cromwell/pull/4472); - Allow for a 'name-for-call-caching-purposes' to override the backend name [link](https://github.com/broadinstitute/cromwell/pull/4490); - User-service-account auth for Pipelines API v2 [link](https://github.com/broadinstitute/cromwell/pull/4566); - Add the Token Queue info to Cromwell log [link](https://github.com/broadinstitute/cromwell/pull/4567); - Labels query performance update [link](https://github.com/broadinstitute/cromwell/pull/4610); - Workflow validation for labels PATCH goes to summary not metadata [link](https://github.com/broadinstitute/cromwell/pull/4617); - CI Updates [link](https://github.com/broadinstitute/cromwell/commit/1a739fc7aabb1c557d96b57944c50ddc2006236a),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4675
https://github.com/broadinstitute/cromwell/pull/4675:187,Performance,cache,cache,187,This PR cherry picks the below features/functionalities from Cromwell 37 onto 36_hotfix:. - Statsd logging in CromIAM [link](https://github.com/broadinstitute/cromwell/pull/4293); - Call cache copy fail blacklisting [link](https://github.com/broadinstitute/cromwell/pull/4359); - Forbidden failures are recognized/treated as IO Failures [link](https://github.com/broadinstitute/cromwell/pull/4376); - Auto-sizing boot disk in PAPI v2 [link](https://github.com/broadinstitute/cromwell/pull/4472); - Allow for a 'name-for-call-caching-purposes' to override the backend name [link](https://github.com/broadinstitute/cromwell/pull/4490); - User-service-account auth for Pipelines API v2 [link](https://github.com/broadinstitute/cromwell/pull/4566); - Add the Token Queue info to Cromwell log [link](https://github.com/broadinstitute/cromwell/pull/4567); - Labels query performance update [link](https://github.com/broadinstitute/cromwell/pull/4610); - Workflow validation for labels PATCH goes to summary not metadata [link](https://github.com/broadinstitute/cromwell/pull/4617); - CI Updates [link](https://github.com/broadinstitute/cromwell/commit/1a739fc7aabb1c557d96b57944c50ddc2006236a),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4675
https://github.com/broadinstitute/cromwell/pull/4675:761,Performance,Queue,Queue,761,This PR cherry picks the below features/functionalities from Cromwell 37 onto 36_hotfix:. - Statsd logging in CromIAM [link](https://github.com/broadinstitute/cromwell/pull/4293); - Call cache copy fail blacklisting [link](https://github.com/broadinstitute/cromwell/pull/4359); - Forbidden failures are recognized/treated as IO Failures [link](https://github.com/broadinstitute/cromwell/pull/4376); - Auto-sizing boot disk in PAPI v2 [link](https://github.com/broadinstitute/cromwell/pull/4472); - Allow for a 'name-for-call-caching-purposes' to override the backend name [link](https://github.com/broadinstitute/cromwell/pull/4490); - User-service-account auth for Pipelines API v2 [link](https://github.com/broadinstitute/cromwell/pull/4566); - Add the Token Queue info to Cromwell log [link](https://github.com/broadinstitute/cromwell/pull/4567); - Labels query performance update [link](https://github.com/broadinstitute/cromwell/pull/4610); - Workflow validation for labels PATCH goes to summary not metadata [link](https://github.com/broadinstitute/cromwell/pull/4617); - CI Updates [link](https://github.com/broadinstitute/cromwell/commit/1a739fc7aabb1c557d96b57944c50ddc2006236a),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4675
https://github.com/broadinstitute/cromwell/pull/4675:865,Performance,perform,performance,865,This PR cherry picks the below features/functionalities from Cromwell 37 onto 36_hotfix:. - Statsd logging in CromIAM [link](https://github.com/broadinstitute/cromwell/pull/4293); - Call cache copy fail blacklisting [link](https://github.com/broadinstitute/cromwell/pull/4359); - Forbidden failures are recognized/treated as IO Failures [link](https://github.com/broadinstitute/cromwell/pull/4376); - Auto-sizing boot disk in PAPI v2 [link](https://github.com/broadinstitute/cromwell/pull/4472); - Allow for a 'name-for-call-caching-purposes' to override the backend name [link](https://github.com/broadinstitute/cromwell/pull/4490); - User-service-account auth for Pipelines API v2 [link](https://github.com/broadinstitute/cromwell/pull/4566); - Add the Token Queue info to Cromwell log [link](https://github.com/broadinstitute/cromwell/pull/4567); - Labels query performance update [link](https://github.com/broadinstitute/cromwell/pull/4610); - Workflow validation for labels PATCH goes to summary not metadata [link](https://github.com/broadinstitute/cromwell/pull/4617); - CI Updates [link](https://github.com/broadinstitute/cromwell/commit/1a739fc7aabb1c557d96b57944c50ddc2006236a),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4675
https://github.com/broadinstitute/cromwell/pull/4675:957,Security,validat,validation,957,This PR cherry picks the below features/functionalities from Cromwell 37 onto 36_hotfix:. - Statsd logging in CromIAM [link](https://github.com/broadinstitute/cromwell/pull/4293); - Call cache copy fail blacklisting [link](https://github.com/broadinstitute/cromwell/pull/4359); - Forbidden failures are recognized/treated as IO Failures [link](https://github.com/broadinstitute/cromwell/pull/4376); - Auto-sizing boot disk in PAPI v2 [link](https://github.com/broadinstitute/cromwell/pull/4472); - Allow for a 'name-for-call-caching-purposes' to override the backend name [link](https://github.com/broadinstitute/cromwell/pull/4490); - User-service-account auth for Pipelines API v2 [link](https://github.com/broadinstitute/cromwell/pull/4566); - Add the Token Queue info to Cromwell log [link](https://github.com/broadinstitute/cromwell/pull/4567); - Labels query performance update [link](https://github.com/broadinstitute/cromwell/pull/4610); - Workflow validation for labels PATCH goes to summary not metadata [link](https://github.com/broadinstitute/cromwell/pull/4617); - CI Updates [link](https://github.com/broadinstitute/cromwell/commit/1a739fc7aabb1c557d96b57944c50ddc2006236a),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4675
https://github.com/broadinstitute/cromwell/pull/4675:99,Testability,log,logging,99,This PR cherry picks the below features/functionalities from Cromwell 37 onto 36_hotfix:. - Statsd logging in CromIAM [link](https://github.com/broadinstitute/cromwell/pull/4293); - Call cache copy fail blacklisting [link](https://github.com/broadinstitute/cromwell/pull/4359); - Forbidden failures are recognized/treated as IO Failures [link](https://github.com/broadinstitute/cromwell/pull/4376); - Auto-sizing boot disk in PAPI v2 [link](https://github.com/broadinstitute/cromwell/pull/4472); - Allow for a 'name-for-call-caching-purposes' to override the backend name [link](https://github.com/broadinstitute/cromwell/pull/4490); - User-service-account auth for Pipelines API v2 [link](https://github.com/broadinstitute/cromwell/pull/4566); - Add the Token Queue info to Cromwell log [link](https://github.com/broadinstitute/cromwell/pull/4567); - Labels query performance update [link](https://github.com/broadinstitute/cromwell/pull/4610); - Workflow validation for labels PATCH goes to summary not metadata [link](https://github.com/broadinstitute/cromwell/pull/4617); - CI Updates [link](https://github.com/broadinstitute/cromwell/commit/1a739fc7aabb1c557d96b57944c50ddc2006236a),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4675
https://github.com/broadinstitute/cromwell/pull/4675:784,Testability,log,log,784,This PR cherry picks the below features/functionalities from Cromwell 37 onto 36_hotfix:. - Statsd logging in CromIAM [link](https://github.com/broadinstitute/cromwell/pull/4293); - Call cache copy fail blacklisting [link](https://github.com/broadinstitute/cromwell/pull/4359); - Forbidden failures are recognized/treated as IO Failures [link](https://github.com/broadinstitute/cromwell/pull/4376); - Auto-sizing boot disk in PAPI v2 [link](https://github.com/broadinstitute/cromwell/pull/4472); - Allow for a 'name-for-call-caching-purposes' to override the backend name [link](https://github.com/broadinstitute/cromwell/pull/4490); - User-service-account auth for Pipelines API v2 [link](https://github.com/broadinstitute/cromwell/pull/4566); - Add the Token Queue info to Cromwell log [link](https://github.com/broadinstitute/cromwell/pull/4567); - Labels query performance update [link](https://github.com/broadinstitute/cromwell/pull/4610); - Workflow validation for labels PATCH goes to summary not metadata [link](https://github.com/broadinstitute/cromwell/pull/4617); - CI Updates [link](https://github.com/broadinstitute/cromwell/commit/1a739fc7aabb1c557d96b57944c50ddc2006236a),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4675
https://github.com/broadinstitute/cromwell/issues/4676:320,Deployability,configurat,configuration,320,"Hello, I am running Cromwell 36 configured with the GCS/JES backend to run jobs on GCP. When running massive batches of workflows, I frequently encounter the IP-address quota from Google. To avoid this, I've reconfigured my default VPC to allow private google access (see #1325). I've added the following to my Cromwell configuration (other unrelated configuration entries removed):; ```; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; default-runtime-attributes {; noAddress: true; }; }; }; }; }; ```. This appears to have the desired effect, as my instances are now launching without an external IP, however, the jobs end up failing because docker cannot fetch the image `stedolan/jq` (as it resides on docker hub). Is there a way to configure Cromwell to use a different image for that pipeline action?. I could reconfigure the VPC to allow access to docker(hub), but that would require connecting a NAT instance which would increase the cost of using Cromwell. ---. Edit: Cromwell 36. Sorry!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676
https://github.com/broadinstitute/cromwell/issues/4676:351,Deployability,configurat,configuration,351,"Hello, I am running Cromwell 36 configured with the GCS/JES backend to run jobs on GCP. When running massive batches of workflows, I frequently encounter the IP-address quota from Google. To avoid this, I've reconfigured my default VPC to allow private google access (see #1325). I've added the following to my Cromwell configuration (other unrelated configuration entries removed):; ```; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; default-runtime-attributes {; noAddress: true; }; }; }; }; }; ```. This appears to have the desired effect, as my instances are now launching without an external IP, however, the jobs end up failing because docker cannot fetch the image `stedolan/jq` (as it resides on docker hub). Is there a way to configure Cromwell to use a different image for that pipeline action?. I could reconfigure the VPC to allow access to docker(hub), but that would require connecting a NAT instance which would increase the cost of using Cromwell. ---. Edit: Cromwell 36. Sorry!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676
https://github.com/broadinstitute/cromwell/issues/4676:478,Deployability,pipeline,pipelines,478,"Hello, I am running Cromwell 36 configured with the GCS/JES backend to run jobs on GCP. When running massive batches of workflows, I frequently encounter the IP-address quota from Google. To avoid this, I've reconfigured my default VPC to allow private google access (see #1325). I've added the following to my Cromwell configuration (other unrelated configuration entries removed):; ```; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; default-runtime-attributes {; noAddress: true; }; }; }; }; }; ```. This appears to have the desired effect, as my instances are now launching without an external IP, however, the jobs end up failing because docker cannot fetch the image `stedolan/jq` (as it resides on docker hub). Is there a way to configure Cromwell to use a different image for that pipeline action?. I could reconfigure the VPC to allow access to docker(hub), but that would require connecting a NAT instance which would increase the cost of using Cromwell. ---. Edit: Cromwell 36. Sorry!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676
https://github.com/broadinstitute/cromwell/issues/4676:497,Deployability,Pipeline,PipelinesApiLifecycleActorFactory,497,"Hello, I am running Cromwell 36 configured with the GCS/JES backend to run jobs on GCP. When running massive batches of workflows, I frequently encounter the IP-address quota from Google. To avoid this, I've reconfigured my default VPC to allow private google access (see #1325). I've added the following to my Cromwell configuration (other unrelated configuration entries removed):; ```; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; default-runtime-attributes {; noAddress: true; }; }; }; }; }; ```. This appears to have the desired effect, as my instances are now launching without an external IP, however, the jobs end up failing because docker cannot fetch the image `stedolan/jq` (as it resides on docker hub). Is there a way to configure Cromwell to use a different image for that pipeline action?. I could reconfigure the VPC to allow access to docker(hub), but that would require connecting a NAT instance which would increase the cost of using Cromwell. ---. Edit: Cromwell 36. Sorry!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676
https://github.com/broadinstitute/cromwell/issues/4676:896,Deployability,pipeline,pipeline,896,"Hello, I am running Cromwell 36 configured with the GCS/JES backend to run jobs on GCP. When running massive batches of workflows, I frequently encounter the IP-address quota from Google. To avoid this, I've reconfigured my default VPC to allow private google access (see #1325). I've added the following to my Cromwell configuration (other unrelated configuration entries removed):; ```; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; default-runtime-attributes {; noAddress: true; }; }; }; }; }; ```. This appears to have the desired effect, as my instances are now launching without an external IP, however, the jobs end up failing because docker cannot fetch the image `stedolan/jq` (as it resides on docker hub). Is there a way to configure Cromwell to use a different image for that pipeline action?. I could reconfigure the VPC to allow access to docker(hub), but that would require connecting a NAT instance which would increase the cost of using Cromwell. ---. Edit: Cromwell 36. Sorry!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676
https://github.com/broadinstitute/cromwell/issues/4676:32,Modifiability,config,configured,32,"Hello, I am running Cromwell 36 configured with the GCS/JES backend to run jobs on GCP. When running massive batches of workflows, I frequently encounter the IP-address quota from Google. To avoid this, I've reconfigured my default VPC to allow private google access (see #1325). I've added the following to my Cromwell configuration (other unrelated configuration entries removed):; ```; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; default-runtime-attributes {; noAddress: true; }; }; }; }; }; ```. This appears to have the desired effect, as my instances are now launching without an external IP, however, the jobs end up failing because docker cannot fetch the image `stedolan/jq` (as it resides on docker hub). Is there a way to configure Cromwell to use a different image for that pipeline action?. I could reconfigure the VPC to allow access to docker(hub), but that would require connecting a NAT instance which would increase the cost of using Cromwell. ---. Edit: Cromwell 36. Sorry!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676
https://github.com/broadinstitute/cromwell/issues/4676:320,Modifiability,config,configuration,320,"Hello, I am running Cromwell 36 configured with the GCS/JES backend to run jobs on GCP. When running massive batches of workflows, I frequently encounter the IP-address quota from Google. To avoid this, I've reconfigured my default VPC to allow private google access (see #1325). I've added the following to my Cromwell configuration (other unrelated configuration entries removed):; ```; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; default-runtime-attributes {; noAddress: true; }; }; }; }; }; ```. This appears to have the desired effect, as my instances are now launching without an external IP, however, the jobs end up failing because docker cannot fetch the image `stedolan/jq` (as it resides on docker hub). Is there a way to configure Cromwell to use a different image for that pipeline action?. I could reconfigure the VPC to allow access to docker(hub), but that would require connecting a NAT instance which would increase the cost of using Cromwell. ---. Edit: Cromwell 36. Sorry!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676
https://github.com/broadinstitute/cromwell/issues/4676:351,Modifiability,config,configuration,351,"Hello, I am running Cromwell 36 configured with the GCS/JES backend to run jobs on GCP. When running massive batches of workflows, I frequently encounter the IP-address quota from Google. To avoid this, I've reconfigured my default VPC to allow private google access (see #1325). I've added the following to my Cromwell configuration (other unrelated configuration entries removed):; ```; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; default-runtime-attributes {; noAddress: true; }; }; }; }; }; ```. This appears to have the desired effect, as my instances are now launching without an external IP, however, the jobs end up failing because docker cannot fetch the image `stedolan/jq` (as it resides on docker hub). Is there a way to configure Cromwell to use a different image for that pipeline action?. I could reconfigure the VPC to allow access to docker(hub), but that would require connecting a NAT instance which would increase the cost of using Cromwell. ---. Edit: Cromwell 36. Sorry!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676
https://github.com/broadinstitute/cromwell/issues/4676:533,Modifiability,config,config,533,"Hello, I am running Cromwell 36 configured with the GCS/JES backend to run jobs on GCP. When running massive batches of workflows, I frequently encounter the IP-address quota from Google. To avoid this, I've reconfigured my default VPC to allow private google access (see #1325). I've added the following to my Cromwell configuration (other unrelated configuration entries removed):; ```; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; default-runtime-attributes {; noAddress: true; }; }; }; }; }; ```. This appears to have the desired effect, as my instances are now launching without an external IP, however, the jobs end up failing because docker cannot fetch the image `stedolan/jq` (as it resides on docker hub). Is there a way to configure Cromwell to use a different image for that pipeline action?. I could reconfigure the VPC to allow access to docker(hub), but that would require connecting a NAT instance which would increase the cost of using Cromwell. ---. Edit: Cromwell 36. Sorry!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676
https://github.com/broadinstitute/cromwell/issues/4676:843,Modifiability,config,configure,843,"Hello, I am running Cromwell 36 configured with the GCS/JES backend to run jobs on GCP. When running massive batches of workflows, I frequently encounter the IP-address quota from Google. To avoid this, I've reconfigured my default VPC to allow private google access (see #1325). I've added the following to my Cromwell configuration (other unrelated configuration entries removed):; ```; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; default-runtime-attributes {; noAddress: true; }; }; }; }; }; ```. This appears to have the desired effect, as my instances are now launching without an external IP, however, the jobs end up failing because docker cannot fetch the image `stedolan/jq` (as it resides on docker hub). Is there a way to configure Cromwell to use a different image for that pipeline action?. I could reconfigure the VPC to allow access to docker(hub), but that would require connecting a NAT instance which would increase the cost of using Cromwell. ---. Edit: Cromwell 36. Sorry!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676
https://github.com/broadinstitute/cromwell/issues/4676:191,Safety,avoid,avoid,191,"Hello, I am running Cromwell 36 configured with the GCS/JES backend to run jobs on GCP. When running massive batches of workflows, I frequently encounter the IP-address quota from Google. To avoid this, I've reconfigured my default VPC to allow private google access (see #1325). I've added the following to my Cromwell configuration (other unrelated configuration entries removed):; ```; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; default-runtime-attributes {; noAddress: true; }; }; }; }; }; ```. This appears to have the desired effect, as my instances are now launching without an external IP, however, the jobs end up failing because docker cannot fetch the image `stedolan/jq` (as it resides on docker hub). Is there a way to configure Cromwell to use a different image for that pipeline action?. I could reconfigure the VPC to allow access to docker(hub), but that would require connecting a NAT instance which would increase the cost of using Cromwell. ---. Edit: Cromwell 36. Sorry!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676
https://github.com/broadinstitute/cromwell/issues/4676:260,Security,access,access,260,"Hello, I am running Cromwell 36 configured with the GCS/JES backend to run jobs on GCP. When running massive batches of workflows, I frequently encounter the IP-address quota from Google. To avoid this, I've reconfigured my default VPC to allow private google access (see #1325). I've added the following to my Cromwell configuration (other unrelated configuration entries removed):; ```; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; default-runtime-attributes {; noAddress: true; }; }; }; }; }; ```. This appears to have the desired effect, as my instances are now launching without an external IP, however, the jobs end up failing because docker cannot fetch the image `stedolan/jq` (as it resides on docker hub). Is there a way to configure Cromwell to use a different image for that pipeline action?. I could reconfigure the VPC to allow access to docker(hub), but that would require connecting a NAT instance which would increase the cost of using Cromwell. ---. Edit: Cromwell 36. Sorry!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676
https://github.com/broadinstitute/cromwell/issues/4676:951,Security,access,access,951,"Hello, I am running Cromwell 36 configured with the GCS/JES backend to run jobs on GCP. When running massive batches of workflows, I frequently encounter the IP-address quota from Google. To avoid this, I've reconfigured my default VPC to allow private google access (see #1325). I've added the following to my Cromwell configuration (other unrelated configuration entries removed):; ```; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; default-runtime-attributes {; noAddress: true; }; }; }; }; }; ```. This appears to have the desired effect, as my instances are now launching without an external IP, however, the jobs end up failing because docker cannot fetch the image `stedolan/jq` (as it resides on docker hub). Is there a way to configure Cromwell to use a different image for that pipeline action?. I could reconfigure the VPC to allow access to docker(hub), but that would require connecting a NAT instance which would increase the cost of using Cromwell. ---. Edit: Cromwell 36. Sorry!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676
https://github.com/broadinstitute/cromwell/issues/4677:382,Availability,echo,echoHello,382,"Please check this issue.; When I configure the aws, I followed up this page. (https://docs.opendata.aws/genomics-workflows/; ); I did not use the all-in-one template. With 3 step configure, I setup the cromwell server (Custom AMI -> VPC..and etc -> cromwell server instance). <!-- Which backend are you running? -->; aws. <!-- Paste/Attach your workflow if possible: -->; ```; task echoHello{; command {; echo ""Hello AWS!""; }; runtime {; docker: ""ubuntu:latest""; }. }. workflow printHelloAndGoodbye {; call echoHello; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; [cromwell-server.log](https://github.com/broadinstitute/cromwell/files/2897001/cromwell-server.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4677
https://github.com/broadinstitute/cromwell/issues/4677:405,Availability,echo,echo,405,"Please check this issue.; When I configure the aws, I followed up this page. (https://docs.opendata.aws/genomics-workflows/; ); I did not use the all-in-one template. With 3 step configure, I setup the cromwell server (Custom AMI -> VPC..and etc -> cromwell server instance). <!-- Which backend are you running? -->; aws. <!-- Paste/Attach your workflow if possible: -->; ```; task echoHello{; command {; echo ""Hello AWS!""; }; runtime {; docker: ""ubuntu:latest""; }. }. workflow printHelloAndGoodbye {; call echoHello; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; [cromwell-server.log](https://github.com/broadinstitute/cromwell/files/2897001/cromwell-server.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4677
https://github.com/broadinstitute/cromwell/issues/4677:507,Availability,echo,echoHello,507,"Please check this issue.; When I configure the aws, I followed up this page. (https://docs.opendata.aws/genomics-workflows/; ); I did not use the all-in-one template. With 3 step configure, I setup the cromwell server (Custom AMI -> VPC..and etc -> cromwell server instance). <!-- Which backend are you running? -->; aws. <!-- Paste/Attach your workflow if possible: -->; ```; task echoHello{; command {; echo ""Hello AWS!""; }; runtime {; docker: ""ubuntu:latest""; }. }. workflow printHelloAndGoodbye {; call echoHello; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; [cromwell-server.log](https://github.com/broadinstitute/cromwell/files/2897001/cromwell-server.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4677
https://github.com/broadinstitute/cromwell/issues/4677:542,Deployability,configurat,configuration,542,"Please check this issue.; When I configure the aws, I followed up this page. (https://docs.opendata.aws/genomics-workflows/; ); I did not use the all-in-one template. With 3 step configure, I setup the cromwell server (Custom AMI -> VPC..and etc -> cromwell server instance). <!-- Which backend are you running? -->; aws. <!-- Paste/Attach your workflow if possible: -->; ```; task echoHello{; command {; echo ""Hello AWS!""; }; runtime {; docker: ""ubuntu:latest""; }. }. workflow printHelloAndGoodbye {; call echoHello; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; [cromwell-server.log](https://github.com/broadinstitute/cromwell/files/2897001/cromwell-server.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4677
https://github.com/broadinstitute/cromwell/issues/4677:33,Modifiability,config,configure,33,"Please check this issue.; When I configure the aws, I followed up this page. (https://docs.opendata.aws/genomics-workflows/; ); I did not use the all-in-one template. With 3 step configure, I setup the cromwell server (Custom AMI -> VPC..and etc -> cromwell server instance). <!-- Which backend are you running? -->; aws. <!-- Paste/Attach your workflow if possible: -->; ```; task echoHello{; command {; echo ""Hello AWS!""; }; runtime {; docker: ""ubuntu:latest""; }. }. workflow printHelloAndGoodbye {; call echoHello; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; [cromwell-server.log](https://github.com/broadinstitute/cromwell/files/2897001/cromwell-server.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4677
https://github.com/broadinstitute/cromwell/issues/4677:179,Modifiability,config,configure,179,"Please check this issue.; When I configure the aws, I followed up this page. (https://docs.opendata.aws/genomics-workflows/; ); I did not use the all-in-one template. With 3 step configure, I setup the cromwell server (Custom AMI -> VPC..and etc -> cromwell server instance). <!-- Which backend are you running? -->; aws. <!-- Paste/Attach your workflow if possible: -->; ```; task echoHello{; command {; echo ""Hello AWS!""; }; runtime {; docker: ""ubuntu:latest""; }. }. workflow printHelloAndGoodbye {; call echoHello; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; [cromwell-server.log](https://github.com/broadinstitute/cromwell/files/2897001/cromwell-server.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4677
https://github.com/broadinstitute/cromwell/issues/4677:542,Modifiability,config,configuration,542,"Please check this issue.; When I configure the aws, I followed up this page. (https://docs.opendata.aws/genomics-workflows/; ); I did not use the all-in-one template. With 3 step configure, I setup the cromwell server (Custom AMI -> VPC..and etc -> cromwell server instance). <!-- Which backend are you running? -->; aws. <!-- Paste/Attach your workflow if possible: -->; ```; task echoHello{; command {; echo ""Hello AWS!""; }; runtime {; docker: ""ubuntu:latest""; }. }. workflow printHelloAndGoodbye {; call echoHello; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; [cromwell-server.log](https://github.com/broadinstitute/cromwell/files/2897001/cromwell-server.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4677
https://github.com/broadinstitute/cromwell/issues/4677:587,Security,PASSWORD,PASSWORDS,587,"Please check this issue.; When I configure the aws, I followed up this page. (https://docs.opendata.aws/genomics-workflows/; ); I did not use the all-in-one template. With 3 step configure, I setup the cromwell server (Custom AMI -> VPC..and etc -> cromwell server instance). <!-- Which backend are you running? -->; aws. <!-- Paste/Attach your workflow if possible: -->; ```; task echoHello{; command {; echo ""Hello AWS!""; }; runtime {; docker: ""ubuntu:latest""; }. }. workflow printHelloAndGoodbye {; call echoHello; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; [cromwell-server.log](https://github.com/broadinstitute/cromwell/files/2897001/cromwell-server.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4677
https://github.com/broadinstitute/cromwell/issues/4677:657,Testability,log,log,657,"Please check this issue.; When I configure the aws, I followed up this page. (https://docs.opendata.aws/genomics-workflows/; ); I did not use the all-in-one template. With 3 step configure, I setup the cromwell server (Custom AMI -> VPC..and etc -> cromwell server instance). <!-- Which backend are you running? -->; aws. <!-- Paste/Attach your workflow if possible: -->; ```; task echoHello{; command {; echo ""Hello AWS!""; }; runtime {; docker: ""ubuntu:latest""; }. }. workflow printHelloAndGoodbye {; call echoHello; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; [cromwell-server.log](https://github.com/broadinstitute/cromwell/files/2897001/cromwell-server.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4677
https://github.com/broadinstitute/cromwell/issues/4677:735,Testability,log,log,735,"Please check this issue.; When I configure the aws, I followed up this page. (https://docs.opendata.aws/genomics-workflows/; ); I did not use the all-in-one template. With 3 step configure, I setup the cromwell server (Custom AMI -> VPC..and etc -> cromwell server instance). <!-- Which backend are you running? -->; aws. <!-- Paste/Attach your workflow if possible: -->; ```; task echoHello{; command {; echo ""Hello AWS!""; }; runtime {; docker: ""ubuntu:latest""; }. }. workflow printHelloAndGoodbye {; call echoHello; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; [cromwell-server.log](https://github.com/broadinstitute/cromwell/files/2897001/cromwell-server.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4677
https://github.com/broadinstitute/cromwell/issues/4678:1014,Deployability,configurat,configuration,1014,"Hello,. It seems to me that cromwell (at least `cromwell-37.jar`) can run `version 1.0` WDL scripts. Would you confirm this? It would also be helpful if you had this info readily on the ReadTheDocs page (https://github.com/broadinstitute/cromwell/blob/develop/docs/LanguageSupport.md). Thank you,; Azza . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4678
https://github.com/broadinstitute/cromwell/issues/4678:1014,Modifiability,config,configuration,1014,"Hello,. It seems to me that cromwell (at least `cromwell-37.jar`) can run `version 1.0` WDL scripts. Would you confirm this? It would also be helpful if you had this info readily on the ReadTheDocs page (https://github.com/broadinstitute/cromwell/blob/develop/docs/LanguageSupport.md). Thank you,; Azza . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4678
https://github.com/broadinstitute/cromwell/issues/4678:1059,Security,PASSWORD,PASSWORDS,1059,"Hello,. It seems to me that cromwell (at least `cromwell-37.jar`) can run `version 1.0` WDL scripts. Would you confirm this? It would also be helpful if you had this info readily on the ReadTheDocs page (https://github.com/broadinstitute/cromwell/blob/develop/docs/LanguageSupport.md). Thank you,; Azza . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4678
https://github.com/broadinstitute/cromwell/issues/4678:352,Usability,feedback,feedback,352,"Hello,. It seems to me that cromwell (at least `cromwell-37.jar`) can run `version 1.0` WDL scripts. Would you confirm this? It would also be helpful if you had this info readily on the ReadTheDocs page (https://github.com/broadinstitute/cromwell/blob/develop/docs/LanguageSupport.md). Thank you,; Azza . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4678
https://github.com/broadinstitute/cromwell/issues/4679:32,Deployability,patch,patch,32,"It's very frustrating to try to patch labels on a workflow to which you have read but not write permission. Right now, there's no way for JM to know which workflows can or cannot be patched, without trying and failing. It would be awesome if the CromIAM `/query` could add into the response JSON an indication of the permissions of the requester w.r.t each workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4679
https://github.com/broadinstitute/cromwell/issues/4679:182,Deployability,patch,patched,182,"It's very frustrating to try to patch labels on a workflow to which you have read but not write permission. Right now, there's no way for JM to know which workflows can or cannot be patched, without trying and failing. It would be awesome if the CromIAM `/query` could add into the response JSON an indication of the permissions of the requester w.r.t each workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4679
https://github.com/broadinstitute/cromwell/issues/4681:65,Availability,error,errors,65,"FYI @cjllanwarne . FireCloud production is seeing a lot of these errors on a regular basis:; ```; message:2019-02-26 18:12:58 [cromwell-system-akka.actor.default-dispatcher-1160] ; ERROR c.e.w.t.JobExecutionTokenDispenserActor - Job execution token returned from incorrect actor: <snip>-EngineJobExecutionActor-CHSworkflow.GenerateAndRunScript:; ```. You can see more of these errors in production Kibana with a simple query on ""Job execution token returned from incorrect"". Current cromwell [version](https://api.firecloud.org/#!/Version/executionEngineVersion):; ```; {; ""cromwell"": ""36-7de344f""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4681
https://github.com/broadinstitute/cromwell/issues/4681:181,Availability,ERROR,ERROR,181,"FYI @cjllanwarne . FireCloud production is seeing a lot of these errors on a regular basis:; ```; message:2019-02-26 18:12:58 [cromwell-system-akka.actor.default-dispatcher-1160] ; ERROR c.e.w.t.JobExecutionTokenDispenserActor - Job execution token returned from incorrect actor: <snip>-EngineJobExecutionActor-CHSworkflow.GenerateAndRunScript:; ```. You can see more of these errors in production Kibana with a simple query on ""Job execution token returned from incorrect"". Current cromwell [version](https://api.firecloud.org/#!/Version/executionEngineVersion):; ```; {; ""cromwell"": ""36-7de344f""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4681
https://github.com/broadinstitute/cromwell/issues/4681:377,Availability,error,errors,377,"FYI @cjllanwarne . FireCloud production is seeing a lot of these errors on a regular basis:; ```; message:2019-02-26 18:12:58 [cromwell-system-akka.actor.default-dispatcher-1160] ; ERROR c.e.w.t.JobExecutionTokenDispenserActor - Job execution token returned from incorrect actor: <snip>-EngineJobExecutionActor-CHSworkflow.GenerateAndRunScript:; ```. You can see more of these errors in production Kibana with a simple query on ""Job execution token returned from incorrect"". Current cromwell [version](https://api.firecloud.org/#!/Version/executionEngineVersion):; ```; {; ""cromwell"": ""36-7de344f""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4681
https://github.com/broadinstitute/cromwell/issues/4681:98,Integrability,message,message,98,"FYI @cjllanwarne . FireCloud production is seeing a lot of these errors on a regular basis:; ```; message:2019-02-26 18:12:58 [cromwell-system-akka.actor.default-dispatcher-1160] ; ERROR c.e.w.t.JobExecutionTokenDispenserActor - Job execution token returned from incorrect actor: <snip>-EngineJobExecutionActor-CHSworkflow.GenerateAndRunScript:; ```. You can see more of these errors in production Kibana with a simple query on ""Job execution token returned from incorrect"". Current cromwell [version](https://api.firecloud.org/#!/Version/executionEngineVersion):; ```; {; ""cromwell"": ""36-7de344f""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4681
https://github.com/broadinstitute/cromwell/issues/4681:412,Usability,simpl,simple,412,"FYI @cjllanwarne . FireCloud production is seeing a lot of these errors on a regular basis:; ```; message:2019-02-26 18:12:58 [cromwell-system-akka.actor.default-dispatcher-1160] ; ERROR c.e.w.t.JobExecutionTokenDispenserActor - Job execution token returned from incorrect actor: <snip>-EngineJobExecutionActor-CHSworkflow.GenerateAndRunScript:; ```. You can see more of these errors in production Kibana with a simple query on ""Job execution token returned from incorrect"". Current cromwell [version](https://api.firecloud.org/#!/Version/executionEngineVersion):; ```; {; ""cromwell"": ""36-7de344f""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4681
https://github.com/broadinstitute/cromwell/issues/4682:41,Availability,avail,available,41,"Cromwell release 36.1 has a docker image available for the release, but nowhere in the docs is the cromwell docker container documented (as far as I can tell).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4682
https://github.com/broadinstitute/cromwell/issues/4682:9,Deployability,release,release,9,"Cromwell release 36.1 has a docker image available for the release, but nowhere in the docs is the cromwell docker container documented (as far as I can tell).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4682
https://github.com/broadinstitute/cromwell/issues/4682:59,Deployability,release,release,59,"Cromwell release 36.1 has a docker image available for the release, but nowhere in the docs is the cromwell docker container documented (as far as I can tell).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4682
https://github.com/broadinstitute/cromwell/issues/4684:35,Modifiability,rewrite,rewrite,35,"Quick spike to explore if a `join` rewrite of `includeSubworkflows` might provide a performance boost analogous to the `join` rewrite for `labels`. . - ~write a `join` formulation of `includeSubworkflows`~; - ~clone dev Cloud SQL~; - harvest ""before"" and ""after"" versions of a sample `includeSubworkflows` query from Cromwell debug sessions (careful not to restart jobs in the dev clone); - compare performance of these queries between cold restarts of the dev clone",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4684
https://github.com/broadinstitute/cromwell/issues/4684:126,Modifiability,rewrite,rewrite,126,"Quick spike to explore if a `join` rewrite of `includeSubworkflows` might provide a performance boost analogous to the `join` rewrite for `labels`. . - ~write a `join` formulation of `includeSubworkflows`~; - ~clone dev Cloud SQL~; - harvest ""before"" and ""after"" versions of a sample `includeSubworkflows` query from Cromwell debug sessions (careful not to restart jobs in the dev clone); - compare performance of these queries between cold restarts of the dev clone",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4684
https://github.com/broadinstitute/cromwell/issues/4684:84,Performance,perform,performance,84,"Quick spike to explore if a `join` rewrite of `includeSubworkflows` might provide a performance boost analogous to the `join` rewrite for `labels`. . - ~write a `join` formulation of `includeSubworkflows`~; - ~clone dev Cloud SQL~; - harvest ""before"" and ""after"" versions of a sample `includeSubworkflows` query from Cromwell debug sessions (careful not to restart jobs in the dev clone); - compare performance of these queries between cold restarts of the dev clone",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4684
https://github.com/broadinstitute/cromwell/issues/4684:399,Performance,perform,performance,399,"Quick spike to explore if a `join` rewrite of `includeSubworkflows` might provide a performance boost analogous to the `join` rewrite for `labels`. . - ~write a `join` formulation of `includeSubworkflows`~; - ~clone dev Cloud SQL~; - harvest ""before"" and ""after"" versions of a sample `includeSubworkflows` query from Cromwell debug sessions (careful not to restart jobs in the dev clone); - compare performance of these queries between cold restarts of the dev clone",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4684
https://github.com/broadinstitute/cromwell/issues/4685:257,Availability,error,error,257,"I would like to specify an optional array of strings as a runtime attribute. As such, I tried this in my provider configuration:. runtime-attributes = """"""; Array[String]? mounts = []; """""". Unfortunately, this fails horribly:. ```; [2019-02-27 16:26:09,15] [error] Unsupported config runtime attribute WomMaybeEmptyArrayType(WomStringType) mounts; java.lang.RuntimeException: Unsupported config runtime attribute WomMaybeEmptyArrayType(WomStringType) mounts; ```. The error clearly indicates that this type is unsupported. What is the reasoning behind that, or am I just doing it wrong?. (This is with Cromwell 36.1)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4685
https://github.com/broadinstitute/cromwell/issues/4685:467,Availability,error,error,467,"I would like to specify an optional array of strings as a runtime attribute. As such, I tried this in my provider configuration:. runtime-attributes = """"""; Array[String]? mounts = []; """""". Unfortunately, this fails horribly:. ```; [2019-02-27 16:26:09,15] [error] Unsupported config runtime attribute WomMaybeEmptyArrayType(WomStringType) mounts; java.lang.RuntimeException: Unsupported config runtime attribute WomMaybeEmptyArrayType(WomStringType) mounts; ```. The error clearly indicates that this type is unsupported. What is the reasoning behind that, or am I just doing it wrong?. (This is with Cromwell 36.1)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4685
https://github.com/broadinstitute/cromwell/issues/4685:114,Deployability,configurat,configuration,114,"I would like to specify an optional array of strings as a runtime attribute. As such, I tried this in my provider configuration:. runtime-attributes = """"""; Array[String]? mounts = []; """""". Unfortunately, this fails horribly:. ```; [2019-02-27 16:26:09,15] [error] Unsupported config runtime attribute WomMaybeEmptyArrayType(WomStringType) mounts; java.lang.RuntimeException: Unsupported config runtime attribute WomMaybeEmptyArrayType(WomStringType) mounts; ```. The error clearly indicates that this type is unsupported. What is the reasoning behind that, or am I just doing it wrong?. (This is with Cromwell 36.1)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4685
https://github.com/broadinstitute/cromwell/issues/4685:114,Modifiability,config,configuration,114,"I would like to specify an optional array of strings as a runtime attribute. As such, I tried this in my provider configuration:. runtime-attributes = """"""; Array[String]? mounts = []; """""". Unfortunately, this fails horribly:. ```; [2019-02-27 16:26:09,15] [error] Unsupported config runtime attribute WomMaybeEmptyArrayType(WomStringType) mounts; java.lang.RuntimeException: Unsupported config runtime attribute WomMaybeEmptyArrayType(WomStringType) mounts; ```. The error clearly indicates that this type is unsupported. What is the reasoning behind that, or am I just doing it wrong?. (This is with Cromwell 36.1)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4685
https://github.com/broadinstitute/cromwell/issues/4685:276,Modifiability,config,config,276,"I would like to specify an optional array of strings as a runtime attribute. As such, I tried this in my provider configuration:. runtime-attributes = """"""; Array[String]? mounts = []; """""". Unfortunately, this fails horribly:. ```; [2019-02-27 16:26:09,15] [error] Unsupported config runtime attribute WomMaybeEmptyArrayType(WomStringType) mounts; java.lang.RuntimeException: Unsupported config runtime attribute WomMaybeEmptyArrayType(WomStringType) mounts; ```. The error clearly indicates that this type is unsupported. What is the reasoning behind that, or am I just doing it wrong?. (This is with Cromwell 36.1)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4685
https://github.com/broadinstitute/cromwell/issues/4685:387,Modifiability,config,config,387,"I would like to specify an optional array of strings as a runtime attribute. As such, I tried this in my provider configuration:. runtime-attributes = """"""; Array[String]? mounts = []; """""". Unfortunately, this fails horribly:. ```; [2019-02-27 16:26:09,15] [error] Unsupported config runtime attribute WomMaybeEmptyArrayType(WomStringType) mounts; java.lang.RuntimeException: Unsupported config runtime attribute WomMaybeEmptyArrayType(WomStringType) mounts; ```. The error clearly indicates that this type is unsupported. What is the reasoning behind that, or am I just doing it wrong?. (This is with Cromwell 36.1)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4685
https://github.com/broadinstitute/cromwell/issues/4685:473,Usability,clear,clearly,473,"I would like to specify an optional array of strings as a runtime attribute. As such, I tried this in my provider configuration:. runtime-attributes = """"""; Array[String]? mounts = []; """""". Unfortunately, this fails horribly:. ```; [2019-02-27 16:26:09,15] [error] Unsupported config runtime attribute WomMaybeEmptyArrayType(WomStringType) mounts; java.lang.RuntimeException: Unsupported config runtime attribute WomMaybeEmptyArrayType(WomStringType) mounts; ```. The error clearly indicates that this type is unsupported. What is the reasoning behind that, or am I just doing it wrong?. (This is with Cromwell 36.1)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4685
https://github.com/broadinstitute/cromwell/issues/4686:585,Availability,error,error,585,"I have a working cromwell/AWS batch configuration.; I have a simple workflow called three_task_sequence.wdl which I am able to run on AWS backend, and see the outputs in s3. However, submitting this job to my cromwell server:; `curl -X POST ""http://172.20.1.67:8001/api/workflows/v1"" -H ""accept: application/json"" -F ""workflowSource=@three_task_sequence.wdl"" -F ""workflowOptions=@workflow_options.json""; `; Where workflow_options.json content is:; ```; {; ""final_workflow_outputs_dir"": ""s3://nrglab-cromwell-genomics/cromwell-execution/out_bin_test""; }. ```. I'm getting the following error at the end of the workflow cromwell log:. ````; 2019-02-28 08:30:32,167 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:717,Availability,ERROR,ERROR,717,"I have a working cromwell/AWS batch configuration.; I have a simple workflow called three_task_sequence.wdl which I am able to run on AWS backend, and see the outputs in s3. However, submitting this job to my cromwell server:; `curl -X POST ""http://172.20.1.67:8001/api/workflows/v1"" -H ""accept: application/json"" -F ""workflowSource=@three_task_sequence.wdl"" -F ""workflowOptions=@workflow_options.json""; `; Where workflow_options.json content is:; ```; {; ""final_workflow_outputs_dir"": ""s3://nrglab-cromwell-genomics/cromwell-execution/out_bin_test""; }. ```. I'm getting the following error at the end of the workflow cromwell log:. ````; 2019-02-28 08:30:32,167 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:6906,Availability,ERROR,ERROR,6906,"lyOrElse(CopyWorkflowOutputsActor.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowOutputsActor.aroundReceive(CopyWorkflowOutputsActor.scala:28); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-02-28 08:30:32,176 cromwell-system-akka.dispatchers.engine-dispatcher-28 ERROR - WorkflowManagerActor Workflow bd18e464-59a2-44cf-80c2-b4d93bdfe0ce failed (during FinalizingWorkflowState): software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerException",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:36,Deployability,configurat,configuration,36,"I have a working cromwell/AWS batch configuration.; I have a simple workflow called three_task_sequence.wdl which I am able to run on AWS backend, and see the outputs in s3. However, submitting this job to my cromwell server:; `curl -X POST ""http://172.20.1.67:8001/api/workflows/v1"" -H ""accept: application/json"" -F ""workflowSource=@three_task_sequence.wdl"" -F ""workflowOptions=@workflow_options.json""; `; Where workflow_options.json content is:; ```; {; ""final_workflow_outputs_dir"": ""s3://nrglab-cromwell-genomics/cromwell-execution/out_bin_test""; }. ```. I'm getting the following error at the end of the workflow cromwell log:. ````; 2019-02-28 08:30:32,167 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:982,Deployability,pipeline,pipeline,982,"I have a working cromwell/AWS batch configuration.; I have a simple workflow called three_task_sequence.wdl which I am able to run on AWS backend, and see the outputs in s3. However, submitting this job to my cromwell server:; `curl -X POST ""http://172.20.1.67:8001/api/workflows/v1"" -H ""accept: application/json"" -F ""workflowSource=@three_task_sequence.wdl"" -F ""workflowOptions=@workflow_options.json""; `; Where workflow_options.json content is:; ```; {; ""final_workflow_outputs_dir"": ""s3://nrglab-cromwell-genomics/cromwell-execution/out_bin_test""; }. ```. I'm getting the following error at the end of the workflow cromwell log:. ````; 2019-02-28 08:30:32,167 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:1106,Deployability,pipeline,pipeline,1106,"ich I am able to run on AWS backend, and see the outputs in s3. However, submitting this job to my cromwell server:; `curl -X POST ""http://172.20.1.67:8001/api/workflows/v1"" -H ""accept: application/json"" -F ""workflowSource=@three_task_sequence.wdl"" -F ""workflowOptions=@workflow_options.json""; `; Where workflow_options.json content is:; ```; {; ""final_workflow_outputs_dir"": ""s3://nrglab-cromwell-genomics/cromwell-execution/out_bin_test""; }. ```. I'm getting the following error at the end of the workflow cromwell log:. ````; 2019-02-28 08:30:32,167 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:1224,Deployability,pipeline,pipeline,1224,"curl -X POST ""http://172.20.1.67:8001/api/workflows/v1"" -H ""accept: application/json"" -F ""workflowSource=@three_task_sequence.wdl"" -F ""workflowOptions=@workflow_options.json""; `; Where workflow_options.json content is:; ```; {; ""final_workflow_outputs_dir"": ""s3://nrglab-cromwell-genomics/cromwell-execution/out_bin_test""; }. ```. I'm getting the following error at the end of the workflow cromwell log:. ````; 2019-02-28 08:30:32,167 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:1335,Deployability,pipeline,pipeline,1335,"_task_sequence.wdl"" -F ""workflowOptions=@workflow_options.json""; `; Where workflow_options.json content is:; ```; {; ""final_workflow_outputs_dir"": ""s3://nrglab-cromwell-genomics/cromwell-execution/out_bin_test""; }. ```. I'm getting the following error at the end of the workflow cromwell log:. ````; 2019-02-28 08:30:32,167 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:1446,Deployability,pipeline,pipeline,1446,"`; {; ""final_workflow_outputs_dir"": ""s3://nrglab-cromwell-genomics/cromwell-execution/out_bin_test""; }. ```. I'm getting the following error at the end of the workflow cromwell log:. ````; 2019-02-28 08:30:32,167 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(Strea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:1587,Deployability,pipeline,pipeline,1587,"at the end of the workflow cromwell log:. ````; 2019-02-28 08:30:32,167 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:1714,Deployability,pipeline,pipeline,1714,RROR - Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.ama,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:1841,Deployability,pipeline,pipeline,1841,del.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:1959,Deployability,pipeline,pipeline,1959,n.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.am,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:2075,Deployability,pipeline,pipeline,2075,re.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.j,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:2176,Deployability,pipeline,pipeline,2176,a:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipeli,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:2277,Deployability,pipeline,pipeline,2277,eStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stage,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:2608,Deployability,pipeline,pipeline,2608,ptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImp,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:2740,Deployability,pipeline,pipeline,2740,HandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.ja,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:2863,Deployability,pipeline,pipeline,2863,tage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:2986,Deployability,pipeline,pipeline,2986,RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); 	at software.amazon.awssdk.core.cl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:3127,Deployability,pipeline,pipeline,3127,va:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientH,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:3268,Deployability,pipeline,pipeline,3268,http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.listBuckets(DefaultS3Client.java:1749); 	at software.amazon.awssdk.se,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:3419,Deployability,pipeline,pipeline,3419,mManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.listBuckets(DefaultS3Client.java:1749); 	at software.amazon.awssdk.services.s3.S3Client.listBuckets(S3Client.java:2184); 	at org.lerch.s3fs.S3FileStore.getBucket(S3FileStore.java:93); 	at org.lerch.s3fs.S3FileStore.getBu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:7196,Deployability,pipeline,pipeline,7196,"(CopyWorkflowOutputsActor.scala:28); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-02-28 08:30:32,176 cromwell-system-akka.dispatchers.engine-dispatcher-28 ERROR - WorkflowManagerActor Workflow bd18e464-59a2-44cf-80c2-b4d93bdfe0ce failed (during FinalizingWorkflowState): software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableSta",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:7320,Deployability,pipeline,pipeline,7320,"invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-02-28 08:30:32,176 cromwell-system-akka.dispatchers.engine-dispatcher-28 ERROR - WorkflowManagerActor Workflow bd18e464-59a2-44cf-80c2-b4d93bdfe0ce failed (during FinalizingWorkflowState): software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:7438,Deployability,pipeline,pipeline,7438,"n(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-02-28 08:30:32,176 cromwell-system-akka.dispatchers.engine-dispatcher-28 ERROR - WorkflowManagerActor Workflow bd18e464-59a2-44cf-80c2-b4d93bdfe0ce failed (during FinalizingWorkflowState): software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:7549,Deployability,pipeline,pipeline,7549,"k.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-02-28 08:30:32,176 cromwell-system-akka.dispatchers.engine-dispatcher-28 ERROR - WorkflowManagerActor Workflow bd18e464-59a2-44cf-80c2-b4d93bdfe0ce failed (during FinalizingWorkflowState): software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:7660,Deployability,pipeline,pipeline,7660,"39); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-02-28 08:30:32,176 cromwell-system-akka.dispatchers.engine-dispatcher-28 ERROR - WorkflowManagerActor Workflow bd18e464-59a2-44cf-80c2-b4d93bdfe0ce failed (during FinalizingWorkflowState): software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(Strea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:7801,Deployability,pipeline,pipeline,7801,"orkerThread.java:107); 2019-02-28 08:30:32,176 cromwell-system-akka.dispatchers.engine-dispatcher-28 ERROR - WorkflowManagerActor Workflow bd18e464-59a2-44cf-80c2-b4d93bdfe0ce failed (during FinalizingWorkflowState): software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:7928,Deployability,pipeline,pipeline,7928,or Workflow bd18e464-59a2-44cf-80c2-b4d93bdfe0ce failed (during FinalizingWorkflowState): software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.ama,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:8055,Deployability,pipeline,pipeline,8055,del.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:8173,Deployability,pipeline,pipeline,8173,n.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.am,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:8289,Deployability,pipeline,pipeline,8289,re.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.j,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:8390,Deployability,pipeline,pipeline,8390,a:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipeli,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:8491,Deployability,pipeline,pipeline,8491,eStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stage,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:8822,Deployability,pipeline,pipeline,8822,ptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImp,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:8954,Deployability,pipeline,pipeline,8954,HandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.ja,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:9077,Deployability,pipeline,pipeline,9077,tage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:9200,Deployability,pipeline,pipeline,9200,RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); 	at software.amazon.awssdk.core.cl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:9341,Deployability,pipeline,pipeline,9341,va:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientH,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:9482,Deployability,pipeline,pipeline,9482,http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.listBuckets(DefaultS3Client.java:1749); 	at software.amazon.awssdk.se,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:9633,Deployability,pipeline,pipeline,9633,mManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.listBuckets(DefaultS3Client.java:1749); 	at software.amazon.awssdk.services.s3.S3Client.listBuckets(S3Client.java:2184); 	at org.lerch.s3fs.S3FileStore.getBucket(S3FileStore.java:93); 	at org.lerch.s3fs.S3FileStore.getBu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:36,Modifiability,config,configuration,36,"I have a working cromwell/AWS batch configuration.; I have a simple workflow called three_task_sequence.wdl which I am able to run on AWS backend, and see the outputs in s3. However, submitting this job to my cromwell server:; `curl -X POST ""http://172.20.1.67:8001/api/workflows/v1"" -H ""accept: application/json"" -F ""workflowSource=@three_task_sequence.wdl"" -F ""workflowOptions=@workflow_options.json""; `; Where workflow_options.json content is:; ```; {; ""final_workflow_outputs_dir"": ""s3://nrglab-cromwell-genomics/cromwell-execution/out_bin_test""; }. ```. I'm getting the following error at the end of the workflow cromwell log:. ````; 2019-02-28 08:30:32,167 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:5798,Performance,perform,performActionThenRespond,5798,mwell.core.path.BetterFileMethods.createDirectories(BetterFileMethods.scala:99); 	at cromwell.core.path.BetterFileMethods.createDirectories$(BetterFileMethods.scala:98); 	at cromwell.filesystems.s3.S3Path.createDirectories(S3PathBuilder.scala:156); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowOutputsActor.$anonfun$copyWorkflowOutputs$1(CopyWorkflowOutputsActor.scala:56); 	at scala.collection.immutable.List.map(List.scala:283); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowOutputsActor.copyWorkflowOutputs(CopyWorkflowOutputsActor.scala:54); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowOutputsActor.afterAll(CopyWorkflowOutputsActor.scala:108); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowOutputsActor$$anonfun$receive$1.$anonfun$applyOrElse$1(CopyWorkflowOutputsActor.scala:36); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowOutputsActor.cromwell$engine$workflow$lifecycle$finalization$CopyWorkflowOutputsActor$$performActionThenRespond(CopyWorkflowOutputsActor.scala:43); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowOutputsActor$$anonfun$receive$1.applyOrElse(CopyWorkflowOutputsActor.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowOutputsActor.aroundReceive(CopyWorkflowOutputsActor.scala:28); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerT,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:12012,Performance,perform,performActionThenRespond,12012,mwell.core.path.BetterFileMethods.createDirectories(BetterFileMethods.scala:99); 	at cromwell.core.path.BetterFileMethods.createDirectories$(BetterFileMethods.scala:98); 	at cromwell.filesystems.s3.S3Path.createDirectories(S3PathBuilder.scala:156); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowOutputsActor.$anonfun$copyWorkflowOutputs$1(CopyWorkflowOutputsActor.scala:56); 	at scala.collection.immutable.List.map(List.scala:283); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowOutputsActor.copyWorkflowOutputs(CopyWorkflowOutputsActor.scala:54); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowOutputsActor.afterAll(CopyWorkflowOutputsActor.scala:108); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowOutputsActor$$anonfun$receive$1.$anonfun$applyOrElse$1(CopyWorkflowOutputsActor.scala:36); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowOutputsActor.cromwell$engine$workflow$lifecycle$finalization$CopyWorkflowOutputsActor$$performActionThenRespond(CopyWorkflowOutputsActor.scala:43); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowOutputsActor$$anonfun$receive$1.applyOrElse(CopyWorkflowOutputsActor.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowOutputsActor.aroundReceive(CopyWorkflowOutputsActor.scala:28); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerT,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:725,Security,Access,Access,725,"I have a working cromwell/AWS batch configuration.; I have a simple workflow called three_task_sequence.wdl which I am able to run on AWS backend, and see the outputs in s3. However, submitting this job to my cromwell server:; `curl -X POST ""http://172.20.1.67:8001/api/workflows/v1"" -H ""accept: application/json"" -F ""workflowSource=@three_task_sequence.wdl"" -F ""workflowOptions=@workflow_options.json""; `; Where workflow_options.json content is:; ```; {; ""final_workflow_outputs_dir"": ""s3://nrglab-cromwell-genomics/cromwell-execution/out_bin_test""; }. ```. I'm getting the following error at the end of the workflow cromwell log:. ````; 2019-02-28 08:30:32,167 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:862,Security,Access,Access,862,"I have a working cromwell/AWS batch configuration.; I have a simple workflow called three_task_sequence.wdl which I am able to run on AWS backend, and see the outputs in s3. However, submitting this job to my cromwell server:; `curl -X POST ""http://172.20.1.67:8001/api/workflows/v1"" -H ""accept: application/json"" -F ""workflowSource=@three_task_sequence.wdl"" -F ""workflowOptions=@workflow_options.json""; `; Where workflow_options.json content is:; ```; {; ""final_workflow_outputs_dir"": ""s3://nrglab-cromwell-genomics/cromwell-execution/out_bin_test""; }. ```. I'm getting the following error at the end of the workflow cromwell log:. ````; 2019-02-28 08:30:32,167 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:7076,Security,Access,Access,7076,"mwell.engine.workflow.lifecycle.finalization.CopyWorkflowOutputsActor.aroundReceive(CopyWorkflowOutputsActor.scala:28); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-02-28 08:30:32,176 cromwell-system-akka.dispatchers.engine-dispatcher-28 ERROR - WorkflowManagerActor Workflow bd18e464-59a2-44cf-80c2-b4d93bdfe0ce failed (during FinalizingWorkflowState): software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(Retryabl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:627,Testability,log,log,627,"I have a working cromwell/AWS batch configuration.; I have a simple workflow called three_task_sequence.wdl which I am able to run on AWS backend, and see the outputs in s3. However, submitting this job to my cromwell server:; `curl -X POST ""http://172.20.1.67:8001/api/workflows/v1"" -H ""accept: application/json"" -F ""workflowSource=@three_task_sequence.wdl"" -F ""workflowOptions=@workflow_options.json""; `; Where workflow_options.json content is:; ```; {; ""final_workflow_outputs_dir"": ""s3://nrglab-cromwell-genomics/cromwell-execution/out_bin_test""; }. ```. I'm getting the following error at the end of the workflow cromwell log:. ````; 2019-02-28 08:30:32,167 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4686:61,Usability,simpl,simple,61,"I have a working cromwell/AWS batch configuration.; I have a simple workflow called three_task_sequence.wdl which I am able to run on AWS backend, and see the outputs in s3. However, submitting this job to my cromwell server:; `curl -X POST ""http://172.20.1.67:8001/api/workflows/v1"" -H ""accept: application/json"" -F ""workflowSource=@three_task_sequence.wdl"" -F ""workflowOptions=@workflow_options.json""; `; Where workflow_options.json content is:; ```; {; ""final_workflow_outputs_dir"": ""s3://nrglab-cromwell-genomics/cromwell-execution/out_bin_test""; }. ```. I'm getting the following error at the end of the workflow cromwell log:. ````; 2019-02-28 08:30:32,167 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686
https://github.com/broadinstitute/cromwell/issues/4687:70,Availability,error,error,70,"While testing cromwell-36 with AWS batch I was able to reproduce this error:. ```; 2019-02-25 09:38:52,508 cromwell-system-akka.dispatchers.engine-dispatcher-24 ERROR - WorkflowManagerActor Workflow b6b9322c-3929-4b72-9598-45d97dfb858d failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'print_nach_nachman_meuman.out': [Attempted 1 time(s)] - IOException: Could not read from s3://nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log: Cannot access file: s3://s3.amazonaws.com/nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:867); ```. The error occurs when running many sub-workflows within a single wrapping workflow.; The environment is configured correctly, and the test usually passes when running <30 subworkflows. Here are the workflows:. run_multiple_test.wdl; ```; import ""three_task_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687
https://github.com/broadinstitute/cromwell/issues/4687:161,Availability,ERROR,ERROR,161,"While testing cromwell-36 with AWS batch I was able to reproduce this error:. ```; 2019-02-25 09:38:52,508 cromwell-system-akka.dispatchers.engine-dispatcher-24 ERROR - WorkflowManagerActor Workflow b6b9322c-3929-4b72-9598-45d97dfb858d failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'print_nach_nachman_meuman.out': [Attempted 1 time(s)] - IOException: Could not read from s3://nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log: Cannot access file: s3://s3.amazonaws.com/nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:867); ```. The error occurs when running many sub-workflows within a single wrapping workflow.; The environment is configured correctly, and the test usually passes when running <30 subworkflows. Here are the workflows:. run_multiple_test.wdl; ```; import ""three_task_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687
https://github.com/broadinstitute/cromwell/issues/4687:1206,Availability,error,error,1206,"(during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'print_nach_nachman_meuman.out': [Attempted 1 time(s)] - IOException: Could not read from s3://nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log: Cannot access file: s3://s3.amazonaws.com/nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:867); ```. The error occurs when running many sub-workflows within a single wrapping workflow.; The environment is configured correctly, and the test usually passes when running <30 subworkflows. Here are the workflows:. run_multiple_test.wdl; ```; import ""three_task_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker: ""ubuntu:latest""; 	 maxRetries: 3; }; }. task print_nach_nachman{; Array[String] previous. command{; echo ${sep=' ' previous} "" nachman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687
https://github.com/broadinstitute/cromwell/issues/4687:1912,Availability,echo,echo,1912,"ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:867); ```. The error occurs when running many sub-workflows within a single wrapping workflow.; The environment is configured correctly, and the test usually passes when running <30 subworkflows. Here are the workflows:. run_multiple_test.wdl; ```; import ""three_task_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker: ""ubuntu:latest""; 	 maxRetries: 3; }; }. task print_nach_nachman{; Array[String] previous. command{; echo ${sep=' ' previous} "" nachman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; ; }. task print_nach_nachman_meuman{; Array[String] previous. command{; echo ${sep=' ' previous} "" meuman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; }; ```. Here is the cromwell-conf:; ```; // aws.conf; include required(classpath(""application"")). webservice {; port = 8001; interface = 0.0.0.0; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""us-east-1""; }. engine {; filesystems {; s3 { auth = ""default"" }; }; }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; root = ""s3://nrglab-c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687
https://github.com/broadinstitute/cromwell/issues/4687:2103,Availability,echo,echo,2103,"cutionActor.scala:867); ```. The error occurs when running many sub-workflows within a single wrapping workflow.; The environment is configured correctly, and the test usually passes when running <30 subworkflows. Here are the workflows:. run_multiple_test.wdl; ```; import ""three_task_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker: ""ubuntu:latest""; 	 maxRetries: 3; }; }. task print_nach_nachman{; Array[String] previous. command{; echo ${sep=' ' previous} "" nachman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; ; }. task print_nach_nachman_meuman{; Array[String] previous. command{; echo ${sep=' ' previous} "" meuman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; }; ```. Here is the cromwell-conf:; ```; // aws.conf; include required(classpath(""application"")). webservice {; port = 8001; interface = 0.0.0.0; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""us-east-1""; }. engine {; filesystems {; s3 { auth = ""default"" }; }; }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; root = ""s3://nrglab-cromwell-genomics/cromwell-execution""; auth = ""default"". numSubmitAttempts = 3; numCreateDefinitionAttempts = 3. concurrent-job-limit = 100. default-runtime-attributes {; queueArn: ""arn:aws:batch:us-east-1:66:job",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687
https://github.com/broadinstitute/cromwell/issues/4687:2323,Availability,echo,echo,2323,"e the workflows:. run_multiple_test.wdl; ```; import ""three_task_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker: ""ubuntu:latest""; 	 maxRetries: 3; }; }. task print_nach_nachman{; Array[String] previous. command{; echo ${sep=' ' previous} "" nachman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; ; }. task print_nach_nachman_meuman{; Array[String] previous. command{; echo ${sep=' ' previous} "" meuman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; }; ```. Here is the cromwell-conf:; ```; // aws.conf; include required(classpath(""application"")). webservice {; port = 8001; interface = 0.0.0.0; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""us-east-1""; }. engine {; filesystems {; s3 { auth = ""default"" }; }; }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; root = ""s3://nrglab-cromwell-genomics/cromwell-execution""; auth = ""default"". numSubmitAttempts = 3; numCreateDefinitionAttempts = 3. concurrent-job-limit = 100. default-runtime-attributes {; queueArn: ""arn:aws:batch:us-east-1:66:job-queue/GenomicsDefaultQueue""; }. filesystems {; s3 {; auth = ""default""; }; }; }; }; }; }. system {; job-rate-control {; jobs = 1; per = 1 second; }; }; ```. Would appreciate help on this.; I wonder if cromwell was ever te",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687
https://github.com/broadinstitute/cromwell/issues/4687:1267,Integrability,wrap,wrapping,1267,"(during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'print_nach_nachman_meuman.out': [Attempted 1 time(s)] - IOException: Could not read from s3://nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log: Cannot access file: s3://s3.amazonaws.com/nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:867); ```. The error occurs when running many sub-workflows within a single wrapping workflow.; The environment is configured correctly, and the test usually passes when running <30 subworkflows. Here are the workflows:. run_multiple_test.wdl; ```; import ""three_task_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker: ""ubuntu:latest""; 	 maxRetries: 3; }; }. task print_nach_nachman{; Array[String] previous. command{; echo ${sep=' ' previous} "" nachman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687
https://github.com/broadinstitute/cromwell/issues/4687:2595,Integrability,interface,interface,2595,"sk_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker: ""ubuntu:latest""; 	 maxRetries: 3; }; }. task print_nach_nachman{; Array[String] previous. command{; echo ${sep=' ' previous} "" nachman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; ; }. task print_nach_nachman_meuman{; Array[String] previous. command{; echo ${sep=' ' previous} "" meuman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; }; ```. Here is the cromwell-conf:; ```; // aws.conf; include required(classpath(""application"")). webservice {; port = 8001; interface = 0.0.0.0; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""us-east-1""; }. engine {; filesystems {; s3 { auth = ""default"" }; }; }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; root = ""s3://nrglab-cromwell-genomics/cromwell-execution""; auth = ""default"". numSubmitAttempts = 3; numCreateDefinitionAttempts = 3. concurrent-job-limit = 100. default-runtime-attributes {; queueArn: ""arn:aws:batch:us-east-1:66:job-queue/GenomicsDefaultQueue""; }. filesystems {; s3 {; auth = ""default""; }; }; }; }; }; }. system {; job-rate-control {; jobs = 1; per = 1 second; }; }; ```. Would appreciate help on this.; I wonder if cromwell was ever tested for many parallel sub-workflows running on AWS?. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687
https://github.com/broadinstitute/cromwell/issues/4687:1306,Modifiability,config,configured,1306,"$2: Failed to evaluate job outputs:; Bad output 'print_nach_nachman_meuman.out': [Attempted 1 time(s)] - IOException: Could not read from s3://nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log: Cannot access file: s3://s3.amazonaws.com/nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:867); ```. The error occurs when running many sub-workflows within a single wrapping workflow.; The environment is configured correctly, and the test usually passes when running <30 subworkflows. Here are the workflows:. run_multiple_test.wdl; ```; import ""three_task_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker: ""ubuntu:latest""; 	 maxRetries: 3; }; }. task print_nach_nachman{; Array[String] previous. command{; echo ${sep=' ' previous} "" nachman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; ; }. task print_nach_nachman_meuman{; Array[String] previous. command{; echo ${sep=' ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687
https://github.com/broadinstitute/cromwell/issues/4687:2932,Modifiability,config,config,2932,"sk_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker: ""ubuntu:latest""; 	 maxRetries: 3; }; }. task print_nach_nachman{; Array[String] previous. command{; echo ${sep=' ' previous} "" nachman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; ; }. task print_nach_nachman_meuman{; Array[String] previous. command{; echo ${sep=' ' previous} "" meuman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; }; ```. Here is the cromwell-conf:; ```; // aws.conf; include required(classpath(""application"")). webservice {; port = 8001; interface = 0.0.0.0; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""us-east-1""; }. engine {; filesystems {; s3 { auth = ""default"" }; }; }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; root = ""s3://nrglab-cromwell-genomics/cromwell-execution""; auth = ""default"". numSubmitAttempts = 3; numCreateDefinitionAttempts = 3. concurrent-job-limit = 100. default-runtime-attributes {; queueArn: ""arn:aws:batch:us-east-1:66:job-queue/GenomicsDefaultQueue""; }. filesystems {; s3 {; auth = ""default""; }; }; }; }; }; }. system {; job-rate-control {; jobs = 1; per = 1 second; }; }; ```. Would appreciate help on this.; I wonder if cromwell was ever tested for many parallel sub-workflows running on AWS?. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687
https://github.com/broadinstitute/cromwell/issues/4687:3075,Performance,concurren,concurrent-job-limit,3075,"sk_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker: ""ubuntu:latest""; 	 maxRetries: 3; }; }. task print_nach_nachman{; Array[String] previous. command{; echo ${sep=' ' previous} "" nachman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; ; }. task print_nach_nachman_meuman{; Array[String] previous. command{; echo ${sep=' ' previous} "" meuman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; }; ```. Here is the cromwell-conf:; ```; // aws.conf; include required(classpath(""application"")). webservice {; port = 8001; interface = 0.0.0.0; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""us-east-1""; }. engine {; filesystems {; s3 { auth = ""default"" }; }; }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; root = ""s3://nrglab-cromwell-genomics/cromwell-execution""; auth = ""default"". numSubmitAttempts = 3; numCreateDefinitionAttempts = 3. concurrent-job-limit = 100. default-runtime-attributes {; queueArn: ""arn:aws:batch:us-east-1:66:job-queue/GenomicsDefaultQueue""; }. filesystems {; s3 {; auth = ""default""; }; }; }; }; }; }. system {; job-rate-control {; jobs = 1; per = 1 second; }; }; ```. Would appreciate help on this.; I wonder if cromwell was ever tested for many parallel sub-workflows running on AWS?. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687
https://github.com/broadinstitute/cromwell/issues/4687:3133,Performance,queue,queueArn,3133,"sk_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker: ""ubuntu:latest""; 	 maxRetries: 3; }; }. task print_nach_nachman{; Array[String] previous. command{; echo ${sep=' ' previous} "" nachman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; ; }. task print_nach_nachman_meuman{; Array[String] previous. command{; echo ${sep=' ' previous} "" meuman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; }; ```. Here is the cromwell-conf:; ```; // aws.conf; include required(classpath(""application"")). webservice {; port = 8001; interface = 0.0.0.0; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""us-east-1""; }. engine {; filesystems {; s3 { auth = ""default"" }; }; }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; root = ""s3://nrglab-cromwell-genomics/cromwell-execution""; auth = ""default"". numSubmitAttempts = 3; numCreateDefinitionAttempts = 3. concurrent-job-limit = 100. default-runtime-attributes {; queueArn: ""arn:aws:batch:us-east-1:66:job-queue/GenomicsDefaultQueue""; }. filesystems {; s3 {; auth = ""default""; }; }; }; }; }; }. system {; job-rate-control {; jobs = 1; per = 1 second; }; }; ```. Would appreciate help on this.; I wonder if cromwell was ever tested for many parallel sub-workflows running on AWS?. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687
https://github.com/broadinstitute/cromwell/issues/4687:3175,Performance,queue,queue,3175,"sk_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker: ""ubuntu:latest""; 	 maxRetries: 3; }; }. task print_nach_nachman{; Array[String] previous. command{; echo ${sep=' ' previous} "" nachman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; ; }. task print_nach_nachman_meuman{; Array[String] previous. command{; echo ${sep=' ' previous} "" meuman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; }; ```. Here is the cromwell-conf:; ```; // aws.conf; include required(classpath(""application"")). webservice {; port = 8001; interface = 0.0.0.0; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""us-east-1""; }. engine {; filesystems {; s3 { auth = ""default"" }; }; }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; root = ""s3://nrglab-cromwell-genomics/cromwell-execution""; auth = ""default"". numSubmitAttempts = 3; numCreateDefinitionAttempts = 3. concurrent-job-limit = 100. default-runtime-attributes {; queueArn: ""arn:aws:batch:us-east-1:66:job-queue/GenomicsDefaultQueue""; }. filesystems {; s3 {; auth = ""default""; }; }; }; }; }; }. system {; job-rate-control {; jobs = 1; per = 1 second; }; }; ```. Would appreciate help on this.; I wonder if cromwell was ever tested for many parallel sub-workflows running on AWS?. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687
https://github.com/broadinstitute/cromwell/issues/4687:758,Security,access,access,758,"While testing cromwell-36 with AWS batch I was able to reproduce this error:. ```; 2019-02-25 09:38:52,508 cromwell-system-akka.dispatchers.engine-dispatcher-24 ERROR - WorkflowManagerActor Workflow b6b9322c-3929-4b72-9598-45d97dfb858d failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'print_nach_nachman_meuman.out': [Attempted 1 time(s)] - IOException: Could not read from s3://nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log: Cannot access file: s3://s3.amazonaws.com/nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:867); ```. The error occurs when running many sub-workflows within a single wrapping workflow.; The environment is configured correctly, and the test usually passes when running <30 subworkflows. Here are the workflows:. run_multiple_test.wdl; ```; import ""three_task_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687
https://github.com/broadinstitute/cromwell/issues/4687:6,Testability,test,testing,6,"While testing cromwell-36 with AWS batch I was able to reproduce this error:. ```; 2019-02-25 09:38:52,508 cromwell-system-akka.dispatchers.engine-dispatcher-24 ERROR - WorkflowManagerActor Workflow b6b9322c-3929-4b72-9598-45d97dfb858d failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'print_nach_nachman_meuman.out': [Attempted 1 time(s)] - IOException: Could not read from s3://nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log: Cannot access file: s3://s3.amazonaws.com/nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:867); ```. The error occurs when running many sub-workflows within a single wrapping workflow.; The environment is configured correctly, and the test usually passes when running <30 subworkflows. Here are the workflows:. run_multiple_test.wdl; ```; import ""three_task_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687
https://github.com/broadinstitute/cromwell/issues/4687:746,Testability,log,log,746,"While testing cromwell-36 with AWS batch I was able to reproduce this error:. ```; 2019-02-25 09:38:52,508 cromwell-system-akka.dispatchers.engine-dispatcher-24 ERROR - WorkflowManagerActor Workflow b6b9322c-3929-4b72-9598-45d97dfb858d failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'print_nach_nachman_meuman.out': [Attempted 1 time(s)] - IOException: Could not read from s3://nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log: Cannot access file: s3://s3.amazonaws.com/nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:867); ```. The error occurs when running many sub-workflows within a single wrapping workflow.; The environment is configured correctly, and the test usually passes when running <30 subworkflows. Here are the workflows:. run_multiple_test.wdl; ```; import ""three_task_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687
https://github.com/broadinstitute/cromwell/issues/4687:1061,Testability,log,log,1061," error:. ```; 2019-02-25 09:38:52,508 cromwell-system-akka.dispatchers.engine-dispatcher-24 ERROR - WorkflowManagerActor Workflow b6b9322c-3929-4b72-9598-45d97dfb858d failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'print_nach_nachman_meuman.out': [Attempted 1 time(s)] - IOException: Could not read from s3://nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log: Cannot access file: s3://s3.amazonaws.com/nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:867); ```. The error occurs when running many sub-workflows within a single wrapping workflow.; The environment is configured correctly, and the test usually passes when running <30 subworkflows. Here are the workflows:. run_multiple_test.wdl; ```; import ""three_task_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker: ""ubuntu:latest""; 	 maxRetries: 3; }; }. task print_nach_nachman{; A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687
https://github.com/broadinstitute/cromwell/issues/4687:1336,Testability,test,test,1336,"$2: Failed to evaluate job outputs:; Bad output 'print_nach_nachman_meuman.out': [Attempted 1 time(s)] - IOException: Could not read from s3://nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log: Cannot access file: s3://s3.amazonaws.com/nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:867); ```. The error occurs when running many sub-workflows within a single wrapping workflow.; The environment is configured correctly, and the test usually passes when running <30 subworkflows. Here are the workflows:. run_multiple_test.wdl; ```; import ""three_task_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker: ""ubuntu:latest""; 	 maxRetries: 3; }; }. task print_nach_nachman{; Array[String] previous. command{; echo ${sep=' ' previous} "" nachman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; ; }. task print_nach_nachman_meuman{; Array[String] previous. command{; echo ${sep=' ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687
https://github.com/broadinstitute/cromwell/issues/4687:3393,Testability,test,tested,3393,"sk_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker: ""ubuntu:latest""; 	 maxRetries: 3; }; }. task print_nach_nachman{; Array[String] previous. command{; echo ${sep=' ' previous} "" nachman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; ; }. task print_nach_nachman_meuman{; Array[String] previous. command{; echo ${sep=' ' previous} "" meuman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; }; ```. Here is the cromwell-conf:; ```; // aws.conf; include required(classpath(""application"")). webservice {; port = 8001; interface = 0.0.0.0; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""us-east-1""; }. engine {; filesystems {; s3 { auth = ""default"" }; }; }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; root = ""s3://nrglab-cromwell-genomics/cromwell-execution""; auth = ""default"". numSubmitAttempts = 3; numCreateDefinitionAttempts = 3. concurrent-job-limit = 100. default-runtime-attributes {; queueArn: ""arn:aws:batch:us-east-1:66:job-queue/GenomicsDefaultQueue""; }. filesystems {; s3 {; auth = ""default""; }; }; }; }; }; }. system {; job-rate-control {; jobs = 1; per = 1 second; }; }; ```. Would appreciate help on this.; I wonder if cromwell was ever tested for many parallel sub-workflows running on AWS?. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687
https://github.com/broadinstitute/cromwell/issues/4688:1194,Availability,error,error,1194,"Hi Cromwell Team; ; I am writing in respect to an issue that I am having with using AWS + Cromwell + MySQL server. Not sure if this is the best place to ask because I’m not sure if the issue is Cromwell specific. It might be related to AWS backend configuration. But I couldn’t figure out the problem and I figured you might be able to provide some insight.; ; I have a docker image that has MySQL server installed. I’m using percorna-server-5.6 specifically because I need to use MySQL5.6. There is a Cromwell task which does the following:; 1. Start mysql server. The first line of the WDL task is literally `service mysql start`; 2. Initialize the database and load Vcf files into the database.; 3. Run some SQL query and perform some analysis; ; And the above Cromwell task need to be run for multiple samples. ; ; So, first I did step 1-3 manually without using WDL just to make sure that I can run multiple MySQL docker container just fine. And it worked. So then the next step is test the whole WDL using LOCAL backend. And it ran fine. But when I submit the same WDL script to AWS Batch, the first task will always succeed and the subsequent tasks will always fail with port connection error because all the containers are connecting to port 3306 and port 3306 is already used. Do you know why it is trying to connect to port 3306? My issue was that it worked when running locally. So, I’m wondering if there’s something with how the docker run command was submitted to AWS Batch or EC2 is configured?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4688
https://github.com/broadinstitute/cromwell/issues/4688:248,Deployability,configurat,configuration,248,"Hi Cromwell Team; ; I am writing in respect to an issue that I am having with using AWS + Cromwell + MySQL server. Not sure if this is the best place to ask because I’m not sure if the issue is Cromwell specific. It might be related to AWS backend configuration. But I couldn’t figure out the problem and I figured you might be able to provide some insight.; ; I have a docker image that has MySQL server installed. I’m using percorna-server-5.6 specifically because I need to use MySQL5.6. There is a Cromwell task which does the following:; 1. Start mysql server. The first line of the WDL task is literally `service mysql start`; 2. Initialize the database and load Vcf files into the database.; 3. Run some SQL query and perform some analysis; ; And the above Cromwell task need to be run for multiple samples. ; ; So, first I did step 1-3 manually without using WDL just to make sure that I can run multiple MySQL docker container just fine. And it worked. So then the next step is test the whole WDL using LOCAL backend. And it ran fine. But when I submit the same WDL script to AWS Batch, the first task will always succeed and the subsequent tasks will always fail with port connection error because all the containers are connecting to port 3306 and port 3306 is already used. Do you know why it is trying to connect to port 3306? My issue was that it worked when running locally. So, I’m wondering if there’s something with how the docker run command was submitted to AWS Batch or EC2 is configured?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4688
https://github.com/broadinstitute/cromwell/issues/4688:405,Deployability,install,installed,405,"Hi Cromwell Team; ; I am writing in respect to an issue that I am having with using AWS + Cromwell + MySQL server. Not sure if this is the best place to ask because I’m not sure if the issue is Cromwell specific. It might be related to AWS backend configuration. But I couldn’t figure out the problem and I figured you might be able to provide some insight.; ; I have a docker image that has MySQL server installed. I’m using percorna-server-5.6 specifically because I need to use MySQL5.6. There is a Cromwell task which does the following:; 1. Start mysql server. The first line of the WDL task is literally `service mysql start`; 2. Initialize the database and load Vcf files into the database.; 3. Run some SQL query and perform some analysis; ; And the above Cromwell task need to be run for multiple samples. ; ; So, first I did step 1-3 manually without using WDL just to make sure that I can run multiple MySQL docker container just fine. And it worked. So then the next step is test the whole WDL using LOCAL backend. And it ran fine. But when I submit the same WDL script to AWS Batch, the first task will always succeed and the subsequent tasks will always fail with port connection error because all the containers are connecting to port 3306 and port 3306 is already used. Do you know why it is trying to connect to port 3306? My issue was that it worked when running locally. So, I’m wondering if there’s something with how the docker run command was submitted to AWS Batch or EC2 is configured?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4688
https://github.com/broadinstitute/cromwell/issues/4688:248,Modifiability,config,configuration,248,"Hi Cromwell Team; ; I am writing in respect to an issue that I am having with using AWS + Cromwell + MySQL server. Not sure if this is the best place to ask because I’m not sure if the issue is Cromwell specific. It might be related to AWS backend configuration. But I couldn’t figure out the problem and I figured you might be able to provide some insight.; ; I have a docker image that has MySQL server installed. I’m using percorna-server-5.6 specifically because I need to use MySQL5.6. There is a Cromwell task which does the following:; 1. Start mysql server. The first line of the WDL task is literally `service mysql start`; 2. Initialize the database and load Vcf files into the database.; 3. Run some SQL query and perform some analysis; ; And the above Cromwell task need to be run for multiple samples. ; ; So, first I did step 1-3 manually without using WDL just to make sure that I can run multiple MySQL docker container just fine. And it worked. So then the next step is test the whole WDL using LOCAL backend. And it ran fine. But when I submit the same WDL script to AWS Batch, the first task will always succeed and the subsequent tasks will always fail with port connection error because all the containers are connecting to port 3306 and port 3306 is already used. Do you know why it is trying to connect to port 3306? My issue was that it worked when running locally. So, I’m wondering if there’s something with how the docker run command was submitted to AWS Batch or EC2 is configured?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4688
https://github.com/broadinstitute/cromwell/issues/4688:1498,Modifiability,config,configured,1498,"Hi Cromwell Team; ; I am writing in respect to an issue that I am having with using AWS + Cromwell + MySQL server. Not sure if this is the best place to ask because I’m not sure if the issue is Cromwell specific. It might be related to AWS backend configuration. But I couldn’t figure out the problem and I figured you might be able to provide some insight.; ; I have a docker image that has MySQL server installed. I’m using percorna-server-5.6 specifically because I need to use MySQL5.6. There is a Cromwell task which does the following:; 1. Start mysql server. The first line of the WDL task is literally `service mysql start`; 2. Initialize the database and load Vcf files into the database.; 3. Run some SQL query and perform some analysis; ; And the above Cromwell task need to be run for multiple samples. ; ; So, first I did step 1-3 manually without using WDL just to make sure that I can run multiple MySQL docker container just fine. And it worked. So then the next step is test the whole WDL using LOCAL backend. And it ran fine. But when I submit the same WDL script to AWS Batch, the first task will always succeed and the subsequent tasks will always fail with port connection error because all the containers are connecting to port 3306 and port 3306 is already used. Do you know why it is trying to connect to port 3306? My issue was that it worked when running locally. So, I’m wondering if there’s something with how the docker run command was submitted to AWS Batch or EC2 is configured?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4688
https://github.com/broadinstitute/cromwell/issues/4688:664,Performance,load,load,664,"Hi Cromwell Team; ; I am writing in respect to an issue that I am having with using AWS + Cromwell + MySQL server. Not sure if this is the best place to ask because I’m not sure if the issue is Cromwell specific. It might be related to AWS backend configuration. But I couldn’t figure out the problem and I figured you might be able to provide some insight.; ; I have a docker image that has MySQL server installed. I’m using percorna-server-5.6 specifically because I need to use MySQL5.6. There is a Cromwell task which does the following:; 1. Start mysql server. The first line of the WDL task is literally `service mysql start`; 2. Initialize the database and load Vcf files into the database.; 3. Run some SQL query and perform some analysis; ; And the above Cromwell task need to be run for multiple samples. ; ; So, first I did step 1-3 manually without using WDL just to make sure that I can run multiple MySQL docker container just fine. And it worked. So then the next step is test the whole WDL using LOCAL backend. And it ran fine. But when I submit the same WDL script to AWS Batch, the first task will always succeed and the subsequent tasks will always fail with port connection error because all the containers are connecting to port 3306 and port 3306 is already used. Do you know why it is trying to connect to port 3306? My issue was that it worked when running locally. So, I’m wondering if there’s something with how the docker run command was submitted to AWS Batch or EC2 is configured?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4688
https://github.com/broadinstitute/cromwell/issues/4688:725,Performance,perform,perform,725,"Hi Cromwell Team; ; I am writing in respect to an issue that I am having with using AWS + Cromwell + MySQL server. Not sure if this is the best place to ask because I’m not sure if the issue is Cromwell specific. It might be related to AWS backend configuration. But I couldn’t figure out the problem and I figured you might be able to provide some insight.; ; I have a docker image that has MySQL server installed. I’m using percorna-server-5.6 specifically because I need to use MySQL5.6. There is a Cromwell task which does the following:; 1. Start mysql server. The first line of the WDL task is literally `service mysql start`; 2. Initialize the database and load Vcf files into the database.; 3. Run some SQL query and perform some analysis; ; And the above Cromwell task need to be run for multiple samples. ; ; So, first I did step 1-3 manually without using WDL just to make sure that I can run multiple MySQL docker container just fine. And it worked. So then the next step is test the whole WDL using LOCAL backend. And it ran fine. But when I submit the same WDL script to AWS Batch, the first task will always succeed and the subsequent tasks will always fail with port connection error because all the containers are connecting to port 3306 and port 3306 is already used. Do you know why it is trying to connect to port 3306? My issue was that it worked when running locally. So, I’m wondering if there’s something with how the docker run command was submitted to AWS Batch or EC2 is configured?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4688
https://github.com/broadinstitute/cromwell/issues/4688:987,Testability,test,test,987,"Hi Cromwell Team; ; I am writing in respect to an issue that I am having with using AWS + Cromwell + MySQL server. Not sure if this is the best place to ask because I’m not sure if the issue is Cromwell specific. It might be related to AWS backend configuration. But I couldn’t figure out the problem and I figured you might be able to provide some insight.; ; I have a docker image that has MySQL server installed. I’m using percorna-server-5.6 specifically because I need to use MySQL5.6. There is a Cromwell task which does the following:; 1. Start mysql server. The first line of the WDL task is literally `service mysql start`; 2. Initialize the database and load Vcf files into the database.; 3. Run some SQL query and perform some analysis; ; And the above Cromwell task need to be run for multiple samples. ; ; So, first I did step 1-3 manually without using WDL just to make sure that I can run multiple MySQL docker container just fine. And it worked. So then the next step is test the whole WDL using LOCAL backend. And it ran fine. But when I submit the same WDL script to AWS Batch, the first task will always succeed and the subsequent tasks will always fail with port connection error because all the containers are connecting to port 3306 and port 3306 is already used. Do you know why it is trying to connect to port 3306? My issue was that it worked when running locally. So, I’m wondering if there’s something with how the docker run command was submitted to AWS Batch or EC2 is configured?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4688
https://github.com/broadinstitute/cromwell/issues/4689:498,Availability,error,error,498,"![screenshot-2019-2-28 kibana](https://user-images.githubusercontent.com/1087943/53603858-d5c7bb00-3b80-11e9-9330-a9ac9f9032dc.png). [Kibana link](https://kibana.logit.io/app/kibana#/discover?_g=(refreshInterval:(display:Off,pause:!f,value:0),time:(from:now-90d,mode:quick,to:now))&_a=(columns:!(_source),index:'*-*',interval:auto,query:(query_string:(analyze_wildcard:!t,query:'host:%22gce-cromwell-prod601%22%20AND%20%22Communications%20link%20failure%22')),sort:!('@timestamp',desc))). The same error message showed up in #4360, #3387, and #2519 but in those the ""last packet"" time was short and more or less random, while here it's repeatedly 929,284 milliseconds - or precisely 15 minutes, 30 seconds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4689
https://github.com/broadinstitute/cromwell/issues/4689:504,Integrability,message,message,504,"![screenshot-2019-2-28 kibana](https://user-images.githubusercontent.com/1087943/53603858-d5c7bb00-3b80-11e9-9330-a9ac9f9032dc.png). [Kibana link](https://kibana.logit.io/app/kibana#/discover?_g=(refreshInterval:(display:Off,pause:!f,value:0),time:(from:now-90d,mode:quick,to:now))&_a=(columns:!(_source),index:'*-*',interval:auto,query:(query_string:(analyze_wildcard:!t,query:'host:%22gce-cromwell-prod601%22%20AND%20%22Communications%20link%20failure%22')),sort:!('@timestamp',desc))). The same error message showed up in #4360, #3387, and #2519 but in those the ""last packet"" time was short and more or less random, while here it's repeatedly 929,284 milliseconds - or precisely 15 minutes, 30 seconds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4689
https://github.com/broadinstitute/cromwell/issues/4689:162,Testability,log,logit,162,"![screenshot-2019-2-28 kibana](https://user-images.githubusercontent.com/1087943/53603858-d5c7bb00-3b80-11e9-9330-a9ac9f9032dc.png). [Kibana link](https://kibana.logit.io/app/kibana#/discover?_g=(refreshInterval:(display:Off,pause:!f,value:0),time:(from:now-90d,mode:quick,to:now))&_a=(columns:!(_source),index:'*-*',interval:auto,query:(query_string:(analyze_wildcard:!t,query:'host:%22gce-cromwell-prod601%22%20AND%20%22Communications%20link%20failure%22')),sort:!('@timestamp',desc))). The same error message showed up in #4360, #3387, and #2519 but in those the ""last packet"" time was short and more or less random, while here it's repeatedly 929,284 milliseconds - or precisely 15 minutes, 30 seconds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4689
https://github.com/broadinstitute/cromwell/issues/4689:225,Usability,pause,pause,225,"![screenshot-2019-2-28 kibana](https://user-images.githubusercontent.com/1087943/53603858-d5c7bb00-3b80-11e9-9330-a9ac9f9032dc.png). [Kibana link](https://kibana.logit.io/app/kibana#/discover?_g=(refreshInterval:(display:Off,pause:!f,value:0),time:(from:now-90d,mode:quick,to:now))&_a=(columns:!(_source),index:'*-*',interval:auto,query:(query_string:(analyze_wildcard:!t,query:'host:%22gce-cromwell-prod601%22%20AND%20%22Communications%20link%20failure%22')),sort:!('@timestamp',desc))). The same error message showed up in #4360, #3387, and #2519 but in those the ""last packet"" time was short and more or less random, while here it's repeatedly 929,284 milliseconds - or precisely 15 minutes, 30 seconds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4689
https://github.com/broadinstitute/cromwell/pull/4690:46,Availability,down,downloads,46,"According to [the docs](https://dev.mysql.com/downloads/connector/j/) it is ""highly recommended"" that we upgrade to the latest version, even if our MySQL is behind - `5.6.36-google-log` in our case. Reason I started looking into this was https://github.com/broadinstitute/cromwell/issues/4689",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4690
https://github.com/broadinstitute/cromwell/pull/4690:105,Deployability,upgrade,upgrade,105,"According to [the docs](https://dev.mysql.com/downloads/connector/j/) it is ""highly recommended"" that we upgrade to the latest version, even if our MySQL is behind - `5.6.36-google-log` in our case. Reason I started looking into this was https://github.com/broadinstitute/cromwell/issues/4689",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4690
https://github.com/broadinstitute/cromwell/pull/4690:181,Testability,log,log,181,"According to [the docs](https://dev.mysql.com/downloads/connector/j/) it is ""highly recommended"" that we upgrade to the latest version, even if our MySQL is behind - `5.6.36-google-log` in our case. Reason I started looking into this was https://github.com/broadinstitute/cromwell/issues/4689",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4690
https://github.com/broadinstitute/cromwell/issues/4691:313,Availability,avail,available,313,"Today, when using Cromwell, only 2 GPU types are allowed:. https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L26-L27. However, there's many more gpu types that are actually available:; https://cloud.google.com/compute/docs/gpus/. AC: ; - Cromwell shouldn't whitelist GPU types that are allowed, and instead make it possible to request a GPU machine type of choice.; - If someone asks for a gpu type that's a bad string, record the error returned by Piplines API and add it to our docs to explain that an error that looks like ""..."" means an invalid GPU type, and add a link to the list of available GPU types to the Cromwell docs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4691
https://github.com/broadinstitute/cromwell/issues/4691:571,Availability,error,error,571,"Today, when using Cromwell, only 2 GPU types are allowed:. https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L26-L27. However, there's many more gpu types that are actually available:; https://cloud.google.com/compute/docs/gpus/. AC: ; - Cromwell shouldn't whitelist GPU types that are allowed, and instead make it possible to request a GPU machine type of choice.; - If someone asks for a gpu type that's a bad string, record the error returned by Piplines API and add it to our docs to explain that an error that looks like ""..."" means an invalid GPU type, and add a link to the list of available GPU types to the Cromwell docs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4691
https://github.com/broadinstitute/cromwell/issues/4691:644,Availability,error,error,644,"Today, when using Cromwell, only 2 GPU types are allowed:. https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L26-L27. However, there's many more gpu types that are actually available:; https://cloud.google.com/compute/docs/gpus/. AC: ; - Cromwell shouldn't whitelist GPU types that are allowed, and instead make it possible to request a GPU machine type of choice.; - If someone asks for a gpu type that's a bad string, record the error returned by Piplines API and add it to our docs to explain that an error that looks like ""..."" means an invalid GPU type, and add a link to the list of available GPU types to the Cromwell docs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4691
https://github.com/broadinstitute/cromwell/issues/4691:729,Availability,avail,available,729,"Today, when using Cromwell, only 2 GPU types are allowed:. https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L26-L27. However, there's many more gpu types that are actually available:; https://cloud.google.com/compute/docs/gpus/. AC: ; - Cromwell shouldn't whitelist GPU types that are allowed, and instead make it possible to request a GPU machine type of choice.; - If someone asks for a gpu type that's a bad string, record the error returned by Piplines API and add it to our docs to explain that an error that looks like ""..."" means an invalid GPU type, and add a link to the list of available GPU types to the Cromwell docs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4691
https://github.com/broadinstitute/cromwell/issues/4691:140,Deployability,pipeline,pipelines,140,"Today, when using Cromwell, only 2 GPU types are allowed:. https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L26-L27. However, there's many more gpu types that are actually available:; https://cloud.google.com/compute/docs/gpus/. AC: ; - Cromwell shouldn't whitelist GPU types that are allowed, and instead make it possible to request a GPU machine type of choice.; - If someone asks for a gpu type that's a bad string, record the error returned by Piplines API and add it to our docs to explain that an error that looks like ""..."" means an invalid GPU type, and add a link to the list of available GPU types to the Cromwell docs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4691
https://github.com/broadinstitute/cromwell/issues/4691:196,Deployability,pipeline,pipelines,196,"Today, when using Cromwell, only 2 GPU types are allowed:. https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L26-L27. However, there's many more gpu types that are actually available:; https://cloud.google.com/compute/docs/gpus/. AC: ; - Cromwell shouldn't whitelist GPU types that are allowed, and instead make it possible to request a GPU machine type of choice.; - If someone asks for a gpu type that's a bad string, record the error returned by Piplines API and add it to our docs to explain that an error that looks like ""..."" means an invalid GPU type, and add a link to the list of available GPU types to the Cromwell docs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4691
https://github.com/broadinstitute/cromwell/issues/4691:213,Deployability,Pipeline,PipelinesApiRuntimeAttributes,213,"Today, when using Cromwell, only 2 GPU types are allowed:. https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L26-L27. However, there's many more gpu types that are actually available:; https://cloud.google.com/compute/docs/gpus/. AC: ; - Cromwell shouldn't whitelist GPU types that are allowed, and instead make it possible to request a GPU machine type of choice.; - If someone asks for a gpu type that's a bad string, record the error returned by Piplines API and add it to our docs to explain that an error that looks like ""..."" means an invalid GPU type, and add a link to the list of available GPU types to the Cromwell docs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4691
https://github.com/broadinstitute/cromwell/issues/4692:54,Safety,safe,safe,54,Cromwell Version: 37 (I just saw the reversion. is it safe to revert from 37 to 36.1?). It looks like the only labels that are added to the PAPI2 VMs are the `cromwell-id` and the `task-name`. None of the custom labels are being added to the VM. If this is one of the issues that was solved in v36.1 Then I will gladly revert so long as it is safe!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4692
https://github.com/broadinstitute/cromwell/issues/4692:343,Safety,safe,safe,343,Cromwell Version: 37 (I just saw the reversion. is it safe to revert from 37 to 36.1?). It looks like the only labels that are added to the PAPI2 VMs are the `cromwell-id` and the `task-name`. None of the custom labels are being added to the VM. If this is one of the issues that was solved in v36.1 Then I will gladly revert so long as it is safe!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4692
https://github.com/broadinstitute/cromwell/issues/4696:516,Deployability,configurat,configuration,516,"Per discussion in https://github.com/broadinstitute/cromwell/pull/4039, it would be nice and clean to have different backend providers defined in an examples folder. The user:. - should more easily be able to inspect one backend, as it's not lost in a huge commented file; - from each backend example, a link to the docs (if they exist) should be provided; - per the files being separate, we don't ask the user to uncomment a million lines to use a backend (copy pasta, done). But given separation from the examples configuration file, we need to compensate by having really good instructions for doing this (I will write a nice README). I think this is the right way to go because it will more cleanly show the various backends that Cromwell provides, and how to use. Right now it's a bit overwhelming just looking at that file, and I can only imagine for a new user / someone not super keen on configuration files. @geoffjentry please assign me to this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4696
https://github.com/broadinstitute/cromwell/issues/4696:896,Deployability,configurat,configuration,896,"Per discussion in https://github.com/broadinstitute/cromwell/pull/4039, it would be nice and clean to have different backend providers defined in an examples folder. The user:. - should more easily be able to inspect one backend, as it's not lost in a huge commented file; - from each backend example, a link to the docs (if they exist) should be provided; - per the files being separate, we don't ask the user to uncomment a million lines to use a backend (copy pasta, done). But given separation from the examples configuration file, we need to compensate by having really good instructions for doing this (I will write a nice README). I think this is the right way to go because it will more cleanly show the various backends that Cromwell provides, and how to use. Right now it's a bit overwhelming just looking at that file, and I can only imagine for a new user / someone not super keen on configuration files. @geoffjentry please assign me to this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4696
https://github.com/broadinstitute/cromwell/issues/4696:516,Modifiability,config,configuration,516,"Per discussion in https://github.com/broadinstitute/cromwell/pull/4039, it would be nice and clean to have different backend providers defined in an examples folder. The user:. - should more easily be able to inspect one backend, as it's not lost in a huge commented file; - from each backend example, a link to the docs (if they exist) should be provided; - per the files being separate, we don't ask the user to uncomment a million lines to use a backend (copy pasta, done). But given separation from the examples configuration file, we need to compensate by having really good instructions for doing this (I will write a nice README). I think this is the right way to go because it will more cleanly show the various backends that Cromwell provides, and how to use. Right now it's a bit overwhelming just looking at that file, and I can only imagine for a new user / someone not super keen on configuration files. @geoffjentry please assign me to this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4696
https://github.com/broadinstitute/cromwell/issues/4696:896,Modifiability,config,configuration,896,"Per discussion in https://github.com/broadinstitute/cromwell/pull/4039, it would be nice and clean to have different backend providers defined in an examples folder. The user:. - should more easily be able to inspect one backend, as it's not lost in a huge commented file; - from each backend example, a link to the docs (if they exist) should be provided; - per the files being separate, we don't ask the user to uncomment a million lines to use a backend (copy pasta, done). But given separation from the examples configuration file, we need to compensate by having really good instructions for doing this (I will write a nice README). I think this is the right way to go because it will more cleanly show the various backends that Cromwell provides, and how to use. Right now it's a bit overwhelming just looking at that file, and I can only imagine for a new user / someone not super keen on configuration files. @geoffjentry please assign me to this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4696
https://github.com/broadinstitute/cromwell/pull/4697:696,Usability,feedback,feedback,696,"This pull request will add a folder of example backend providers to cromwell, along with a README.md to (verbosely) describe them. Importantly:. - links to documentation are included in the examples; - the user understands the context of where to provide it under backend -> providers; - the user doesn't have to uncomment a million things. I couldn't find the backend page for AWS - the closest I found was https://cromwell.readthedocs.io/en/stable/tutorials/AwsBatch101/. Is there a backend page, proper?. I've done my best to add notes and documentation, and maybe it would be useful to bring in others with expertise for the various providers? This is a first shot, and I expect we will want feedback from others to get it in good shape!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4697
https://github.com/broadinstitute/cromwell/issues/4698:187,Deployability,pipeline,pipelines,187,"After updating docker container to the latest cromwell:dev from cromwell:37 I started noticing multiple file not found exception for my https://github.com/antonkulaga/rna-seq/blob/master/pipelines/bs-seq/bs_extract_run.wdl pipeline ( sample input is https://github.com/antonkulaga/rna-seq/blob/master/pipelines/bs-seq/inputs/extract_run/bs_extract_SRR948855.json ).I think it has something to do with the fact that my cromwell configuration in docker-swarm uses /data/cromwell-executions as root instead of default /cromwell-executions (my config is here https://github.com/antonkulaga/cromwell-client/blob/master/services/cromwell/app-config/application.conf ), however, there may be other reasons.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4698
https://github.com/broadinstitute/cromwell/issues/4698:223,Deployability,pipeline,pipeline,223,"After updating docker container to the latest cromwell:dev from cromwell:37 I started noticing multiple file not found exception for my https://github.com/antonkulaga/rna-seq/blob/master/pipelines/bs-seq/bs_extract_run.wdl pipeline ( sample input is https://github.com/antonkulaga/rna-seq/blob/master/pipelines/bs-seq/inputs/extract_run/bs_extract_SRR948855.json ).I think it has something to do with the fact that my cromwell configuration in docker-swarm uses /data/cromwell-executions as root instead of default /cromwell-executions (my config is here https://github.com/antonkulaga/cromwell-client/blob/master/services/cromwell/app-config/application.conf ), however, there may be other reasons.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4698
https://github.com/broadinstitute/cromwell/issues/4698:301,Deployability,pipeline,pipelines,301,"After updating docker container to the latest cromwell:dev from cromwell:37 I started noticing multiple file not found exception for my https://github.com/antonkulaga/rna-seq/blob/master/pipelines/bs-seq/bs_extract_run.wdl pipeline ( sample input is https://github.com/antonkulaga/rna-seq/blob/master/pipelines/bs-seq/inputs/extract_run/bs_extract_SRR948855.json ).I think it has something to do with the fact that my cromwell configuration in docker-swarm uses /data/cromwell-executions as root instead of default /cromwell-executions (my config is here https://github.com/antonkulaga/cromwell-client/blob/master/services/cromwell/app-config/application.conf ), however, there may be other reasons.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4698
https://github.com/broadinstitute/cromwell/issues/4698:427,Deployability,configurat,configuration,427,"After updating docker container to the latest cromwell:dev from cromwell:37 I started noticing multiple file not found exception for my https://github.com/antonkulaga/rna-seq/blob/master/pipelines/bs-seq/bs_extract_run.wdl pipeline ( sample input is https://github.com/antonkulaga/rna-seq/blob/master/pipelines/bs-seq/inputs/extract_run/bs_extract_SRR948855.json ).I think it has something to do with the fact that my cromwell configuration in docker-swarm uses /data/cromwell-executions as root instead of default /cromwell-executions (my config is here https://github.com/antonkulaga/cromwell-client/blob/master/services/cromwell/app-config/application.conf ), however, there may be other reasons.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4698
https://github.com/broadinstitute/cromwell/issues/4698:427,Modifiability,config,configuration,427,"After updating docker container to the latest cromwell:dev from cromwell:37 I started noticing multiple file not found exception for my https://github.com/antonkulaga/rna-seq/blob/master/pipelines/bs-seq/bs_extract_run.wdl pipeline ( sample input is https://github.com/antonkulaga/rna-seq/blob/master/pipelines/bs-seq/inputs/extract_run/bs_extract_SRR948855.json ).I think it has something to do with the fact that my cromwell configuration in docker-swarm uses /data/cromwell-executions as root instead of default /cromwell-executions (my config is here https://github.com/antonkulaga/cromwell-client/blob/master/services/cromwell/app-config/application.conf ), however, there may be other reasons.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4698
https://github.com/broadinstitute/cromwell/issues/4698:540,Modifiability,config,config,540,"After updating docker container to the latest cromwell:dev from cromwell:37 I started noticing multiple file not found exception for my https://github.com/antonkulaga/rna-seq/blob/master/pipelines/bs-seq/bs_extract_run.wdl pipeline ( sample input is https://github.com/antonkulaga/rna-seq/blob/master/pipelines/bs-seq/inputs/extract_run/bs_extract_SRR948855.json ).I think it has something to do with the fact that my cromwell configuration in docker-swarm uses /data/cromwell-executions as root instead of default /cromwell-executions (my config is here https://github.com/antonkulaga/cromwell-client/blob/master/services/cromwell/app-config/application.conf ), however, there may be other reasons.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4698
https://github.com/broadinstitute/cromwell/issues/4698:636,Modifiability,config,config,636,"After updating docker container to the latest cromwell:dev from cromwell:37 I started noticing multiple file not found exception for my https://github.com/antonkulaga/rna-seq/blob/master/pipelines/bs-seq/bs_extract_run.wdl pipeline ( sample input is https://github.com/antonkulaga/rna-seq/blob/master/pipelines/bs-seq/inputs/extract_run/bs_extract_SRR948855.json ).I think it has something to do with the fact that my cromwell configuration in docker-swarm uses /data/cromwell-executions as root instead of default /cromwell-executions (my config is here https://github.com/antonkulaga/cromwell-client/blob/master/services/cromwell/app-config/application.conf ), however, there may be other reasons.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4698
https://github.com/broadinstitute/cromwell/issues/4699:159,Performance,cache,cache,159,When you have a workflow that consists out of several subworkflows and only one of them change it takes time when cromwell will run all others and resolve the cache. There should be the way to cache workflows output so if specific subworkflow did not change in terms of WDL or input it should give cache of its output,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4699
https://github.com/broadinstitute/cromwell/issues/4699:193,Performance,cache,cache,193,When you have a workflow that consists out of several subworkflows and only one of them change it takes time when cromwell will run all others and resolve the cache. There should be the way to cache workflows output so if specific subworkflow did not change in terms of WDL or input it should give cache of its output,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4699
https://github.com/broadinstitute/cromwell/issues/4699:298,Performance,cache,cache,298,When you have a workflow that consists out of several subworkflows and only one of them change it takes time when cromwell will run all others and resolve the cache. There should be the way to cache workflows output so if specific subworkflow did not change in terms of WDL or input it should give cache of its output,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4699
https://github.com/broadinstitute/cromwell/issues/4700:394,Availability,error,error,394,"What are the restrictions on the WDL that can appear in the `runtime-attributes` section of a provider definition? Issue #4685 indicates that `Array` types cannot be specified. I have also learnt that you cannot reference symbols that have already been defined. For example:. ```; runtime-attributes = """"""; Int foo?; String mode = if defined(foo) then ""foo"" else ""bar""; """"""; ```. This gives an error on Cromwell's warm up as follows:. ```; [2019-03-04 16:26:34,77] [error] No identifiers should be looked up: foo; ```. The above is just a toy example, but this functionality would be *really* useful. It's not clear, much like the `Array` restriction, why this isn't allowed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4700
https://github.com/broadinstitute/cromwell/issues/4700:466,Availability,error,error,466,"What are the restrictions on the WDL that can appear in the `runtime-attributes` section of a provider definition? Issue #4685 indicates that `Array` types cannot be specified. I have also learnt that you cannot reference symbols that have already been defined. For example:. ```; runtime-attributes = """"""; Int foo?; String mode = if defined(foo) then ""foo"" else ""bar""; """"""; ```. This gives an error on Cromwell's warm up as follows:. ```; [2019-03-04 16:26:34,77] [error] No identifiers should be looked up: foo; ```. The above is just a toy example, but this functionality would be *really* useful. It's not clear, much like the `Array` restriction, why this isn't allowed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4700
https://github.com/broadinstitute/cromwell/issues/4700:189,Usability,learn,learnt,189,"What are the restrictions on the WDL that can appear in the `runtime-attributes` section of a provider definition? Issue #4685 indicates that `Array` types cannot be specified. I have also learnt that you cannot reference symbols that have already been defined. For example:. ```; runtime-attributes = """"""; Int foo?; String mode = if defined(foo) then ""foo"" else ""bar""; """"""; ```. This gives an error on Cromwell's warm up as follows:. ```; [2019-03-04 16:26:34,77] [error] No identifiers should be looked up: foo; ```. The above is just a toy example, but this functionality would be *really* useful. It's not clear, much like the `Array` restriction, why this isn't allowed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4700
https://github.com/broadinstitute/cromwell/issues/4700:610,Usability,clear,clear,610,"What are the restrictions on the WDL that can appear in the `runtime-attributes` section of a provider definition? Issue #4685 indicates that `Array` types cannot be specified. I have also learnt that you cannot reference symbols that have already been defined. For example:. ```; runtime-attributes = """"""; Int foo?; String mode = if defined(foo) then ""foo"" else ""bar""; """"""; ```. This gives an error on Cromwell's warm up as follows:. ```; [2019-03-04 16:26:34,77] [error] No identifiers should be looked up: foo; ```. The above is just a toy example, but this functionality would be *really* useful. It's not clear, much like the `Array` restriction, why this isn't allowed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4700
https://github.com/broadinstitute/cromwell/pull/4701:108,Deployability,upgrade,upgrade,108,"Better theory is https://github.com/broadinstitute/firecloud-develop/pull/1556. IMO the risk/reward for the upgrade no longer checks out, especially since we are in a bit of a crunch mode and relatively ill-equipped to deal with unexpected problems.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4701
https://github.com/broadinstitute/cromwell/pull/4701:88,Safety,risk,risk,88,"Better theory is https://github.com/broadinstitute/firecloud-develop/pull/1556. IMO the risk/reward for the upgrade no longer checks out, especially since we are in a bit of a crunch mode and relatively ill-equipped to deal with unexpected problems.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4701
https://github.com/broadinstitute/cromwell/pull/4702:924,Testability,assert,assert,924,"Sample query shapes:. ### “labelAnd” (used to filter by submission ID, for example) style:; ```; SELECT summaryTable.workflow_execution_uuid, ; summaryTable.workflow_name, ; summaryTable.workflow_status, ; summaryTable.start_timestamp, ; summaryTable.end_timestamp, ; summaryTable.submission_timestamp, ; summaryTable.workflow_metadata_summary_entry_id ; FROM workflow_metadata_summary_entry summaryTable ; JOIN custom_label_entry labelAndTable0 ; ON summaryTable.workflow_execution_uuid = ; labelAndTable0.workflow_execution_uuid ; JOIN custom_label_entry labelAndTable1 ; ON summaryTable.workflow_execution_uuid = ; labelAndTable1.workflow_execution_uuid ; WHERE ( ( labelAndTable0.custom_label_key = ? ; AND labelAndTable0.custom_label_value = ? ) ; AND ( labelAndTable1.custom_label_key = ? ; AND labelAndTable1.custom_label_value = ? ) ) ; ORDER BY workflow_metadata_summary_entry_id DESC ; ```. ### “labelOr” (used to assert at least one collection ID, for example) style:; ```; SELECT summaryTable.workflow_execution_uuid, ; summaryTable.workflow_name, ; summaryTable.workflow_status, ; summaryTable.start_timestamp, ; summaryTable.end_timestamp, ; summaryTable.submission_timestamp, ; summaryTable.workflow_metadata_summary_entry_id ; FROM workflow_metadata_summary_entry summaryTable ; JOIN custom_label_entry labelsOrMixin ; ON summaryTable.workflow_execution_uuid = ; labelsOrMixin.workflow_execution_uuid ; WHERE ( ( labelsOrMixin.custom_label_key = ? ; AND labelsOrMixin.custom_label_value = ? ) ; OR ( labelsOrMixin.custom_label_key = ? ; AND labelsOrMixin.custom_label_value = ? ) ) ; ORDER BY workflow_metadata_summary_entry_id DESC ; ```. ### excludeLabelsAnd style (not sure Job Manager uses this):; ```; SELECT summaryTable.workflow_execution_uuid, ; summaryTable.workflow_name, ; summaryTable.workflow_status, ; summaryTable.start_timestamp, ; summaryTable.end_timestamp, ; summaryTable.submission_timestamp, ; summaryTable.workflow_metadata_summary_entry_id ; FROM workflow_metada",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4702
https://github.com/broadinstitute/cromwell/issues/4703:614,Availability,alive,alive,614,"my code end ENV:. config file: . backend {; providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. filesystems {; local {; localization: [; ""soft-link"",; ""hard-link"",; ""copy""; ]; }; }. runtime-attributes = """"""; String time = ""2-0""; Int cpus = 2; Int memory = 8000; String queue = ""compute""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${time} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${memory} \; --wrap ""/bin/bash ${script}""; """""". job-id-regex = ""Submitted batch job (\\d+).*""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; }; }; }. wdl file :; task SamToFastqAndBwaMem {; ......; ......; command <<<; set -o pipefail; set -e. # set the bash variable needed for the command-line; bash_ref_fasta=${ref_fasta}. java -Dsamjdk.compression_level=${compression_level} ${java_opt} -jar ${gotc_path}picard.jar \; SamToFastq \; INPUT=${input_bam} \; FASTQ=/dev/stdout \; INTERLEAVE=true \; NON_PF=true \; | \; ${bwa_path}${bwa_commandline} /dev/stdin - 2> >(tee ${output_bam_basename}.bwa.stderr.log >&2) \; | \; samtools view -1 - > ${output_bam_basename}.bam. >>>; #runtime {; # backend: ""SLURM""; # memory: mem_size; # cpus: num_cpu; #}; output {; File output_bam = ""${output_bam_basename}.bam""; File bwa_stderr_log = ""${output_bam_basename}.bwa.stderr.log""; }; }. all parameters goes ok, but below are some problems:. 1:; Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Ope",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:1493,Availability,Error,Error,1493," -e. # set the bash variable needed for the command-line; bash_ref_fasta=${ref_fasta}. java -Dsamjdk.compression_level=${compression_level} ${java_opt} -jar ${gotc_path}picard.jar \; SamToFastq \; INPUT=${input_bam} \; FASTQ=/dev/stdout \; INTERLEAVE=true \; NON_PF=true \; | \; ${bwa_path}${bwa_commandline} /dev/stdin - 2> >(tee ${output_bam_basename}.bwa.stderr.log >&2) \; | \; samtools view -1 - > ${output_bam_basename}.bam. >>>; #runtime {; # backend: ""SLURM""; # memory: mem_size; # cpus: num_cpu; #}; output {; File output_bam = ""${output_bam_basename}.bam""; File bwa_stderr_log = ""${output_bam_basename}.bwa.stderr.log""; }; }. all parameters goes ok, but below are some problems:. 1:; Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:3565,Availability,error,error,3565,"dBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-workflow-logs/workflow.8fc94dc1-722b-40d5-9840-9d6e4a66db21.log: File name too long; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:563); ... 32 common frames omitted. cromwell: v36.1; my working dir is: /nfs/disk3/user/gaoyuhui/github/test, has only 2.wdl and 2.json for test, but everytime this simple task is getting recursive and finally file name too long, and when change backend to local, it is the same. I found other topic and change to other dir to run this wdl task, got the same error.; so, can someone check about this? How can I goes well. it is a bug or my mistake??; Yours, sincerely!; Gao",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:500,Integrability,wrap,wrap,500,"my code end ENV:. config file: . backend {; providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. filesystems {; local {; localization: [; ""soft-link"",; ""hard-link"",; ""copy""; ]; }; }. runtime-attributes = """"""; String time = ""2-0""; Int cpus = 2; Int memory = 8000; String queue = ""compute""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${time} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${memory} \; --wrap ""/bin/bash ${script}""; """""". job-id-regex = ""Submitted batch job (\\d+).*""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; }; }; }. wdl file :; task SamToFastqAndBwaMem {; ......; ......; command <<<; set -o pipefail; set -e. # set the bash variable needed for the command-line; bash_ref_fasta=${ref_fasta}. java -Dsamjdk.compression_level=${compression_level} ${java_opt} -jar ${gotc_path}picard.jar \; SamToFastq \; INPUT=${input_bam} \; FASTQ=/dev/stdout \; INTERLEAVE=true \; NON_PF=true \; | \; ${bwa_path}${bwa_commandline} /dev/stdin - 2> >(tee ${output_bam_basename}.bwa.stderr.log >&2) \; | \; samtools view -1 - > ${output_bam_basename}.bam. >>>; #runtime {; # backend: ""SLURM""; # memory: mem_size; # cpus: num_cpu; #}; output {; File output_bam = ""${output_bam_basename}.bam""; File bwa_stderr_log = ""${output_bam_basename}.bwa.stderr.log""; }; }. all parameters goes ok, but below are some problems:. 1:; Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Ope",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:18,Modifiability,config,config,18,"my code end ENV:. config file: . backend {; providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. filesystems {; local {; localization: [; ""soft-link"",; ""hard-link"",; ""copy""; ]; }; }. runtime-attributes = """"""; String time = ""2-0""; Int cpus = 2; Int memory = 8000; String queue = ""compute""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${time} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${memory} \; --wrap ""/bin/bash ${script}""; """""". job-id-regex = ""Submitted batch job (\\d+).*""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; }; }; }. wdl file :; task SamToFastqAndBwaMem {; ......; ......; command <<<; set -o pipefail; set -e. # set the bash variable needed for the command-line; bash_ref_fasta=${ref_fasta}. java -Dsamjdk.compression_level=${compression_level} ${java_opt} -jar ${gotc_path}picard.jar \; SamToFastq \; INPUT=${input_bam} \; FASTQ=/dev/stdout \; INTERLEAVE=true \; NON_PF=true \; | \; ${bwa_path}${bwa_commandline} /dev/stdin - 2> >(tee ${output_bam_basename}.bwa.stderr.log >&2) \; | \; samtools view -1 - > ${output_bam_basename}.bam. >>>; #runtime {; # backend: ""SLURM""; # memory: mem_size; # cpus: num_cpu; #}; output {; File output_bam = ""${output_bam_basename}.bam""; File bwa_stderr_log = ""${output_bam_basename}.bwa.stderr.log""; }; }. all parameters goes ok, but below are some problems:. 1:; Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Ope",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:109,Modifiability,config,config,109,"my code end ENV:. config file: . backend {; providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. filesystems {; local {; localization: [; ""soft-link"",; ""hard-link"",; ""copy""; ]; }; }. runtime-attributes = """"""; String time = ""2-0""; Int cpus = 2; Int memory = 8000; String queue = ""compute""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${time} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${memory} \; --wrap ""/bin/bash ${script}""; """""". job-id-regex = ""Submitted batch job (\\d+).*""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; }; }; }. wdl file :; task SamToFastqAndBwaMem {; ......; ......; command <<<; set -o pipefail; set -e. # set the bash variable needed for the command-line; bash_ref_fasta=${ref_fasta}. java -Dsamjdk.compression_level=${compression_level} ${java_opt} -jar ${gotc_path}picard.jar \; SamToFastq \; INPUT=${input_bam} \; FASTQ=/dev/stdout \; INTERLEAVE=true \; NON_PF=true \; | \; ${bwa_path}${bwa_commandline} /dev/stdin - 2> >(tee ${output_bam_basename}.bwa.stderr.log >&2) \; | \; samtools view -1 - > ${output_bam_basename}.bam. >>>; #runtime {; # backend: ""SLURM""; # memory: mem_size; # cpus: num_cpu; #}; output {; File output_bam = ""${output_bam_basename}.bam""; File bwa_stderr_log = ""${output_bam_basename}.bwa.stderr.log""; }; }. all parameters goes ok, but below are some problems:. 1:; Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Ope",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:116,Modifiability,Config,ConfigBackendLifecycleActorFactory,116,"my code end ENV:. config file: . backend {; providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. filesystems {; local {; localization: [; ""soft-link"",; ""hard-link"",; ""copy""; ]; }; }. runtime-attributes = """"""; String time = ""2-0""; Int cpus = 2; Int memory = 8000; String queue = ""compute""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${time} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${memory} \; --wrap ""/bin/bash ${script}""; """""". job-id-regex = ""Submitted batch job (\\d+).*""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; }; }; }. wdl file :; task SamToFastqAndBwaMem {; ......; ......; command <<<; set -o pipefail; set -e. # set the bash variable needed for the command-line; bash_ref_fasta=${ref_fasta}. java -Dsamjdk.compression_level=${compression_level} ${java_opt} -jar ${gotc_path}picard.jar \; SamToFastq \; INPUT=${input_bam} \; FASTQ=/dev/stdout \; INTERLEAVE=true \; NON_PF=true \; | \; ${bwa_path}${bwa_commandline} /dev/stdin - 2> >(tee ${output_bam_basename}.bwa.stderr.log >&2) \; | \; samtools view -1 - > ${output_bam_basename}.bam. >>>; #runtime {; # backend: ""SLURM""; # memory: mem_size; # cpus: num_cpu; #}; output {; File output_bam = ""${output_bam_basename}.bam""; File bwa_stderr_log = ""${output_bam_basename}.bwa.stderr.log""; }; }. all parameters goes ok, but below are some problems:. 1:; Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Ope",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:153,Modifiability,config,config,153,"my code end ENV:. config file: . backend {; providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. filesystems {; local {; localization: [; ""soft-link"",; ""hard-link"",; ""copy""; ]; }; }. runtime-attributes = """"""; String time = ""2-0""; Int cpus = 2; Int memory = 8000; String queue = ""compute""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${time} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${memory} \; --wrap ""/bin/bash ${script}""; """""". job-id-regex = ""Submitted batch job (\\d+).*""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; }; }; }. wdl file :; task SamToFastqAndBwaMem {; ......; ......; command <<<; set -o pipefail; set -e. # set the bash variable needed for the command-line; bash_ref_fasta=${ref_fasta}. java -Dsamjdk.compression_level=${compression_level} ${java_opt} -jar ${gotc_path}picard.jar \; SamToFastq \; INPUT=${input_bam} \; FASTQ=/dev/stdout \; INTERLEAVE=true \; NON_PF=true \; | \; ${bwa_path}${bwa_commandline} /dev/stdin - 2> >(tee ${output_bam_basename}.bwa.stderr.log >&2) \; | \; samtools view -1 - > ${output_bam_basename}.bam. >>>; #runtime {; # backend: ""SLURM""; # memory: mem_size; # cpus: num_cpu; #}; output {; File output_bam = ""${output_bam_basename}.bam""; File bwa_stderr_log = ""${output_bam_basename}.bwa.stderr.log""; }; }. all parameters goes ok, but below are some problems:. 1:; Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Ope",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:763,Modifiability,variab,variable,763,"my code end ENV:. config file: . backend {; providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. filesystems {; local {; localization: [; ""soft-link"",; ""hard-link"",; ""copy""; ]; }; }. runtime-attributes = """"""; String time = ""2-0""; Int cpus = 2; Int memory = 8000; String queue = ""compute""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${time} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${memory} \; --wrap ""/bin/bash ${script}""; """""". job-id-regex = ""Submitted batch job (\\d+).*""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; }; }; }. wdl file :; task SamToFastqAndBwaMem {; ......; ......; command <<<; set -o pipefail; set -e. # set the bash variable needed for the command-line; bash_ref_fasta=${ref_fasta}. java -Dsamjdk.compression_level=${compression_level} ${java_opt} -jar ${gotc_path}picard.jar \; SamToFastq \; INPUT=${input_bam} \; FASTQ=/dev/stdout \; INTERLEAVE=true \; NON_PF=true \; | \; ${bwa_path}${bwa_commandline} /dev/stdin - 2> >(tee ${output_bam_basename}.bwa.stderr.log >&2) \; | \; samtools view -1 - > ${output_bam_basename}.bam. >>>; #runtime {; # backend: ""SLURM""; # memory: mem_size; # cpus: num_cpu; #}; output {; File output_bam = ""${output_bam_basename}.bam""; File bwa_stderr_log = ""${output_bam_basename}.bwa.stderr.log""; }; }. all parameters goes ok, but below are some problems:. 1:; Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Ope",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:336,Performance,queue,queue,336,"my code end ENV:. config file: . backend {; providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. filesystems {; local {; localization: [; ""soft-link"",; ""hard-link"",; ""copy""; ]; }; }. runtime-attributes = """"""; String time = ""2-0""; Int cpus = 2; Int memory = 8000; String queue = ""compute""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${time} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${memory} \; --wrap ""/bin/bash ${script}""; """""". job-id-regex = ""Submitted batch job (\\d+).*""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; }; }; }. wdl file :; task SamToFastqAndBwaMem {; ......; ......; command <<<; set -o pipefail; set -e. # set the bash variable needed for the command-line; bash_ref_fasta=${ref_fasta}. java -Dsamjdk.compression_level=${compression_level} ${java_opt} -jar ${gotc_path}picard.jar \; SamToFastq \; INPUT=${input_bam} \; FASTQ=/dev/stdout \; INTERLEAVE=true \; NON_PF=true \; | \; ${bwa_path}${bwa_commandline} /dev/stdin - 2> >(tee ${output_bam_basename}.bwa.stderr.log >&2) \; | \; samtools view -1 - > ${output_bam_basename}.bam. >>>; #runtime {; # backend: ""SLURM""; # memory: mem_size; # cpus: num_cpu; #}; output {; File output_bam = ""${output_bam_basename}.bam""; File bwa_stderr_log = ""${output_bam_basename}.bwa.stderr.log""; }; }. all parameters goes ok, but below are some problems:. 1:; Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Ope",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:442,Performance,queue,queue,442,"my code end ENV:. config file: . backend {; providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. filesystems {; local {; localization: [; ""soft-link"",; ""hard-link"",; ""copy""; ]; }; }. runtime-attributes = """"""; String time = ""2-0""; Int cpus = 2; Int memory = 8000; String queue = ""compute""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${time} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${memory} \; --wrap ""/bin/bash ${script}""; """""". job-id-regex = ""Submitted batch job (\\d+).*""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; }; }; }. wdl file :; task SamToFastqAndBwaMem {; ......; ......; command <<<; set -o pipefail; set -e. # set the bash variable needed for the command-line; bash_ref_fasta=${ref_fasta}. java -Dsamjdk.compression_level=${compression_level} ${java_opt} -jar ${gotc_path}picard.jar \; SamToFastq \; INPUT=${input_bam} \; FASTQ=/dev/stdout \; INTERLEAVE=true \; NON_PF=true \; | \; ${bwa_path}${bwa_commandline} /dev/stdin - 2> >(tee ${output_bam_basename}.bwa.stderr.log >&2) \; | \; samtools view -1 - > ${output_bam_basename}.bam. >>>; #runtime {; # backend: ""SLURM""; # memory: mem_size; # cpus: num_cpu; #}; output {; File output_bam = ""${output_bam_basename}.bam""; File bwa_stderr_log = ""${output_bam_basename}.bwa.stderr.log""; }; }. all parameters goes ok, but below are some problems:. 1:; Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Ope",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:2933,Security,validat,validation,2933,"dBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-workflow-logs/workflow.8fc94dc1-722b-40d5-9840-9d6e4a66db21.log: File name too long; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:563); ... 32 common frames omitted. cromwell: v36.1; my working dir is: /nfs/disk3/user/gaoyuhui/github/test, has only 2.wdl and 2.json for test, but everytime this simple task is getting recursive and finally file name too long, and when change backend to local, it is the same. I found other topic and change to other dir to run this wdl task, got the same error.; so, can someone check about this? How can I goes well. it is a bug or my mistake??; Yours, sincerely!; Gao",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:2944,Security,Validat,Validation,2944,"dBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-workflow-logs/workflow.8fc94dc1-722b-40d5-9840-9d6e4a66db21.log: File name too long; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:563); ... 32 common frames omitted. cromwell: v36.1; my working dir is: /nfs/disk3/user/gaoyuhui/github/test, has only 2.wdl and 2.json for test, but everytime this simple task is getting recursive and finally file name too long, and when change backend to local, it is the same. I found other topic and change to other dir to run this wdl task, got the same error.; so, can someone check about this? How can I goes well. it is a bug or my mistake??; Yours, sincerely!; Gao",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:2955,Security,Validat,ValidationTry,2955,"dBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-workflow-logs/workflow.8fc94dc1-722b-40d5-9840-9d6e4a66db21.log: File name too long; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:563); ... 32 common frames omitted. cromwell: v36.1; my working dir is: /nfs/disk3/user/gaoyuhui/github/test, has only 2.wdl and 2.json for test, but everytime this simple task is getting recursive and finally file name too long, and when change backend to local, it is the same. I found other topic and change to other dir to run this wdl task, got the same error.; so, can someone check about this? How can I goes well. it is a bug or my mistake??; Yours, sincerely!; Gao",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:2987,Security,Validat,Validation,2987,"dBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-workflow-logs/workflow.8fc94dc1-722b-40d5-9840-9d6e4a66db21.log: File name too long; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:563); ... 32 common frames omitted. cromwell: v36.1; my working dir is: /nfs/disk3/user/gaoyuhui/github/test, has only 2.wdl and 2.json for test, but everytime this simple task is getting recursive and finally file name too long, and when change backend to local, it is the same. I found other topic and change to other dir to run this wdl task, got the same error.; so, can someone check about this? How can I goes well. it is a bug or my mistake??; Yours, sincerely!; Gao",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:3019,Security,validat,validation,3019,"dBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-workflow-logs/workflow.8fc94dc1-722b-40d5-9840-9d6e4a66db21.log: File name too long; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:563); ... 32 common frames omitted. cromwell: v36.1; my working dir is: /nfs/disk3/user/gaoyuhui/github/test, has only 2.wdl and 2.json for test, but everytime this simple task is getting recursive and finally file name too long, and when change backend to local, it is the same. I found other topic and change to other dir to run this wdl task, got the same error.; so, can someone check about this? How can I goes well. it is a bug or my mistake??; Yours, sincerely!; Gao",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:3030,Security,Validat,Validation,3030,"dBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-workflow-logs/workflow.8fc94dc1-722b-40d5-9840-9d6e4a66db21.log: File name too long; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:563); ... 32 common frames omitted. cromwell: v36.1; my working dir is: /nfs/disk3/user/gaoyuhui/github/test, has only 2.wdl and 2.json for test, but everytime this simple task is getting recursive and finally file name too long, and when change backend to local, it is the same. I found other topic and change to other dir to run this wdl task, got the same error.; so, can someone check about this? How can I goes well. it is a bug or my mistake??; Yours, sincerely!; Gao",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:3041,Security,Validat,ValidationTry,3041,"dBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-workflow-logs/workflow.8fc94dc1-722b-40d5-9840-9d6e4a66db21.log: File name too long; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:563); ... 32 common frames omitted. cromwell: v36.1; my working dir is: /nfs/disk3/user/gaoyuhui/github/test, has only 2.wdl and 2.json for test, but everytime this simple task is getting recursive and finally file name too long, and when change backend to local, it is the same. I found other topic and change to other dir to run this wdl task, got the same error.; so, can someone check about this? How can I goes well. it is a bug or my mistake??; Yours, sincerely!; Gao",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:3073,Security,Validat,Validation,3073,"dBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-workflow-logs/workflow.8fc94dc1-722b-40d5-9840-9d6e4a66db21.log: File name too long; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:563); ... 32 common frames omitted. cromwell: v36.1; my working dir is: /nfs/disk3/user/gaoyuhui/github/test, has only 2.wdl and 2.json for test, but everytime this simple task is getting recursive and finally file name too long, and when change backend to local, it is the same. I found other topic and change to other dir to run this wdl task, got the same error.; so, can someone check about this? How can I goes well. it is a bug or my mistake??; Yours, sincerely!; Gao",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:1108,Testability,log,log,1108,"ActorFactory""; config {. filesystems {; local {; localization: [; ""soft-link"",; ""hard-link"",; ""copy""; ]; }; }. runtime-attributes = """"""; String time = ""2-0""; Int cpus = 2; Int memory = 8000; String queue = ""compute""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${time} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${memory} \; --wrap ""/bin/bash ${script}""; """""". job-id-regex = ""Submitted batch job (\\d+).*""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; }; }; }. wdl file :; task SamToFastqAndBwaMem {; ......; ......; command <<<; set -o pipefail; set -e. # set the bash variable needed for the command-line; bash_ref_fasta=${ref_fasta}. java -Dsamjdk.compression_level=${compression_level} ${java_opt} -jar ${gotc_path}picard.jar \; SamToFastq \; INPUT=${input_bam} \; FASTQ=/dev/stdout \; INTERLEAVE=true \; NON_PF=true \; | \; ${bwa_path}${bwa_commandline} /dev/stdin - 2> >(tee ${output_bam_basename}.bwa.stderr.log >&2) \; | \; samtools view -1 - > ${output_bam_basename}.bam. >>>; #runtime {; # backend: ""SLURM""; # memory: mem_size; # cpus: num_cpu; #}; output {; File output_bam = ""${output_bam_basename}.bam""; File bwa_stderr_log = ""${output_bam_basename}.bwa.stderr.log""; }; }. all parameters goes ok, but below are some problems:. 1:; Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GA",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:1367,Testability,log,log,1367,"; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${time} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${memory} \; --wrap ""/bin/bash ${script}""; """""". job-id-regex = ""Submitted batch job (\\d+).*""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; }; }; }. wdl file :; task SamToFastqAndBwaMem {; ......; ......; command <<<; set -o pipefail; set -e. # set the bash variable needed for the command-line; bash_ref_fasta=${ref_fasta}. java -Dsamjdk.compression_level=${compression_level} ${java_opt} -jar ${gotc_path}picard.jar \; SamToFastq \; INPUT=${input_bam} \; FASTQ=/dev/stdout \; INTERLEAVE=true \; NON_PF=true \; | \; ${bwa_path}${bwa_commandline} /dev/stdin - 2> >(tee ${output_bam_basename}.bwa.stderr.log >&2) \; | \; samtools view -1 - > ${output_bam_basename}.bam. >>>; #runtime {; # backend: ""SLURM""; # memory: mem_size; # cpus: num_cpu; #}; output {; File output_bam = ""${output_bam_basename}.bam""; File bwa_stderr_log = ""${output_bam_basename}.bwa.stderr.log""; }; }. all parameters goes ok, but below are some problems:. 1:; Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:1561,Testability,test,test,1561," -e. # set the bash variable needed for the command-line; bash_ref_fasta=${ref_fasta}. java -Dsamjdk.compression_level=${compression_level} ${java_opt} -jar ${gotc_path}picard.jar \; SamToFastq \; INPUT=${input_bam} \; FASTQ=/dev/stdout \; INTERLEAVE=true \; NON_PF=true \; | \; ${bwa_path}${bwa_commandline} /dev/stdin - 2> >(tee ${output_bam_basename}.bwa.stderr.log >&2) \; | \; samtools view -1 - > ${output_bam_basename}.bam. >>>; #runtime {; # backend: ""SLURM""; # memory: mem_size; # cpus: num_cpu; #}; output {; File output_bam = ""${output_bam_basename}.bam""; File bwa_stderr_log = ""${output_bam_basename}.bwa.stderr.log""; }; }. all parameters goes ok, but below are some problems:. 1:; Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:1706,Testability,test,test,1706," -e. # set the bash variable needed for the command-line; bash_ref_fasta=${ref_fasta}. java -Dsamjdk.compression_level=${compression_level} ${java_opt} -jar ${gotc_path}picard.jar \; SamToFastq \; INPUT=${input_bam} \; FASTQ=/dev/stdout \; INTERLEAVE=true \; NON_PF=true \; | \; ${bwa_path}${bwa_commandline} /dev/stdin - 2> >(tee ${output_bam_basename}.bwa.stderr.log >&2) \; | \; samtools view -1 - > ${output_bam_basename}.bam. >>>; #runtime {; # backend: ""SLURM""; # memory: mem_size; # cpus: num_cpu; #}; output {; File output_bam = ""${output_bam_basename}.bam""; File bwa_stderr_log = ""${output_bam_basename}.bwa.stderr.log""; }; }. all parameters goes ok, but below are some problems:. 1:; Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:1807,Testability,test,test,1807," -e. # set the bash variable needed for the command-line; bash_ref_fasta=${ref_fasta}. java -Dsamjdk.compression_level=${compression_level} ${java_opt} -jar ${gotc_path}picard.jar \; SamToFastq \; INPUT=${input_bam} \; FASTQ=/dev/stdout \; INTERLEAVE=true \; NON_PF=true \; | \; ${bwa_path}${bwa_commandline} /dev/stdin - 2> >(tee ${output_bam_basename}.bwa.stderr.log >&2) \; | \; samtools view -1 - > ${output_bam_basename}.bam. >>>; #runtime {; # backend: ""SLURM""; # memory: mem_size; # cpus: num_cpu; #}; output {; File output_bam = ""${output_bam_basename}.bam""; File bwa_stderr_log = ""${output_bam_basename}.bwa.stderr.log""; }; }. all parameters goes ok, but below are some problems:. 1:; Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:1952,Testability,test,test,1952," -e. # set the bash variable needed for the command-line; bash_ref_fasta=${ref_fasta}. java -Dsamjdk.compression_level=${compression_level} ${java_opt} -jar ${gotc_path}picard.jar \; SamToFastq \; INPUT=${input_bam} \; FASTQ=/dev/stdout \; INTERLEAVE=true \; NON_PF=true \; | \; ${bwa_path}${bwa_commandline} /dev/stdin - 2> >(tee ${output_bam_basename}.bwa.stderr.log >&2) \; | \; samtools view -1 - > ${output_bam_basename}.bam. >>>; #runtime {; # backend: ""SLURM""; # memory: mem_size; # cpus: num_cpu; #}; output {; File output_bam = ""${output_bam_basename}.bam""; File bwa_stderr_log = ""${output_bam_basename}.bwa.stderr.log""; }; }. all parameters goes ok, but below are some problems:. 1:; Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:1992,Testability,test,test,1992," -e. # set the bash variable needed for the command-line; bash_ref_fasta=${ref_fasta}. java -Dsamjdk.compression_level=${compression_level} ${java_opt} -jar ${gotc_path}picard.jar \; SamToFastq \; INPUT=${input_bam} \; FASTQ=/dev/stdout \; INTERLEAVE=true \; NON_PF=true \; | \; ${bwa_path}${bwa_commandline} /dev/stdin - 2> >(tee ${output_bam_basename}.bwa.stderr.log >&2) \; | \; samtools view -1 - > ${output_bam_basename}.bam. >>>; #runtime {; # backend: ""SLURM""; # memory: mem_size; # cpus: num_cpu; #}; output {; File output_bam = ""${output_bam_basename}.bam""; File bwa_stderr_log = ""${output_bam_basename}.bwa.stderr.log""; }; }. all parameters goes ok, but below are some problems:. 1:; Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:2075,Testability,test,test,2075,"stdin - 2> >(tee ${output_bam_basename}.bwa.stderr.log >&2) \; | \; samtools view -1 - > ${output_bam_basename}.bam. >>>; #runtime {; # backend: ""SLURM""; # memory: mem_size; # cpus: num_cpu; #}; output {; File output_bam = ""${output_bam_basename}.bam""; File bwa_stderr_log = ""${output_bam_basename}.bwa.stderr.log""; }; }. all parameters goes ok, but below are some problems:. 1:; Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-workflow-logs/workflow.8fc94dc1-722b-40d5-9840-9d6e4a66db21.log: File name too long; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); at common.validation.Validation$ValidationTry$.to",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:2224,Testability,test,test,2224,"_bam_basename}.bam. >>>; #runtime {; # backend: ""SLURM""; # memory: mem_size; # cpus: num_cpu; #}; output {; File output_bam = ""${output_bam_basename}.bam""; File bwa_stderr_log = ""${output_bam_basename}.bwa.stderr.log""; }; }. all parameters goes ok, but below are some problems:. 1:; Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-workflow-logs/workflow.8fc94dc1-722b-40d5-9840-9d6e4a66db21.log: File name too long; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); at cromwell.backend.standard.StandardAsyncExecutionActor.ins",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:2373,Testability,test,test,2373,".bam""; File bwa_stderr_log = ""${output_bam_basename}.bwa.stderr.log""; }; }. all parameters goes ok, but below are some problems:. 1:; Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-workflow-logs/workflow.8fc94dc1-722b-40d5-9840-9d6e4a66db21.log: File name too long; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:563); ... 32 common frames omitted. cromwell: v36.1; my working dir is: /nfs/disk3/user/gaoyuhui/g",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:2522,Testability,test,test,2522,"on.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-workflow-logs/workflow.8fc94dc1-722b-40d5-9840-9d6e4a66db21.log: File name too long; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:563); ... 32 common frames omitted. cromwell: v36.1; my working dir is: /nfs/disk3/user/gaoyuhui/github/test, has only 2.wdl and 2.json for test, but everytime this simple task is getting recursive and finally file name too long, and when change b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:2671,Testability,test,test,2671,"rVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-workflow-logs/workflow.8fc94dc1-722b-40d5-9840-9d6e4a66db21.log: File name too long; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:563); ... 32 common frames omitted. cromwell: v36.1; my working dir is: /nfs/disk3/user/gaoyuhui/github/test, has only 2.wdl and 2.json for test, but everytime this simple task is getting recursive and finally file name too long, and when change backend to local, it is the same. I found other topic and change to other dir to run this wdl task, got the same error.; so, can someone check about t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:2820,Testability,test,test,2820,"dBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-workflow-logs/workflow.8fc94dc1-722b-40d5-9840-9d6e4a66db21.log: File name too long; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:563); ... 32 common frames omitted. cromwell: v36.1; my working dir is: /nfs/disk3/user/gaoyuhui/github/test, has only 2.wdl and 2.json for test, but everytime this simple task is getting recursive and finally file name too long, and when change backend to local, it is the same. I found other topic and change to other dir to run this wdl task, got the same error.; so, can someone check about this? How can I goes well. it is a bug or my mistake??; Yours, sincerely!; Gao",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:2847,Testability,log,logs,2847,"dBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-workflow-logs/workflow.8fc94dc1-722b-40d5-9840-9d6e4a66db21.log: File name too long; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:563); ... 32 common frames omitted. cromwell: v36.1; my working dir is: /nfs/disk3/user/gaoyuhui/github/test, has only 2.wdl and 2.json for test, but everytime this simple task is getting recursive and finally file name too long, and when change backend to local, it is the same. I found other topic and change to other dir to run this wdl task, got the same error.; so, can someone check about this? How can I goes well. it is a bug or my mistake??; Yours, sincerely!; Gao",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:2898,Testability,log,log,2898,"dBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-workflow-logs/workflow.8fc94dc1-722b-40d5-9840-9d6e4a66db21.log: File name too long; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:563); ... 32 common frames omitted. cromwell: v36.1; my working dir is: /nfs/disk3/user/gaoyuhui/github/test, has only 2.wdl and 2.json for test, but everytime this simple task is getting recursive and finally file name too long, and when change backend to local, it is the same. I found other topic and change to other dir to run this wdl task, got the same error.; so, can someone check about this? How can I goes well. it is a bug or my mistake??; Yours, sincerely!; Gao",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:3310,Testability,test,test,3310,"dBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-workflow-logs/workflow.8fc94dc1-722b-40d5-9840-9d6e4a66db21.log: File name too long; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:563); ... 32 common frames omitted. cromwell: v36.1; my working dir is: /nfs/disk3/user/gaoyuhui/github/test, has only 2.wdl and 2.json for test, but everytime this simple task is getting recursive and finally file name too long, and when change backend to local, it is the same. I found other topic and change to other dir to run this wdl task, got the same error.; so, can someone check about this? How can I goes well. it is a bug or my mistake??; Yours, sincerely!; Gao",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:3346,Testability,test,test,3346,"dBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-workflow-logs/workflow.8fc94dc1-722b-40d5-9840-9d6e4a66db21.log: File name too long; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:563); ... 32 common frames omitted. cromwell: v36.1; my working dir is: /nfs/disk3/user/gaoyuhui/github/test, has only 2.wdl and 2.json for test, but everytime this simple task is getting recursive and finally file name too long, and when change backend to local, it is the same. I found other topic and change to other dir to run this wdl task, got the same error.; so, can someone check about this? How can I goes well. it is a bug or my mistake??; Yours, sincerely!; Gao",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4703:3371,Usability,simpl,simple,3371,"dBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-workflow-logs/workflow.8fc94dc1-722b-40d5-9840-9d6e4a66db21.log: File name too long; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:563); ... 32 common frames omitted. cromwell: v36.1; my working dir is: /nfs/disk3/user/gaoyuhui/github/test, has only 2.wdl and 2.json for test, but everytime this simple task is getting recursive and finally file name too long, and when change backend to local, it is the same. I found other topic and change to other dir to run this wdl task, got the same error.; so, can someone check about this? How can I goes well. it is a bug or my mistake??; Yours, sincerely!; Gao",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703
https://github.com/broadinstitute/cromwell/issues/4708:64,Security,Authoriz,Authorization,64,"**Not working with ""gzip"":**; ```shell; $ curl \; -X GET \; -H ""Authorization: Bearer $(gcloud auth application-default print-access-token)"" \; -H 'Accept-Encoding: gzip' \; ""https://${CAAS_PROD}/api/workflows/v1/d7923869-af12-4a09-9a5d-fa1aaef84e90/metadata?expandSubWorkflows=false"" \; | wc; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:00:05 --:--:-- 0; curl: (18) transfer closed with outstanding read data remaining; 0 0 0; $ ; ```. **Working with ""identity"":**; ```shell; $ curl \; -X GET \; -H ""Authorization: Bearer $(gcloud auth application-default print-access-token)"" \; -H 'Accept-Encoding: identity' \; ""https://${CAAS_PROD}/api/workflows/v1/d7923869-af12-4a09-9a5d-fa1aaef84e90/metadata?expandSubWorkflows=false"" \; | wc; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 16.0M 100 16.0M 0 0 740k 0 0:00:22 0:00:22 --:--:-- 1100k; 0 902345 16872427; $ ; ```. NOTE:; This perhaps may only occur on larger (possibly chunked?) payloads. The above json is 16MB decompressed. If required Rex should be able to give you access to this workflow via FC if you want to reproduce this exact case. A/C:; - curl works with gzip encoding for a 16MB+ json response; - regression test to ensure this doesn't re-occur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4708
https://github.com/broadinstitute/cromwell/issues/4708:126,Security,access,access-token,126,"**Not working with ""gzip"":**; ```shell; $ curl \; -X GET \; -H ""Authorization: Bearer $(gcloud auth application-default print-access-token)"" \; -H 'Accept-Encoding: gzip' \; ""https://${CAAS_PROD}/api/workflows/v1/d7923869-af12-4a09-9a5d-fa1aaef84e90/metadata?expandSubWorkflows=false"" \; | wc; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:00:05 --:--:-- 0; curl: (18) transfer closed with outstanding read data remaining; 0 0 0; $ ; ```. **Working with ""identity"":**; ```shell; $ curl \; -X GET \; -H ""Authorization: Bearer $(gcloud auth application-default print-access-token)"" \; -H 'Accept-Encoding: identity' \; ""https://${CAAS_PROD}/api/workflows/v1/d7923869-af12-4a09-9a5d-fa1aaef84e90/metadata?expandSubWorkflows=false"" \; | wc; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 16.0M 100 16.0M 0 0 740k 0 0:00:22 0:00:22 --:--:-- 1100k; 0 902345 16872427; $ ; ```. NOTE:; This perhaps may only occur on larger (possibly chunked?) payloads. The above json is 16MB decompressed. If required Rex should be able to give you access to this workflow via FC if you want to reproduce this exact case. A/C:; - curl works with gzip encoding for a 16MB+ json response; - regression test to ensure this doesn't re-occur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4708
https://github.com/broadinstitute/cromwell/issues/4708:586,Security,Authoriz,Authorization,586,"**Not working with ""gzip"":**; ```shell; $ curl \; -X GET \; -H ""Authorization: Bearer $(gcloud auth application-default print-access-token)"" \; -H 'Accept-Encoding: gzip' \; ""https://${CAAS_PROD}/api/workflows/v1/d7923869-af12-4a09-9a5d-fa1aaef84e90/metadata?expandSubWorkflows=false"" \; | wc; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:00:05 --:--:-- 0; curl: (18) transfer closed with outstanding read data remaining; 0 0 0; $ ; ```. **Working with ""identity"":**; ```shell; $ curl \; -X GET \; -H ""Authorization: Bearer $(gcloud auth application-default print-access-token)"" \; -H 'Accept-Encoding: identity' \; ""https://${CAAS_PROD}/api/workflows/v1/d7923869-af12-4a09-9a5d-fa1aaef84e90/metadata?expandSubWorkflows=false"" \; | wc; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 16.0M 100 16.0M 0 0 740k 0 0:00:22 0:00:22 --:--:-- 1100k; 0 902345 16872427; $ ; ```. NOTE:; This perhaps may only occur on larger (possibly chunked?) payloads. The above json is 16MB decompressed. If required Rex should be able to give you access to this workflow via FC if you want to reproduce this exact case. A/C:; - curl works with gzip encoding for a 16MB+ json response; - regression test to ensure this doesn't re-occur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4708
https://github.com/broadinstitute/cromwell/issues/4708:648,Security,access,access-token,648,"**Not working with ""gzip"":**; ```shell; $ curl \; -X GET \; -H ""Authorization: Bearer $(gcloud auth application-default print-access-token)"" \; -H 'Accept-Encoding: gzip' \; ""https://${CAAS_PROD}/api/workflows/v1/d7923869-af12-4a09-9a5d-fa1aaef84e90/metadata?expandSubWorkflows=false"" \; | wc; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:00:05 --:--:-- 0; curl: (18) transfer closed with outstanding read data remaining; 0 0 0; $ ; ```. **Working with ""identity"":**; ```shell; $ curl \; -X GET \; -H ""Authorization: Bearer $(gcloud auth application-default print-access-token)"" \; -H 'Accept-Encoding: identity' \; ""https://${CAAS_PROD}/api/workflows/v1/d7923869-af12-4a09-9a5d-fa1aaef84e90/metadata?expandSubWorkflows=false"" \; | wc; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 16.0M 100 16.0M 0 0 740k 0 0:00:22 0:00:22 --:--:-- 1100k; 0 902345 16872427; $ ; ```. NOTE:; This perhaps may only occur on larger (possibly chunked?) payloads. The above json is 16MB decompressed. If required Rex should be able to give you access to this workflow via FC if you want to reproduce this exact case. A/C:; - curl works with gzip encoding for a 16MB+ json response; - regression test to ensure this doesn't re-occur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4708
https://github.com/broadinstitute/cromwell/issues/4708:1168,Security,access,access,1168,"**Not working with ""gzip"":**; ```shell; $ curl \; -X GET \; -H ""Authorization: Bearer $(gcloud auth application-default print-access-token)"" \; -H 'Accept-Encoding: gzip' \; ""https://${CAAS_PROD}/api/workflows/v1/d7923869-af12-4a09-9a5d-fa1aaef84e90/metadata?expandSubWorkflows=false"" \; | wc; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:00:05 --:--:-- 0; curl: (18) transfer closed with outstanding read data remaining; 0 0 0; $ ; ```. **Working with ""identity"":**; ```shell; $ curl \; -X GET \; -H ""Authorization: Bearer $(gcloud auth application-default print-access-token)"" \; -H 'Accept-Encoding: identity' \; ""https://${CAAS_PROD}/api/workflows/v1/d7923869-af12-4a09-9a5d-fa1aaef84e90/metadata?expandSubWorkflows=false"" \; | wc; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 16.0M 100 16.0M 0 0 740k 0 0:00:22 0:00:22 --:--:-- 1100k; 0 902345 16872427; $ ; ```. NOTE:; This perhaps may only occur on larger (possibly chunked?) payloads. The above json is 16MB decompressed. If required Rex should be able to give you access to this workflow via FC if you want to reproduce this exact case. A/C:; - curl works with gzip encoding for a 16MB+ json response; - regression test to ensure this doesn't re-occur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4708
https://github.com/broadinstitute/cromwell/issues/4708:1319,Testability,test,test,1319,"**Not working with ""gzip"":**; ```shell; $ curl \; -X GET \; -H ""Authorization: Bearer $(gcloud auth application-default print-access-token)"" \; -H 'Accept-Encoding: gzip' \; ""https://${CAAS_PROD}/api/workflows/v1/d7923869-af12-4a09-9a5d-fa1aaef84e90/metadata?expandSubWorkflows=false"" \; | wc; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:00:05 --:--:-- 0; curl: (18) transfer closed with outstanding read data remaining; 0 0 0; $ ; ```. **Working with ""identity"":**; ```shell; $ curl \; -X GET \; -H ""Authorization: Bearer $(gcloud auth application-default print-access-token)"" \; -H 'Accept-Encoding: identity' \; ""https://${CAAS_PROD}/api/workflows/v1/d7923869-af12-4a09-9a5d-fa1aaef84e90/metadata?expandSubWorkflows=false"" \; | wc; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 16.0M 100 16.0M 0 0 740k 0 0:00:22 0:00:22 --:--:-- 1100k; 0 902345 16872427; $ ; ```. NOTE:; This perhaps may only occur on larger (possibly chunked?) payloads. The above json is 16MB decompressed. If required Rex should be able to give you access to this workflow via FC if you want to reproduce this exact case. A/C:; - curl works with gzip encoding for a 16MB+ json response; - regression test to ensure this doesn't re-occur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4708
https://github.com/broadinstitute/cromwell/issues/4709:1718,Availability,error,errors,1718," creating a `CallableTaskDefinition` structure. This structure has an empty meta section. When rewriting `foo` in draft-2, this works as expected. . ```wdl; version 1.0. task foo {; input {; String buf ; }; command {}; output {; String s = buf; } ; meta {; type : ""native"",; id: ""applet-xxxx""; }; }; ```. This is the code used to parse the task: ; ```scala; // Extract the only task from a namespace ; def getMainTask(bundle: WomBundle) : CallableTaskDefinition = {; // check if the primary is nonempty ; val task: Option[CallableTaskDefinition] = bundle.primaryCallable match {; case Some(task : CallableTaskDefinition) => Some(task); case Some(exec : ExecutableTaskDefinition) => Some(exec.callableTaskDefinition); case _ => None; }; task match {; case Some(x) => x; case None =>; // primary is empty, check the allCallables map ; if (bundle.allCallables.size != 1); throw new Exception(""WDL file must contains exactly one task""); val (_, task) = bundle.allCallables.head; task match {; case task : CallableTaskDefinition => task; case exec : ExecutableTaskDefinition => exec.callableTaskDefinition; case _ => throw new Exception(""Cannot find task inside WDL file""); 		}; }; }. def parseWdlTask(wfSource: String) : CallableTaskDefinition = {; val languageFactory =; if (wfSource.startsWith(""version 1.0"") ||; wfSource.startsWith(""version draft-3"")) {; new WdlDraft3LanguageFactory(ConfigFactory.empty()); } else {; new WdlDraft2LanguageFactory(ConfigFactory.empty()); }. val bundleChk: Checked[WomBundle] =; languageFactory.getWomBundle(wfSource, ""{}"", List.empty, List(languageFactory)); val womBundle = bundleChk match {; case Left(errors) => throw new Exception(s""""""|WOM validation errors: ; | ${errors} ; |"""""".stripMargin); case Right(bundle) => bundle; }; val task: Option[CallableTaskDefinition] = bundle.primaryCallable match {; case Some(task : CallableTaskDefinition) => Some(task); case Some(exec : ExecutableTaskDefinition) => Some(exec.callableTaskDefinition); case _ => None; }. }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4709
https://github.com/broadinstitute/cromwell/issues/4709:1769,Availability,error,errors,1769," creating a `CallableTaskDefinition` structure. This structure has an empty meta section. When rewriting `foo` in draft-2, this works as expected. . ```wdl; version 1.0. task foo {; input {; String buf ; }; command {}; output {; String s = buf; } ; meta {; type : ""native"",; id: ""applet-xxxx""; }; }; ```. This is the code used to parse the task: ; ```scala; // Extract the only task from a namespace ; def getMainTask(bundle: WomBundle) : CallableTaskDefinition = {; // check if the primary is nonempty ; val task: Option[CallableTaskDefinition] = bundle.primaryCallable match {; case Some(task : CallableTaskDefinition) => Some(task); case Some(exec : ExecutableTaskDefinition) => Some(exec.callableTaskDefinition); case _ => None; }; task match {; case Some(x) => x; case None =>; // primary is empty, check the allCallables map ; if (bundle.allCallables.size != 1); throw new Exception(""WDL file must contains exactly one task""); val (_, task) = bundle.allCallables.head; task match {; case task : CallableTaskDefinition => task; case exec : ExecutableTaskDefinition => exec.callableTaskDefinition; case _ => throw new Exception(""Cannot find task inside WDL file""); 		}; }; }. def parseWdlTask(wfSource: String) : CallableTaskDefinition = {; val languageFactory =; if (wfSource.startsWith(""version 1.0"") ||; wfSource.startsWith(""version draft-3"")) {; new WdlDraft3LanguageFactory(ConfigFactory.empty()); } else {; new WdlDraft2LanguageFactory(ConfigFactory.empty()); }. val bundleChk: Checked[WomBundle] =; languageFactory.getWomBundle(wfSource, ""{}"", List.empty, List(languageFactory)); val womBundle = bundleChk match {; case Left(errors) => throw new Exception(s""""""|WOM validation errors: ; | ${errors} ; |"""""".stripMargin); case Right(bundle) => bundle; }; val task: Option[CallableTaskDefinition] = bundle.primaryCallable match {; case Some(task : CallableTaskDefinition) => Some(task); case Some(exec : ExecutableTaskDefinition) => Some(exec.callableTaskDefinition); case _ => None; }. }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4709
https://github.com/broadinstitute/cromwell/issues/4709:1783,Availability,error,errors,1783," creating a `CallableTaskDefinition` structure. This structure has an empty meta section. When rewriting `foo` in draft-2, this works as expected. . ```wdl; version 1.0. task foo {; input {; String buf ; }; command {}; output {; String s = buf; } ; meta {; type : ""native"",; id: ""applet-xxxx""; }; }; ```. This is the code used to parse the task: ; ```scala; // Extract the only task from a namespace ; def getMainTask(bundle: WomBundle) : CallableTaskDefinition = {; // check if the primary is nonempty ; val task: Option[CallableTaskDefinition] = bundle.primaryCallable match {; case Some(task : CallableTaskDefinition) => Some(task); case Some(exec : ExecutableTaskDefinition) => Some(exec.callableTaskDefinition); case _ => None; }; task match {; case Some(x) => x; case None =>; // primary is empty, check the allCallables map ; if (bundle.allCallables.size != 1); throw new Exception(""WDL file must contains exactly one task""); val (_, task) = bundle.allCallables.head; task match {; case task : CallableTaskDefinition => task; case exec : ExecutableTaskDefinition => exec.callableTaskDefinition; case _ => throw new Exception(""Cannot find task inside WDL file""); 		}; }; }. def parseWdlTask(wfSource: String) : CallableTaskDefinition = {; val languageFactory =; if (wfSource.startsWith(""version 1.0"") ||; wfSource.startsWith(""version draft-3"")) {; new WdlDraft3LanguageFactory(ConfigFactory.empty()); } else {; new WdlDraft2LanguageFactory(ConfigFactory.empty()); }. val bundleChk: Checked[WomBundle] =; languageFactory.getWomBundle(wfSource, ""{}"", List.empty, List(languageFactory)); val womBundle = bundleChk match {; case Left(errors) => throw new Exception(s""""""|WOM validation errors: ; | ${errors} ; |"""""".stripMargin); case Right(bundle) => bundle; }; val task: Option[CallableTaskDefinition] = bundle.primaryCallable match {; case Some(task : CallableTaskDefinition) => Some(task); case Some(exec : ExecutableTaskDefinition) => Some(exec.callableTaskDefinition); case _ => None; }. }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4709
https://github.com/broadinstitute/cromwell/issues/4709:1465,Modifiability,Config,ConfigFactory,1465," creating a `CallableTaskDefinition` structure. This structure has an empty meta section. When rewriting `foo` in draft-2, this works as expected. . ```wdl; version 1.0. task foo {; input {; String buf ; }; command {}; output {; String s = buf; } ; meta {; type : ""native"",; id: ""applet-xxxx""; }; }; ```. This is the code used to parse the task: ; ```scala; // Extract the only task from a namespace ; def getMainTask(bundle: WomBundle) : CallableTaskDefinition = {; // check if the primary is nonempty ; val task: Option[CallableTaskDefinition] = bundle.primaryCallable match {; case Some(task : CallableTaskDefinition) => Some(task); case Some(exec : ExecutableTaskDefinition) => Some(exec.callableTaskDefinition); case _ => None; }; task match {; case Some(x) => x; case None =>; // primary is empty, check the allCallables map ; if (bundle.allCallables.size != 1); throw new Exception(""WDL file must contains exactly one task""); val (_, task) = bundle.allCallables.head; task match {; case task : CallableTaskDefinition => task; case exec : ExecutableTaskDefinition => exec.callableTaskDefinition; case _ => throw new Exception(""Cannot find task inside WDL file""); 		}; }; }. def parseWdlTask(wfSource: String) : CallableTaskDefinition = {; val languageFactory =; if (wfSource.startsWith(""version 1.0"") ||; wfSource.startsWith(""version draft-3"")) {; new WdlDraft3LanguageFactory(ConfigFactory.empty()); } else {; new WdlDraft2LanguageFactory(ConfigFactory.empty()); }. val bundleChk: Checked[WomBundle] =; languageFactory.getWomBundle(wfSource, ""{}"", List.empty, List(languageFactory)); val womBundle = bundleChk match {; case Left(errors) => throw new Exception(s""""""|WOM validation errors: ; | ${errors} ; |"""""".stripMargin); case Right(bundle) => bundle; }; val task: Option[CallableTaskDefinition] = bundle.primaryCallable match {; case Some(task : CallableTaskDefinition) => Some(task); case Some(exec : ExecutableTaskDefinition) => Some(exec.callableTaskDefinition); case _ => None; }. }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4709
https://github.com/broadinstitute/cromwell/issues/4709:1528,Modifiability,Config,ConfigFactory,1528," creating a `CallableTaskDefinition` structure. This structure has an empty meta section. When rewriting `foo` in draft-2, this works as expected. . ```wdl; version 1.0. task foo {; input {; String buf ; }; command {}; output {; String s = buf; } ; meta {; type : ""native"",; id: ""applet-xxxx""; }; }; ```. This is the code used to parse the task: ; ```scala; // Extract the only task from a namespace ; def getMainTask(bundle: WomBundle) : CallableTaskDefinition = {; // check if the primary is nonempty ; val task: Option[CallableTaskDefinition] = bundle.primaryCallable match {; case Some(task : CallableTaskDefinition) => Some(task); case Some(exec : ExecutableTaskDefinition) => Some(exec.callableTaskDefinition); case _ => None; }; task match {; case Some(x) => x; case None =>; // primary is empty, check the allCallables map ; if (bundle.allCallables.size != 1); throw new Exception(""WDL file must contains exactly one task""); val (_, task) = bundle.allCallables.head; task match {; case task : CallableTaskDefinition => task; case exec : ExecutableTaskDefinition => exec.callableTaskDefinition; case _ => throw new Exception(""Cannot find task inside WDL file""); 		}; }; }. def parseWdlTask(wfSource: String) : CallableTaskDefinition = {; val languageFactory =; if (wfSource.startsWith(""version 1.0"") ||; wfSource.startsWith(""version draft-3"")) {; new WdlDraft3LanguageFactory(ConfigFactory.empty()); } else {; new WdlDraft2LanguageFactory(ConfigFactory.empty()); }. val bundleChk: Checked[WomBundle] =; languageFactory.getWomBundle(wfSource, ""{}"", List.empty, List(languageFactory)); val womBundle = bundleChk match {; case Left(errors) => throw new Exception(s""""""|WOM validation errors: ; | ${errors} ; |"""""".stripMargin); case Right(bundle) => bundle; }; val task: Option[CallableTaskDefinition] = bundle.primaryCallable match {; case Some(task : CallableTaskDefinition) => Some(task); case Some(exec : ExecutableTaskDefinition) => Some(exec.callableTaskDefinition); case _ => None; }. }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4709
https://github.com/broadinstitute/cromwell/issues/4709:1758,Security,validat,validation,1758," creating a `CallableTaskDefinition` structure. This structure has an empty meta section. When rewriting `foo` in draft-2, this works as expected. . ```wdl; version 1.0. task foo {; input {; String buf ; }; command {}; output {; String s = buf; } ; meta {; type : ""native"",; id: ""applet-xxxx""; }; }; ```. This is the code used to parse the task: ; ```scala; // Extract the only task from a namespace ; def getMainTask(bundle: WomBundle) : CallableTaskDefinition = {; // check if the primary is nonempty ; val task: Option[CallableTaskDefinition] = bundle.primaryCallable match {; case Some(task : CallableTaskDefinition) => Some(task); case Some(exec : ExecutableTaskDefinition) => Some(exec.callableTaskDefinition); case _ => None; }; task match {; case Some(x) => x; case None =>; // primary is empty, check the allCallables map ; if (bundle.allCallables.size != 1); throw new Exception(""WDL file must contains exactly one task""); val (_, task) = bundle.allCallables.head; task match {; case task : CallableTaskDefinition => task; case exec : ExecutableTaskDefinition => exec.callableTaskDefinition; case _ => throw new Exception(""Cannot find task inside WDL file""); 		}; }; }. def parseWdlTask(wfSource: String) : CallableTaskDefinition = {; val languageFactory =; if (wfSource.startsWith(""version 1.0"") ||; wfSource.startsWith(""version draft-3"")) {; new WdlDraft3LanguageFactory(ConfigFactory.empty()); } else {; new WdlDraft2LanguageFactory(ConfigFactory.empty()); }. val bundleChk: Checked[WomBundle] =; languageFactory.getWomBundle(wfSource, ""{}"", List.empty, List(languageFactory)); val womBundle = bundleChk match {; case Left(errors) => throw new Exception(s""""""|WOM validation errors: ; | ${errors} ; |"""""".stripMargin); case Right(bundle) => bundle; }; val task: Option[CallableTaskDefinition] = bundle.primaryCallable match {; case Some(task : CallableTaskDefinition) => Some(task); case Some(exec : ExecutableTaskDefinition) => Some(exec.callableTaskDefinition); case _ => None; }. }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4709
https://github.com/broadinstitute/cromwell/issues/4710:231,Availability,failure,failure,231,"Hi All, . I am currently running Cromwell 36 in AWS batch (both Server and run mode) in one account, lets say ACCT1, and all of my resource files are cross account in another account (ACCT2). Intermittently jobs will fail due to a failure to download a reference file. Error seen through the Cromwell proxy logs:. > aws s3 cp --sse AES256 --no-progress s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt /cromwell_root//s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt download failed: s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt to ../cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt (""Connection broken: error(104, 'Connection reset by peer')"", error(104, 'Connection reset by `peer')). I'm wondering if it is in the scope of Cromwell on AWS to have a file download retry, or check at least, rather than continuing the task with an empty file which may persist silently causing troubleshooting to be quite difficult. Best,; Adam",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4710
https://github.com/broadinstitute/cromwell/issues/4710:242,Availability,down,download,242,"Hi All, . I am currently running Cromwell 36 in AWS batch (both Server and run mode) in one account, lets say ACCT1, and all of my resource files are cross account in another account (ACCT2). Intermittently jobs will fail due to a failure to download a reference file. Error seen through the Cromwell proxy logs:. > aws s3 cp --sse AES256 --no-progress s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt /cromwell_root//s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt download failed: s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt to ../cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt (""Connection broken: error(104, 'Connection reset by peer')"", error(104, 'Connection reset by `peer')). I'm wondering if it is in the scope of Cromwell on AWS to have a file download retry, or check at least, rather than continuing the task with an empty file which may persist silently causing troubleshooting to be quite difficult. Best,; Adam",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4710
https://github.com/broadinstitute/cromwell/issues/4710:269,Availability,Error,Error,269,"Hi All, . I am currently running Cromwell 36 in AWS batch (both Server and run mode) in one account, lets say ACCT1, and all of my resource files are cross account in another account (ACCT2). Intermittently jobs will fail due to a failure to download a reference file. Error seen through the Cromwell proxy logs:. > aws s3 cp --sse AES256 --no-progress s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt /cromwell_root//s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt download failed: s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt to ../cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt (""Connection broken: error(104, 'Connection reset by peer')"", error(104, 'Connection reset by `peer')). I'm wondering if it is in the scope of Cromwell on AWS to have a file download retry, or check at least, rather than continuing the task with an empty file which may persist silently causing troubleshooting to be quite difficult. Best,; Adam",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4710
https://github.com/broadinstitute/cromwell/issues/4710:514,Availability,down,download,514,"Hi All, . I am currently running Cromwell 36 in AWS batch (both Server and run mode) in one account, lets say ACCT1, and all of my resource files are cross account in another account (ACCT2). Intermittently jobs will fail due to a failure to download a reference file. Error seen through the Cromwell proxy logs:. > aws s3 cp --sse AES256 --no-progress s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt /cromwell_root//s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt download failed: s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt to ../cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt (""Connection broken: error(104, 'Connection reset by peer')"", error(104, 'Connection reset by `peer')). I'm wondering if it is in the scope of Cromwell on AWS to have a file download retry, or check at least, rather than continuing the task with an empty file which may persist silently causing troubleshooting to be quite difficult. Best,; Adam",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4710
https://github.com/broadinstitute/cromwell/issues/4710:717,Availability,error,error,717,"Hi All, . I am currently running Cromwell 36 in AWS batch (both Server and run mode) in one account, lets say ACCT1, and all of my resource files are cross account in another account (ACCT2). Intermittently jobs will fail due to a failure to download a reference file. Error seen through the Cromwell proxy logs:. > aws s3 cp --sse AES256 --no-progress s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt /cromwell_root//s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt download failed: s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt to ../cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt (""Connection broken: error(104, 'Connection reset by peer')"", error(104, 'Connection reset by `peer')). I'm wondering if it is in the scope of Cromwell on AWS to have a file download retry, or check at least, rather than continuing the task with an empty file which may persist silently causing troubleshooting to be quite difficult. Best,; Adam",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4710
https://github.com/broadinstitute/cromwell/issues/4710:758,Availability,error,error,758,"Hi All, . I am currently running Cromwell 36 in AWS batch (both Server and run mode) in one account, lets say ACCT1, and all of my resource files are cross account in another account (ACCT2). Intermittently jobs will fail due to a failure to download a reference file. Error seen through the Cromwell proxy logs:. > aws s3 cp --sse AES256 --no-progress s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt /cromwell_root//s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt download failed: s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt to ../cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt (""Connection broken: error(104, 'Connection reset by peer')"", error(104, 'Connection reset by `peer')). I'm wondering if it is in the scope of Cromwell on AWS to have a file download retry, or check at least, rather than continuing the task with an empty file which may persist silently causing troubleshooting to be quite difficult. Best,; Adam",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4710
https://github.com/broadinstitute/cromwell/issues/4710:870,Availability,down,download,870,"Hi All, . I am currently running Cromwell 36 in AWS batch (both Server and run mode) in one account, lets say ACCT1, and all of my resource files are cross account in another account (ACCT2). Intermittently jobs will fail due to a failure to download a reference file. Error seen through the Cromwell proxy logs:. > aws s3 cp --sse AES256 --no-progress s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt /cromwell_root//s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt download failed: s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt to ../cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt (""Connection broken: error(104, 'Connection reset by peer')"", error(104, 'Connection reset by `peer')). I'm wondering if it is in the scope of Cromwell on AWS to have a file download retry, or check at least, rather than continuing the task with an empty file which may persist silently causing troubleshooting to be quite difficult. Best,; Adam",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4710
https://github.com/broadinstitute/cromwell/issues/4710:307,Testability,log,logs,307,"Hi All, . I am currently running Cromwell 36 in AWS batch (both Server and run mode) in one account, lets say ACCT1, and all of my resource files are cross account in another account (ACCT2). Intermittently jobs will fail due to a failure to download a reference file. Error seen through the Cromwell proxy logs:. > aws s3 cp --sse AES256 --no-progress s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt /cromwell_root//s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt download failed: s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt to ../cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt (""Connection broken: error(104, 'Connection reset by peer')"", error(104, 'Connection reset by `peer')). I'm wondering if it is in the scope of Cromwell on AWS to have a file download retry, or check at least, rather than continuing the task with an empty file which may persist silently causing troubleshooting to be quite difficult. Best,; Adam",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4710
https://github.com/broadinstitute/cromwell/issues/4710:379,Testability,benchmark,benchmark-,379,"Hi All, . I am currently running Cromwell 36 in AWS batch (both Server and run mode) in one account, lets say ACCT1, and all of my resource files are cross account in another account (ACCT2). Intermittently jobs will fail due to a failure to download a reference file. Error seen through the Cromwell proxy logs:. > aws s3 cp --sse AES256 --no-progress s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt /cromwell_root//s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt download failed: s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt to ../cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt (""Connection broken: error(104, 'Connection reset by peer')"", error(104, 'Connection reset by `peer')). I'm wondering if it is in the scope of Cromwell on AWS to have a file download retry, or check at least, rather than continuing the task with an empty file which may persist silently causing troubleshooting to be quite difficult. Best,; Adam",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4710
https://github.com/broadinstitute/cromwell/issues/4710:465,Testability,benchmark,benchmark-,465,"Hi All, . I am currently running Cromwell 36 in AWS batch (both Server and run mode) in one account, lets say ACCT1, and all of my resource files are cross account in another account (ACCT2). Intermittently jobs will fail due to a failure to download a reference file. Error seen through the Cromwell proxy logs:. > aws s3 cp --sse AES256 --no-progress s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt /cromwell_root//s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt download failed: s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt to ../cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt (""Connection broken: error(104, 'Connection reset by peer')"", error(104, 'Connection reset by `peer')). I'm wondering if it is in the scope of Cromwell on AWS to have a file download retry, or check at least, rather than continuing the task with an empty file which may persist silently causing troubleshooting to be quite difficult. Best,; Adam",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4710
https://github.com/broadinstitute/cromwell/issues/4710:557,Testability,benchmark,benchmark-,557,"Hi All, . I am currently running Cromwell 36 in AWS batch (both Server and run mode) in one account, lets say ACCT1, and all of my resource files are cross account in another account (ACCT2). Intermittently jobs will fail due to a failure to download a reference file. Error seen through the Cromwell proxy logs:. > aws s3 cp --sse AES256 --no-progress s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt /cromwell_root//s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt download failed: s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt to ../cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt (""Connection broken: error(104, 'Connection reset by peer')"", error(104, 'Connection reset by `peer')). I'm wondering if it is in the scope of Cromwell on AWS to have a file download retry, or check at least, rather than continuing the task with an empty file which may persist silently causing troubleshooting to be quite difficult. Best,; Adam",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4710
https://github.com/broadinstitute/cromwell/issues/4710:647,Testability,benchmark,benchmark-,647,"Hi All, . I am currently running Cromwell 36 in AWS batch (both Server and run mode) in one account, lets say ACCT1, and all of my resource files are cross account in another account (ACCT2). Intermittently jobs will fail due to a failure to download a reference file. Error seen through the Cromwell proxy logs:. > aws s3 cp --sse AES256 --no-progress s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt /cromwell_root//s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt download failed: s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt to ../cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt (""Connection broken: error(104, 'Connection reset by peer')"", error(104, 'Connection reset by `peer')). I'm wondering if it is in the scope of Cromwell on AWS to have a file download retry, or check at least, rather than continuing the task with an empty file which may persist silently causing troubleshooting to be quite difficult. Best,; Adam",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4710
https://github.com/broadinstitute/cromwell/issues/4712:5,Testability,test,test,5,This test is verifying an incorrect behavior: https://github.com/broadinstitute/cromwell/blob/develop/wom/src/test/scala/wom/types/WomMapTypeSpec.scala#L77. See also:. - https://github.com/broadinstitute/cromwell/issues/4663 (I think this one is working as expected because we can't coerce a `WomMap` to a `WomString`),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4712
https://github.com/broadinstitute/cromwell/issues/4712:110,Testability,test,test,110,This test is verifying an incorrect behavior: https://github.com/broadinstitute/cromwell/blob/develop/wom/src/test/scala/wom/types/WomMapTypeSpec.scala#L77. See also:. - https://github.com/broadinstitute/cromwell/issues/4663 (I think this one is working as expected because we can't coerce a `WomMap` to a `WomString`),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4712
https://github.com/broadinstitute/cromwell/issues/4722:62,Testability,test,test,62,"try follow wdl code in cromwell36; ```; version 1.0. workflow test {; Map[String,String] aa= {""key1"": ""value1"", ""key2"": ""value2""}; call example { input:map=aa }; }. task example {; input {; Map[String, String] map; }. command {; cat ${write_map(map)}; }; }; ```. get a strange tmp file ; ""; cat 377f5912-77cb-409f-8888-ac46fef2c448/call-example/execution/write_map_6b29aeb5bdc2e2cffaf159a56d2ad1f4.tmp; key1 key2; value1 value2. ""; but when using draft-2, the output is ok; ""; cat 7435a991-a7be-400c-b503-94df322a619d/call-example/execution/write_map_4b4637e67d71685228a1ff8b94e7d3e1.tmp; key1 value1; key2 value2; ""; can someone fix this in version1.0 ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4722
https://github.com/broadinstitute/cromwell/issues/4723:198,Deployability,deploy,deployment,198,"Cromwell publishes no metadata regarding which Cromwell instances have processed a workflow. This has been quite reasonable up to now since it has only been possible to have one worker Cromwell per deployment, but that is now changing with horizontal Cromwell. It would useful for operations and provenance to know which instances have been involved in processing a workflow. Things to think about:. - This data can be multi-valued; - Time indexes are important; - Stable Cromwell identifiers are important. Conversely, ephemeral Cromwell IDs that evaporate on restarts aren't useful.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4723
https://github.com/broadinstitute/cromwell/issues/4724:372,Availability,echo,echo,372,"According to of the WDL spec, if an undefined optional value is used in a string interpolation expression, the result of the expression is the empty string. However this doesn't seem to work correctly if there is more than two arguments to `+`, e.g. (x + y + z). ```; version 1.0. workflow Test {; call Tester; }. task Tester {; input {; String? optional; }; command <<<; echo interpolation 1: ~{optional + "" something2""}; echo interpolation 2: ~{""something1 "" + optional}; echo interpolation 3: ~{""something1 "" + optional + "" something2""}; >>>; output {; String out = read_string(stdout()); }; }; ```; When run, example 1 and 2 produces no output but example 3 produces output. I would expect 3 to also produce no output.; ```; $ java -jar cromwell-38.jar run /tmp/test.wdl ; ...; {; ""outputs"": {; ""Test.Tester.out"": ""interpolation 1:\ninterpolation 2:\ninterpolation 3: something2""; },; ""id"": ""8e2523af-b6eb-45d9-901f-8c7e0b6bd726""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4724
https://github.com/broadinstitute/cromwell/issues/4724:423,Availability,echo,echo,423,"According to of the WDL spec, if an undefined optional value is used in a string interpolation expression, the result of the expression is the empty string. However this doesn't seem to work correctly if there is more than two arguments to `+`, e.g. (x + y + z). ```; version 1.0. workflow Test {; call Tester; }. task Tester {; input {; String? optional; }; command <<<; echo interpolation 1: ~{optional + "" something2""}; echo interpolation 2: ~{""something1 "" + optional}; echo interpolation 3: ~{""something1 "" + optional + "" something2""}; >>>; output {; String out = read_string(stdout()); }; }; ```; When run, example 1 and 2 produces no output but example 3 produces output. I would expect 3 to also produce no output.; ```; $ java -jar cromwell-38.jar run /tmp/test.wdl ; ...; {; ""outputs"": {; ""Test.Tester.out"": ""interpolation 1:\ninterpolation 2:\ninterpolation 3: something2""; },; ""id"": ""8e2523af-b6eb-45d9-901f-8c7e0b6bd726""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4724
https://github.com/broadinstitute/cromwell/issues/4724:474,Availability,echo,echo,474,"According to of the WDL spec, if an undefined optional value is used in a string interpolation expression, the result of the expression is the empty string. However this doesn't seem to work correctly if there is more than two arguments to `+`, e.g. (x + y + z). ```; version 1.0. workflow Test {; call Tester; }. task Tester {; input {; String? optional; }; command <<<; echo interpolation 1: ~{optional + "" something2""}; echo interpolation 2: ~{""something1 "" + optional}; echo interpolation 3: ~{""something1 "" + optional + "" something2""}; >>>; output {; String out = read_string(stdout()); }; }; ```; When run, example 1 and 2 produces no output but example 3 produces output. I would expect 3 to also produce no output.; ```; $ java -jar cromwell-38.jar run /tmp/test.wdl ; ...; {; ""outputs"": {; ""Test.Tester.out"": ""interpolation 1:\ninterpolation 2:\ninterpolation 3: something2""; },; ""id"": ""8e2523af-b6eb-45d9-901f-8c7e0b6bd726""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4724
https://github.com/broadinstitute/cromwell/issues/4724:290,Testability,Test,Test,290,"According to of the WDL spec, if an undefined optional value is used in a string interpolation expression, the result of the expression is the empty string. However this doesn't seem to work correctly if there is more than two arguments to `+`, e.g. (x + y + z). ```; version 1.0. workflow Test {; call Tester; }. task Tester {; input {; String? optional; }; command <<<; echo interpolation 1: ~{optional + "" something2""}; echo interpolation 2: ~{""something1 "" + optional}; echo interpolation 3: ~{""something1 "" + optional + "" something2""}; >>>; output {; String out = read_string(stdout()); }; }; ```; When run, example 1 and 2 produces no output but example 3 produces output. I would expect 3 to also produce no output.; ```; $ java -jar cromwell-38.jar run /tmp/test.wdl ; ...; {; ""outputs"": {; ""Test.Tester.out"": ""interpolation 1:\ninterpolation 2:\ninterpolation 3: something2""; },; ""id"": ""8e2523af-b6eb-45d9-901f-8c7e0b6bd726""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4724
https://github.com/broadinstitute/cromwell/issues/4724:303,Testability,Test,Tester,303,"According to of the WDL spec, if an undefined optional value is used in a string interpolation expression, the result of the expression is the empty string. However this doesn't seem to work correctly if there is more than two arguments to `+`, e.g. (x + y + z). ```; version 1.0. workflow Test {; call Tester; }. task Tester {; input {; String? optional; }; command <<<; echo interpolation 1: ~{optional + "" something2""}; echo interpolation 2: ~{""something1 "" + optional}; echo interpolation 3: ~{""something1 "" + optional + "" something2""}; >>>; output {; String out = read_string(stdout()); }; }; ```; When run, example 1 and 2 produces no output but example 3 produces output. I would expect 3 to also produce no output.; ```; $ java -jar cromwell-38.jar run /tmp/test.wdl ; ...; {; ""outputs"": {; ""Test.Tester.out"": ""interpolation 1:\ninterpolation 2:\ninterpolation 3: something2""; },; ""id"": ""8e2523af-b6eb-45d9-901f-8c7e0b6bd726""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4724
https://github.com/broadinstitute/cromwell/issues/4724:319,Testability,Test,Tester,319,"According to of the WDL spec, if an undefined optional value is used in a string interpolation expression, the result of the expression is the empty string. However this doesn't seem to work correctly if there is more than two arguments to `+`, e.g. (x + y + z). ```; version 1.0. workflow Test {; call Tester; }. task Tester {; input {; String? optional; }; command <<<; echo interpolation 1: ~{optional + "" something2""}; echo interpolation 2: ~{""something1 "" + optional}; echo interpolation 3: ~{""something1 "" + optional + "" something2""}; >>>; output {; String out = read_string(stdout()); }; }; ```; When run, example 1 and 2 produces no output but example 3 produces output. I would expect 3 to also produce no output.; ```; $ java -jar cromwell-38.jar run /tmp/test.wdl ; ...; {; ""outputs"": {; ""Test.Tester.out"": ""interpolation 1:\ninterpolation 2:\ninterpolation 3: something2""; },; ""id"": ""8e2523af-b6eb-45d9-901f-8c7e0b6bd726""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4724
https://github.com/broadinstitute/cromwell/issues/4724:766,Testability,test,test,766,"According to of the WDL spec, if an undefined optional value is used in a string interpolation expression, the result of the expression is the empty string. However this doesn't seem to work correctly if there is more than two arguments to `+`, e.g. (x + y + z). ```; version 1.0. workflow Test {; call Tester; }. task Tester {; input {; String? optional; }; command <<<; echo interpolation 1: ~{optional + "" something2""}; echo interpolation 2: ~{""something1 "" + optional}; echo interpolation 3: ~{""something1 "" + optional + "" something2""}; >>>; output {; String out = read_string(stdout()); }; }; ```; When run, example 1 and 2 produces no output but example 3 produces output. I would expect 3 to also produce no output.; ```; $ java -jar cromwell-38.jar run /tmp/test.wdl ; ...; {; ""outputs"": {; ""Test.Tester.out"": ""interpolation 1:\ninterpolation 2:\ninterpolation 3: something2""; },; ""id"": ""8e2523af-b6eb-45d9-901f-8c7e0b6bd726""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4724
https://github.com/broadinstitute/cromwell/issues/4724:800,Testability,Test,Test,800,"According to of the WDL spec, if an undefined optional value is used in a string interpolation expression, the result of the expression is the empty string. However this doesn't seem to work correctly if there is more than two arguments to `+`, e.g. (x + y + z). ```; version 1.0. workflow Test {; call Tester; }. task Tester {; input {; String? optional; }; command <<<; echo interpolation 1: ~{optional + "" something2""}; echo interpolation 2: ~{""something1 "" + optional}; echo interpolation 3: ~{""something1 "" + optional + "" something2""}; >>>; output {; String out = read_string(stdout()); }; }; ```; When run, example 1 and 2 produces no output but example 3 produces output. I would expect 3 to also produce no output.; ```; $ java -jar cromwell-38.jar run /tmp/test.wdl ; ...; {; ""outputs"": {; ""Test.Tester.out"": ""interpolation 1:\ninterpolation 2:\ninterpolation 3: something2""; },; ""id"": ""8e2523af-b6eb-45d9-901f-8c7e0b6bd726""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4724
https://github.com/broadinstitute/cromwell/issues/4724:805,Testability,Test,Tester,805,"According to of the WDL spec, if an undefined optional value is used in a string interpolation expression, the result of the expression is the empty string. However this doesn't seem to work correctly if there is more than two arguments to `+`, e.g. (x + y + z). ```; version 1.0. workflow Test {; call Tester; }. task Tester {; input {; String? optional; }; command <<<; echo interpolation 1: ~{optional + "" something2""}; echo interpolation 2: ~{""something1 "" + optional}; echo interpolation 3: ~{""something1 "" + optional + "" something2""}; >>>; output {; String out = read_string(stdout()); }; }; ```; When run, example 1 and 2 produces no output but example 3 produces output. I would expect 3 to also produce no output.; ```; $ java -jar cromwell-38.jar run /tmp/test.wdl ; ...; {; ""outputs"": {; ""Test.Tester.out"": ""interpolation 1:\ninterpolation 2:\ninterpolation 3: something2""; },; ""id"": ""8e2523af-b6eb-45d9-901f-8c7e0b6bd726""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4724
https://github.com/broadinstitute/cromwell/issues/4725:19,Modifiability,config,config,19,"With a local MySQL config, Local backend and call caching enabled, the following Centaur tests consistently fail for me regardless of horicromtal or unicromtal:. - [x] draft3_call_cache_capoeira_local; - [x] call_cache_capoeira_local; - [ ] cwl_dynamic_initial_workdir; - [x] long_cmd",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725
https://github.com/broadinstitute/cromwell/issues/4725:89,Testability,test,tests,89,"With a local MySQL config, Local backend and call caching enabled, the following Centaur tests consistently fail for me regardless of horicromtal or unicromtal:. - [x] draft3_call_cache_capoeira_local; - [x] call_cache_capoeira_local; - [ ] cwl_dynamic_initial_workdir; - [x] long_cmd",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725
https://github.com/broadinstitute/cromwell/issues/4728:709,Deployability,configurat,configuration,709,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4728
https://github.com/broadinstitute/cromwell/issues/4728:709,Modifiability,config,configuration,709,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4728
https://github.com/broadinstitute/cromwell/issues/4728:754,Security,PASSWORD,PASSWORDS,754,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4728
https://github.com/broadinstitute/cromwell/issues/4728:47,Usability,feedback,feedback,47,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4728
https://github.com/broadinstitute/cromwell/issues/4731:531,Availability,error,error,531,"Hi!. I'm having some trouble request s3 objects that are outside my current region (I get a Status Code: 301). . Backend: AWS Batch; Filesystem: S3; Region : `ap-southeast-2`. I'm attempting to run a small genomics pipeline that is trying to request some of the [`broad-reference` open data set](; https://registry.opendata.aws/broad-references) on AWS S3. I can see that open data set exists in `us-east-1`. . Specifically, I'm requesting (`s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta`) and I'm receiving the same error 5 times.; ```; [2019-03-12 11:27:21,50] [error] WorkflowManagerActor Workflow 434834fb-cb24-4bd2-ba44-8a1c929b11f5 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null). 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:217); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:187); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:182); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731
https://github.com/broadinstitute/cromwell/issues/4731:578,Availability,error,error,578,"Hi!. I'm having some trouble request s3 objects that are outside my current region (I get a Status Code: 301). . Backend: AWS Batch; Filesystem: S3; Region : `ap-southeast-2`. I'm attempting to run a small genomics pipeline that is trying to request some of the [`broad-reference` open data set](; https://registry.opendata.aws/broad-references) on AWS S3. I can see that open data set exists in `us-east-1`. . Specifically, I'm requesting (`s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta`) and I'm receiving the same error 5 times.; ```; [2019-03-12 11:27:21,50] [error] WorkflowManagerActor Workflow 434834fb-cb24-4bd2-ba44-8a1c929b11f5 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null). 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:217); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:187); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:182); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731
https://github.com/broadinstitute/cromwell/issues/4731:215,Deployability,pipeline,pipeline,215,"Hi!. I'm having some trouble request s3 objects that are outside my current region (I get a Status Code: 301). . Backend: AWS Batch; Filesystem: S3; Region : `ap-southeast-2`. I'm attempting to run a small genomics pipeline that is trying to request some of the [`broad-reference` open data set](; https://registry.opendata.aws/broad-references) on AWS S3. I can see that open data set exists in `us-east-1`. . Specifically, I'm requesting (`s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta`) and I'm receiving the same error 5 times.; ```; [2019-03-12 11:27:21,50] [error] WorkflowManagerActor Workflow 434834fb-cb24-4bd2-ba44-8a1c929b11f5 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null). 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:217); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:187); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:182); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731
https://github.com/broadinstitute/cromwell/issues/4731:3543,Deployability,configurat,configuration,3543,"SM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:138); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. I'm basically using the standard aws configuration file for Cromwell:. ```hocon; include required(classpath(""application"")). aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 3; numCreateDefinitionAttempts = 3; root = ""s3://$bucketName/cromwell-execution""; auth = ""default""; concurrent-job-limit = 16; default-runtime-attributes {; queueArn = ""arn:aws:batch:ap-southeast-2:$arn""; }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; ```. I've contacted AWS Support, to find out if I could fully (region) qualify the S3 locator (something like [these examples](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html#access-bucket-intro): `s3://us-east-1.amazonaws.com/broad-references/.../file`. . AWS basically said no, and they",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731
https://github.com/broadinstitute/cromwell/issues/4731:3543,Modifiability,config,configuration,3543,"SM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:138); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. I'm basically using the standard aws configuration file for Cromwell:. ```hocon; include required(classpath(""application"")). aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 3; numCreateDefinitionAttempts = 3; root = ""s3://$bucketName/cromwell-execution""; auth = ""default""; concurrent-job-limit = 16; default-runtime-attributes {; queueArn = ""arn:aws:batch:ap-southeast-2:$arn""; }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; ```. I've contacted AWS Support, to find out if I could fully (region) qualify the S3 locator (something like [these examples](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html#access-bucket-intro): `s3://us-east-1.amazonaws.com/broad-references/.../file`. . AWS basically said no, and they",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731
https://github.com/broadinstitute/cromwell/issues/4731:3945,Modifiability,config,config,3945,"r.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. I'm basically using the standard aws configuration file for Cromwell:. ```hocon; include required(classpath(""application"")). aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 3; numCreateDefinitionAttempts = 3; root = ""s3://$bucketName/cromwell-execution""; auth = ""default""; concurrent-job-limit = 16; default-runtime-attributes {; queueArn = ""arn:aws:batch:ap-southeast-2:$arn""; }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; ```. I've contacted AWS Support, to find out if I could fully (region) qualify the S3 locator (something like [these examples](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html#access-bucket-intro): `s3://us-east-1.amazonaws.com/broad-references/.../file`. . AWS basically said no, and they directed me towards https://github.com/aws/aws-sdk-java/issues/1366 (their aws-sdk-java) with an [`enableForceGlobalBucketAccess`](https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3Builder.html#enableForceGlobalBucketAccess--) option on a `AmazonS3Builder`. . I've tried to have a search through Cromwell to work out where this setting could be placed, but I'm a bit lost with project structure and Scala.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731
https://github.com/broadinstitute/cromwell/issues/4731:4075,Performance,concurren,concurrent-job-limit,4075,"r.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. I'm basically using the standard aws configuration file for Cromwell:. ```hocon; include required(classpath(""application"")). aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 3; numCreateDefinitionAttempts = 3; root = ""s3://$bucketName/cromwell-execution""; auth = ""default""; concurrent-job-limit = 16; default-runtime-attributes {; queueArn = ""arn:aws:batch:ap-southeast-2:$arn""; }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; ```. I've contacted AWS Support, to find out if I could fully (region) qualify the S3 locator (something like [these examples](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html#access-bucket-intro): `s3://us-east-1.amazonaws.com/broad-references/.../file`. . AWS basically said no, and they directed me towards https://github.com/aws/aws-sdk-java/issues/1366 (their aws-sdk-java) with an [`enableForceGlobalBucketAccess`](https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3Builder.html#enableForceGlobalBucketAccess--) option on a `AmazonS3Builder`. . I've tried to have a search through Cromwell to work out where this setting could be placed, but I'm a bit lost with project structure and Scala.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731
https://github.com/broadinstitute/cromwell/issues/4731:4132,Performance,queue,queueArn,4132,"r.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. I'm basically using the standard aws configuration file for Cromwell:. ```hocon; include required(classpath(""application"")). aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 3; numCreateDefinitionAttempts = 3; root = ""s3://$bucketName/cromwell-execution""; auth = ""default""; concurrent-job-limit = 16; default-runtime-attributes {; queueArn = ""arn:aws:batch:ap-southeast-2:$arn""; }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; ```. I've contacted AWS Support, to find out if I could fully (region) qualify the S3 locator (something like [these examples](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html#access-bucket-intro): `s3://us-east-1.amazonaws.com/broad-references/.../file`. . AWS basically said no, and they directed me towards https://github.com/aws/aws-sdk-java/issues/1366 (their aws-sdk-java) with an [`enableForceGlobalBucketAccess`](https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3Builder.html#enableForceGlobalBucketAccess--) option on a `AmazonS3Builder`. . I've tried to have a search through Cromwell to work out where this setting could be placed, but I'm a bit lost with project structure and Scala.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731
https://github.com/broadinstitute/cromwell/issues/4731:4428,Security,access,access-bucket-intro,4428,"r.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. I'm basically using the standard aws configuration file for Cromwell:. ```hocon; include required(classpath(""application"")). aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 3; numCreateDefinitionAttempts = 3; root = ""s3://$bucketName/cromwell-execution""; auth = ""default""; concurrent-job-limit = 16; default-runtime-attributes {; queueArn = ""arn:aws:batch:ap-southeast-2:$arn""; }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; ```. I've contacted AWS Support, to find out if I could fully (region) qualify the S3 locator (something like [these examples](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html#access-bucket-intro): `s3://us-east-1.amazonaws.com/broad-references/.../file`. . AWS basically said no, and they directed me towards https://github.com/aws/aws-sdk-java/issues/1366 (their aws-sdk-java) with an [`enableForceGlobalBucketAccess`](https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3Builder.html#enableForceGlobalBucketAccess--) option on a `AmazonS3Builder`. . I've tried to have a search through Cromwell to work out where this setting could be placed, but I'm a bit lost with project structure and Scala.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731
https://github.com/broadinstitute/cromwell/issues/4731:2185,Testability,Log,LoggingFSM,2185, Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null). 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:217); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:187); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:182); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:138); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:138); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:138); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731
https://github.com/broadinstitute/cromwell/issues/4731:2278,Testability,Log,LoggingFSM,2278, S3Client; Status Code: 301; Request ID: null). 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:217); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:187); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:182); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:138); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:138); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:138); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731
https://github.com/broadinstitute/cromwell/issues/4731:2333,Testability,Log,LoggingFSM,2333,mwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:217); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:187); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:182); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:138); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:138); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:138); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731
https://github.com/broadinstitute/cromwell/issues/4732:170,Availability,error,errors,170,"Hello,. I posted a couple of issues on the bcbio-nextgen tracker and I thought I'd link them here as well, in case anyone can help, as these seem to be Cromwell-specific errors. Running bcbio-nextgen with IPython parallel instead of Cromwell works fine for the scenario I've set up. In summary, I'm trying to run variant calling with bcbio-nextgen using Cromwell and 100 small FASTQ files (50-100 reads each), for testing, on an SGE cluster. The main issue seems to be that the master node is coming up under heavy load, for some reason, and, at some point, the Cromwell jobs stop getting scheduled. . This is described in more detail here: https://github.com/bcbio/bcbio-nextgen/issues/2721. A second issue, which might be related, is with the file `cromwell_work/persist/metadata.lobs`. It gets quite big, at 36 GB, for the scenario that I've described earlier, and I'm wondering if this is normal. . I'm also seeing some error messages (`java.sql.BatchUpdateException: lob is no longer valid`) when running bcbio-nextgen with Cromwell, in the same scenario as above. Could this somehow be related to the high cluster load issues as well? . More details are here: https://github.com/bcbio/bcbio-nextgen/issues/2722.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4732
https://github.com/broadinstitute/cromwell/issues/4732:924,Availability,error,error,924,"Hello,. I posted a couple of issues on the bcbio-nextgen tracker and I thought I'd link them here as well, in case anyone can help, as these seem to be Cromwell-specific errors. Running bcbio-nextgen with IPython parallel instead of Cromwell works fine for the scenario I've set up. In summary, I'm trying to run variant calling with bcbio-nextgen using Cromwell and 100 small FASTQ files (50-100 reads each), for testing, on an SGE cluster. The main issue seems to be that the master node is coming up under heavy load, for some reason, and, at some point, the Cromwell jobs stop getting scheduled. . This is described in more detail here: https://github.com/bcbio/bcbio-nextgen/issues/2721. A second issue, which might be related, is with the file `cromwell_work/persist/metadata.lobs`. It gets quite big, at 36 GB, for the scenario that I've described earlier, and I'm wondering if this is normal. . I'm also seeing some error messages (`java.sql.BatchUpdateException: lob is no longer valid`) when running bcbio-nextgen with Cromwell, in the same scenario as above. Could this somehow be related to the high cluster load issues as well? . More details are here: https://github.com/bcbio/bcbio-nextgen/issues/2722.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4732
https://github.com/broadinstitute/cromwell/issues/4732:589,Energy Efficiency,schedul,scheduled,589,"Hello,. I posted a couple of issues on the bcbio-nextgen tracker and I thought I'd link them here as well, in case anyone can help, as these seem to be Cromwell-specific errors. Running bcbio-nextgen with IPython parallel instead of Cromwell works fine for the scenario I've set up. In summary, I'm trying to run variant calling with bcbio-nextgen using Cromwell and 100 small FASTQ files (50-100 reads each), for testing, on an SGE cluster. The main issue seems to be that the master node is coming up under heavy load, for some reason, and, at some point, the Cromwell jobs stop getting scheduled. . This is described in more detail here: https://github.com/bcbio/bcbio-nextgen/issues/2721. A second issue, which might be related, is with the file `cromwell_work/persist/metadata.lobs`. It gets quite big, at 36 GB, for the scenario that I've described earlier, and I'm wondering if this is normal. . I'm also seeing some error messages (`java.sql.BatchUpdateException: lob is no longer valid`) when running bcbio-nextgen with Cromwell, in the same scenario as above. Could this somehow be related to the high cluster load issues as well? . More details are here: https://github.com/bcbio/bcbio-nextgen/issues/2722.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4732
https://github.com/broadinstitute/cromwell/issues/4732:930,Integrability,message,messages,930,"Hello,. I posted a couple of issues on the bcbio-nextgen tracker and I thought I'd link them here as well, in case anyone can help, as these seem to be Cromwell-specific errors. Running bcbio-nextgen with IPython parallel instead of Cromwell works fine for the scenario I've set up. In summary, I'm trying to run variant calling with bcbio-nextgen using Cromwell and 100 small FASTQ files (50-100 reads each), for testing, on an SGE cluster. The main issue seems to be that the master node is coming up under heavy load, for some reason, and, at some point, the Cromwell jobs stop getting scheduled. . This is described in more detail here: https://github.com/bcbio/bcbio-nextgen/issues/2721. A second issue, which might be related, is with the file `cromwell_work/persist/metadata.lobs`. It gets quite big, at 36 GB, for the scenario that I've described earlier, and I'm wondering if this is normal. . I'm also seeing some error messages (`java.sql.BatchUpdateException: lob is no longer valid`) when running bcbio-nextgen with Cromwell, in the same scenario as above. Could this somehow be related to the high cluster load issues as well? . More details are here: https://github.com/bcbio/bcbio-nextgen/issues/2722.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4732
https://github.com/broadinstitute/cromwell/issues/4732:515,Performance,load,load,515,"Hello,. I posted a couple of issues on the bcbio-nextgen tracker and I thought I'd link them here as well, in case anyone can help, as these seem to be Cromwell-specific errors. Running bcbio-nextgen with IPython parallel instead of Cromwell works fine for the scenario I've set up. In summary, I'm trying to run variant calling with bcbio-nextgen using Cromwell and 100 small FASTQ files (50-100 reads each), for testing, on an SGE cluster. The main issue seems to be that the master node is coming up under heavy load, for some reason, and, at some point, the Cromwell jobs stop getting scheduled. . This is described in more detail here: https://github.com/bcbio/bcbio-nextgen/issues/2721. A second issue, which might be related, is with the file `cromwell_work/persist/metadata.lobs`. It gets quite big, at 36 GB, for the scenario that I've described earlier, and I'm wondering if this is normal. . I'm also seeing some error messages (`java.sql.BatchUpdateException: lob is no longer valid`) when running bcbio-nextgen with Cromwell, in the same scenario as above. Could this somehow be related to the high cluster load issues as well? . More details are here: https://github.com/bcbio/bcbio-nextgen/issues/2722.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4732
https://github.com/broadinstitute/cromwell/issues/4732:1120,Performance,load,load,1120,"Hello,. I posted a couple of issues on the bcbio-nextgen tracker and I thought I'd link them here as well, in case anyone can help, as these seem to be Cromwell-specific errors. Running bcbio-nextgen with IPython parallel instead of Cromwell works fine for the scenario I've set up. In summary, I'm trying to run variant calling with bcbio-nextgen using Cromwell and 100 small FASTQ files (50-100 reads each), for testing, on an SGE cluster. The main issue seems to be that the master node is coming up under heavy load, for some reason, and, at some point, the Cromwell jobs stop getting scheduled. . This is described in more detail here: https://github.com/bcbio/bcbio-nextgen/issues/2721. A second issue, which might be related, is with the file `cromwell_work/persist/metadata.lobs`. It gets quite big, at 36 GB, for the scenario that I've described earlier, and I'm wondering if this is normal. . I'm also seeing some error messages (`java.sql.BatchUpdateException: lob is no longer valid`) when running bcbio-nextgen with Cromwell, in the same scenario as above. Could this somehow be related to the high cluster load issues as well? . More details are here: https://github.com/bcbio/bcbio-nextgen/issues/2722.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4732
https://github.com/broadinstitute/cromwell/issues/4732:414,Testability,test,testing,414,"Hello,. I posted a couple of issues on the bcbio-nextgen tracker and I thought I'd link them here as well, in case anyone can help, as these seem to be Cromwell-specific errors. Running bcbio-nextgen with IPython parallel instead of Cromwell works fine for the scenario I've set up. In summary, I'm trying to run variant calling with bcbio-nextgen using Cromwell and 100 small FASTQ files (50-100 reads each), for testing, on an SGE cluster. The main issue seems to be that the master node is coming up under heavy load, for some reason, and, at some point, the Cromwell jobs stop getting scheduled. . This is described in more detail here: https://github.com/bcbio/bcbio-nextgen/issues/2721. A second issue, which might be related, is with the file `cromwell_work/persist/metadata.lobs`. It gets quite big, at 36 GB, for the scenario that I've described earlier, and I'm wondering if this is normal. . I'm also seeing some error messages (`java.sql.BatchUpdateException: lob is no longer valid`) when running bcbio-nextgen with Cromwell, in the same scenario as above. Could this somehow be related to the high cluster load issues as well? . More details are here: https://github.com/bcbio/bcbio-nextgen/issues/2722.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4732
https://github.com/broadinstitute/cromwell/issues/4737:123,Availability,avail,available,123,WAAS will have different `/status` requirements than the reader or writer Cromwells. This could involve (depending on time available) either:; * Generalizing the current status implementation to allow callers of `/status` to opt-in/opt-out of status information they do or don't care about; * Creating a new specialist health implementation which knows what WAAS would care about.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4737
https://github.com/broadinstitute/cromwell/issues/4737:105,Integrability,depend,depending,105,WAAS will have different `/status` requirements than the reader or writer Cromwells. This could involve (depending on time available) either:; * Generalizing the current status implementation to allow callers of `/status` to opt-in/opt-out of status information they do or don't care about; * Creating a new specialist health implementation which knows what WAAS would care about.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4737
https://github.com/broadinstitute/cromwell/issues/4738:9,Deployability,configurat,configuration,9,Create a configuration file suitable for WAAS. Outcomes:; * A PR in firecloud-develop with the new configuration; * Documentation on how to configure for WAAS; * [Optional] An example `.conf` file?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4738
https://github.com/broadinstitute/cromwell/issues/4738:99,Deployability,configurat,configuration,99,Create a configuration file suitable for WAAS. Outcomes:; * A PR in firecloud-develop with the new configuration; * Documentation on how to configure for WAAS; * [Optional] An example `.conf` file?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4738
https://github.com/broadinstitute/cromwell/issues/4738:9,Modifiability,config,configuration,9,Create a configuration file suitable for WAAS. Outcomes:; * A PR in firecloud-develop with the new configuration; * Documentation on how to configure for WAAS; * [Optional] An example `.conf` file?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4738
https://github.com/broadinstitute/cromwell/issues/4738:99,Modifiability,config,configuration,99,Create a configuration file suitable for WAAS. Outcomes:; * A PR in firecloud-develop with the new configuration; * Documentation on how to configure for WAAS; * [Optional] An example `.conf` file?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4738
https://github.com/broadinstitute/cromwell/issues/4738:140,Modifiability,config,configure,140,Create a configuration file suitable for WAAS. Outcomes:; * A PR in firecloud-develop with the new configuration; * Documentation on how to configure for WAAS; * [Optional] An example `.conf` file?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4738
https://github.com/broadinstitute/cromwell/issues/4740:582,Availability,error,errors,582,"### Possible Workaround. Try using the `default` AWS auth scheme along with the [AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) setup on the system / environment. ### Background. Starting in cromwell 37, the AWS S3 SDK was upgraded from `2.0.0-preview-9` to `2.3.9`. Along with this upgrade several cromwell calls to the S3 SDK were updated to match changes in the library. Post upgrade, the existing Cromwell AWS CI tests continue to pass, however there have been reports of permissions problems or other errors.; - https://gatkforums.broadinstitute.org/firecloud/discussion/comment/56245/#Comment_56245; - https://github.com/broadinstitute/cromwell/issues/4541; - https://github.com/broadinstitute/cromwell/issues/4686; - https://github.com/broadinstitute/cromwell/issues/4731. Because the CI tests are passing, and they use default credentials, it is possible that each of these issues may be also be worked around by using the [AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) scheme. It is unclear how the very similar SDK calls worked with `2.0.0-preview-9` and not `2.3.9`. However the fix might include passing in credentials via the `Map env` / `Properties props`. | Type | Cromwell copy targeting `2.3.9` | ""Original"" targeting `2.0.0-preview-9` |; |----------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|; | (Attempted) setting of key/secret from props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740
https://github.com/broadinstitute/cromwell/issues/4740:3929,Availability,error,errors,3929,"ilesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L81) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L82) |; | Region not set by props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L59) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L57) |; | (Attempted) props read from env | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L95-L96) | [elerch-sdk2/S3FileSystemProvider.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L94-L95) |. ### Acceptance Criteria. The existing four centaur tests (two run in Travis, two nightly in Jenkins) should pass using non-default credentials:; - Switch the `aws_application.conf` to use auth scheme `custom_keys` in a rendered template with secrets embedded; - Remove the files `aws_credentials.ctmpl` and `aws_config`; - Update `testCentaurAws.sh` removing `AWS_SHARED_CREDENTIALS_FILE` and `AWS_CONFIG_FILE` exports; - **Maybe**: It isn't clear that installing the `awscli` should be required. Remove this from the `testCentaurAws.sh` and see if the tests still work using just the embedded Java S3 SDK.; - Update `.gitignore` replacing `aws_credentials` with `aws_application.conf`; - **Maybe**: Add a second backend that **does** use some form of default authorization; - If adding this test, please ensure that the non-default credentials are NOT accidentally falling-back to the default credentials. ### TL;DR. **It's unclear how cromwell calls to S3 broke in 37+, but CI testing with non-default credentials should hopefully reproduce reported errors.**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740
https://github.com/broadinstitute/cromwell/issues/4740:299,Deployability,upgrade,upgraded,299,"### Possible Workaround. Try using the `default` AWS auth scheme along with the [AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) setup on the system / environment. ### Background. Starting in cromwell 37, the AWS S3 SDK was upgraded from `2.0.0-preview-9` to `2.3.9`. Along with this upgrade several cromwell calls to the S3 SDK were updated to match changes in the library. Post upgrade, the existing Cromwell AWS CI tests continue to pass, however there have been reports of permissions problems or other errors.; - https://gatkforums.broadinstitute.org/firecloud/discussion/comment/56245/#Comment_56245; - https://github.com/broadinstitute/cromwell/issues/4541; - https://github.com/broadinstitute/cromwell/issues/4686; - https://github.com/broadinstitute/cromwell/issues/4731. Because the CI tests are passing, and they use default credentials, it is possible that each of these issues may be also be worked around by using the [AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) scheme. It is unclear how the very similar SDK calls worked with `2.0.0-preview-9` and not `2.3.9`. However the fix might include passing in credentials via the `Map env` / `Properties props`. | Type | Cromwell copy targeting `2.3.9` | ""Original"" targeting `2.0.0-preview-9` |; |----------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|; | (Attempted) setting of key/secret from props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740
https://github.com/broadinstitute/cromwell/issues/4740:359,Deployability,upgrade,upgrade,359,"### Possible Workaround. Try using the `default` AWS auth scheme along with the [AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) setup on the system / environment. ### Background. Starting in cromwell 37, the AWS S3 SDK was upgraded from `2.0.0-preview-9` to `2.3.9`. Along with this upgrade several cromwell calls to the S3 SDK were updated to match changes in the library. Post upgrade, the existing Cromwell AWS CI tests continue to pass, however there have been reports of permissions problems or other errors.; - https://gatkforums.broadinstitute.org/firecloud/discussion/comment/56245/#Comment_56245; - https://github.com/broadinstitute/cromwell/issues/4541; - https://github.com/broadinstitute/cromwell/issues/4686; - https://github.com/broadinstitute/cromwell/issues/4731. Because the CI tests are passing, and they use default credentials, it is possible that each of these issues may be also be worked around by using the [AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) scheme. It is unclear how the very similar SDK calls worked with `2.0.0-preview-9` and not `2.3.9`. However the fix might include passing in credentials via the `Map env` / `Properties props`. | Type | Cromwell copy targeting `2.3.9` | ""Original"" targeting `2.0.0-preview-9` |; |----------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|; | (Attempted) setting of key/secret from props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740
https://github.com/broadinstitute/cromwell/issues/4740:409,Deployability,update,updated,409,"### Possible Workaround. Try using the `default` AWS auth scheme along with the [AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) setup on the system / environment. ### Background. Starting in cromwell 37, the AWS S3 SDK was upgraded from `2.0.0-preview-9` to `2.3.9`. Along with this upgrade several cromwell calls to the S3 SDK were updated to match changes in the library. Post upgrade, the existing Cromwell AWS CI tests continue to pass, however there have been reports of permissions problems or other errors.; - https://gatkforums.broadinstitute.org/firecloud/discussion/comment/56245/#Comment_56245; - https://github.com/broadinstitute/cromwell/issues/4541; - https://github.com/broadinstitute/cromwell/issues/4686; - https://github.com/broadinstitute/cromwell/issues/4731. Because the CI tests are passing, and they use default credentials, it is possible that each of these issues may be also be worked around by using the [AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) scheme. It is unclear how the very similar SDK calls worked with `2.0.0-preview-9` and not `2.3.9`. However the fix might include passing in credentials via the `Map env` / `Properties props`. | Type | Cromwell copy targeting `2.3.9` | ""Original"" targeting `2.0.0-preview-9` |; |----------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|; | (Attempted) setting of key/secret from props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740
https://github.com/broadinstitute/cromwell/issues/4740:455,Deployability,upgrade,upgrade,455,"### Possible Workaround. Try using the `default` AWS auth scheme along with the [AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) setup on the system / environment. ### Background. Starting in cromwell 37, the AWS S3 SDK was upgraded from `2.0.0-preview-9` to `2.3.9`. Along with this upgrade several cromwell calls to the S3 SDK were updated to match changes in the library. Post upgrade, the existing Cromwell AWS CI tests continue to pass, however there have been reports of permissions problems or other errors.; - https://gatkforums.broadinstitute.org/firecloud/discussion/comment/56245/#Comment_56245; - https://github.com/broadinstitute/cromwell/issues/4541; - https://github.com/broadinstitute/cromwell/issues/4686; - https://github.com/broadinstitute/cromwell/issues/4731. Because the CI tests are passing, and they use default credentials, it is possible that each of these issues may be also be worked around by using the [AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) scheme. It is unclear how the very similar SDK calls worked with `2.0.0-preview-9` and not `2.3.9`. However the fix might include passing in credentials via the `Map env` / `Properties props`. | Type | Cromwell copy targeting `2.3.9` | ""Original"" targeting `2.0.0-preview-9` |; |----------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|; | (Attempted) setting of key/secret from props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740
https://github.com/broadinstitute/cromwell/issues/4740:3200,Deployability,Update,Update,3200,"ilesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L81) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L82) |; | Region not set by props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L59) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L57) |; | (Attempted) props read from env | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L95-L96) | [elerch-sdk2/S3FileSystemProvider.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L94-L95) |. ### Acceptance Criteria. The existing four centaur tests (two run in Travis, two nightly in Jenkins) should pass using non-default credentials:; - Switch the `aws_application.conf` to use auth scheme `custom_keys` in a rendered template with secrets embedded; - Remove the files `aws_credentials.ctmpl` and `aws_config`; - Update `testCentaurAws.sh` removing `AWS_SHARED_CREDENTIALS_FILE` and `AWS_CONFIG_FILE` exports; - **Maybe**: It isn't clear that installing the `awscli` should be required. Remove this from the `testCentaurAws.sh` and see if the tests still work using just the embedded Java S3 SDK.; - Update `.gitignore` replacing `aws_credentials` with `aws_application.conf`; - **Maybe**: Add a second backend that **does** use some form of default authorization; - If adding this test, please ensure that the non-default credentials are NOT accidentally falling-back to the default credentials. ### TL;DR. **It's unclear how cromwell calls to S3 broke in 37+, but CI testing with non-default credentials should hopefully reproduce reported errors.**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740
https://github.com/broadinstitute/cromwell/issues/4740:3330,Deployability,install,installing,3330,"ilesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L81) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L82) |; | Region not set by props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L59) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L57) |; | (Attempted) props read from env | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L95-L96) | [elerch-sdk2/S3FileSystemProvider.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L94-L95) |. ### Acceptance Criteria. The existing four centaur tests (two run in Travis, two nightly in Jenkins) should pass using non-default credentials:; - Switch the `aws_application.conf` to use auth scheme `custom_keys` in a rendered template with secrets embedded; - Remove the files `aws_credentials.ctmpl` and `aws_config`; - Update `testCentaurAws.sh` removing `AWS_SHARED_CREDENTIALS_FILE` and `AWS_CONFIG_FILE` exports; - **Maybe**: It isn't clear that installing the `awscli` should be required. Remove this from the `testCentaurAws.sh` and see if the tests still work using just the embedded Java S3 SDK.; - Update `.gitignore` replacing `aws_credentials` with `aws_application.conf`; - **Maybe**: Add a second backend that **does** use some form of default authorization; - If adding this test, please ensure that the non-default credentials are NOT accidentally falling-back to the default credentials. ### TL;DR. **It's unclear how cromwell calls to S3 broke in 37+, but CI testing with non-default credentials should hopefully reproduce reported errors.**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740
https://github.com/broadinstitute/cromwell/issues/4740:3487,Deployability,Update,Update,3487,"ilesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L81) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L82) |; | Region not set by props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L59) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L57) |; | (Attempted) props read from env | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L95-L96) | [elerch-sdk2/S3FileSystemProvider.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L94-L95) |. ### Acceptance Criteria. The existing four centaur tests (two run in Travis, two nightly in Jenkins) should pass using non-default credentials:; - Switch the `aws_application.conf` to use auth scheme `custom_keys` in a rendered template with secrets embedded; - Remove the files `aws_credentials.ctmpl` and `aws_config`; - Update `testCentaurAws.sh` removing `AWS_SHARED_CREDENTIALS_FILE` and `AWS_CONFIG_FILE` exports; - **Maybe**: It isn't clear that installing the `awscli` should be required. Remove this from the `testCentaurAws.sh` and see if the tests still work using just the embedded Java S3 SDK.; - Update `.gitignore` replacing `aws_credentials` with `aws_application.conf`; - **Maybe**: Add a second backend that **does** use some form of default authorization; - If adding this test, please ensure that the non-default credentials are NOT accidentally falling-back to the default credentials. ### TL;DR. **It's unclear how cromwell calls to S3 broke in 37+, but CI testing with non-default credentials should hopefully reproduce reported errors.**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740
https://github.com/broadinstitute/cromwell/issues/4740:3637,Security,authoriz,authorization,3637,"ilesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L81) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L82) |; | Region not set by props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L59) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L57) |; | (Attempted) props read from env | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L95-L96) | [elerch-sdk2/S3FileSystemProvider.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L94-L95) |. ### Acceptance Criteria. The existing four centaur tests (two run in Travis, two nightly in Jenkins) should pass using non-default credentials:; - Switch the `aws_application.conf` to use auth scheme `custom_keys` in a rendered template with secrets embedded; - Remove the files `aws_credentials.ctmpl` and `aws_config`; - Update `testCentaurAws.sh` removing `AWS_SHARED_CREDENTIALS_FILE` and `AWS_CONFIG_FILE` exports; - **Maybe**: It isn't clear that installing the `awscli` should be required. Remove this from the `testCentaurAws.sh` and see if the tests still work using just the embedded Java S3 SDK.; - Update `.gitignore` replacing `aws_credentials` with `aws_application.conf`; - **Maybe**: Add a second backend that **does** use some form of default authorization; - If adding this test, please ensure that the non-default credentials are NOT accidentally falling-back to the default credentials. ### TL;DR. **It's unclear how cromwell calls to S3 broke in 37+, but CI testing with non-default credentials should hopefully reproduce reported errors.**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740
https://github.com/broadinstitute/cromwell/issues/4740:493,Testability,test,tests,493,"### Possible Workaround. Try using the `default` AWS auth scheme along with the [AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) setup on the system / environment. ### Background. Starting in cromwell 37, the AWS S3 SDK was upgraded from `2.0.0-preview-9` to `2.3.9`. Along with this upgrade several cromwell calls to the S3 SDK were updated to match changes in the library. Post upgrade, the existing Cromwell AWS CI tests continue to pass, however there have been reports of permissions problems or other errors.; - https://gatkforums.broadinstitute.org/firecloud/discussion/comment/56245/#Comment_56245; - https://github.com/broadinstitute/cromwell/issues/4541; - https://github.com/broadinstitute/cromwell/issues/4686; - https://github.com/broadinstitute/cromwell/issues/4731. Because the CI tests are passing, and they use default credentials, it is possible that each of these issues may be also be worked around by using the [AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) scheme. It is unclear how the very similar SDK calls worked with `2.0.0-preview-9` and not `2.3.9`. However the fix might include passing in credentials via the `Map env` / `Properties props`. | Type | Cromwell copy targeting `2.3.9` | ""Original"" targeting `2.0.0-preview-9` |; |----------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|; | (Attempted) setting of key/secret from props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740
https://github.com/broadinstitute/cromwell/issues/4740:871,Testability,test,tests,871,"### Possible Workaround. Try using the `default` AWS auth scheme along with the [AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) setup on the system / environment. ### Background. Starting in cromwell 37, the AWS S3 SDK was upgraded from `2.0.0-preview-9` to `2.3.9`. Along with this upgrade several cromwell calls to the S3 SDK were updated to match changes in the library. Post upgrade, the existing Cromwell AWS CI tests continue to pass, however there have been reports of permissions problems or other errors.; - https://gatkforums.broadinstitute.org/firecloud/discussion/comment/56245/#Comment_56245; - https://github.com/broadinstitute/cromwell/issues/4541; - https://github.com/broadinstitute/cromwell/issues/4686; - https://github.com/broadinstitute/cromwell/issues/4731. Because the CI tests are passing, and they use default credentials, it is possible that each of these issues may be also be worked around by using the [AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) scheme. It is unclear how the very similar SDK calls worked with `2.0.0-preview-9` and not `2.3.9`. However the fix might include passing in credentials via the `Map env` / `Properties props`. | Type | Cromwell copy targeting `2.3.9` | ""Original"" targeting `2.0.0-preview-9` |; |----------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|; | (Attempted) setting of key/secret from props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740
https://github.com/broadinstitute/cromwell/issues/4740:2928,Testability,test,tests,2928,"ilesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L81) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L82) |; | Region not set by props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L59) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L57) |; | (Attempted) props read from env | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L95-L96) | [elerch-sdk2/S3FileSystemProvider.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L94-L95) |. ### Acceptance Criteria. The existing four centaur tests (two run in Travis, two nightly in Jenkins) should pass using non-default credentials:; - Switch the `aws_application.conf` to use auth scheme `custom_keys` in a rendered template with secrets embedded; - Remove the files `aws_credentials.ctmpl` and `aws_config`; - Update `testCentaurAws.sh` removing `AWS_SHARED_CREDENTIALS_FILE` and `AWS_CONFIG_FILE` exports; - **Maybe**: It isn't clear that installing the `awscli` should be required. Remove this from the `testCentaurAws.sh` and see if the tests still work using just the embedded Java S3 SDK.; - Update `.gitignore` replacing `aws_credentials` with `aws_application.conf`; - **Maybe**: Add a second backend that **does** use some form of default authorization; - If adding this test, please ensure that the non-default credentials are NOT accidentally falling-back to the default credentials. ### TL;DR. **It's unclear how cromwell calls to S3 broke in 37+, but CI testing with non-default credentials should hopefully reproduce reported errors.**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740
https://github.com/broadinstitute/cromwell/issues/4740:3208,Testability,test,testCentaurAws,3208,"ilesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L81) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L82) |; | Region not set by props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L59) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L57) |; | (Attempted) props read from env | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L95-L96) | [elerch-sdk2/S3FileSystemProvider.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L94-L95) |. ### Acceptance Criteria. The existing four centaur tests (two run in Travis, two nightly in Jenkins) should pass using non-default credentials:; - Switch the `aws_application.conf` to use auth scheme `custom_keys` in a rendered template with secrets embedded; - Remove the files `aws_credentials.ctmpl` and `aws_config`; - Update `testCentaurAws.sh` removing `AWS_SHARED_CREDENTIALS_FILE` and `AWS_CONFIG_FILE` exports; - **Maybe**: It isn't clear that installing the `awscli` should be required. Remove this from the `testCentaurAws.sh` and see if the tests still work using just the embedded Java S3 SDK.; - Update `.gitignore` replacing `aws_credentials` with `aws_application.conf`; - **Maybe**: Add a second backend that **does** use some form of default authorization; - If adding this test, please ensure that the non-default credentials are NOT accidentally falling-back to the default credentials. ### TL;DR. **It's unclear how cromwell calls to S3 broke in 37+, but CI testing with non-default credentials should hopefully reproduce reported errors.**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740
https://github.com/broadinstitute/cromwell/issues/4740:3396,Testability,test,testCentaurAws,3396,"ilesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L81) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L82) |; | Region not set by props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L59) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L57) |; | (Attempted) props read from env | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L95-L96) | [elerch-sdk2/S3FileSystemProvider.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L94-L95) |. ### Acceptance Criteria. The existing four centaur tests (two run in Travis, two nightly in Jenkins) should pass using non-default credentials:; - Switch the `aws_application.conf` to use auth scheme `custom_keys` in a rendered template with secrets embedded; - Remove the files `aws_credentials.ctmpl` and `aws_config`; - Update `testCentaurAws.sh` removing `AWS_SHARED_CREDENTIALS_FILE` and `AWS_CONFIG_FILE` exports; - **Maybe**: It isn't clear that installing the `awscli` should be required. Remove this from the `testCentaurAws.sh` and see if the tests still work using just the embedded Java S3 SDK.; - Update `.gitignore` replacing `aws_credentials` with `aws_application.conf`; - **Maybe**: Add a second backend that **does** use some form of default authorization; - If adding this test, please ensure that the non-default credentials are NOT accidentally falling-back to the default credentials. ### TL;DR. **It's unclear how cromwell calls to S3 broke in 37+, but CI testing with non-default credentials should hopefully reproduce reported errors.**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740
https://github.com/broadinstitute/cromwell/issues/4740:3430,Testability,test,tests,3430,"ilesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L81) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L82) |; | Region not set by props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L59) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L57) |; | (Attempted) props read from env | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L95-L96) | [elerch-sdk2/S3FileSystemProvider.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L94-L95) |. ### Acceptance Criteria. The existing four centaur tests (two run in Travis, two nightly in Jenkins) should pass using non-default credentials:; - Switch the `aws_application.conf` to use auth scheme `custom_keys` in a rendered template with secrets embedded; - Remove the files `aws_credentials.ctmpl` and `aws_config`; - Update `testCentaurAws.sh` removing `AWS_SHARED_CREDENTIALS_FILE` and `AWS_CONFIG_FILE` exports; - **Maybe**: It isn't clear that installing the `awscli` should be required. Remove this from the `testCentaurAws.sh` and see if the tests still work using just the embedded Java S3 SDK.; - Update `.gitignore` replacing `aws_credentials` with `aws_application.conf`; - **Maybe**: Add a second backend that **does** use some form of default authorization; - If adding this test, please ensure that the non-default credentials are NOT accidentally falling-back to the default credentials. ### TL;DR. **It's unclear how cromwell calls to S3 broke in 37+, but CI testing with non-default credentials should hopefully reproduce reported errors.**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740
https://github.com/broadinstitute/cromwell/issues/4740:3669,Testability,test,test,3669,"ilesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L81) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L82) |; | Region not set by props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L59) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L57) |; | (Attempted) props read from env | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L95-L96) | [elerch-sdk2/S3FileSystemProvider.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L94-L95) |. ### Acceptance Criteria. The existing four centaur tests (two run in Travis, two nightly in Jenkins) should pass using non-default credentials:; - Switch the `aws_application.conf` to use auth scheme `custom_keys` in a rendered template with secrets embedded; - Remove the files `aws_credentials.ctmpl` and `aws_config`; - Update `testCentaurAws.sh` removing `AWS_SHARED_CREDENTIALS_FILE` and `AWS_CONFIG_FILE` exports; - **Maybe**: It isn't clear that installing the `awscli` should be required. Remove this from the `testCentaurAws.sh` and see if the tests still work using just the embedded Java S3 SDK.; - Update `.gitignore` replacing `aws_credentials` with `aws_application.conf`; - **Maybe**: Add a second backend that **does** use some form of default authorization; - If adding this test, please ensure that the non-default credentials are NOT accidentally falling-back to the default credentials. ### TL;DR. **It's unclear how cromwell calls to S3 broke in 37+, but CI testing with non-default credentials should hopefully reproduce reported errors.**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740
https://github.com/broadinstitute/cromwell/issues/4740:3856,Testability,test,testing,3856,"ilesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L81) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L82) |; | Region not set by props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L59) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L57) |; | (Attempted) props read from env | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L95-L96) | [elerch-sdk2/S3FileSystemProvider.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L94-L95) |. ### Acceptance Criteria. The existing four centaur tests (two run in Travis, two nightly in Jenkins) should pass using non-default credentials:; - Switch the `aws_application.conf` to use auth scheme `custom_keys` in a rendered template with secrets embedded; - Remove the files `aws_credentials.ctmpl` and `aws_config`; - Update `testCentaurAws.sh` removing `AWS_SHARED_CREDENTIALS_FILE` and `AWS_CONFIG_FILE` exports; - **Maybe**: It isn't clear that installing the `awscli` should be required. Remove this from the `testCentaurAws.sh` and see if the tests still work using just the embedded Java S3 SDK.; - Update `.gitignore` replacing `aws_credentials` with `aws_application.conf`; - **Maybe**: Add a second backend that **does** use some form of default authorization; - If adding this test, please ensure that the non-default credentials are NOT accidentally falling-back to the default credentials. ### TL;DR. **It's unclear how cromwell calls to S3 broke in 37+, but CI testing with non-default credentials should hopefully reproduce reported errors.**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740
https://github.com/broadinstitute/cromwell/issues/4740:160,Usability,guid,guide,160,"### Possible Workaround. Try using the `default` AWS auth scheme along with the [AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) setup on the system / environment. ### Background. Starting in cromwell 37, the AWS S3 SDK was upgraded from `2.0.0-preview-9` to `2.3.9`. Along with this upgrade several cromwell calls to the S3 SDK were updated to match changes in the library. Post upgrade, the existing Cromwell AWS CI tests continue to pass, however there have been reports of permissions problems or other errors.; - https://gatkforums.broadinstitute.org/firecloud/discussion/comment/56245/#Comment_56245; - https://github.com/broadinstitute/cromwell/issues/4541; - https://github.com/broadinstitute/cromwell/issues/4686; - https://github.com/broadinstitute/cromwell/issues/4731. Because the CI tests are passing, and they use default credentials, it is possible that each of these issues may be also be worked around by using the [AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) scheme. It is unclear how the very similar SDK calls worked with `2.0.0-preview-9` and not `2.3.9`. However the fix might include passing in credentials via the `Map env` / `Properties props`. | Type | Cromwell copy targeting `2.3.9` | ""Original"" targeting `2.0.0-preview-9` |; |----------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|; | (Attempted) setting of key/secret from props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740
https://github.com/broadinstitute/cromwell/issues/4740:1087,Usability,guid,guide,1087,"AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) setup on the system / environment. ### Background. Starting in cromwell 37, the AWS S3 SDK was upgraded from `2.0.0-preview-9` to `2.3.9`. Along with this upgrade several cromwell calls to the S3 SDK were updated to match changes in the library. Post upgrade, the existing Cromwell AWS CI tests continue to pass, however there have been reports of permissions problems or other errors.; - https://gatkforums.broadinstitute.org/firecloud/discussion/comment/56245/#Comment_56245; - https://github.com/broadinstitute/cromwell/issues/4541; - https://github.com/broadinstitute/cromwell/issues/4686; - https://github.com/broadinstitute/cromwell/issues/4731. Because the CI tests are passing, and they use default credentials, it is possible that each of these issues may be also be worked around by using the [AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) scheme. It is unclear how the very similar SDK calls worked with `2.0.0-preview-9` and not `2.3.9`. However the fix might include passing in credentials via the `Map env` / `Properties props`. | Type | Cromwell copy targeting `2.3.9` | ""Original"" targeting `2.0.0-preview-9` |; |----------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|; | (Attempted) setting of key/secret from props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L81) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-F",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740
https://github.com/broadinstitute/cromwell/issues/4740:3319,Usability,clear,clear,3319,"ilesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L81) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L82) |; | Region not set by props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L59) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L57) |; | (Attempted) props read from env | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L95-L96) | [elerch-sdk2/S3FileSystemProvider.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L94-L95) |. ### Acceptance Criteria. The existing four centaur tests (two run in Travis, two nightly in Jenkins) should pass using non-default credentials:; - Switch the `aws_application.conf` to use auth scheme `custom_keys` in a rendered template with secrets embedded; - Remove the files `aws_credentials.ctmpl` and `aws_config`; - Update `testCentaurAws.sh` removing `AWS_SHARED_CREDENTIALS_FILE` and `AWS_CONFIG_FILE` exports; - **Maybe**: It isn't clear that installing the `awscli` should be required. Remove this from the `testCentaurAws.sh` and see if the tests still work using just the embedded Java S3 SDK.; - Update `.gitignore` replacing `aws_credentials` with `aws_application.conf`; - **Maybe**: Add a second backend that **does** use some form of default authorization; - If adding this test, please ensure that the non-default credentials are NOT accidentally falling-back to the default credentials. ### TL;DR. **It's unclear how cromwell calls to S3 broke in 37+, but CI testing with non-default credentials should hopefully reproduce reported errors.**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740
https://github.com/broadinstitute/cromwell/issues/4741:1741,Availability,error,error,1741,"It seems that the protocol for setting runtime attributes is to do so within the task, thus allowing expressions based on their values. Say,. ```wdl; task foo {; Int cpus. runtime {; cpus = cpus; }. command {; ./my_binary --threads ${cpus * 2}; }; }; ```. However, a lot of the time, it's not appropriate (in a ""separation of concerns"" sense) to thread the value through the task invocation. For example, you may be setting a Unix group under which all data should be accessed, defining credentials, etc. We're doing this by using the `default_runtime_attributes`, passed in as workflow options. However, these are not visible to the task. This is what we'd like to be able to do, for example:. Workflow Options:; ```json; {; ""default_runtime_attributes"": {; ""AUTH_USER"": ""foo"",; ""AUTH_TOKEN"": ""bar""; }; }; ```. Workflow:; ```wdl; task {; command {; export AUTH_USER=""${AUTH_USER}"" # Taken from default_runtime_attributes; export AUTH_TOKEN=""${AUTH_TOKEN}"" # Taken from default_runtime_attributes; ./my_authenticated_command; }; }; ```. At the moment, this will fail, as `AUTH_USER` and `AUTH_TOKEN` are not defined within the task. Even if you explicitly define it in the task (`String AUTH_USER`, etc.), Cromwell won't automatically seed this from the default options. I can see why it would be useful to define the inputs in the task, for clarity's sake. I'm just thinking out aloud -- so this is very much half-baked -- but perhaps an option would therefore be to have an additional keyword that made it explicit that the task value was to be taken from options:. ```wdl; task {; Int something; runtime String AUTH_USER # ""runtime"" implies this is from the runtime attributes (default or otherwise); runtime String AUTH_TOKEN # Raise an error if undefined or modified within the task. command {; export AUTH_USER=""${AUTH_USER}""; export AUTH_TOKEN=""${AUTH_TOKEN}""; ./my_authenticated_command -n ${something}; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4741
https://github.com/broadinstitute/cromwell/issues/4741:18,Integrability,protocol,protocol,18,"It seems that the protocol for setting runtime attributes is to do so within the task, thus allowing expressions based on their values. Say,. ```wdl; task foo {; Int cpus. runtime {; cpus = cpus; }. command {; ./my_binary --threads ${cpus * 2}; }; }; ```. However, a lot of the time, it's not appropriate (in a ""separation of concerns"" sense) to thread the value through the task invocation. For example, you may be setting a Unix group under which all data should be accessed, defining credentials, etc. We're doing this by using the `default_runtime_attributes`, passed in as workflow options. However, these are not visible to the task. This is what we'd like to be able to do, for example:. Workflow Options:; ```json; {; ""default_runtime_attributes"": {; ""AUTH_USER"": ""foo"",; ""AUTH_TOKEN"": ""bar""; }; }; ```. Workflow:; ```wdl; task {; command {; export AUTH_USER=""${AUTH_USER}"" # Taken from default_runtime_attributes; export AUTH_TOKEN=""${AUTH_TOKEN}"" # Taken from default_runtime_attributes; ./my_authenticated_command; }; }; ```. At the moment, this will fail, as `AUTH_USER` and `AUTH_TOKEN` are not defined within the task. Even if you explicitly define it in the task (`String AUTH_USER`, etc.), Cromwell won't automatically seed this from the default options. I can see why it would be useful to define the inputs in the task, for clarity's sake. I'm just thinking out aloud -- so this is very much half-baked -- but perhaps an option would therefore be to have an additional keyword that made it explicit that the task value was to be taken from options:. ```wdl; task {; Int something; runtime String AUTH_USER # ""runtime"" implies this is from the runtime attributes (default or otherwise); runtime String AUTH_TOKEN # Raise an error if undefined or modified within the task. command {; export AUTH_USER=""${AUTH_USER}""; export AUTH_TOKEN=""${AUTH_TOKEN}""; ./my_authenticated_command -n ${something}; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4741
https://github.com/broadinstitute/cromwell/issues/4741:468,Security,access,accessed,468,"It seems that the protocol for setting runtime attributes is to do so within the task, thus allowing expressions based on their values. Say,. ```wdl; task foo {; Int cpus. runtime {; cpus = cpus; }. command {; ./my_binary --threads ${cpus * 2}; }; }; ```. However, a lot of the time, it's not appropriate (in a ""separation of concerns"" sense) to thread the value through the task invocation. For example, you may be setting a Unix group under which all data should be accessed, defining credentials, etc. We're doing this by using the `default_runtime_attributes`, passed in as workflow options. However, these are not visible to the task. This is what we'd like to be able to do, for example:. Workflow Options:; ```json; {; ""default_runtime_attributes"": {; ""AUTH_USER"": ""foo"",; ""AUTH_TOKEN"": ""bar""; }; }; ```. Workflow:; ```wdl; task {; command {; export AUTH_USER=""${AUTH_USER}"" # Taken from default_runtime_attributes; export AUTH_TOKEN=""${AUTH_TOKEN}"" # Taken from default_runtime_attributes; ./my_authenticated_command; }; }; ```. At the moment, this will fail, as `AUTH_USER` and `AUTH_TOKEN` are not defined within the task. Even if you explicitly define it in the task (`String AUTH_USER`, etc.), Cromwell won't automatically seed this from the default options. I can see why it would be useful to define the inputs in the task, for clarity's sake. I'm just thinking out aloud -- so this is very much half-baked -- but perhaps an option would therefore be to have an additional keyword that made it explicit that the task value was to be taken from options:. ```wdl; task {; Int something; runtime String AUTH_USER # ""runtime"" implies this is from the runtime attributes (default or otherwise); runtime String AUTH_TOKEN # Raise an error if undefined or modified within the task. command {; export AUTH_USER=""${AUTH_USER}""; export AUTH_TOKEN=""${AUTH_TOKEN}""; ./my_authenticated_command -n ${something}; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4741
https://github.com/broadinstitute/cromwell/issues/4742:59,Testability,test,tester,59,Make a hybrid of something a lot like Kristian's [deadlock tester docker-compose](https://github.com/broadinstitute/cromwell/blob/develop/src/ci/bin/testDockerDeadlock.sh) with the existing Centaur system. The result should be Centaur running the full PAPI test suite against a horizontal Cromwell system.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4742
https://github.com/broadinstitute/cromwell/issues/4742:149,Testability,test,testDockerDeadlock,149,Make a hybrid of something a lot like Kristian's [deadlock tester docker-compose](https://github.com/broadinstitute/cromwell/blob/develop/src/ci/bin/testDockerDeadlock.sh) with the existing Centaur system. The result should be Centaur running the full PAPI test suite against a horizontal Cromwell system.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4742
https://github.com/broadinstitute/cromwell/issues/4742:257,Testability,test,test,257,Make a hybrid of something a lot like Kristian's [deadlock tester docker-compose](https://github.com/broadinstitute/cromwell/blob/develop/src/ci/bin/testDockerDeadlock.sh) with the existing Centaur system. The result should be Centaur running the full PAPI test suite against a horizontal Cromwell system.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4742
https://github.com/broadinstitute/cromwell/pull/4744:218,Availability,failure,failures,218,"- Added comment that WDL can only handle increasing version numbers; - Sort through the first page of releases instead of using the latest-release-by-date; - Exit WDL commands that contain unset variables or have pipe failures (set -uo pipefail); - Exit WDL commands on the first error (set -e); - Log WDL commands verbosely as they run (set -x); - Replaced usages of docker/python runtimes with brew'ed jq; - Remove call to sbt test from minor releases, thus operating like major releases; - Made the WDL input ""organization"" mandatory instead of optional; - Copy release notes for major releases from develop instead of master; - Copy release notes for minor releases from hotfix branches; - Pointed to correct homebrew pull request template; - Added additional homebrew test as required in the homebrew pull request template; - Fail the WDL call/workflow if any of homebrew's build/test/verify tasks fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4744
https://github.com/broadinstitute/cromwell/pull/4744:280,Availability,error,error,280,"- Added comment that WDL can only handle increasing version numbers; - Sort through the first page of releases instead of using the latest-release-by-date; - Exit WDL commands that contain unset variables or have pipe failures (set -uo pipefail); - Exit WDL commands on the first error (set -e); - Log WDL commands verbosely as they run (set -x); - Replaced usages of docker/python runtimes with brew'ed jq; - Remove call to sbt test from minor releases, thus operating like major releases; - Made the WDL input ""organization"" mandatory instead of optional; - Copy release notes for major releases from develop instead of master; - Copy release notes for minor releases from hotfix branches; - Pointed to correct homebrew pull request template; - Added additional homebrew test as required in the homebrew pull request template; - Fail the WDL call/workflow if any of homebrew's build/test/verify tasks fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4744
https://github.com/broadinstitute/cromwell/pull/4744:102,Deployability,release,releases,102,"- Added comment that WDL can only handle increasing version numbers; - Sort through the first page of releases instead of using the latest-release-by-date; - Exit WDL commands that contain unset variables or have pipe failures (set -uo pipefail); - Exit WDL commands on the first error (set -e); - Log WDL commands verbosely as they run (set -x); - Replaced usages of docker/python runtimes with brew'ed jq; - Remove call to sbt test from minor releases, thus operating like major releases; - Made the WDL input ""organization"" mandatory instead of optional; - Copy release notes for major releases from develop instead of master; - Copy release notes for minor releases from hotfix branches; - Pointed to correct homebrew pull request template; - Added additional homebrew test as required in the homebrew pull request template; - Fail the WDL call/workflow if any of homebrew's build/test/verify tasks fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4744
https://github.com/broadinstitute/cromwell/pull/4744:139,Deployability,release,release-by-date,139,"- Added comment that WDL can only handle increasing version numbers; - Sort through the first page of releases instead of using the latest-release-by-date; - Exit WDL commands that contain unset variables or have pipe failures (set -uo pipefail); - Exit WDL commands on the first error (set -e); - Log WDL commands verbosely as they run (set -x); - Replaced usages of docker/python runtimes with brew'ed jq; - Remove call to sbt test from minor releases, thus operating like major releases; - Made the WDL input ""organization"" mandatory instead of optional; - Copy release notes for major releases from develop instead of master; - Copy release notes for minor releases from hotfix branches; - Pointed to correct homebrew pull request template; - Added additional homebrew test as required in the homebrew pull request template; - Fail the WDL call/workflow if any of homebrew's build/test/verify tasks fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4744
https://github.com/broadinstitute/cromwell/pull/4744:445,Deployability,release,releases,445,"- Added comment that WDL can only handle increasing version numbers; - Sort through the first page of releases instead of using the latest-release-by-date; - Exit WDL commands that contain unset variables or have pipe failures (set -uo pipefail); - Exit WDL commands on the first error (set -e); - Log WDL commands verbosely as they run (set -x); - Replaced usages of docker/python runtimes with brew'ed jq; - Remove call to sbt test from minor releases, thus operating like major releases; - Made the WDL input ""organization"" mandatory instead of optional; - Copy release notes for major releases from develop instead of master; - Copy release notes for minor releases from hotfix branches; - Pointed to correct homebrew pull request template; - Added additional homebrew test as required in the homebrew pull request template; - Fail the WDL call/workflow if any of homebrew's build/test/verify tasks fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4744
https://github.com/broadinstitute/cromwell/pull/4744:481,Deployability,release,releases,481,"- Added comment that WDL can only handle increasing version numbers; - Sort through the first page of releases instead of using the latest-release-by-date; - Exit WDL commands that contain unset variables or have pipe failures (set -uo pipefail); - Exit WDL commands on the first error (set -e); - Log WDL commands verbosely as they run (set -x); - Replaced usages of docker/python runtimes with brew'ed jq; - Remove call to sbt test from minor releases, thus operating like major releases; - Made the WDL input ""organization"" mandatory instead of optional; - Copy release notes for major releases from develop instead of master; - Copy release notes for minor releases from hotfix branches; - Pointed to correct homebrew pull request template; - Added additional homebrew test as required in the homebrew pull request template; - Fail the WDL call/workflow if any of homebrew's build/test/verify tasks fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4744
https://github.com/broadinstitute/cromwell/pull/4744:565,Deployability,release,release,565,"- Added comment that WDL can only handle increasing version numbers; - Sort through the first page of releases instead of using the latest-release-by-date; - Exit WDL commands that contain unset variables or have pipe failures (set -uo pipefail); - Exit WDL commands on the first error (set -e); - Log WDL commands verbosely as they run (set -x); - Replaced usages of docker/python runtimes with brew'ed jq; - Remove call to sbt test from minor releases, thus operating like major releases; - Made the WDL input ""organization"" mandatory instead of optional; - Copy release notes for major releases from develop instead of master; - Copy release notes for minor releases from hotfix branches; - Pointed to correct homebrew pull request template; - Added additional homebrew test as required in the homebrew pull request template; - Fail the WDL call/workflow if any of homebrew's build/test/verify tasks fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4744
https://github.com/broadinstitute/cromwell/pull/4744:589,Deployability,release,releases,589,"- Added comment that WDL can only handle increasing version numbers; - Sort through the first page of releases instead of using the latest-release-by-date; - Exit WDL commands that contain unset variables or have pipe failures (set -uo pipefail); - Exit WDL commands on the first error (set -e); - Log WDL commands verbosely as they run (set -x); - Replaced usages of docker/python runtimes with brew'ed jq; - Remove call to sbt test from minor releases, thus operating like major releases; - Made the WDL input ""organization"" mandatory instead of optional; - Copy release notes for major releases from develop instead of master; - Copy release notes for minor releases from hotfix branches; - Pointed to correct homebrew pull request template; - Added additional homebrew test as required in the homebrew pull request template; - Fail the WDL call/workflow if any of homebrew's build/test/verify tasks fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4744
https://github.com/broadinstitute/cromwell/pull/4744:637,Deployability,release,release,637,"- Added comment that WDL can only handle increasing version numbers; - Sort through the first page of releases instead of using the latest-release-by-date; - Exit WDL commands that contain unset variables or have pipe failures (set -uo pipefail); - Exit WDL commands on the first error (set -e); - Log WDL commands verbosely as they run (set -x); - Replaced usages of docker/python runtimes with brew'ed jq; - Remove call to sbt test from minor releases, thus operating like major releases; - Made the WDL input ""organization"" mandatory instead of optional; - Copy release notes for major releases from develop instead of master; - Copy release notes for minor releases from hotfix branches; - Pointed to correct homebrew pull request template; - Added additional homebrew test as required in the homebrew pull request template; - Fail the WDL call/workflow if any of homebrew's build/test/verify tasks fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4744
https://github.com/broadinstitute/cromwell/pull/4744:661,Deployability,release,releases,661,"- Added comment that WDL can only handle increasing version numbers; - Sort through the first page of releases instead of using the latest-release-by-date; - Exit WDL commands that contain unset variables or have pipe failures (set -uo pipefail); - Exit WDL commands on the first error (set -e); - Log WDL commands verbosely as they run (set -x); - Replaced usages of docker/python runtimes with brew'ed jq; - Remove call to sbt test from minor releases, thus operating like major releases; - Made the WDL input ""organization"" mandatory instead of optional; - Copy release notes for major releases from develop instead of master; - Copy release notes for minor releases from hotfix branches; - Pointed to correct homebrew pull request template; - Added additional homebrew test as required in the homebrew pull request template; - Fail the WDL call/workflow if any of homebrew's build/test/verify tasks fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4744
https://github.com/broadinstitute/cromwell/pull/4744:675,Deployability,hotfix,hotfix,675,"- Added comment that WDL can only handle increasing version numbers; - Sort through the first page of releases instead of using the latest-release-by-date; - Exit WDL commands that contain unset variables or have pipe failures (set -uo pipefail); - Exit WDL commands on the first error (set -e); - Log WDL commands verbosely as they run (set -x); - Replaced usages of docker/python runtimes with brew'ed jq; - Remove call to sbt test from minor releases, thus operating like major releases; - Made the WDL input ""organization"" mandatory instead of optional; - Copy release notes for major releases from develop instead of master; - Copy release notes for minor releases from hotfix branches; - Pointed to correct homebrew pull request template; - Added additional homebrew test as required in the homebrew pull request template; - Fail the WDL call/workflow if any of homebrew's build/test/verify tasks fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4744
https://github.com/broadinstitute/cromwell/pull/4744:195,Modifiability,variab,variables,195,"- Added comment that WDL can only handle increasing version numbers; - Sort through the first page of releases instead of using the latest-release-by-date; - Exit WDL commands that contain unset variables or have pipe failures (set -uo pipefail); - Exit WDL commands on the first error (set -e); - Log WDL commands verbosely as they run (set -x); - Replaced usages of docker/python runtimes with brew'ed jq; - Remove call to sbt test from minor releases, thus operating like major releases; - Made the WDL input ""organization"" mandatory instead of optional; - Copy release notes for major releases from develop instead of master; - Copy release notes for minor releases from hotfix branches; - Pointed to correct homebrew pull request template; - Added additional homebrew test as required in the homebrew pull request template; - Fail the WDL call/workflow if any of homebrew's build/test/verify tasks fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4744
https://github.com/broadinstitute/cromwell/pull/4744:298,Testability,Log,Log,298,"- Added comment that WDL can only handle increasing version numbers; - Sort through the first page of releases instead of using the latest-release-by-date; - Exit WDL commands that contain unset variables or have pipe failures (set -uo pipefail); - Exit WDL commands on the first error (set -e); - Log WDL commands verbosely as they run (set -x); - Replaced usages of docker/python runtimes with brew'ed jq; - Remove call to sbt test from minor releases, thus operating like major releases; - Made the WDL input ""organization"" mandatory instead of optional; - Copy release notes for major releases from develop instead of master; - Copy release notes for minor releases from hotfix branches; - Pointed to correct homebrew pull request template; - Added additional homebrew test as required in the homebrew pull request template; - Fail the WDL call/workflow if any of homebrew's build/test/verify tasks fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4744
https://github.com/broadinstitute/cromwell/pull/4744:429,Testability,test,test,429,"- Added comment that WDL can only handle increasing version numbers; - Sort through the first page of releases instead of using the latest-release-by-date; - Exit WDL commands that contain unset variables or have pipe failures (set -uo pipefail); - Exit WDL commands on the first error (set -e); - Log WDL commands verbosely as they run (set -x); - Replaced usages of docker/python runtimes with brew'ed jq; - Remove call to sbt test from minor releases, thus operating like major releases; - Made the WDL input ""organization"" mandatory instead of optional; - Copy release notes for major releases from develop instead of master; - Copy release notes for minor releases from hotfix branches; - Pointed to correct homebrew pull request template; - Added additional homebrew test as required in the homebrew pull request template; - Fail the WDL call/workflow if any of homebrew's build/test/verify tasks fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4744
https://github.com/broadinstitute/cromwell/pull/4744:773,Testability,test,test,773,"- Added comment that WDL can only handle increasing version numbers; - Sort through the first page of releases instead of using the latest-release-by-date; - Exit WDL commands that contain unset variables or have pipe failures (set -uo pipefail); - Exit WDL commands on the first error (set -e); - Log WDL commands verbosely as they run (set -x); - Replaced usages of docker/python runtimes with brew'ed jq; - Remove call to sbt test from minor releases, thus operating like major releases; - Made the WDL input ""organization"" mandatory instead of optional; - Copy release notes for major releases from develop instead of master; - Copy release notes for minor releases from hotfix branches; - Pointed to correct homebrew pull request template; - Added additional homebrew test as required in the homebrew pull request template; - Fail the WDL call/workflow if any of homebrew's build/test/verify tasks fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4744
https://github.com/broadinstitute/cromwell/pull/4744:885,Testability,test,test,885,"- Added comment that WDL can only handle increasing version numbers; - Sort through the first page of releases instead of using the latest-release-by-date; - Exit WDL commands that contain unset variables or have pipe failures (set -uo pipefail); - Exit WDL commands on the first error (set -e); - Log WDL commands verbosely as they run (set -x); - Replaced usages of docker/python runtimes with brew'ed jq; - Remove call to sbt test from minor releases, thus operating like major releases; - Made the WDL input ""organization"" mandatory instead of optional; - Copy release notes for major releases from develop instead of master; - Copy release notes for minor releases from hotfix branches; - Pointed to correct homebrew pull request template; - Added additional homebrew test as required in the homebrew pull request template; - Fail the WDL call/workflow if any of homebrew's build/test/verify tasks fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4744
https://github.com/broadinstitute/cromwell/pull/4747:20,Testability,test,tests,20,"Fixes three centaur tests: input_mirror, sizeenginefunction, if_then_…else_expressions",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4747
https://github.com/broadinstitute/cromwell/pull/4748:88,Modifiability,config,config,88,"Just some lessons learned while trying to run tests on the Local backend using a non-CI config, though the main lesson learned was to just use the CI config if possible.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4748
https://github.com/broadinstitute/cromwell/pull/4748:150,Modifiability,config,config,150,"Just some lessons learned while trying to run tests on the Local backend using a non-CI config, though the main lesson learned was to just use the CI config if possible.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4748
https://github.com/broadinstitute/cromwell/pull/4748:46,Testability,test,tests,46,"Just some lessons learned while trying to run tests on the Local backend using a non-CI config, though the main lesson learned was to just use the CI config if possible.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4748
https://github.com/broadinstitute/cromwell/pull/4748:18,Usability,learn,learned,18,"Just some lessons learned while trying to run tests on the Local backend using a non-CI config, though the main lesson learned was to just use the CI config if possible.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4748
https://github.com/broadinstitute/cromwell/pull/4748:119,Usability,learn,learned,119,"Just some lessons learned while trying to run tests on the Local backend using a non-CI config, though the main lesson learned was to just use the CI config if possible.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4748
https://github.com/broadinstitute/cromwell/pull/4749:5,Availability,down,download,5,"When download input from S3 to instance, it may fail. But the workflow will continue to run.; Retry and exit after 5 times.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4749
https://github.com/broadinstitute/cromwell/issues/4750:45,Modifiability,config,config,45,"Introducing the concept of a ""lord"" into the config, with the responsibilities:. * Summarize; * Liquibase. To complement this effort, these functions should be turned *off* in worker & reader Cromwells",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4750
https://github.com/broadinstitute/cromwell/issues/4755:1225,Availability,ERROR,ERROR,1225,"low task is tryting to do the following assignation...:. ```; Array[File]? y ; # ...; Array[File] x = select_first([y, []]); ```; where ```y``` is a task argument that is assigned the value of a ```Array[File]+``` argument by the invoking workflow. Eventually I fixed this by unraveling the unnecessary conversions since in this case there is no need for a ""?"" in the ```y``` nor the ```x``` or the invokation of ```select_first```; however I have to say that I don't see why this ""coversion"" would be invalid but I'm not much of a wdl or scala expert. Now the for-sure issue here is that instead of failing indicating what is going on the workflow was still running and the offending task(s) were reported as ""Starting"" in the metadata and the timing and they stayed that way forever. . In order to find out what was going on I needed to install and run a locally v36 server (I usually use dsde-method's community cromwell servers). The logs show first the causing wdl bug like so:. ```; [ERROR] [03/19/2019 09:52:14.444] [cromwell-system-akka.dispatchers.engine-dispatcher-47] [akka://cromwell-system ... Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomAnyType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([""gs:// .... 70.tsv.gz""])), []); ```; Notice that Skipped most of the message text showing (what I think are) the important bits . . This meesage is follow for java exception directly dump into the log output file. java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(W....z""])), []); at wom.values.WomArray$.apply(WomArray.scala:34); at wom.values.WomArray$.apply(WomArray.scala:38); at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:108); at cats.data.Validated.map(Validated.scala:194); ... After this exception there is a log [ERROR] entry appears reporting the exception an",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755
https://github.com/broadinstitute/cromwell/issues/4755:2189,Availability,ERROR,ERROR,2189,"ns since in this case there is no need for a ""?"" in the ```y``` nor the ```x``` or the invokation of ```select_first```; however I have to say that I don't see why this ""coversion"" would be invalid but I'm not much of a wdl or scala expert. Now the for-sure issue here is that instead of failing indicating what is going on the workflow was still running and the offending task(s) were reported as ""Starting"" in the metadata and the timing and they stayed that way forever. . In order to find out what was going on I needed to install and run a locally v36 server (I usually use dsde-method's community cromwell servers). The logs show first the causing wdl bug like so:. ```; [ERROR] [03/19/2019 09:52:14.444] [cromwell-system-akka.dispatchers.engine-dispatcher-47] [akka://cromwell-system ... Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomAnyType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([""gs:// .... 70.tsv.gz""])), []); ```; Notice that Skipped most of the message text showing (what I think are) the important bits . . This meesage is follow for java exception directly dump into the log output file. java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(W....z""])), []); at wom.values.WomArray$.apply(WomArray.scala:34); at wom.values.WomArray$.apply(WomArray.scala:38); at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:108); at cats.data.Validated.map(Validated.scala:194); ... After this exception there is a log [ERROR] entry appears reporting the exception and exception stack trace. The timing diagram show the tasks hanging in the ""Starting"" state forever and the metadata does not report anything apart than these tasks are ""Starting"". So the error is silenced and the only recurse left is to abort. A re-submit of the workflow would just get stuck in the same place.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755
https://github.com/broadinstitute/cromwell/issues/4755:2423,Availability,error,error,2423,"ns since in this case there is no need for a ""?"" in the ```y``` nor the ```x``` or the invokation of ```select_first```; however I have to say that I don't see why this ""coversion"" would be invalid but I'm not much of a wdl or scala expert. Now the for-sure issue here is that instead of failing indicating what is going on the workflow was still running and the offending task(s) were reported as ""Starting"" in the metadata and the timing and they stayed that way forever. . In order to find out what was going on I needed to install and run a locally v36 server (I usually use dsde-method's community cromwell servers). The logs show first the causing wdl bug like so:. ```; [ERROR] [03/19/2019 09:52:14.444] [cromwell-system-akka.dispatchers.engine-dispatcher-47] [akka://cromwell-system ... Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomAnyType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([""gs:// .... 70.tsv.gz""])), []); ```; Notice that Skipped most of the message text showing (what I think are) the important bits . . This meesage is follow for java exception directly dump into the log output file. java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(W....z""])), []); at wom.values.WomArray$.apply(WomArray.scala:34); at wom.values.WomArray$.apply(WomArray.scala:38); at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:108); at cats.data.Validated.map(Validated.scala:194); ... After this exception there is a log [ERROR] entry appears reporting the exception and exception stack trace. The timing diagram show the tasks hanging in the ""Starting"" state forever and the metadata does not report anything apart than these tasks are ""Starting"". So the error is silenced and the only recurse left is to abort. A re-submit of the workflow would just get stuck in the same place.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755
https://github.com/broadinstitute/cromwell/issues/4755:1074,Deployability,install,install,1074," this because the resulting .jar file name contains a ""35"" instead (```server/target/scala-2.12/cromwell-35-71debed-SNAP.jar```). A subworkflow task is tryting to do the following assignation...:. ```; Array[File]? y ; # ...; Array[File] x = select_first([y, []]); ```; where ```y``` is a task argument that is assigned the value of a ```Array[File]+``` argument by the invoking workflow. Eventually I fixed this by unraveling the unnecessary conversions since in this case there is no need for a ""?"" in the ```y``` nor the ```x``` or the invokation of ```select_first```; however I have to say that I don't see why this ""coversion"" would be invalid but I'm not much of a wdl or scala expert. Now the for-sure issue here is that instead of failing indicating what is going on the workflow was still running and the offending task(s) were reported as ""Starting"" in the metadata and the timing and they stayed that way forever. . In order to find out what was going on I needed to install and run a locally v36 server (I usually use dsde-method's community cromwell servers). The logs show first the causing wdl bug like so:. ```; [ERROR] [03/19/2019 09:52:14.444] [cromwell-system-akka.dispatchers.engine-dispatcher-47] [akka://cromwell-system ... Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomAnyType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([""gs:// .... 70.tsv.gz""])), []); ```; Notice that Skipped most of the message text showing (what I think are) the important bits . . This meesage is follow for java exception directly dump into the log output file. java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(W....z""])), []); at wom.values.WomArray$.apply(WomArray.scala:34); at wom.values.WomArray$.apply(WomArray.scala:38); at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:10",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755
https://github.com/broadinstitute/cromwell/issues/4755:1608,Integrability,message,message,1608,"ns since in this case there is no need for a ""?"" in the ```y``` nor the ```x``` or the invokation of ```select_first```; however I have to say that I don't see why this ""coversion"" would be invalid but I'm not much of a wdl or scala expert. Now the for-sure issue here is that instead of failing indicating what is going on the workflow was still running and the offending task(s) were reported as ""Starting"" in the metadata and the timing and they stayed that way forever. . In order to find out what was going on I needed to install and run a locally v36 server (I usually use dsde-method's community cromwell servers). The logs show first the causing wdl bug like so:. ```; [ERROR] [03/19/2019 09:52:14.444] [cromwell-system-akka.dispatchers.engine-dispatcher-47] [akka://cromwell-system ... Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomAnyType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([""gs:// .... 70.tsv.gz""])), []); ```; Notice that Skipped most of the message text showing (what I think are) the important bits . . This meesage is follow for java exception directly dump into the log output file. java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(W....z""])), []); at wom.values.WomArray$.apply(WomArray.scala:34); at wom.values.WomArray$.apply(WomArray.scala:38); at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:108); at cats.data.Validated.map(Validated.scala:194); ... After this exception there is a log [ERROR] entry appears reporting the exception and exception stack trace. The timing diagram show the tasks hanging in the ""Starting"" state forever and the metadata does not report anything apart than these tasks are ""Starting"". So the error is silenced and the only recurse left is to abort. A re-submit of the workflow would just get stuck in the same place.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755
https://github.com/broadinstitute/cromwell/issues/4755:2473,Safety,abort,abort,2473,"ns since in this case there is no need for a ""?"" in the ```y``` nor the ```x``` or the invokation of ```select_first```; however I have to say that I don't see why this ""coversion"" would be invalid but I'm not much of a wdl or scala expert. Now the for-sure issue here is that instead of failing indicating what is going on the workflow was still running and the offending task(s) were reported as ""Starting"" in the metadata and the timing and they stayed that way forever. . In order to find out what was going on I needed to install and run a locally v36 server (I usually use dsde-method's community cromwell servers). The logs show first the causing wdl bug like so:. ```; [ERROR] [03/19/2019 09:52:14.444] [cromwell-system-akka.dispatchers.engine-dispatcher-47] [akka://cromwell-system ... Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomAnyType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([""gs:// .... 70.tsv.gz""])), []); ```; Notice that Skipped most of the message text showing (what I think are) the important bits . . This meesage is follow for java exception directly dump into the log output file. java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(W....z""])), []); at wom.values.WomArray$.apply(WomArray.scala:34); at wom.values.WomArray$.apply(WomArray.scala:38); at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:108); at cats.data.Validated.map(Validated.scala:194); ... After this exception there is a log [ERROR] entry appears reporting the exception and exception stack trace. The timing diagram show the tasks hanging in the ""Starting"" state forever and the metadata does not report anything apart than these tasks are ""Starting"". So the error is silenced and the only recurse left is to abort. A re-submit of the workflow would just get stuck in the same place.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755
https://github.com/broadinstitute/cromwell/issues/4755:2112,Security,Validat,Validated,2112,"ns since in this case there is no need for a ""?"" in the ```y``` nor the ```x``` or the invokation of ```select_first```; however I have to say that I don't see why this ""coversion"" would be invalid but I'm not much of a wdl or scala expert. Now the for-sure issue here is that instead of failing indicating what is going on the workflow was still running and the offending task(s) were reported as ""Starting"" in the metadata and the timing and they stayed that way forever. . In order to find out what was going on I needed to install and run a locally v36 server (I usually use dsde-method's community cromwell servers). The logs show first the causing wdl bug like so:. ```; [ERROR] [03/19/2019 09:52:14.444] [cromwell-system-akka.dispatchers.engine-dispatcher-47] [akka://cromwell-system ... Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomAnyType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([""gs:// .... 70.tsv.gz""])), []); ```; Notice that Skipped most of the message text showing (what I think are) the important bits . . This meesage is follow for java exception directly dump into the log output file. java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(W....z""])), []); at wom.values.WomArray$.apply(WomArray.scala:34); at wom.values.WomArray$.apply(WomArray.scala:38); at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:108); at cats.data.Validated.map(Validated.scala:194); ... After this exception there is a log [ERROR] entry appears reporting the exception and exception stack trace. The timing diagram show the tasks hanging in the ""Starting"" state forever and the metadata does not report anything apart than these tasks are ""Starting"". So the error is silenced and the only recurse left is to abort. A re-submit of the workflow would just get stuck in the same place.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755
https://github.com/broadinstitute/cromwell/issues/4755:2126,Security,Validat,Validated,2126,"ns since in this case there is no need for a ""?"" in the ```y``` nor the ```x``` or the invokation of ```select_first```; however I have to say that I don't see why this ""coversion"" would be invalid but I'm not much of a wdl or scala expert. Now the for-sure issue here is that instead of failing indicating what is going on the workflow was still running and the offending task(s) were reported as ""Starting"" in the metadata and the timing and they stayed that way forever. . In order to find out what was going on I needed to install and run a locally v36 server (I usually use dsde-method's community cromwell servers). The logs show first the causing wdl bug like so:. ```; [ERROR] [03/19/2019 09:52:14.444] [cromwell-system-akka.dispatchers.engine-dispatcher-47] [akka://cromwell-system ... Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomAnyType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([""gs:// .... 70.tsv.gz""])), []); ```; Notice that Skipped most of the message text showing (what I think are) the important bits . . This meesage is follow for java exception directly dump into the log output file. java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(W....z""])), []); at wom.values.WomArray$.apply(WomArray.scala:34); at wom.values.WomArray$.apply(WomArray.scala:38); at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:108); at cats.data.Validated.map(Validated.scala:194); ... After this exception there is a log [ERROR] entry appears reporting the exception and exception stack trace. The timing diagram show the tasks hanging in the ""Starting"" state forever and the metadata does not report anything apart than these tasks are ""Starting"". So the error is silenced and the only recurse left is to abort. A re-submit of the workflow would just get stuck in the same place.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755
https://github.com/broadinstitute/cromwell/issues/4755:1173,Testability,log,logs,1173,"omwell-35-71debed-SNAP.jar```). A subworkflow task is tryting to do the following assignation...:. ```; Array[File]? y ; # ...; Array[File] x = select_first([y, []]); ```; where ```y``` is a task argument that is assigned the value of a ```Array[File]+``` argument by the invoking workflow. Eventually I fixed this by unraveling the unnecessary conversions since in this case there is no need for a ""?"" in the ```y``` nor the ```x``` or the invokation of ```select_first```; however I have to say that I don't see why this ""coversion"" would be invalid but I'm not much of a wdl or scala expert. Now the for-sure issue here is that instead of failing indicating what is going on the workflow was still running and the offending task(s) were reported as ""Starting"" in the metadata and the timing and they stayed that way forever. . In order to find out what was going on I needed to install and run a locally v36 server (I usually use dsde-method's community cromwell servers). The logs show first the causing wdl bug like so:. ```; [ERROR] [03/19/2019 09:52:14.444] [cromwell-system-akka.dispatchers.engine-dispatcher-47] [akka://cromwell-system ... Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomAnyType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([""gs:// .... 70.tsv.gz""])), []); ```; Notice that Skipped most of the message text showing (what I think are) the important bits . . This meesage is follow for java exception directly dump into the log output file. java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(W....z""])), []); at wom.values.WomArray$.apply(WomArray.scala:34); at wom.values.WomArray$.apply(WomArray.scala:38); at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:108); at cats.data.Validated.map(Validated.scala:194); ... After this exception there is a log [ERRO",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755
https://github.com/broadinstitute/cromwell/issues/4755:1736,Testability,log,log,1736,"ns since in this case there is no need for a ""?"" in the ```y``` nor the ```x``` or the invokation of ```select_first```; however I have to say that I don't see why this ""coversion"" would be invalid but I'm not much of a wdl or scala expert. Now the for-sure issue here is that instead of failing indicating what is going on the workflow was still running and the offending task(s) were reported as ""Starting"" in the metadata and the timing and they stayed that way forever. . In order to find out what was going on I needed to install and run a locally v36 server (I usually use dsde-method's community cromwell servers). The logs show first the causing wdl bug like so:. ```; [ERROR] [03/19/2019 09:52:14.444] [cromwell-system-akka.dispatchers.engine-dispatcher-47] [akka://cromwell-system ... Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomAnyType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([""gs:// .... 70.tsv.gz""])), []); ```; Notice that Skipped most of the message text showing (what I think are) the important bits . . This meesage is follow for java exception directly dump into the log output file. java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(W....z""])), []); at wom.values.WomArray$.apply(WomArray.scala:34); at wom.values.WomArray$.apply(WomArray.scala:38); at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:108); at cats.data.Validated.map(Validated.scala:194); ... After this exception there is a log [ERROR] entry appears reporting the exception and exception stack trace. The timing diagram show the tasks hanging in the ""Starting"" state forever and the metadata does not report anything apart than these tasks are ""Starting"". So the error is silenced and the only recurse left is to abort. A re-submit of the workflow would just get stuck in the same place.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755
https://github.com/broadinstitute/cromwell/issues/4755:2184,Testability,log,log,2184,"ns since in this case there is no need for a ""?"" in the ```y``` nor the ```x``` or the invokation of ```select_first```; however I have to say that I don't see why this ""coversion"" would be invalid but I'm not much of a wdl or scala expert. Now the for-sure issue here is that instead of failing indicating what is going on the workflow was still running and the offending task(s) were reported as ""Starting"" in the metadata and the timing and they stayed that way forever. . In order to find out what was going on I needed to install and run a locally v36 server (I usually use dsde-method's community cromwell servers). The logs show first the causing wdl bug like so:. ```; [ERROR] [03/19/2019 09:52:14.444] [cromwell-system-akka.dispatchers.engine-dispatcher-47] [akka://cromwell-system ... Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomAnyType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([""gs:// .... 70.tsv.gz""])), []); ```; Notice that Skipped most of the message text showing (what I think are) the important bits . . This meesage is follow for java exception directly dump into the log output file. java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(W....z""])), []); at wom.values.WomArray$.apply(WomArray.scala:34); at wom.values.WomArray$.apply(WomArray.scala:38); at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:108); at cats.data.Validated.map(Validated.scala:194); ... After this exception there is a log [ERROR] entry appears reporting the exception and exception stack trace. The timing diagram show the tasks hanging in the ""Starting"" state forever and the metadata does not report anything apart than these tasks are ""Starting"". So the error is silenced and the only recurse left is to abort. A re-submit of the workflow would just get stuck in the same place.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755
https://github.com/broadinstitute/cromwell/issues/4756:210,Availability,Failure,Failures,210,"When Cromwell is finding a successful call cache hit, it is failing to copy the file even though the user has access to it. This causes the workflow to be rerun rather than copying the cached results. . Inside Failures you can see the following error:; ```[Attempted 1 time(s)] - HttpResponseException: 400 Bad Request { ""error"": { ""errors"": [ { ""domain"": ""global"", ""reason"": ""required"", ""message"": ""Bucket is requester pays bucket but no user project provided."" } ], ""code"": 400, ""message"": ""Bucket is requester pays bucket but no user project provided."" } }```. https://portal.firecloud.org/#workspaces/dvoet-prod-test-20190305-3/dvoet_tutorial_requester_pays/monitor/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/102e99b1-26b2-4bf4-80ec-fcc02c32136d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4756
https://github.com/broadinstitute/cromwell/issues/4756:245,Availability,error,error,245,"When Cromwell is finding a successful call cache hit, it is failing to copy the file even though the user has access to it. This causes the workflow to be rerun rather than copying the cached results. . Inside Failures you can see the following error:; ```[Attempted 1 time(s)] - HttpResponseException: 400 Bad Request { ""error"": { ""errors"": [ { ""domain"": ""global"", ""reason"": ""required"", ""message"": ""Bucket is requester pays bucket but no user project provided."" } ], ""code"": 400, ""message"": ""Bucket is requester pays bucket but no user project provided."" } }```. https://portal.firecloud.org/#workspaces/dvoet-prod-test-20190305-3/dvoet_tutorial_requester_pays/monitor/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/102e99b1-26b2-4bf4-80ec-fcc02c32136d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4756
https://github.com/broadinstitute/cromwell/issues/4756:322,Availability,error,error,322,"When Cromwell is finding a successful call cache hit, it is failing to copy the file even though the user has access to it. This causes the workflow to be rerun rather than copying the cached results. . Inside Failures you can see the following error:; ```[Attempted 1 time(s)] - HttpResponseException: 400 Bad Request { ""error"": { ""errors"": [ { ""domain"": ""global"", ""reason"": ""required"", ""message"": ""Bucket is requester pays bucket but no user project provided."" } ], ""code"": 400, ""message"": ""Bucket is requester pays bucket but no user project provided."" } }```. https://portal.firecloud.org/#workspaces/dvoet-prod-test-20190305-3/dvoet_tutorial_requester_pays/monitor/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/102e99b1-26b2-4bf4-80ec-fcc02c32136d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4756
https://github.com/broadinstitute/cromwell/issues/4756:333,Availability,error,errors,333,"When Cromwell is finding a successful call cache hit, it is failing to copy the file even though the user has access to it. This causes the workflow to be rerun rather than copying the cached results. . Inside Failures you can see the following error:; ```[Attempted 1 time(s)] - HttpResponseException: 400 Bad Request { ""error"": { ""errors"": [ { ""domain"": ""global"", ""reason"": ""required"", ""message"": ""Bucket is requester pays bucket but no user project provided."" } ], ""code"": 400, ""message"": ""Bucket is requester pays bucket but no user project provided."" } }```. https://portal.firecloud.org/#workspaces/dvoet-prod-test-20190305-3/dvoet_tutorial_requester_pays/monitor/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/102e99b1-26b2-4bf4-80ec-fcc02c32136d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4756
https://github.com/broadinstitute/cromwell/issues/4756:662,Energy Efficiency,monitor,monitor,662,"When Cromwell is finding a successful call cache hit, it is failing to copy the file even though the user has access to it. This causes the workflow to be rerun rather than copying the cached results. . Inside Failures you can see the following error:; ```[Attempted 1 time(s)] - HttpResponseException: 400 Bad Request { ""error"": { ""errors"": [ { ""domain"": ""global"", ""reason"": ""required"", ""message"": ""Bucket is requester pays bucket but no user project provided."" } ], ""code"": 400, ""message"": ""Bucket is requester pays bucket but no user project provided."" } }```. https://portal.firecloud.org/#workspaces/dvoet-prod-test-20190305-3/dvoet_tutorial_requester_pays/monitor/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/102e99b1-26b2-4bf4-80ec-fcc02c32136d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4756
https://github.com/broadinstitute/cromwell/issues/4756:389,Integrability,message,message,389,"When Cromwell is finding a successful call cache hit, it is failing to copy the file even though the user has access to it. This causes the workflow to be rerun rather than copying the cached results. . Inside Failures you can see the following error:; ```[Attempted 1 time(s)] - HttpResponseException: 400 Bad Request { ""error"": { ""errors"": [ { ""domain"": ""global"", ""reason"": ""required"", ""message"": ""Bucket is requester pays bucket but no user project provided."" } ], ""code"": 400, ""message"": ""Bucket is requester pays bucket but no user project provided."" } }```. https://portal.firecloud.org/#workspaces/dvoet-prod-test-20190305-3/dvoet_tutorial_requester_pays/monitor/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/102e99b1-26b2-4bf4-80ec-fcc02c32136d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4756
https://github.com/broadinstitute/cromwell/issues/4756:482,Integrability,message,message,482,"When Cromwell is finding a successful call cache hit, it is failing to copy the file even though the user has access to it. This causes the workflow to be rerun rather than copying the cached results. . Inside Failures you can see the following error:; ```[Attempted 1 time(s)] - HttpResponseException: 400 Bad Request { ""error"": { ""errors"": [ { ""domain"": ""global"", ""reason"": ""required"", ""message"": ""Bucket is requester pays bucket but no user project provided."" } ], ""code"": 400, ""message"": ""Bucket is requester pays bucket but no user project provided."" } }```. https://portal.firecloud.org/#workspaces/dvoet-prod-test-20190305-3/dvoet_tutorial_requester_pays/monitor/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/102e99b1-26b2-4bf4-80ec-fcc02c32136d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4756
https://github.com/broadinstitute/cromwell/issues/4756:43,Performance,cache,cache,43,"When Cromwell is finding a successful call cache hit, it is failing to copy the file even though the user has access to it. This causes the workflow to be rerun rather than copying the cached results. . Inside Failures you can see the following error:; ```[Attempted 1 time(s)] - HttpResponseException: 400 Bad Request { ""error"": { ""errors"": [ { ""domain"": ""global"", ""reason"": ""required"", ""message"": ""Bucket is requester pays bucket but no user project provided."" } ], ""code"": 400, ""message"": ""Bucket is requester pays bucket but no user project provided."" } }```. https://portal.firecloud.org/#workspaces/dvoet-prod-test-20190305-3/dvoet_tutorial_requester_pays/monitor/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/102e99b1-26b2-4bf4-80ec-fcc02c32136d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4756
https://github.com/broadinstitute/cromwell/issues/4756:185,Performance,cache,cached,185,"When Cromwell is finding a successful call cache hit, it is failing to copy the file even though the user has access to it. This causes the workflow to be rerun rather than copying the cached results. . Inside Failures you can see the following error:; ```[Attempted 1 time(s)] - HttpResponseException: 400 Bad Request { ""error"": { ""errors"": [ { ""domain"": ""global"", ""reason"": ""required"", ""message"": ""Bucket is requester pays bucket but no user project provided."" } ], ""code"": 400, ""message"": ""Bucket is requester pays bucket but no user project provided."" } }```. https://portal.firecloud.org/#workspaces/dvoet-prod-test-20190305-3/dvoet_tutorial_requester_pays/monitor/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/102e99b1-26b2-4bf4-80ec-fcc02c32136d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4756
https://github.com/broadinstitute/cromwell/issues/4756:110,Security,access,access,110,"When Cromwell is finding a successful call cache hit, it is failing to copy the file even though the user has access to it. This causes the workflow to be rerun rather than copying the cached results. . Inside Failures you can see the following error:; ```[Attempted 1 time(s)] - HttpResponseException: 400 Bad Request { ""error"": { ""errors"": [ { ""domain"": ""global"", ""reason"": ""required"", ""message"": ""Bucket is requester pays bucket but no user project provided."" } ], ""code"": 400, ""message"": ""Bucket is requester pays bucket but no user project provided."" } }```. https://portal.firecloud.org/#workspaces/dvoet-prod-test-20190305-3/dvoet_tutorial_requester_pays/monitor/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/102e99b1-26b2-4bf4-80ec-fcc02c32136d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4756
https://github.com/broadinstitute/cromwell/issues/4756:616,Testability,test,test-,616,"When Cromwell is finding a successful call cache hit, it is failing to copy the file even though the user has access to it. This causes the workflow to be rerun rather than copying the cached results. . Inside Failures you can see the following error:; ```[Attempted 1 time(s)] - HttpResponseException: 400 Bad Request { ""error"": { ""errors"": [ { ""domain"": ""global"", ""reason"": ""required"", ""message"": ""Bucket is requester pays bucket but no user project provided."" } ], ""code"": 400, ""message"": ""Bucket is requester pays bucket but no user project provided."" } }```. https://portal.firecloud.org/#workspaces/dvoet-prod-test-20190305-3/dvoet_tutorial_requester_pays/monitor/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/102e99b1-26b2-4bf4-80ec-fcc02c32136d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4756
https://github.com/broadinstitute/cromwell/issues/4767:53,Safety,abort,abort,53,Rawls is periodically getting 404s when calling our `abort` endpoint. It resends the 404 every minute.; ; These accumulate over time so eventually Rawls is sending multiple aborts per minute.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4767
https://github.com/broadinstitute/cromwell/issues/4767:173,Safety,abort,aborts,173,Rawls is periodically getting 404s when calling our `abort` endpoint. It resends the 404 every minute.; ; These accumulate over time so eventually Rawls is sending multiple aborts per minute.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4767
https://github.com/broadinstitute/cromwell/issues/4770:135,Availability,error,error,135,"When the workflow logs are copied over from a bucket to an RP-enabled bucket, Cromwell fails to do the operation today and returns the error:; <img width=""1261"" alt=""Screen Shot 2019-03-25 at 12 57 13 PM"" src=""https://user-images.githubusercontent.com/14941133/54967379-ceac8680-4f4d-11e9-9f2a-3c2b0edab7f6.png"">. AC: Correct and incorporate the workflow log copying behavior to retry such failures with the defined google project.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4770
https://github.com/broadinstitute/cromwell/issues/4770:390,Availability,failure,failures,390,"When the workflow logs are copied over from a bucket to an RP-enabled bucket, Cromwell fails to do the operation today and returns the error:; <img width=""1261"" alt=""Screen Shot 2019-03-25 at 12 57 13 PM"" src=""https://user-images.githubusercontent.com/14941133/54967379-ceac8680-4f4d-11e9-9f2a-3c2b0edab7f6.png"">. AC: Correct and incorporate the workflow log copying behavior to retry such failures with the defined google project.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4770
https://github.com/broadinstitute/cromwell/issues/4770:18,Testability,log,logs,18,"When the workflow logs are copied over from a bucket to an RP-enabled bucket, Cromwell fails to do the operation today and returns the error:; <img width=""1261"" alt=""Screen Shot 2019-03-25 at 12 57 13 PM"" src=""https://user-images.githubusercontent.com/14941133/54967379-ceac8680-4f4d-11e9-9f2a-3c2b0edab7f6.png"">. AC: Correct and incorporate the workflow log copying behavior to retry such failures with the defined google project.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4770
https://github.com/broadinstitute/cromwell/issues/4770:355,Testability,log,log,355,"When the workflow logs are copied over from a bucket to an RP-enabled bucket, Cromwell fails to do the operation today and returns the error:; <img width=""1261"" alt=""Screen Shot 2019-03-25 at 12 57 13 PM"" src=""https://user-images.githubusercontent.com/14941133/54967379-ceac8680-4f4d-11e9-9f2a-3c2b0edab7f6.png"">. AC: Correct and incorporate the workflow log copying behavior to retry such failures with the defined google project.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4770
https://github.com/broadinstitute/cromwell/issues/4771:757,Availability,error,error,757,"Today when using Cromwell to copy cache hits to an RP enabled bucket (as the execution directory), there are issues where the the copying of outputs times out:. ```; ""hitFailures"":[{""e831b105-209d-44a4-b550-92ac77c2076e:test.hello:-1"":[{""message"":""The Cache hit copying actor timed out waiting for a response to copy gs://fc-ec2179fb-ab57-4867-b548-b6728060cdef/92241ad8-4f27-4112-826c-8c5230d9a2e0/test/e831b105-209d-44a4-b550-92ac77c2076e/call-hello/stdout to gs://fc-ec2179fb-ab57-4867-b548-b6728060cdef/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/test/102e99b1-26b2-4bf4-80ec-fcc02c32136d/call-hello/stdout"",""causedBy"":[]}]; ```. AC: For the functions used to copy cached outputs, incorporate retry logic when the action fails due to a `400 UserProjectMissing error`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4771
https://github.com/broadinstitute/cromwell/issues/4771:238,Integrability,message,message,238,"Today when using Cromwell to copy cache hits to an RP enabled bucket (as the execution directory), there are issues where the the copying of outputs times out:. ```; ""hitFailures"":[{""e831b105-209d-44a4-b550-92ac77c2076e:test.hello:-1"":[{""message"":""The Cache hit copying actor timed out waiting for a response to copy gs://fc-ec2179fb-ab57-4867-b548-b6728060cdef/92241ad8-4f27-4112-826c-8c5230d9a2e0/test/e831b105-209d-44a4-b550-92ac77c2076e/call-hello/stdout to gs://fc-ec2179fb-ab57-4867-b548-b6728060cdef/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/test/102e99b1-26b2-4bf4-80ec-fcc02c32136d/call-hello/stdout"",""causedBy"":[]}]; ```. AC: For the functions used to copy cached outputs, incorporate retry logic when the action fails due to a `400 UserProjectMissing error`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4771
https://github.com/broadinstitute/cromwell/issues/4771:34,Performance,cache,cache,34,"Today when using Cromwell to copy cache hits to an RP enabled bucket (as the execution directory), there are issues where the the copying of outputs times out:. ```; ""hitFailures"":[{""e831b105-209d-44a4-b550-92ac77c2076e:test.hello:-1"":[{""message"":""The Cache hit copying actor timed out waiting for a response to copy gs://fc-ec2179fb-ab57-4867-b548-b6728060cdef/92241ad8-4f27-4112-826c-8c5230d9a2e0/test/e831b105-209d-44a4-b550-92ac77c2076e/call-hello/stdout to gs://fc-ec2179fb-ab57-4867-b548-b6728060cdef/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/test/102e99b1-26b2-4bf4-80ec-fcc02c32136d/call-hello/stdout"",""causedBy"":[]}]; ```. AC: For the functions used to copy cached outputs, incorporate retry logic when the action fails due to a `400 UserProjectMissing error`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4771
https://github.com/broadinstitute/cromwell/issues/4771:252,Performance,Cache,Cache,252,"Today when using Cromwell to copy cache hits to an RP enabled bucket (as the execution directory), there are issues where the the copying of outputs times out:. ```; ""hitFailures"":[{""e831b105-209d-44a4-b550-92ac77c2076e:test.hello:-1"":[{""message"":""The Cache hit copying actor timed out waiting for a response to copy gs://fc-ec2179fb-ab57-4867-b548-b6728060cdef/92241ad8-4f27-4112-826c-8c5230d9a2e0/test/e831b105-209d-44a4-b550-92ac77c2076e/call-hello/stdout to gs://fc-ec2179fb-ab57-4867-b548-b6728060cdef/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/test/102e99b1-26b2-4bf4-80ec-fcc02c32136d/call-hello/stdout"",""causedBy"":[]}]; ```. AC: For the functions used to copy cached outputs, incorporate retry logic when the action fails due to a `400 UserProjectMissing error`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4771
https://github.com/broadinstitute/cromwell/issues/4771:662,Performance,cache,cached,662,"Today when using Cromwell to copy cache hits to an RP enabled bucket (as the execution directory), there are issues where the the copying of outputs times out:. ```; ""hitFailures"":[{""e831b105-209d-44a4-b550-92ac77c2076e:test.hello:-1"":[{""message"":""The Cache hit copying actor timed out waiting for a response to copy gs://fc-ec2179fb-ab57-4867-b548-b6728060cdef/92241ad8-4f27-4112-826c-8c5230d9a2e0/test/e831b105-209d-44a4-b550-92ac77c2076e/call-hello/stdout to gs://fc-ec2179fb-ab57-4867-b548-b6728060cdef/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/test/102e99b1-26b2-4bf4-80ec-fcc02c32136d/call-hello/stdout"",""causedBy"":[]}]; ```. AC: For the functions used to copy cached outputs, incorporate retry logic when the action fails due to a `400 UserProjectMissing error`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4771
https://github.com/broadinstitute/cromwell/issues/4771:220,Testability,test,test,220,"Today when using Cromwell to copy cache hits to an RP enabled bucket (as the execution directory), there are issues where the the copying of outputs times out:. ```; ""hitFailures"":[{""e831b105-209d-44a4-b550-92ac77c2076e:test.hello:-1"":[{""message"":""The Cache hit copying actor timed out waiting for a response to copy gs://fc-ec2179fb-ab57-4867-b548-b6728060cdef/92241ad8-4f27-4112-826c-8c5230d9a2e0/test/e831b105-209d-44a4-b550-92ac77c2076e/call-hello/stdout to gs://fc-ec2179fb-ab57-4867-b548-b6728060cdef/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/test/102e99b1-26b2-4bf4-80ec-fcc02c32136d/call-hello/stdout"",""causedBy"":[]}]; ```. AC: For the functions used to copy cached outputs, incorporate retry logic when the action fails due to a `400 UserProjectMissing error`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4771
https://github.com/broadinstitute/cromwell/issues/4771:399,Testability,test,test,399,"Today when using Cromwell to copy cache hits to an RP enabled bucket (as the execution directory), there are issues where the the copying of outputs times out:. ```; ""hitFailures"":[{""e831b105-209d-44a4-b550-92ac77c2076e:test.hello:-1"":[{""message"":""The Cache hit copying actor timed out waiting for a response to copy gs://fc-ec2179fb-ab57-4867-b548-b6728060cdef/92241ad8-4f27-4112-826c-8c5230d9a2e0/test/e831b105-209d-44a4-b550-92ac77c2076e/call-hello/stdout to gs://fc-ec2179fb-ab57-4867-b548-b6728060cdef/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/test/102e99b1-26b2-4bf4-80ec-fcc02c32136d/call-hello/stdout"",""causedBy"":[]}]; ```. AC: For the functions used to copy cached outputs, incorporate retry logic when the action fails due to a `400 UserProjectMissing error`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4771
https://github.com/broadinstitute/cromwell/issues/4771:544,Testability,test,test,544,"Today when using Cromwell to copy cache hits to an RP enabled bucket (as the execution directory), there are issues where the the copying of outputs times out:. ```; ""hitFailures"":[{""e831b105-209d-44a4-b550-92ac77c2076e:test.hello:-1"":[{""message"":""The Cache hit copying actor timed out waiting for a response to copy gs://fc-ec2179fb-ab57-4867-b548-b6728060cdef/92241ad8-4f27-4112-826c-8c5230d9a2e0/test/e831b105-209d-44a4-b550-92ac77c2076e/call-hello/stdout to gs://fc-ec2179fb-ab57-4867-b548-b6728060cdef/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/test/102e99b1-26b2-4bf4-80ec-fcc02c32136d/call-hello/stdout"",""causedBy"":[]}]; ```. AC: For the functions used to copy cached outputs, incorporate retry logic when the action fails due to a `400 UserProjectMissing error`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4771
https://github.com/broadinstitute/cromwell/issues/4771:696,Testability,log,logic,696,"Today when using Cromwell to copy cache hits to an RP enabled bucket (as the execution directory), there are issues where the the copying of outputs times out:. ```; ""hitFailures"":[{""e831b105-209d-44a4-b550-92ac77c2076e:test.hello:-1"":[{""message"":""The Cache hit copying actor timed out waiting for a response to copy gs://fc-ec2179fb-ab57-4867-b548-b6728060cdef/92241ad8-4f27-4112-826c-8c5230d9a2e0/test/e831b105-209d-44a4-b550-92ac77c2076e/call-hello/stdout to gs://fc-ec2179fb-ab57-4867-b548-b6728060cdef/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/test/102e99b1-26b2-4bf4-80ec-fcc02c32136d/call-hello/stdout"",""causedBy"":[]}]; ```. AC: For the functions used to copy cached outputs, incorporate retry logic when the action fails due to a `400 UserProjectMissing error`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4771
https://github.com/broadinstitute/cromwell/issues/4772:257,Availability,failure,failures,257,"Version: Cromwell v38; Backend: PAPI v2. Running a hello world workflow on Pipelines API v2 against inputs originating from a Requester Pays bucket. The job finishes running, generates a 0 return code and detritus logs (stdout/stderr). However the workflow failures with an error message (full message attached).; [failed-workflow-metadata.txt](https://github.com/broadinstitute/cromwell/files/3008597/failed-workflow-metadata.txt). ```Unable to complete JES Api Request. Caught NPE while processing operation projects/dvoet-prod-test-20190305-3/operations/11506961050484846699```. Searched the code and found the error here:; https://github.com/broadinstitute/cromwell/blob/2d8ff3b0962bbe84828445fd3ac77d2379e499c2/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/request/GetRequestHandler.scala#L49-L83. Operation metadata attached as well.; [operation-metadata.yaml.txt](https://github.com/broadinstitute/cromwell/files/3008623/operation-metadata.yaml.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4772
https://github.com/broadinstitute/cromwell/issues/4772:274,Availability,error,error,274,"Version: Cromwell v38; Backend: PAPI v2. Running a hello world workflow on Pipelines API v2 against inputs originating from a Requester Pays bucket. The job finishes running, generates a 0 return code and detritus logs (stdout/stderr). However the workflow failures with an error message (full message attached).; [failed-workflow-metadata.txt](https://github.com/broadinstitute/cromwell/files/3008597/failed-workflow-metadata.txt). ```Unable to complete JES Api Request. Caught NPE while processing operation projects/dvoet-prod-test-20190305-3/operations/11506961050484846699```. Searched the code and found the error here:; https://github.com/broadinstitute/cromwell/blob/2d8ff3b0962bbe84828445fd3ac77d2379e499c2/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/request/GetRequestHandler.scala#L49-L83. Operation metadata attached as well.; [operation-metadata.yaml.txt](https://github.com/broadinstitute/cromwell/files/3008623/operation-metadata.yaml.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4772
https://github.com/broadinstitute/cromwell/issues/4772:614,Availability,error,error,614,"Version: Cromwell v38; Backend: PAPI v2. Running a hello world workflow on Pipelines API v2 against inputs originating from a Requester Pays bucket. The job finishes running, generates a 0 return code and detritus logs (stdout/stderr). However the workflow failures with an error message (full message attached).; [failed-workflow-metadata.txt](https://github.com/broadinstitute/cromwell/files/3008597/failed-workflow-metadata.txt). ```Unable to complete JES Api Request. Caught NPE while processing operation projects/dvoet-prod-test-20190305-3/operations/11506961050484846699```. Searched the code and found the error here:; https://github.com/broadinstitute/cromwell/blob/2d8ff3b0962bbe84828445fd3ac77d2379e499c2/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/request/GetRequestHandler.scala#L49-L83. Operation metadata attached as well.; [operation-metadata.yaml.txt](https://github.com/broadinstitute/cromwell/files/3008623/operation-metadata.yaml.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4772
https://github.com/broadinstitute/cromwell/issues/4772:75,Deployability,Pipeline,Pipelines,75,"Version: Cromwell v38; Backend: PAPI v2. Running a hello world workflow on Pipelines API v2 against inputs originating from a Requester Pays bucket. The job finishes running, generates a 0 return code and detritus logs (stdout/stderr). However the workflow failures with an error message (full message attached).; [failed-workflow-metadata.txt](https://github.com/broadinstitute/cromwell/files/3008597/failed-workflow-metadata.txt). ```Unable to complete JES Api Request. Caught NPE while processing operation projects/dvoet-prod-test-20190305-3/operations/11506961050484846699```. Searched the code and found the error here:; https://github.com/broadinstitute/cromwell/blob/2d8ff3b0962bbe84828445fd3ac77d2379e499c2/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/request/GetRequestHandler.scala#L49-L83. Operation metadata attached as well.; [operation-metadata.yaml.txt](https://github.com/broadinstitute/cromwell/files/3008623/operation-metadata.yaml.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4772
https://github.com/broadinstitute/cromwell/issues/4772:741,Deployability,pipeline,pipelines,741,"Version: Cromwell v38; Backend: PAPI v2. Running a hello world workflow on Pipelines API v2 against inputs originating from a Requester Pays bucket. The job finishes running, generates a 0 return code and detritus logs (stdout/stderr). However the workflow failures with an error message (full message attached).; [failed-workflow-metadata.txt](https://github.com/broadinstitute/cromwell/files/3008597/failed-workflow-metadata.txt). ```Unable to complete JES Api Request. Caught NPE while processing operation projects/dvoet-prod-test-20190305-3/operations/11506961050484846699```. Searched the code and found the error here:; https://github.com/broadinstitute/cromwell/blob/2d8ff3b0962bbe84828445fd3ac77d2379e499c2/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/request/GetRequestHandler.scala#L49-L83. Operation metadata attached as well.; [operation-metadata.yaml.txt](https://github.com/broadinstitute/cromwell/files/3008623/operation-metadata.yaml.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4772
https://github.com/broadinstitute/cromwell/issues/4772:799,Deployability,pipeline,pipelines,799,"Version: Cromwell v38; Backend: PAPI v2. Running a hello world workflow on Pipelines API v2 against inputs originating from a Requester Pays bucket. The job finishes running, generates a 0 return code and detritus logs (stdout/stderr). However the workflow failures with an error message (full message attached).; [failed-workflow-metadata.txt](https://github.com/broadinstitute/cromwell/files/3008597/failed-workflow-metadata.txt). ```Unable to complete JES Api Request. Caught NPE while processing operation projects/dvoet-prod-test-20190305-3/operations/11506961050484846699```. Searched the code and found the error here:; https://github.com/broadinstitute/cromwell/blob/2d8ff3b0962bbe84828445fd3ac77d2379e499c2/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/request/GetRequestHandler.scala#L49-L83. Operation metadata attached as well.; [operation-metadata.yaml.txt](https://github.com/broadinstitute/cromwell/files/3008623/operation-metadata.yaml.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4772
https://github.com/broadinstitute/cromwell/issues/4772:280,Integrability,message,message,280,"Version: Cromwell v38; Backend: PAPI v2. Running a hello world workflow on Pipelines API v2 against inputs originating from a Requester Pays bucket. The job finishes running, generates a 0 return code and detritus logs (stdout/stderr). However the workflow failures with an error message (full message attached).; [failed-workflow-metadata.txt](https://github.com/broadinstitute/cromwell/files/3008597/failed-workflow-metadata.txt). ```Unable to complete JES Api Request. Caught NPE while processing operation projects/dvoet-prod-test-20190305-3/operations/11506961050484846699```. Searched the code and found the error here:; https://github.com/broadinstitute/cromwell/blob/2d8ff3b0962bbe84828445fd3ac77d2379e499c2/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/request/GetRequestHandler.scala#L49-L83. Operation metadata attached as well.; [operation-metadata.yaml.txt](https://github.com/broadinstitute/cromwell/files/3008623/operation-metadata.yaml.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4772
https://github.com/broadinstitute/cromwell/issues/4772:294,Integrability,message,message,294,"Version: Cromwell v38; Backend: PAPI v2. Running a hello world workflow on Pipelines API v2 against inputs originating from a Requester Pays bucket. The job finishes running, generates a 0 return code and detritus logs (stdout/stderr). However the workflow failures with an error message (full message attached).; [failed-workflow-metadata.txt](https://github.com/broadinstitute/cromwell/files/3008597/failed-workflow-metadata.txt). ```Unable to complete JES Api Request. Caught NPE while processing operation projects/dvoet-prod-test-20190305-3/operations/11506961050484846699```. Searched the code and found the error here:; https://github.com/broadinstitute/cromwell/blob/2d8ff3b0962bbe84828445fd3ac77d2379e499c2/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/request/GetRequestHandler.scala#L49-L83. Operation metadata attached as well.; [operation-metadata.yaml.txt](https://github.com/broadinstitute/cromwell/files/3008623/operation-metadata.yaml.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4772
https://github.com/broadinstitute/cromwell/issues/4772:214,Testability,log,logs,214,"Version: Cromwell v38; Backend: PAPI v2. Running a hello world workflow on Pipelines API v2 against inputs originating from a Requester Pays bucket. The job finishes running, generates a 0 return code and detritus logs (stdout/stderr). However the workflow failures with an error message (full message attached).; [failed-workflow-metadata.txt](https://github.com/broadinstitute/cromwell/files/3008597/failed-workflow-metadata.txt). ```Unable to complete JES Api Request. Caught NPE while processing operation projects/dvoet-prod-test-20190305-3/operations/11506961050484846699```. Searched the code and found the error here:; https://github.com/broadinstitute/cromwell/blob/2d8ff3b0962bbe84828445fd3ac77d2379e499c2/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/request/GetRequestHandler.scala#L49-L83. Operation metadata attached as well.; [operation-metadata.yaml.txt](https://github.com/broadinstitute/cromwell/files/3008623/operation-metadata.yaml.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4772
https://github.com/broadinstitute/cromwell/issues/4772:530,Testability,test,test-,530,"Version: Cromwell v38; Backend: PAPI v2. Running a hello world workflow on Pipelines API v2 against inputs originating from a Requester Pays bucket. The job finishes running, generates a 0 return code and detritus logs (stdout/stderr). However the workflow failures with an error message (full message attached).; [failed-workflow-metadata.txt](https://github.com/broadinstitute/cromwell/files/3008597/failed-workflow-metadata.txt). ```Unable to complete JES Api Request. Caught NPE while processing operation projects/dvoet-prod-test-20190305-3/operations/11506961050484846699```. Searched the code and found the error here:; https://github.com/broadinstitute/cromwell/blob/2d8ff3b0962bbe84828445fd3ac77d2379e499c2/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/request/GetRequestHandler.scala#L49-L83. Operation metadata attached as well.; [operation-metadata.yaml.txt](https://github.com/broadinstitute/cromwell/files/3008623/operation-metadata.yaml.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4772
https://github.com/broadinstitute/cromwell/pull/4773:1,Deployability,pipeline,pipeline,1,"`pipeline.getResources.getVirtualMachine.getPreemptible` is intended to be `null` if there was no preemption. Aaron says,; >Client code should be updated to properly access these fields even if they are absent. Closes https://github.com/broadinstitute/cromwell/issues/4772",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4773
https://github.com/broadinstitute/cromwell/pull/4773:146,Deployability,update,updated,146,"`pipeline.getResources.getVirtualMachine.getPreemptible` is intended to be `null` if there was no preemption. Aaron says,; >Client code should be updated to properly access these fields even if they are absent. Closes https://github.com/broadinstitute/cromwell/issues/4772",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4773
https://github.com/broadinstitute/cromwell/pull/4773:166,Security,access,access,166,"`pipeline.getResources.getVirtualMachine.getPreemptible` is intended to be `null` if there was no preemption. Aaron says,; >Client code should be updated to properly access these fields even if they are absent. Closes https://github.com/broadinstitute/cromwell/issues/4772",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4773
https://github.com/broadinstitute/cromwell/issues/4774:251,Availability,robust,robustness,251,"In https://github.com/broadinstitute/cromwell/issues/4772 we learned that Google reserves the right to omit keys from the operation metadata map instead of supplying the default values. In https://github.com/broadinstitute/cromwell/pull/4773 we added robustness for when a particular key, `preemptible`, is absent. Look at the `interpretOperationStatus` functions for PAPI v1 and v2, research which keys Google might chose to omit in the future, and make sure they don't break.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4774
https://github.com/broadinstitute/cromwell/issues/4774:61,Usability,learn,learned,61,"In https://github.com/broadinstitute/cromwell/issues/4772 we learned that Google reserves the right to omit keys from the operation metadata map instead of supplying the default values. In https://github.com/broadinstitute/cromwell/pull/4773 we added robustness for when a particular key, `preemptible`, is absent. Look at the `interpretOperationStatus` functions for PAPI v1 and v2, research which keys Google might chose to omit in the future, and make sure they don't break.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4774
https://github.com/broadinstitute/cromwell/issues/4775:669,Testability,test,tests,669,"Cromwell is correctly placing the file into the execution directory, but it's not correctly mapping the path in the command. My issue specifically:. Issue: I need `file.txt` to be in the execution directory, however when constructing the command it uses the original path to the file in the `inputs/` directory:; ```; 'cat' '/path/to/cromwell-executions/.../inputs/1450116529/file.txt'; ```. when it should be:; ```; 'cat' '/path/to/cromwell-executions/.../execution/file.txt'; ```. For me, this is important for tools (like `tabix`) that only produce their files at the source of the original file, and it's hard to glob outside the execution directory. I noticed the tests in #3001 don't really cover the updating of the path.; I think the underlying problem might be similar to #4533 (where I originally left this comment), but I'm not convinced they're the same issue. ----; CommandLineTool:; ```cwl; baseCommand: cat; class: CommandLineTool; cwlVersion: v1.0; id: ListDirectory; inputs:; file:; inputBinding:; position: 5; label: file_to_localise; type: File; outputs:; out: ; type: File; outputBinding:; glob: $(inputs.file.basename) # breaks without .basename; requirements:; InitialWorkDirRequirement:; listing:; - $(inputs.file); InlineJavascriptRequirement: {}; ```; Inputs:; ```yaml; file:; class: File; path: /path/to/file.txt; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4775
https://github.com/broadinstitute/cromwell/pull/4776:59,Modifiability,config,config,59,Add akka http request-timeout idle-timeout examples in the config; To allow users to get metadata results from large workflows. Also delete the now duplicated cromwell.examples.conf file; And delete the backends section which has been split into; separate files. https://gatkforums.broadinstitute.org/wdl/discussion/10209/retrieving-metadata-for-large-workflows. https://github.com/broadinstitute/cromwell/issues/2519,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4776
https://github.com/broadinstitute/cromwell/pull/4776:22,Safety,timeout,timeout,22,Add akka http request-timeout idle-timeout examples in the config; To allow users to get metadata results from large workflows. Also delete the now duplicated cromwell.examples.conf file; And delete the backends section which has been split into; separate files. https://gatkforums.broadinstitute.org/wdl/discussion/10209/retrieving-metadata-for-large-workflows. https://github.com/broadinstitute/cromwell/issues/2519,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4776
https://github.com/broadinstitute/cromwell/pull/4776:35,Safety,timeout,timeout,35,Add akka http request-timeout idle-timeout examples in the config; To allow users to get metadata results from large workflows. Also delete the now duplicated cromwell.examples.conf file; And delete the backends section which has been split into; separate files. https://gatkforums.broadinstitute.org/wdl/discussion/10209/retrieving-metadata-for-large-workflows. https://github.com/broadinstitute/cromwell/issues/2519,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4776
https://github.com/broadinstitute/cromwell/issues/4777:46,Availability,avail,available,46,"Cromwell (38, in this case) is saturating the available connections to our managed MySQL database. Our DBAs increased the limit and Cromwell proceeded to fill up these slots. How can we limit the number of concurrent connections to a MySQL database? There doesn't seem to be any configuration option for this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4777
https://github.com/broadinstitute/cromwell/issues/4777:279,Deployability,configurat,configuration,279,"Cromwell (38, in this case) is saturating the available connections to our managed MySQL database. Our DBAs increased the limit and Cromwell proceeded to fill up these slots. How can we limit the number of concurrent connections to a MySQL database? There doesn't seem to be any configuration option for this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4777
https://github.com/broadinstitute/cromwell/issues/4777:279,Modifiability,config,configuration,279,"Cromwell (38, in this case) is saturating the available connections to our managed MySQL database. Our DBAs increased the limit and Cromwell proceeded to fill up these slots. How can we limit the number of concurrent connections to a MySQL database? There doesn't seem to be any configuration option for this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4777
https://github.com/broadinstitute/cromwell/issues/4777:206,Performance,concurren,concurrent,206,"Cromwell (38, in this case) is saturating the available connections to our managed MySQL database. Our DBAs increased the limit and Cromwell proceeded to fill up these slots. How can we limit the number of concurrent connections to a MySQL database? There doesn't seem to be any configuration option for this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4777
https://github.com/broadinstitute/cromwell/issues/4781:88,Deployability,configurat,configuration,88,"Horicromtal wants, please see also (and give preference to) the issue for our [existing configuration](https://github.com/broadinstitute/cromwell/issues/4780).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4781
https://github.com/broadinstitute/cromwell/issues/4781:88,Modifiability,config,configuration,88,"Horicromtal wants, please see also (and give preference to) the issue for our [existing configuration](https://github.com/broadinstitute/cromwell/issues/4780).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4781
https://github.com/broadinstitute/cromwell/issues/4783:27,Testability,test,testing,27,"During horizontal Cromwell testing it was observed that the Cromwell not being addressed by Centaur could be grossly misconfigured so that it did no useful work or might topple over during a test run. The test would usually not notice this and success would be declared when really horizontality hadn't been tested. We should figure out the general shape of a ""correct"" horizontal run looks like and assert that this actually occurs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4783
https://github.com/broadinstitute/cromwell/issues/4783:191,Testability,test,test,191,"During horizontal Cromwell testing it was observed that the Cromwell not being addressed by Centaur could be grossly misconfigured so that it did no useful work or might topple over during a test run. The test would usually not notice this and success would be declared when really horizontality hadn't been tested. We should figure out the general shape of a ""correct"" horizontal run looks like and assert that this actually occurs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4783
https://github.com/broadinstitute/cromwell/issues/4783:205,Testability,test,test,205,"During horizontal Cromwell testing it was observed that the Cromwell not being addressed by Centaur could be grossly misconfigured so that it did no useful work or might topple over during a test run. The test would usually not notice this and success would be declared when really horizontality hadn't been tested. We should figure out the general shape of a ""correct"" horizontal run looks like and assert that this actually occurs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4783
https://github.com/broadinstitute/cromwell/issues/4783:308,Testability,test,tested,308,"During horizontal Cromwell testing it was observed that the Cromwell not being addressed by Centaur could be grossly misconfigured so that it did no useful work or might topple over during a test run. The test would usually not notice this and success would be declared when really horizontality hadn't been tested. We should figure out the general shape of a ""correct"" horizontal run looks like and assert that this actually occurs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4783
https://github.com/broadinstitute/cromwell/issues/4783:400,Testability,assert,assert,400,"During horizontal Cromwell testing it was observed that the Cromwell not being addressed by Centaur could be grossly misconfigured so that it did no useful work or might topple over during a test run. The test would usually not notice this and success would be declared when really horizontality hadn't been tested. We should figure out the general shape of a ""correct"" horizontal run looks like and assert that this actually occurs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4783
https://github.com/broadinstitute/cromwell/pull/4785:199,Availability,failure,failures,199,- Additional wiring for the cromwell terminator.; - Reduced duplicate calls to ConfigFactory.load().; - Increased ability to pass around test configs.; - Provide names for more actors.; - Instrument failures in batch actors.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4785
https://github.com/broadinstitute/cromwell/pull/4785:52,Energy Efficiency,Reduce,Reduced,52,- Additional wiring for the cromwell terminator.; - Reduced duplicate calls to ConfigFactory.load().; - Increased ability to pass around test configs.; - Provide names for more actors.; - Instrument failures in batch actors.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4785
https://github.com/broadinstitute/cromwell/pull/4785:79,Modifiability,Config,ConfigFactory,79,- Additional wiring for the cromwell terminator.; - Reduced duplicate calls to ConfigFactory.load().; - Increased ability to pass around test configs.; - Provide names for more actors.; - Instrument failures in batch actors.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4785
https://github.com/broadinstitute/cromwell/pull/4785:142,Modifiability,config,configs,142,- Additional wiring for the cromwell terminator.; - Reduced duplicate calls to ConfigFactory.load().; - Increased ability to pass around test configs.; - Provide names for more actors.; - Instrument failures in batch actors.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4785
https://github.com/broadinstitute/cromwell/pull/4785:93,Performance,load,load,93,- Additional wiring for the cromwell terminator.; - Reduced duplicate calls to ConfigFactory.load().; - Increased ability to pass around test configs.; - Provide names for more actors.; - Instrument failures in batch actors.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4785
https://github.com/broadinstitute/cromwell/pull/4785:137,Testability,test,test,137,- Additional wiring for the cromwell terminator.; - Reduced duplicate calls to ConfigFactory.load().; - Increased ability to pass around test configs.; - Provide names for more actors.; - Instrument failures in batch actors.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4785
https://github.com/broadinstitute/cromwell/issues/4786:17,Deployability,upgrade,upgrade,17,Make sure engine upgrade works in horizontal.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4786
https://github.com/broadinstitute/cromwell/issues/4788:1061,Deployability,install,install-agent,1061,"The easiest way to capture and compare metrics from performance runs is to push them into GCP Stackdriver Monitoring. We have an abstraction in the service registry for Instrumentation implementations, see the [StatsDInstrumentationServiceActor].(https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/instrumentation/impl/statsd/StatsDInstrumentationServiceActor.scala). ## Library; `libraryDependencies += ""com.google.cloud"" % ""google-cloud-monitoring"" % ""1.66.0""`. ## Acceptance Criteria . * Metrics can be labeled from global config settings. Namely: ; * Global config setting for Role (reader, worker, summarizer); * Setting for Host name (e.g. ""gce-cromwell-perfX""); * Setting for stable identifier for the perf test (e.g. ""call-cache-whitelist"").; * Logger can be configured using Service Account from Config.; * Performance runs are visible in GCP Stackdriver graphs with all labels. # Reference. * [Configuring stackdriver agent, useful for service account creation](https://cloud.google.com/monitoring/agent/install-agent); * [v3 monitoring javadoc](https://googleapis.github.io/google-cloud-java/google-cloud-clients/apidocs/com/google/cloud/monitoring/v3/package-summary.html); * [GCP Client Libraries for monitoring](https://cloud.google.com/monitoring/docs/reference/libraries)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4788
https://github.com/broadinstitute/cromwell/issues/4788:106,Energy Efficiency,Monitor,Monitoring,106,"The easiest way to capture and compare metrics from performance runs is to push them into GCP Stackdriver Monitoring. We have an abstraction in the service registry for Instrumentation implementations, see the [StatsDInstrumentationServiceActor].(https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/instrumentation/impl/statsd/StatsDInstrumentationServiceActor.scala). ## Library; `libraryDependencies += ""com.google.cloud"" % ""google-cloud-monitoring"" % ""1.66.0""`. ## Acceptance Criteria . * Metrics can be labeled from global config settings. Namely: ; * Global config setting for Role (reader, worker, summarizer); * Setting for Host name (e.g. ""gce-cromwell-perfX""); * Setting for stable identifier for the perf test (e.g. ""call-cache-whitelist"").; * Logger can be configured using Service Account from Config.; * Performance runs are visible in GCP Stackdriver graphs with all labels. # Reference. * [Configuring stackdriver agent, useful for service account creation](https://cloud.google.com/monitoring/agent/install-agent); * [v3 monitoring javadoc](https://googleapis.github.io/google-cloud-java/google-cloud-clients/apidocs/com/google/cloud/monitoring/v3/package-summary.html); * [GCP Client Libraries for monitoring](https://cloud.google.com/monitoring/docs/reference/libraries)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4788
https://github.com/broadinstitute/cromwell/issues/4788:486,Energy Efficiency,monitor,monitoring,486,"The easiest way to capture and compare metrics from performance runs is to push them into GCP Stackdriver Monitoring. We have an abstraction in the service registry for Instrumentation implementations, see the [StatsDInstrumentationServiceActor].(https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/instrumentation/impl/statsd/StatsDInstrumentationServiceActor.scala). ## Library; `libraryDependencies += ""com.google.cloud"" % ""google-cloud-monitoring"" % ""1.66.0""`. ## Acceptance Criteria . * Metrics can be labeled from global config settings. Namely: ; * Global config setting for Role (reader, worker, summarizer); * Setting for Host name (e.g. ""gce-cromwell-perfX""); * Setting for stable identifier for the perf test (e.g. ""call-cache-whitelist"").; * Logger can be configured using Service Account from Config.; * Performance runs are visible in GCP Stackdriver graphs with all labels. # Reference. * [Configuring stackdriver agent, useful for service account creation](https://cloud.google.com/monitoring/agent/install-agent); * [v3 monitoring javadoc](https://googleapis.github.io/google-cloud-java/google-cloud-clients/apidocs/com/google/cloud/monitoring/v3/package-summary.html); * [GCP Client Libraries for monitoring](https://cloud.google.com/monitoring/docs/reference/libraries)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4788
https://github.com/broadinstitute/cromwell/issues/4788:1044,Energy Efficiency,monitor,monitoring,1044,"The easiest way to capture and compare metrics from performance runs is to push them into GCP Stackdriver Monitoring. We have an abstraction in the service registry for Instrumentation implementations, see the [StatsDInstrumentationServiceActor].(https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/instrumentation/impl/statsd/StatsDInstrumentationServiceActor.scala). ## Library; `libraryDependencies += ""com.google.cloud"" % ""google-cloud-monitoring"" % ""1.66.0""`. ## Acceptance Criteria . * Metrics can be labeled from global config settings. Namely: ; * Global config setting for Role (reader, worker, summarizer); * Setting for Host name (e.g. ""gce-cromwell-perfX""); * Setting for stable identifier for the perf test (e.g. ""call-cache-whitelist"").; * Logger can be configured using Service Account from Config.; * Performance runs are visible in GCP Stackdriver graphs with all labels. # Reference. * [Configuring stackdriver agent, useful for service account creation](https://cloud.google.com/monitoring/agent/install-agent); * [v3 monitoring javadoc](https://googleapis.github.io/google-cloud-java/google-cloud-clients/apidocs/com/google/cloud/monitoring/v3/package-summary.html); * [GCP Client Libraries for monitoring](https://cloud.google.com/monitoring/docs/reference/libraries)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4788
https://github.com/broadinstitute/cromwell/issues/4788:1083,Energy Efficiency,monitor,monitoring,1083,"The easiest way to capture and compare metrics from performance runs is to push them into GCP Stackdriver Monitoring. We have an abstraction in the service registry for Instrumentation implementations, see the [StatsDInstrumentationServiceActor].(https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/instrumentation/impl/statsd/StatsDInstrumentationServiceActor.scala). ## Library; `libraryDependencies += ""com.google.cloud"" % ""google-cloud-monitoring"" % ""1.66.0""`. ## Acceptance Criteria . * Metrics can be labeled from global config settings. Namely: ; * Global config setting for Role (reader, worker, summarizer); * Setting for Host name (e.g. ""gce-cromwell-perfX""); * Setting for stable identifier for the perf test (e.g. ""call-cache-whitelist"").; * Logger can be configured using Service Account from Config.; * Performance runs are visible in GCP Stackdriver graphs with all labels. # Reference. * [Configuring stackdriver agent, useful for service account creation](https://cloud.google.com/monitoring/agent/install-agent); * [v3 monitoring javadoc](https://googleapis.github.io/google-cloud-java/google-cloud-clients/apidocs/com/google/cloud/monitoring/v3/package-summary.html); * [GCP Client Libraries for monitoring](https://cloud.google.com/monitoring/docs/reference/libraries)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4788
https://github.com/broadinstitute/cromwell/issues/4788:1196,Energy Efficiency,monitor,monitoring,1196,"The easiest way to capture and compare metrics from performance runs is to push them into GCP Stackdriver Monitoring. We have an abstraction in the service registry for Instrumentation implementations, see the [StatsDInstrumentationServiceActor].(https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/instrumentation/impl/statsd/StatsDInstrumentationServiceActor.scala). ## Library; `libraryDependencies += ""com.google.cloud"" % ""google-cloud-monitoring"" % ""1.66.0""`. ## Acceptance Criteria . * Metrics can be labeled from global config settings. Namely: ; * Global config setting for Role (reader, worker, summarizer); * Setting for Host name (e.g. ""gce-cromwell-perfX""); * Setting for stable identifier for the perf test (e.g. ""call-cache-whitelist"").; * Logger can be configured using Service Account from Config.; * Performance runs are visible in GCP Stackdriver graphs with all labels. # Reference. * [Configuring stackdriver agent, useful for service account creation](https://cloud.google.com/monitoring/agent/install-agent); * [v3 monitoring javadoc](https://googleapis.github.io/google-cloud-java/google-cloud-clients/apidocs/com/google/cloud/monitoring/v3/package-summary.html); * [GCP Client Libraries for monitoring](https://cloud.google.com/monitoring/docs/reference/libraries)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4788
https://github.com/broadinstitute/cromwell/issues/4788:1261,Energy Efficiency,monitor,monitoring,1261,"The easiest way to capture and compare metrics from performance runs is to push them into GCP Stackdriver Monitoring. We have an abstraction in the service registry for Instrumentation implementations, see the [StatsDInstrumentationServiceActor].(https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/instrumentation/impl/statsd/StatsDInstrumentationServiceActor.scala). ## Library; `libraryDependencies += ""com.google.cloud"" % ""google-cloud-monitoring"" % ""1.66.0""`. ## Acceptance Criteria . * Metrics can be labeled from global config settings. Namely: ; * Global config setting for Role (reader, worker, summarizer); * Setting for Host name (e.g. ""gce-cromwell-perfX""); * Setting for stable identifier for the perf test (e.g. ""call-cache-whitelist"").; * Logger can be configured using Service Account from Config.; * Performance runs are visible in GCP Stackdriver graphs with all labels. # Reference. * [Configuring stackdriver agent, useful for service account creation](https://cloud.google.com/monitoring/agent/install-agent); * [v3 monitoring javadoc](https://googleapis.github.io/google-cloud-java/google-cloud-clients/apidocs/com/google/cloud/monitoring/v3/package-summary.html); * [GCP Client Libraries for monitoring](https://cloud.google.com/monitoring/docs/reference/libraries)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4788
https://github.com/broadinstitute/cromwell/issues/4788:1298,Energy Efficiency,monitor,monitoring,1298,"The easiest way to capture and compare metrics from performance runs is to push them into GCP Stackdriver Monitoring. We have an abstraction in the service registry for Instrumentation implementations, see the [StatsDInstrumentationServiceActor].(https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/instrumentation/impl/statsd/StatsDInstrumentationServiceActor.scala). ## Library; `libraryDependencies += ""com.google.cloud"" % ""google-cloud-monitoring"" % ""1.66.0""`. ## Acceptance Criteria . * Metrics can be labeled from global config settings. Namely: ; * Global config setting for Role (reader, worker, summarizer); * Setting for Host name (e.g. ""gce-cromwell-perfX""); * Setting for stable identifier for the perf test (e.g. ""call-cache-whitelist"").; * Logger can be configured using Service Account from Config.; * Performance runs are visible in GCP Stackdriver graphs with all labels. # Reference. * [Configuring stackdriver agent, useful for service account creation](https://cloud.google.com/monitoring/agent/install-agent); * [v3 monitoring javadoc](https://googleapis.github.io/google-cloud-java/google-cloud-clients/apidocs/com/google/cloud/monitoring/v3/package-summary.html); * [GCP Client Libraries for monitoring](https://cloud.google.com/monitoring/docs/reference/libraries)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4788
https://github.com/broadinstitute/cromwell/issues/4788:573,Modifiability,config,config,573,"The easiest way to capture and compare metrics from performance runs is to push them into GCP Stackdriver Monitoring. We have an abstraction in the service registry for Instrumentation implementations, see the [StatsDInstrumentationServiceActor].(https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/instrumentation/impl/statsd/StatsDInstrumentationServiceActor.scala). ## Library; `libraryDependencies += ""com.google.cloud"" % ""google-cloud-monitoring"" % ""1.66.0""`. ## Acceptance Criteria . * Metrics can be labeled from global config settings. Namely: ; * Global config setting for Role (reader, worker, summarizer); * Setting for Host name (e.g. ""gce-cromwell-perfX""); * Setting for stable identifier for the perf test (e.g. ""call-cache-whitelist"").; * Logger can be configured using Service Account from Config.; * Performance runs are visible in GCP Stackdriver graphs with all labels. # Reference. * [Configuring stackdriver agent, useful for service account creation](https://cloud.google.com/monitoring/agent/install-agent); * [v3 monitoring javadoc](https://googleapis.github.io/google-cloud-java/google-cloud-clients/apidocs/com/google/cloud/monitoring/v3/package-summary.html); * [GCP Client Libraries for monitoring](https://cloud.google.com/monitoring/docs/reference/libraries)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4788
https://github.com/broadinstitute/cromwell/issues/4788:609,Modifiability,config,config,609,"The easiest way to capture and compare metrics from performance runs is to push them into GCP Stackdriver Monitoring. We have an abstraction in the service registry for Instrumentation implementations, see the [StatsDInstrumentationServiceActor].(https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/instrumentation/impl/statsd/StatsDInstrumentationServiceActor.scala). ## Library; `libraryDependencies += ""com.google.cloud"" % ""google-cloud-monitoring"" % ""1.66.0""`. ## Acceptance Criteria . * Metrics can be labeled from global config settings. Namely: ; * Global config setting for Role (reader, worker, summarizer); * Setting for Host name (e.g. ""gce-cromwell-perfX""); * Setting for stable identifier for the perf test (e.g. ""call-cache-whitelist"").; * Logger can be configured using Service Account from Config.; * Performance runs are visible in GCP Stackdriver graphs with all labels. # Reference. * [Configuring stackdriver agent, useful for service account creation](https://cloud.google.com/monitoring/agent/install-agent); * [v3 monitoring javadoc](https://googleapis.github.io/google-cloud-java/google-cloud-clients/apidocs/com/google/cloud/monitoring/v3/package-summary.html); * [GCP Client Libraries for monitoring](https://cloud.google.com/monitoring/docs/reference/libraries)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4788
https://github.com/broadinstitute/cromwell/issues/4788:814,Modifiability,config,configured,814,"The easiest way to capture and compare metrics from performance runs is to push them into GCP Stackdriver Monitoring. We have an abstraction in the service registry for Instrumentation implementations, see the [StatsDInstrumentationServiceActor].(https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/instrumentation/impl/statsd/StatsDInstrumentationServiceActor.scala). ## Library; `libraryDependencies += ""com.google.cloud"" % ""google-cloud-monitoring"" % ""1.66.0""`. ## Acceptance Criteria . * Metrics can be labeled from global config settings. Namely: ; * Global config setting for Role (reader, worker, summarizer); * Setting for Host name (e.g. ""gce-cromwell-perfX""); * Setting for stable identifier for the perf test (e.g. ""call-cache-whitelist"").; * Logger can be configured using Service Account from Config.; * Performance runs are visible in GCP Stackdriver graphs with all labels. # Reference. * [Configuring stackdriver agent, useful for service account creation](https://cloud.google.com/monitoring/agent/install-agent); * [v3 monitoring javadoc](https://googleapis.github.io/google-cloud-java/google-cloud-clients/apidocs/com/google/cloud/monitoring/v3/package-summary.html); * [GCP Client Libraries for monitoring](https://cloud.google.com/monitoring/docs/reference/libraries)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4788
https://github.com/broadinstitute/cromwell/issues/4788:852,Modifiability,Config,Config,852,"The easiest way to capture and compare metrics from performance runs is to push them into GCP Stackdriver Monitoring. We have an abstraction in the service registry for Instrumentation implementations, see the [StatsDInstrumentationServiceActor].(https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/instrumentation/impl/statsd/StatsDInstrumentationServiceActor.scala). ## Library; `libraryDependencies += ""com.google.cloud"" % ""google-cloud-monitoring"" % ""1.66.0""`. ## Acceptance Criteria . * Metrics can be labeled from global config settings. Namely: ; * Global config setting for Role (reader, worker, summarizer); * Setting for Host name (e.g. ""gce-cromwell-perfX""); * Setting for stable identifier for the perf test (e.g. ""call-cache-whitelist"").; * Logger can be configured using Service Account from Config.; * Performance runs are visible in GCP Stackdriver graphs with all labels. # Reference. * [Configuring stackdriver agent, useful for service account creation](https://cloud.google.com/monitoring/agent/install-agent); * [v3 monitoring javadoc](https://googleapis.github.io/google-cloud-java/google-cloud-clients/apidocs/com/google/cloud/monitoring/v3/package-summary.html); * [GCP Client Libraries for monitoring](https://cloud.google.com/monitoring/docs/reference/libraries)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4788
https://github.com/broadinstitute/cromwell/issues/4788:951,Modifiability,Config,Configuring,951,"The easiest way to capture and compare metrics from performance runs is to push them into GCP Stackdriver Monitoring. We have an abstraction in the service registry for Instrumentation implementations, see the [StatsDInstrumentationServiceActor].(https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/instrumentation/impl/statsd/StatsDInstrumentationServiceActor.scala). ## Library; `libraryDependencies += ""com.google.cloud"" % ""google-cloud-monitoring"" % ""1.66.0""`. ## Acceptance Criteria . * Metrics can be labeled from global config settings. Namely: ; * Global config setting for Role (reader, worker, summarizer); * Setting for Host name (e.g. ""gce-cromwell-perfX""); * Setting for stable identifier for the perf test (e.g. ""call-cache-whitelist"").; * Logger can be configured using Service Account from Config.; * Performance runs are visible in GCP Stackdriver graphs with all labels. # Reference. * [Configuring stackdriver agent, useful for service account creation](https://cloud.google.com/monitoring/agent/install-agent); * [v3 monitoring javadoc](https://googleapis.github.io/google-cloud-java/google-cloud-clients/apidocs/com/google/cloud/monitoring/v3/package-summary.html); * [GCP Client Libraries for monitoring](https://cloud.google.com/monitoring/docs/reference/libraries)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4788
https://github.com/broadinstitute/cromwell/issues/4788:52,Performance,perform,performance,52,"The easiest way to capture and compare metrics from performance runs is to push them into GCP Stackdriver Monitoring. We have an abstraction in the service registry for Instrumentation implementations, see the [StatsDInstrumentationServiceActor].(https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/instrumentation/impl/statsd/StatsDInstrumentationServiceActor.scala). ## Library; `libraryDependencies += ""com.google.cloud"" % ""google-cloud-monitoring"" % ""1.66.0""`. ## Acceptance Criteria . * Metrics can be labeled from global config settings. Namely: ; * Global config setting for Role (reader, worker, summarizer); * Setting for Host name (e.g. ""gce-cromwell-perfX""); * Setting for stable identifier for the perf test (e.g. ""call-cache-whitelist"").; * Logger can be configured using Service Account from Config.; * Performance runs are visible in GCP Stackdriver graphs with all labels. # Reference. * [Configuring stackdriver agent, useful for service account creation](https://cloud.google.com/monitoring/agent/install-agent); * [v3 monitoring javadoc](https://googleapis.github.io/google-cloud-java/google-cloud-clients/apidocs/com/google/cloud/monitoring/v3/package-summary.html); * [GCP Client Libraries for monitoring](https://cloud.google.com/monitoring/docs/reference/libraries)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4788
https://github.com/broadinstitute/cromwell/issues/4788:778,Performance,cache,cache-whitelist,778,"The easiest way to capture and compare metrics from performance runs is to push them into GCP Stackdriver Monitoring. We have an abstraction in the service registry for Instrumentation implementations, see the [StatsDInstrumentationServiceActor].(https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/instrumentation/impl/statsd/StatsDInstrumentationServiceActor.scala). ## Library; `libraryDependencies += ""com.google.cloud"" % ""google-cloud-monitoring"" % ""1.66.0""`. ## Acceptance Criteria . * Metrics can be labeled from global config settings. Namely: ; * Global config setting for Role (reader, worker, summarizer); * Setting for Host name (e.g. ""gce-cromwell-perfX""); * Setting for stable identifier for the perf test (e.g. ""call-cache-whitelist"").; * Logger can be configured using Service Account from Config.; * Performance runs are visible in GCP Stackdriver graphs with all labels. # Reference. * [Configuring stackdriver agent, useful for service account creation](https://cloud.google.com/monitoring/agent/install-agent); * [v3 monitoring javadoc](https://googleapis.github.io/google-cloud-java/google-cloud-clients/apidocs/com/google/cloud/monitoring/v3/package-summary.html); * [GCP Client Libraries for monitoring](https://cloud.google.com/monitoring/docs/reference/libraries)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4788
https://github.com/broadinstitute/cromwell/issues/4788:863,Performance,Perform,Performance,863,"The easiest way to capture and compare metrics from performance runs is to push them into GCP Stackdriver Monitoring. We have an abstraction in the service registry for Instrumentation implementations, see the [StatsDInstrumentationServiceActor].(https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/instrumentation/impl/statsd/StatsDInstrumentationServiceActor.scala). ## Library; `libraryDependencies += ""com.google.cloud"" % ""google-cloud-monitoring"" % ""1.66.0""`. ## Acceptance Criteria . * Metrics can be labeled from global config settings. Namely: ; * Global config setting for Role (reader, worker, summarizer); * Setting for Host name (e.g. ""gce-cromwell-perfX""); * Setting for stable identifier for the perf test (e.g. ""call-cache-whitelist"").; * Logger can be configured using Service Account from Config.; * Performance runs are visible in GCP Stackdriver graphs with all labels. # Reference. * [Configuring stackdriver agent, useful for service account creation](https://cloud.google.com/monitoring/agent/install-agent); * [v3 monitoring javadoc](https://googleapis.github.io/google-cloud-java/google-cloud-clients/apidocs/com/google/cloud/monitoring/v3/package-summary.html); * [GCP Client Libraries for monitoring](https://cloud.google.com/monitoring/docs/reference/libraries)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4788
https://github.com/broadinstitute/cromwell/issues/4788:761,Testability,test,test,761,"The easiest way to capture and compare metrics from performance runs is to push them into GCP Stackdriver Monitoring. We have an abstraction in the service registry for Instrumentation implementations, see the [StatsDInstrumentationServiceActor].(https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/instrumentation/impl/statsd/StatsDInstrumentationServiceActor.scala). ## Library; `libraryDependencies += ""com.google.cloud"" % ""google-cloud-monitoring"" % ""1.66.0""`. ## Acceptance Criteria . * Metrics can be labeled from global config settings. Namely: ; * Global config setting for Role (reader, worker, summarizer); * Setting for Host name (e.g. ""gce-cromwell-perfX""); * Setting for stable identifier for the perf test (e.g. ""call-cache-whitelist"").; * Logger can be configured using Service Account from Config.; * Performance runs are visible in GCP Stackdriver graphs with all labels. # Reference. * [Configuring stackdriver agent, useful for service account creation](https://cloud.google.com/monitoring/agent/install-agent); * [v3 monitoring javadoc](https://googleapis.github.io/google-cloud-java/google-cloud-clients/apidocs/com/google/cloud/monitoring/v3/package-summary.html); * [GCP Client Libraries for monitoring](https://cloud.google.com/monitoring/docs/reference/libraries)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4788
https://github.com/broadinstitute/cromwell/issues/4788:800,Testability,Log,Logger,800,"The easiest way to capture and compare metrics from performance runs is to push them into GCP Stackdriver Monitoring. We have an abstraction in the service registry for Instrumentation implementations, see the [StatsDInstrumentationServiceActor].(https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/instrumentation/impl/statsd/StatsDInstrumentationServiceActor.scala). ## Library; `libraryDependencies += ""com.google.cloud"" % ""google-cloud-monitoring"" % ""1.66.0""`. ## Acceptance Criteria . * Metrics can be labeled from global config settings. Namely: ; * Global config setting for Role (reader, worker, summarizer); * Setting for Host name (e.g. ""gce-cromwell-perfX""); * Setting for stable identifier for the perf test (e.g. ""call-cache-whitelist"").; * Logger can be configured using Service Account from Config.; * Performance runs are visible in GCP Stackdriver graphs with all labels. # Reference. * [Configuring stackdriver agent, useful for service account creation](https://cloud.google.com/monitoring/agent/install-agent); * [v3 monitoring javadoc](https://googleapis.github.io/google-cloud-java/google-cloud-clients/apidocs/com/google/cloud/monitoring/v3/package-summary.html); * [GCP Client Libraries for monitoring](https://cloud.google.com/monitoring/docs/reference/libraries)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4788
https://github.com/broadinstitute/cromwell/pull/4789:342,Usability,clear,clearly,342,"Closes #4500 . Requires https://github.com/openwdl/wdl/pull/303. My description over on OpenWDL:. >Right now, the grammar does not correctly handle strings like; >```; >String q1 = ""leading text \"" trailing text""; >```; >The example above is sliced up into one string `""leading text ""` and three bare symbols `trailing`, `text`, `""` which is clearly wrong.; >; >This PR fixes the issue by preventing quotes that follow the escape character `\` from marking the end of a string.; >; >In regex terms, it adds a negative lookbehind for `\` inside the positive lookahead that stops at `""`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4789
https://github.com/broadinstitute/cromwell/issues/4791:28,Performance,perform,performance,28,"# Why; To improve copy fail performance because it introduced I/O bottlenecks -- specifically to improve the workshop experience; # What; Confirm that the existing test is either the right one, or replace it with the right test case of a theoretical ; # Measure. * \# of results from Call cache requests; * Total time it takes to get through results & return answer; * How many elements per bucket it is looking at before blacklisting; * How long does each bucket take?. # Assert; * Still returns call cache hit -or-; * Runs the job",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4791
https://github.com/broadinstitute/cromwell/issues/4791:66,Performance,bottleneck,bottlenecks,66,"# Why; To improve copy fail performance because it introduced I/O bottlenecks -- specifically to improve the workshop experience; # What; Confirm that the existing test is either the right one, or replace it with the right test case of a theoretical ; # Measure. * \# of results from Call cache requests; * Total time it takes to get through results & return answer; * How many elements per bucket it is looking at before blacklisting; * How long does each bucket take?. # Assert; * Still returns call cache hit -or-; * Runs the job",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4791
https://github.com/broadinstitute/cromwell/issues/4791:289,Performance,cache,cache,289,"# Why; To improve copy fail performance because it introduced I/O bottlenecks -- specifically to improve the workshop experience; # What; Confirm that the existing test is either the right one, or replace it with the right test case of a theoretical ; # Measure. * \# of results from Call cache requests; * Total time it takes to get through results & return answer; * How many elements per bucket it is looking at before blacklisting; * How long does each bucket take?. # Assert; * Still returns call cache hit -or-; * Runs the job",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4791
https://github.com/broadinstitute/cromwell/issues/4791:502,Performance,cache,cache,502,"# Why; To improve copy fail performance because it introduced I/O bottlenecks -- specifically to improve the workshop experience; # What; Confirm that the existing test is either the right one, or replace it with the right test case of a theoretical ; # Measure. * \# of results from Call cache requests; * Total time it takes to get through results & return answer; * How many elements per bucket it is looking at before blacklisting; * How long does each bucket take?. # Assert; * Still returns call cache hit -or-; * Runs the job",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4791
https://github.com/broadinstitute/cromwell/issues/4791:164,Testability,test,test,164,"# Why; To improve copy fail performance because it introduced I/O bottlenecks -- specifically to improve the workshop experience; # What; Confirm that the existing test is either the right one, or replace it with the right test case of a theoretical ; # Measure. * \# of results from Call cache requests; * Total time it takes to get through results & return answer; * How many elements per bucket it is looking at before blacklisting; * How long does each bucket take?. # Assert; * Still returns call cache hit -or-; * Runs the job",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4791
https://github.com/broadinstitute/cromwell/issues/4791:223,Testability,test,test,223,"# Why; To improve copy fail performance because it introduced I/O bottlenecks -- specifically to improve the workshop experience; # What; Confirm that the existing test is either the right one, or replace it with the right test case of a theoretical ; # Measure. * \# of results from Call cache requests; * Total time it takes to get through results & return answer; * How many elements per bucket it is looking at before blacklisting; * How long does each bucket take?. # Assert; * Still returns call cache hit -or-; * Runs the job",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4791
https://github.com/broadinstitute/cromwell/issues/4791:473,Testability,Assert,Assert,473,"# Why; To improve copy fail performance because it introduced I/O bottlenecks -- specifically to improve the workshop experience; # What; Confirm that the existing test is either the right one, or replace it with the right test case of a theoretical ; # Measure. * \# of results from Call cache requests; * Total time it takes to get through results & return answer; * How many elements per bucket it is looking at before blacklisting; * How long does each bucket take?. # Assert; * Still returns call cache hit -or-; * Runs the job",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4791
https://github.com/broadinstitute/cromwell/issues/4792:8,Energy Efficiency,Green,Green,8,"## Why; Green team submits a lot of workflows at once, and then immediately uses the /query endpoint but gets stale data; ## Author's Proposed approach ; 1. Submit many jobs, let them run to completion; 1. Turn on summarizer; ## Measure; * How long it took to summarize the data; ## Challenges to consider; ### How to tell when summarization is finished?; Can poll DB table to see where summary pointer is vs. size of metadata table",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4792
https://github.com/broadinstitute/cromwell/issues/4793:84,Availability,error,errors,84,Submit large N (perhaps 10K) workflows as fast as we can; ## Measure; * Observe any errors from submit endpoint; * How well is the summarizer keeping up with Metadata; * All workflows complete in < X Seconds; * CPU/memory usage of the Cromwell server ; * CPU/memory usage of the Database (directly from stackdriver) (TODO: Not sure we care?); ## Possible solutions; ### Mock-ify the Google backend; ## Goal; Submit workflows is successful and all return in < X seconds,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4793
https://github.com/broadinstitute/cromwell/issues/4793:370,Testability,Mock,Mock-ify,370,Submit large N (perhaps 10K) workflows as fast as we can; ## Measure; * Observe any errors from submit endpoint; * How well is the summarizer keeping up with Metadata; * All workflows complete in < X Seconds; * CPU/memory usage of the Cromwell server ; * CPU/memory usage of the Database (directly from stackdriver) (TODO: Not sure we care?); ## Possible solutions; ### Mock-ify the Google backend; ## Goal; Submit workflows is successful and all return in < X seconds,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4793
https://github.com/broadinstitute/cromwell/issues/4794:66,Deployability,continuous,continuously,66,"## Motivation; * Test longevity of Cromwell, to prove that it can continuously run at scale over multiple days.; * This is Green team ideally running 20k jobs/day okr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4794
https://github.com/broadinstitute/cromwell/issues/4794:123,Energy Efficiency,Green,Green,123,"## Motivation; * Test longevity of Cromwell, to prove that it can continuously run at scale over multiple days.; * This is Green team ideally running 20k jobs/day okr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4794
https://github.com/broadinstitute/cromwell/issues/4794:17,Testability,Test,Test,17,"## Motivation; * Test longevity of Cromwell, to prove that it can continuously run at scale over multiple days.; * This is Green team ideally running 20k jobs/day okr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4794
https://github.com/broadinstitute/cromwell/issues/4795:188,Performance,load,load,188,"## Why; Emerald empire scattered 100k wide and it didn’t go well. * Cromwell pegged CPU and was unresponsive to HTTP calls, forcing the process manager to kill it; * Papi v2 deadlocked w/ load (talked to Aaron Kemp & Henry Ferrara, No-op for us. ## What; Send 100k wide scatter to Cromwell, make sure it handles it gracefully. ## Measure. * Time it took to complete scatter (TODO: not sure at which point to consider ""complete"", almost certainly it should be before the WDL is run and thus avoids the inherent variance of the backend ); * CPU (should not be pegged); * HTTP Responsiveness (should respond to HTTP calls in a reasonable time: < 2s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4795
https://github.com/broadinstitute/cromwell/issues/4795:490,Safety,avoid,avoids,490,"## Why; Emerald empire scattered 100k wide and it didn’t go well. * Cromwell pegged CPU and was unresponsive to HTTP calls, forcing the process manager to kill it; * Papi v2 deadlocked w/ load (talked to Aaron Kemp & Henry Ferrara, No-op for us. ## What; Send 100k wide scatter to Cromwell, make sure it handles it gracefully. ## Measure. * Time it took to complete scatter (TODO: not sure at which point to consider ""complete"", almost certainly it should be before the WDL is run and thus avoids the inherent variance of the backend ); * CPU (should not be pegged); * HTTP Responsiveness (should respond to HTTP calls in a reasonable time: < 2s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4795
https://github.com/broadinstitute/cromwell/issues/4795:574,Usability,Responsiv,Responsiveness,574,"## Why; Emerald empire scattered 100k wide and it didn’t go well. * Cromwell pegged CPU and was unresponsive to HTTP calls, forcing the process manager to kill it; * Papi v2 deadlocked w/ load (talked to Aaron Kemp & Henry Ferrara, No-op for us. ## What; Send 100k wide scatter to Cromwell, make sure it handles it gracefully. ## Measure. * Time it took to complete scatter (TODO: not sure at which point to consider ""complete"", almost certainly it should be before the WDL is run and thus avoids the inherent variance of the backend ); * CPU (should not be pegged); * HTTP Responsiveness (should respond to HTTP calls in a reasonable time: < 2s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4795
https://github.com/broadinstitute/cromwell/issues/4796:54,Availability,resilien,resilience,54,"In order to improve our scalability, performance, and resilience, we would like to be able to demonstrate measurable improvements. This epic lists the performance tests we would like to run as part of this effort, as well as what to measure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4796
https://github.com/broadinstitute/cromwell/issues/4796:24,Performance,scalab,scalability,24,"In order to improve our scalability, performance, and resilience, we would like to be able to demonstrate measurable improvements. This epic lists the performance tests we would like to run as part of this effort, as well as what to measure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4796
https://github.com/broadinstitute/cromwell/issues/4796:37,Performance,perform,performance,37,"In order to improve our scalability, performance, and resilience, we would like to be able to demonstrate measurable improvements. This epic lists the performance tests we would like to run as part of this effort, as well as what to measure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4796
https://github.com/broadinstitute/cromwell/issues/4796:151,Performance,perform,performance,151,"In order to improve our scalability, performance, and resilience, we would like to be able to demonstrate measurable improvements. This epic lists the performance tests we would like to run as part of this effort, as well as what to measure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4796
https://github.com/broadinstitute/cromwell/issues/4796:163,Testability,test,tests,163,"In order to improve our scalability, performance, and resilience, we would like to be able to demonstrate measurable improvements. This epic lists the performance tests we would like to run as part of this effort, as well as what to measure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4796
https://github.com/broadinstitute/cromwell/pull/4797:659,Testability,test,test,659,"When Cromwell returns the name for the docker container, there are often non compliant characters for the unix filesystem ('/', '@', ':'). When pulling and rebuilding the container, replace non alphabetical, numerical or some symbols (`[A-Za-z0-9._-]`) with an underscore (`_`). And ensure that Singularity isn't constantly rebuilding containers with no reason. Even if this doesn't get merged, I think it could potentially be valuable for someone. Basically, I just added these three lines to the `docker-submit` script for SLURM (applicable for all batch systems). It might be valuable to change `-f` to `-x`, to check if a file is executable by you (`help test`). ```bash; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME.sif; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4797
https://github.com/broadinstitute/cromwell/issues/4798:31,Deployability,deploy,deployed,31,"## Today. Cromwell workers are deployed as a Terraform `instance,` most likely w/ a `count` of 1.*. ## What we want. ### # of instances. The `instance` stanza should reflect a minimum # of instances = `1`, max=`4`. The 1 represents the normal case where we want 1 Cromwell running, and only scales up when necessary. . ### Autoscaling. Should be set to `CPU Usage` where target == `50%`. Add an additional `group` stanza that creates a GCP instance group and refers to the Cromwell workers.*. *Raph Luckom made this presumption, the author does not have access to verify.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4798
https://github.com/broadinstitute/cromwell/issues/4798:554,Security,access,access,554,"## Today. Cromwell workers are deployed as a Terraform `instance,` most likely w/ a `count` of 1.*. ## What we want. ### # of instances. The `instance` stanza should reflect a minimum # of instances = `1`, max=`4`. The 1 represents the normal case where we want 1 Cromwell running, and only scales up when necessary. . ### Autoscaling. Should be set to `CPU Usage` where target == `50%`. Add an additional `group` stanza that creates a GCP instance group and refers to the Cromwell workers.*. *Raph Luckom made this presumption, the author does not have access to verify.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4798
https://github.com/broadinstitute/cromwell/issues/4799:33,Deployability,deploy,deployed,33,"Acceptance Criteria:. DevOps has deployed an instance ready to be configured using `summarizer` for `INSTANCE_TYPE` environment variable, [seen in this firecloud-develop PR](https://github.com/broadinstitute/firecloud-develop/pull/1590). In detail this consists of a terraform `instance` stanza with `count=1` or omitted.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4799
https://github.com/broadinstitute/cromwell/issues/4799:66,Modifiability,config,configured,66,"Acceptance Criteria:. DevOps has deployed an instance ready to be configured using `summarizer` for `INSTANCE_TYPE` environment variable, [seen in this firecloud-develop PR](https://github.com/broadinstitute/firecloud-develop/pull/1590). In detail this consists of a terraform `instance` stanza with `count=1` or omitted.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4799
https://github.com/broadinstitute/cromwell/issues/4799:128,Modifiability,variab,variable,128,"Acceptance Criteria:. DevOps has deployed an instance ready to be configured using `summarizer` for `INSTANCE_TYPE` environment variable, [seen in this firecloud-develop PR](https://github.com/broadinstitute/firecloud-develop/pull/1590). In detail this consists of a terraform `instance` stanza with `count=1` or omitted.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4799
https://github.com/broadinstitute/cromwell/issues/4800:49,Deployability,release,release,49,How will The Great Horizontaling happen? Will we release Cromwell 4X and then sometime later change the configuration of that same version to be horizontal? Or will we release Cromwell 4X leaping into horizontal for the first time? Or different scenarios for different environments?. Whichever paths to horizontal are supported should have integration tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4800
https://github.com/broadinstitute/cromwell/issues/4800:104,Deployability,configurat,configuration,104,How will The Great Horizontaling happen? Will we release Cromwell 4X and then sometime later change the configuration of that same version to be horizontal? Or will we release Cromwell 4X leaping into horizontal for the first time? Or different scenarios for different environments?. Whichever paths to horizontal are supported should have integration tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4800
https://github.com/broadinstitute/cromwell/issues/4800:168,Deployability,release,release,168,How will The Great Horizontaling happen? Will we release Cromwell 4X and then sometime later change the configuration of that same version to be horizontal? Or will we release Cromwell 4X leaping into horizontal for the first time? Or different scenarios for different environments?. Whichever paths to horizontal are supported should have integration tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4800
https://github.com/broadinstitute/cromwell/issues/4800:340,Deployability,integrat,integration,340,How will The Great Horizontaling happen? Will we release Cromwell 4X and then sometime later change the configuration of that same version to be horizontal? Or will we release Cromwell 4X leaping into horizontal for the first time? Or different scenarios for different environments?. Whichever paths to horizontal are supported should have integration tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4800
https://github.com/broadinstitute/cromwell/issues/4800:340,Integrability,integrat,integration,340,How will The Great Horizontaling happen? Will we release Cromwell 4X and then sometime later change the configuration of that same version to be horizontal? Or will we release Cromwell 4X leaping into horizontal for the first time? Or different scenarios for different environments?. Whichever paths to horizontal are supported should have integration tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4800
https://github.com/broadinstitute/cromwell/issues/4800:104,Modifiability,config,configuration,104,How will The Great Horizontaling happen? Will we release Cromwell 4X and then sometime later change the configuration of that same version to be horizontal? Or will we release Cromwell 4X leaping into horizontal for the first time? Or different scenarios for different environments?. Whichever paths to horizontal are supported should have integration tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4800
https://github.com/broadinstitute/cromwell/issues/4800:352,Testability,test,tests,352,How will The Great Horizontaling happen? Will we release Cromwell 4X and then sometime later change the configuration of that same version to be horizontal? Or will we release Cromwell 4X leaping into horizontal for the first time? Or different scenarios for different environments?. Whichever paths to horizontal are supported should have integration tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4800
https://github.com/broadinstitute/cromwell/issues/4801:21,Deployability,upgrade,upgrade,21,"NOTE: if the PAPI v2 upgrade happens in all environments before horizontaling, this ticket becomes unnecessary and should be closed as a noop. OTHERWISE:. Adapt the existing Centaur test for PAPI v1 to PAPI v2 upgrade for a horizontal Cromwell configuration.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4801
https://github.com/broadinstitute/cromwell/issues/4801:210,Deployability,upgrade,upgrade,210,"NOTE: if the PAPI v2 upgrade happens in all environments before horizontaling, this ticket becomes unnecessary and should be closed as a noop. OTHERWISE:. Adapt the existing Centaur test for PAPI v1 to PAPI v2 upgrade for a horizontal Cromwell configuration.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4801
https://github.com/broadinstitute/cromwell/issues/4801:244,Deployability,configurat,configuration,244,"NOTE: if the PAPI v2 upgrade happens in all environments before horizontaling, this ticket becomes unnecessary and should be closed as a noop. OTHERWISE:. Adapt the existing Centaur test for PAPI v1 to PAPI v2 upgrade for a horizontal Cromwell configuration.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4801
https://github.com/broadinstitute/cromwell/issues/4801:155,Energy Efficiency,Adapt,Adapt,155,"NOTE: if the PAPI v2 upgrade happens in all environments before horizontaling, this ticket becomes unnecessary and should be closed as a noop. OTHERWISE:. Adapt the existing Centaur test for PAPI v1 to PAPI v2 upgrade for a horizontal Cromwell configuration.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4801
https://github.com/broadinstitute/cromwell/issues/4801:155,Modifiability,Adapt,Adapt,155,"NOTE: if the PAPI v2 upgrade happens in all environments before horizontaling, this ticket becomes unnecessary and should be closed as a noop. OTHERWISE:. Adapt the existing Centaur test for PAPI v1 to PAPI v2 upgrade for a horizontal Cromwell configuration.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4801
https://github.com/broadinstitute/cromwell/issues/4801:244,Modifiability,config,configuration,244,"NOTE: if the PAPI v2 upgrade happens in all environments before horizontaling, this ticket becomes unnecessary and should be closed as a noop. OTHERWISE:. Adapt the existing Centaur test for PAPI v1 to PAPI v2 upgrade for a horizontal Cromwell configuration.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4801
https://github.com/broadinstitute/cromwell/issues/4801:182,Testability,test,test,182,"NOTE: if the PAPI v2 upgrade happens in all environments before horizontaling, this ticket becomes unnecessary and should be closed as a noop. OTHERWISE:. Adapt the existing Centaur test for PAPI v1 to PAPI v2 upgrade for a horizontal Cromwell configuration.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4801
https://github.com/broadinstitute/cromwell/issues/4802:110,Availability,error,error,110,"Starting Cromwell on Windows and running any CWL workflow causes the server to crash and exit:; ```; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-50]: ; java.lang.UnsatisfiedLinkError: could not locate stub library in jar file. Tried [jni/x86_64-Windows/jffi-1.2.dll, /jni/x86_64-Windows/jffi-1.2.dll]; ```; What's worse, if one starts Cromwell back up it picks up that same workflow from the workflow store and immediately crashes. Notably, one can't even run WDLs at this point because the server does not stay up. The only remedy is to edit rows in the DB. I realize running on Windows is a self-inflicted wound in my case, hence ""low priority"". It does seem likely that not every researcher who wants to use Cromwell works somewhere that can afford to issue Macs. [jython_trace.txt](https://github.com/broadinstitute/cromwell/files/3048143/jython_trace.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4802
https://github.com/broadinstitute/cromwell/issues/4802:236,Testability,stub,stub,236,"Starting Cromwell on Windows and running any CWL workflow causes the server to crash and exit:; ```; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-50]: ; java.lang.UnsatisfiedLinkError: could not locate stub library in jar file. Tried [jni/x86_64-Windows/jffi-1.2.dll, /jni/x86_64-Windows/jffi-1.2.dll]; ```; What's worse, if one starts Cromwell back up it picks up that same workflow from the workflow store and immediately crashes. Notably, one can't even run WDLs at this point because the server does not stay up. The only remedy is to edit rows in the DB. I realize running on Windows is a self-inflicted wound in my case, hence ""low priority"". It does seem likely that not every researcher who wants to use Cromwell works somewhere that can afford to issue Macs. [jython_trace.txt](https://github.com/broadinstitute/cromwell/files/3048143/jython_trace.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4802
https://github.com/broadinstitute/cromwell/issues/4803:22,Deployability,upgrade,upgrade,22,"With the mysql driver upgrade, it's not necessary to either set a MySQL database time zone *or* specify the timezone in the JDBC connection string. We should probably document this in our getting started documentation and/or configuration examples. One (not exhaustive) example of where this type of documentation might make sense: https://cromwell.readthedocs.io/en/develop/tutorials/PersistentServer/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4803
https://github.com/broadinstitute/cromwell/issues/4803:225,Deployability,configurat,configuration,225,"With the mysql driver upgrade, it's not necessary to either set a MySQL database time zone *or* specify the timezone in the JDBC connection string. We should probably document this in our getting started documentation and/or configuration examples. One (not exhaustive) example of where this type of documentation might make sense: https://cromwell.readthedocs.io/en/develop/tutorials/PersistentServer/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4803
https://github.com/broadinstitute/cromwell/issues/4803:225,Modifiability,config,configuration,225,"With the mysql driver upgrade, it's not necessary to either set a MySQL database time zone *or* specify the timezone in the JDBC connection string. We should probably document this in our getting started documentation and/or configuration examples. One (not exhaustive) example of where this type of documentation might make sense: https://cromwell.readthedocs.io/en/develop/tutorials/PersistentServer/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4803
https://github.com/broadinstitute/cromwell/issues/4805:624,Availability,Error,Error,624,"**Backend:** AWS. **Workflow:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-workflow.wdl; **First input json:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-parameters.json; **Second input json is LIKE this one, but refers to a batch of 100 input datasets:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-batchofOne.json. **Config:** ; Installed the cromwell version in PR #4790. . **Error:**; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hitFailures"": [; {; ""dd860da7-bed8-4e70-812c-227f4e6fead8:Panel_BWA_GATK4_Samtools_Var_Annotate_Split.SamToFastq:0"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ],; ""message"": ""[Attempted 1 time(s)] - S3Exception: The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ```. This version of Cromwell does seem to successfully access and copy a cached file from a previous workflow at least on the first task in a shard. This workflow is essentially a batch in which each row of a batch file is passed to a shard and then the tasks run independently on each input dataset and they never gather. However, when the files get larger than the single test data set it seems it can't get to the previous file in order to determine if there's a hit.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4805
https://github.com/broadinstitute/cromwell/issues/4805:576,Deployability,Install,Installed,576,"**Backend:** AWS. **Workflow:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-workflow.wdl; **First input json:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-parameters.json; **Second input json is LIKE this one, but refers to a batch of 100 input datasets:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-batchofOne.json. **Config:** ; Installed the cromwell version in PR #4790. . **Error:**; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hitFailures"": [; {; ""dd860da7-bed8-4e70-812c-227f4e6fead8:Panel_BWA_GATK4_Samtools_Var_Annotate_Split.SamToFastq:0"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ],; ""message"": ""[Attempted 1 time(s)] - S3Exception: The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ```. This version of Cromwell does seem to successfully access and copy a cached file from a previous workflow at least on the first task in a shard. This workflow is essentially a batch in which each row of a batch file is passed to a shard and then the tasks run independently on each input dataset and they never gather. However, when the files get larger than the single test data set it seems it can't get to the previous file in order to determine if there's a hit.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4805
https://github.com/broadinstitute/cromwell/issues/4805:934,Integrability,message,message,934,"**Backend:** AWS. **Workflow:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-workflow.wdl; **First input json:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-parameters.json; **Second input json is LIKE this one, but refers to a batch of 100 input datasets:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-batchofOne.json. **Config:** ; Installed the cromwell version in PR #4790. . **Error:**; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hitFailures"": [; {; ""dd860da7-bed8-4e70-812c-227f4e6fead8:Panel_BWA_GATK4_Samtools_Var_Annotate_Split.SamToFastq:0"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ],; ""message"": ""[Attempted 1 time(s)] - S3Exception: The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ```. This version of Cromwell does seem to successfully access and copy a cached file from a previous workflow at least on the first task in a shard. This workflow is essentially a batch in which each row of a batch file is passed to a shard and then the tasks run independently on each input dataset and they never gather. However, when the files get larger than the single test data set it seems it can't get to the previous file in order to determine if there's a hit.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4805
https://github.com/broadinstitute/cromwell/issues/4805:1115,Integrability,message,message,1115,"**Backend:** AWS. **Workflow:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-workflow.wdl; **First input json:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-parameters.json; **Second input json is LIKE this one, but refers to a batch of 100 input datasets:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-batchofOne.json. **Config:** ; Installed the cromwell version in PR #4790. . **Error:**; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hitFailures"": [; {; ""dd860da7-bed8-4e70-812c-227f4e6fead8:Panel_BWA_GATK4_Samtools_Var_Annotate_Split.SamToFastq:0"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ],; ""message"": ""[Attempted 1 time(s)] - S3Exception: The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ```. This version of Cromwell does seem to successfully access and copy a cached file from a previous workflow at least on the first task in a shard. This workflow is essentially a batch in which each row of a batch file is passed to a shard and then the tasks run independently on each input dataset and they never gather. However, when the files get larger than the single test data set it seems it can't get to the previous file in order to determine if there's a hit.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4805
https://github.com/broadinstitute/cromwell/issues/4805:564,Modifiability,Config,Config,564,"**Backend:** AWS. **Workflow:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-workflow.wdl; **First input json:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-parameters.json; **Second input json is LIKE this one, but refers to a batch of 100 input datasets:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-batchofOne.json. **Config:** ; Installed the cromwell version in PR #4790. . **Error:**; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hitFailures"": [; {; ""dd860da7-bed8-4e70-812c-227f4e6fead8:Panel_BWA_GATK4_Samtools_Var_Annotate_Split.SamToFastq:0"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ],; ""message"": ""[Attempted 1 time(s)] - S3Exception: The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ```. This version of Cromwell does seem to successfully access and copy a cached file from a previous workflow at least on the first task in a shard. This workflow is essentially a batch in which each row of a batch file is passed to a shard and then the tasks run independently on each input dataset and they never gather. However, when the files get larger than the single test data set it seems it can't get to the previous file in order to determine if there's a hit.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4805
https://github.com/broadinstitute/cromwell/issues/4805:710,Performance,Cache,Cache,710,"**Backend:** AWS. **Workflow:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-workflow.wdl; **First input json:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-parameters.json; **Second input json is LIKE this one, but refers to a batch of 100 input datasets:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-batchofOne.json. **Config:** ; Installed the cromwell version in PR #4790. . **Error:**; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hitFailures"": [; {; ""dd860da7-bed8-4e70-812c-227f4e6fead8:Panel_BWA_GATK4_Samtools_Var_Annotate_Split.SamToFastq:0"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ],; ""message"": ""[Attempted 1 time(s)] - S3Exception: The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ```. This version of Cromwell does seem to successfully access and copy a cached file from a previous workflow at least on the first task in a shard. This workflow is essentially a batch in which each row of a batch file is passed to a shard and then the tasks run independently on each input dataset and they never gather. However, when the files get larger than the single test data set it seems it can't get to the previous file in order to determine if there's a hit.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4805
https://github.com/broadinstitute/cromwell/issues/4805:1402,Performance,cache,cached,1402,"**Backend:** AWS. **Workflow:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-workflow.wdl; **First input json:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-parameters.json; **Second input json is LIKE this one, but refers to a batch of 100 input datasets:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-batchofOne.json. **Config:** ; Installed the cromwell version in PR #4790. . **Error:**; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hitFailures"": [; {; ""dd860da7-bed8-4e70-812c-227f4e6fead8:Panel_BWA_GATK4_Samtools_Var_Annotate_Split.SamToFastq:0"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ],; ""message"": ""[Attempted 1 time(s)] - S3Exception: The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ```. This version of Cromwell does seem to successfully access and copy a cached file from a previous workflow at least on the first task in a shard. This workflow is essentially a batch in which each row of a batch file is passed to a shard and then the tasks run independently on each input dataset and they never gather. However, when the files get larger than the single test data set it seems it can't get to the previous file in order to determine if there's a hit.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4805
https://github.com/broadinstitute/cromwell/issues/4805:1384,Security,access,access,1384,"**Backend:** AWS. **Workflow:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-workflow.wdl; **First input json:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-parameters.json; **Second input json is LIKE this one, but refers to a batch of 100 input datasets:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-batchofOne.json. **Config:** ; Installed the cromwell version in PR #4790. . **Error:**; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hitFailures"": [; {; ""dd860da7-bed8-4e70-812c-227f4e6fead8:Panel_BWA_GATK4_Samtools_Var_Annotate_Split.SamToFastq:0"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ],; ""message"": ""[Attempted 1 time(s)] - S3Exception: The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ```. This version of Cromwell does seem to successfully access and copy a cached file from a previous workflow at least on the first task in a shard. This workflow is essentially a batch in which each row of a batch file is passed to a shard and then the tasks run independently on each input dataset and they never gather. However, when the files get larger than the single test data set it seems it can't get to the previous file in order to determine if there's a hit.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4805
https://github.com/broadinstitute/cromwell/issues/4805:1703,Testability,test,test,1703,"**Backend:** AWS. **Workflow:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-workflow.wdl; **First input json:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-parameters.json; **Second input json is LIKE this one, but refers to a batch of 100 input datasets:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-batchofOne.json. **Config:** ; Installed the cromwell version in PR #4790. . **Error:**; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hitFailures"": [; {; ""dd860da7-bed8-4e70-812c-227f4e6fead8:Panel_BWA_GATK4_Samtools_Var_Annotate_Split.SamToFastq:0"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ],; ""message"": ""[Attempted 1 time(s)] - S3Exception: The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ```. This version of Cromwell does seem to successfully access and copy a cached file from a previous workflow at least on the first task in a shard. This workflow is essentially a batch in which each row of a batch file is passed to a shard and then the tasks run independently on each input dataset and they never gather. However, when the files get larger than the single test data set it seems it can't get to the previous file in order to determine if there's a hit.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4805
https://github.com/broadinstitute/cromwell/issues/4806:506,Deployability,configurat,configuration,506,"#### Background. For background information see [this design doc](https://docs.google.com/document/d/1QqXuURg1HlwAymaQkwCL6v9HA5NWsp1pbribvGafNt0/edit?ts=5c6c875c#heading=h.56f418pyjwq8). #### Concept. * Allow Cromwell to spin up PAPIv2 jobs on high security networks ; * At project-creation time, scripts may create a high-security network for that project and record the network name in the project metadata.; * If these fields exist, Cromwell should honor them. #### Proposal. * Use a key in Cromwell's configuration to locate the appropriate project metadata; * For example, perhaps: ; ```; backend {; providers {; PAPIv2 {; config {; backend.providers.PAPIv2.config.vpc {; name: ""terra-network""; subnetwork: ""terra-subnetwork""; }; }; }; }; }; ```; * When about to submit a job to PAPI, see whether the `name` field exists in the configuration.; * If so, check whether the specified label exists in the Google project, eg:; ```; $ gcloud projects describe my-fc-project. createTime: '2017-07-07T17:07:10.345Z'; labels:; terra-network: firecloud; terra-subnetwork: firecloud; lifecycleState: ACTIVE; name: my-fc-project; ```; * If so, deduce a `NETWORK_PATH` by combining the `GOOGLE_PROJECT_ID` (from workflow options) and `NETWORK_NAME` (the label value) as: `projects/GOOGLE_PROJECT_ID/global/networks/NETWORK_NAME`; * Include this in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; }; ```; * If both the `name` and `subnetwork` fields are defined in configuration, and both exist as project labels:; * The `SUBNETWORK` is the raw value of the `subnetwork` label in the google project; * We additionally provide the subnetwork in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; subnetwork: SUBNETWORK; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4806
https://github.com/broadinstitute/cromwell/issues/4806:834,Deployability,configurat,configuration,834,"#### Background. For background information see [this design doc](https://docs.google.com/document/d/1QqXuURg1HlwAymaQkwCL6v9HA5NWsp1pbribvGafNt0/edit?ts=5c6c875c#heading=h.56f418pyjwq8). #### Concept. * Allow Cromwell to spin up PAPIv2 jobs on high security networks ; * At project-creation time, scripts may create a high-security network for that project and record the network name in the project metadata.; * If these fields exist, Cromwell should honor them. #### Proposal. * Use a key in Cromwell's configuration to locate the appropriate project metadata; * For example, perhaps: ; ```; backend {; providers {; PAPIv2 {; config {; backend.providers.PAPIv2.config.vpc {; name: ""terra-network""; subnetwork: ""terra-subnetwork""; }; }; }; }; }; ```; * When about to submit a job to PAPI, see whether the `name` field exists in the configuration.; * If so, check whether the specified label exists in the Google project, eg:; ```; $ gcloud projects describe my-fc-project. createTime: '2017-07-07T17:07:10.345Z'; labels:; terra-network: firecloud; terra-subnetwork: firecloud; lifecycleState: ACTIVE; name: my-fc-project; ```; * If so, deduce a `NETWORK_PATH` by combining the `GOOGLE_PROJECT_ID` (from workflow options) and `NETWORK_NAME` (the label value) as: `projects/GOOGLE_PROJECT_ID/global/networks/NETWORK_NAME`; * Include this in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; }; ```; * If both the `name` and `subnetwork` fields are defined in configuration, and both exist as project labels:; * The `SUBNETWORK` is the raw value of the `subnetwork` label in the google project; * We additionally provide the subnetwork in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; subnetwork: SUBNETWORK; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4806
https://github.com/broadinstitute/cromwell/issues/4806:1365,Deployability,pipeline,pipeline,1365,"#### Background. For background information see [this design doc](https://docs.google.com/document/d/1QqXuURg1HlwAymaQkwCL6v9HA5NWsp1pbribvGafNt0/edit?ts=5c6c875c#heading=h.56f418pyjwq8). #### Concept. * Allow Cromwell to spin up PAPIv2 jobs on high security networks ; * At project-creation time, scripts may create a high-security network for that project and record the network name in the project metadata.; * If these fields exist, Cromwell should honor them. #### Proposal. * Use a key in Cromwell's configuration to locate the appropriate project metadata; * For example, perhaps: ; ```; backend {; providers {; PAPIv2 {; config {; backend.providers.PAPIv2.config.vpc {; name: ""terra-network""; subnetwork: ""terra-subnetwork""; }; }; }; }; }; ```; * When about to submit a job to PAPI, see whether the `name` field exists in the configuration.; * If so, check whether the specified label exists in the Google project, eg:; ```; $ gcloud projects describe my-fc-project. createTime: '2017-07-07T17:07:10.345Z'; labels:; terra-network: firecloud; terra-subnetwork: firecloud; lifecycleState: ACTIVE; name: my-fc-project; ```; * If so, deduce a `NETWORK_PATH` by combining the `GOOGLE_PROJECT_ID` (from workflow options) and `NETWORK_NAME` (the label value) as: `projects/GOOGLE_PROJECT_ID/global/networks/NETWORK_NAME`; * Include this in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; }; ```; * If both the `name` and `subnetwork` fields are defined in configuration, and both exist as project labels:; * The `SUBNETWORK` is the raw value of the `subnetwork` label in the google project; * We additionally provide the subnetwork in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; subnetwork: SUBNETWORK; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4806
https://github.com/broadinstitute/cromwell/issues/4806:1499,Deployability,configurat,configuration,1499,"#### Background. For background information see [this design doc](https://docs.google.com/document/d/1QqXuURg1HlwAymaQkwCL6v9HA5NWsp1pbribvGafNt0/edit?ts=5c6c875c#heading=h.56f418pyjwq8). #### Concept. * Allow Cromwell to spin up PAPIv2 jobs on high security networks ; * At project-creation time, scripts may create a high-security network for that project and record the network name in the project metadata.; * If these fields exist, Cromwell should honor them. #### Proposal. * Use a key in Cromwell's configuration to locate the appropriate project metadata; * For example, perhaps: ; ```; backend {; providers {; PAPIv2 {; config {; backend.providers.PAPIv2.config.vpc {; name: ""terra-network""; subnetwork: ""terra-subnetwork""; }; }; }; }; }; ```; * When about to submit a job to PAPI, see whether the `name` field exists in the configuration.; * If so, check whether the specified label exists in the Google project, eg:; ```; $ gcloud projects describe my-fc-project. createTime: '2017-07-07T17:07:10.345Z'; labels:; terra-network: firecloud; terra-subnetwork: firecloud; lifecycleState: ACTIVE; name: my-fc-project; ```; * If so, deduce a `NETWORK_PATH` by combining the `GOOGLE_PROJECT_ID` (from workflow options) and `NETWORK_NAME` (the label value) as: `projects/GOOGLE_PROJECT_ID/global/networks/NETWORK_NAME`; * Include this in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; }; ```; * If both the `name` and `subnetwork` fields are defined in configuration, and both exist as project labels:; * The `SUBNETWORK` is the raw value of the `subnetwork` label in the google project; * We additionally provide the subnetwork in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; subnetwork: SUBNETWORK; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4806
https://github.com/broadinstitute/cromwell/issues/4806:1702,Deployability,pipeline,pipeline,1702,"#### Background. For background information see [this design doc](https://docs.google.com/document/d/1QqXuURg1HlwAymaQkwCL6v9HA5NWsp1pbribvGafNt0/edit?ts=5c6c875c#heading=h.56f418pyjwq8). #### Concept. * Allow Cromwell to spin up PAPIv2 jobs on high security networks ; * At project-creation time, scripts may create a high-security network for that project and record the network name in the project metadata.; * If these fields exist, Cromwell should honor them. #### Proposal. * Use a key in Cromwell's configuration to locate the appropriate project metadata; * For example, perhaps: ; ```; backend {; providers {; PAPIv2 {; config {; backend.providers.PAPIv2.config.vpc {; name: ""terra-network""; subnetwork: ""terra-subnetwork""; }; }; }; }; }; ```; * When about to submit a job to PAPI, see whether the `name` field exists in the configuration.; * If so, check whether the specified label exists in the Google project, eg:; ```; $ gcloud projects describe my-fc-project. createTime: '2017-07-07T17:07:10.345Z'; labels:; terra-network: firecloud; terra-subnetwork: firecloud; lifecycleState: ACTIVE; name: my-fc-project; ```; * If so, deduce a `NETWORK_PATH` by combining the `GOOGLE_PROJECT_ID` (from workflow options) and `NETWORK_NAME` (the label value) as: `projects/GOOGLE_PROJECT_ID/global/networks/NETWORK_NAME`; * Include this in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; }; ```; * If both the `name` and `subnetwork` fields are defined in configuration, and both exist as project labels:; * The `SUBNETWORK` is the raw value of the `subnetwork` label in the google project; * We additionally provide the subnetwork in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; subnetwork: SUBNETWORK; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4806
https://github.com/broadinstitute/cromwell/issues/4806:506,Modifiability,config,configuration,506,"#### Background. For background information see [this design doc](https://docs.google.com/document/d/1QqXuURg1HlwAymaQkwCL6v9HA5NWsp1pbribvGafNt0/edit?ts=5c6c875c#heading=h.56f418pyjwq8). #### Concept. * Allow Cromwell to spin up PAPIv2 jobs on high security networks ; * At project-creation time, scripts may create a high-security network for that project and record the network name in the project metadata.; * If these fields exist, Cromwell should honor them. #### Proposal. * Use a key in Cromwell's configuration to locate the appropriate project metadata; * For example, perhaps: ; ```; backend {; providers {; PAPIv2 {; config {; backend.providers.PAPIv2.config.vpc {; name: ""terra-network""; subnetwork: ""terra-subnetwork""; }; }; }; }; }; ```; * When about to submit a job to PAPI, see whether the `name` field exists in the configuration.; * If so, check whether the specified label exists in the Google project, eg:; ```; $ gcloud projects describe my-fc-project. createTime: '2017-07-07T17:07:10.345Z'; labels:; terra-network: firecloud; terra-subnetwork: firecloud; lifecycleState: ACTIVE; name: my-fc-project; ```; * If so, deduce a `NETWORK_PATH` by combining the `GOOGLE_PROJECT_ID` (from workflow options) and `NETWORK_NAME` (the label value) as: `projects/GOOGLE_PROJECT_ID/global/networks/NETWORK_NAME`; * Include this in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; }; ```; * If both the `name` and `subnetwork` fields are defined in configuration, and both exist as project labels:; * The `SUBNETWORK` is the raw value of the `subnetwork` label in the google project; * We additionally provide the subnetwork in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; subnetwork: SUBNETWORK; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4806
https://github.com/broadinstitute/cromwell/issues/4806:629,Modifiability,config,config,629,"#### Background. For background information see [this design doc](https://docs.google.com/document/d/1QqXuURg1HlwAymaQkwCL6v9HA5NWsp1pbribvGafNt0/edit?ts=5c6c875c#heading=h.56f418pyjwq8). #### Concept. * Allow Cromwell to spin up PAPIv2 jobs on high security networks ; * At project-creation time, scripts may create a high-security network for that project and record the network name in the project metadata.; * If these fields exist, Cromwell should honor them. #### Proposal. * Use a key in Cromwell's configuration to locate the appropriate project metadata; * For example, perhaps: ; ```; backend {; providers {; PAPIv2 {; config {; backend.providers.PAPIv2.config.vpc {; name: ""terra-network""; subnetwork: ""terra-subnetwork""; }; }; }; }; }; ```; * When about to submit a job to PAPI, see whether the `name` field exists in the configuration.; * If so, check whether the specified label exists in the Google project, eg:; ```; $ gcloud projects describe my-fc-project. createTime: '2017-07-07T17:07:10.345Z'; labels:; terra-network: firecloud; terra-subnetwork: firecloud; lifecycleState: ACTIVE; name: my-fc-project; ```; * If so, deduce a `NETWORK_PATH` by combining the `GOOGLE_PROJECT_ID` (from workflow options) and `NETWORK_NAME` (the label value) as: `projects/GOOGLE_PROJECT_ID/global/networks/NETWORK_NAME`; * Include this in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; }; ```; * If both the `name` and `subnetwork` fields are defined in configuration, and both exist as project labels:; * The `SUBNETWORK` is the raw value of the `subnetwork` label in the google project; * We additionally provide the subnetwork in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; subnetwork: SUBNETWORK; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4806
https://github.com/broadinstitute/cromwell/issues/4806:664,Modifiability,config,config,664,"#### Background. For background information see [this design doc](https://docs.google.com/document/d/1QqXuURg1HlwAymaQkwCL6v9HA5NWsp1pbribvGafNt0/edit?ts=5c6c875c#heading=h.56f418pyjwq8). #### Concept. * Allow Cromwell to spin up PAPIv2 jobs on high security networks ; * At project-creation time, scripts may create a high-security network for that project and record the network name in the project metadata.; * If these fields exist, Cromwell should honor them. #### Proposal. * Use a key in Cromwell's configuration to locate the appropriate project metadata; * For example, perhaps: ; ```; backend {; providers {; PAPIv2 {; config {; backend.providers.PAPIv2.config.vpc {; name: ""terra-network""; subnetwork: ""terra-subnetwork""; }; }; }; }; }; ```; * When about to submit a job to PAPI, see whether the `name` field exists in the configuration.; * If so, check whether the specified label exists in the Google project, eg:; ```; $ gcloud projects describe my-fc-project. createTime: '2017-07-07T17:07:10.345Z'; labels:; terra-network: firecloud; terra-subnetwork: firecloud; lifecycleState: ACTIVE; name: my-fc-project; ```; * If so, deduce a `NETWORK_PATH` by combining the `GOOGLE_PROJECT_ID` (from workflow options) and `NETWORK_NAME` (the label value) as: `projects/GOOGLE_PROJECT_ID/global/networks/NETWORK_NAME`; * Include this in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; }; ```; * If both the `name` and `subnetwork` fields are defined in configuration, and both exist as project labels:; * The `SUBNETWORK` is the raw value of the `subnetwork` label in the google project; * We additionally provide the subnetwork in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; subnetwork: SUBNETWORK; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4806
https://github.com/broadinstitute/cromwell/issues/4806:834,Modifiability,config,configuration,834,"#### Background. For background information see [this design doc](https://docs.google.com/document/d/1QqXuURg1HlwAymaQkwCL6v9HA5NWsp1pbribvGafNt0/edit?ts=5c6c875c#heading=h.56f418pyjwq8). #### Concept. * Allow Cromwell to spin up PAPIv2 jobs on high security networks ; * At project-creation time, scripts may create a high-security network for that project and record the network name in the project metadata.; * If these fields exist, Cromwell should honor them. #### Proposal. * Use a key in Cromwell's configuration to locate the appropriate project metadata; * For example, perhaps: ; ```; backend {; providers {; PAPIv2 {; config {; backend.providers.PAPIv2.config.vpc {; name: ""terra-network""; subnetwork: ""terra-subnetwork""; }; }; }; }; }; ```; * When about to submit a job to PAPI, see whether the `name` field exists in the configuration.; * If so, check whether the specified label exists in the Google project, eg:; ```; $ gcloud projects describe my-fc-project. createTime: '2017-07-07T17:07:10.345Z'; labels:; terra-network: firecloud; terra-subnetwork: firecloud; lifecycleState: ACTIVE; name: my-fc-project; ```; * If so, deduce a `NETWORK_PATH` by combining the `GOOGLE_PROJECT_ID` (from workflow options) and `NETWORK_NAME` (the label value) as: `projects/GOOGLE_PROJECT_ID/global/networks/NETWORK_NAME`; * Include this in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; }; ```; * If both the `name` and `subnetwork` fields are defined in configuration, and both exist as project labels:; * The `SUBNETWORK` is the raw value of the `subnetwork` label in the google project; * We additionally provide the subnetwork in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; subnetwork: SUBNETWORK; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4806
https://github.com/broadinstitute/cromwell/issues/4806:1499,Modifiability,config,configuration,1499,"#### Background. For background information see [this design doc](https://docs.google.com/document/d/1QqXuURg1HlwAymaQkwCL6v9HA5NWsp1pbribvGafNt0/edit?ts=5c6c875c#heading=h.56f418pyjwq8). #### Concept. * Allow Cromwell to spin up PAPIv2 jobs on high security networks ; * At project-creation time, scripts may create a high-security network for that project and record the network name in the project metadata.; * If these fields exist, Cromwell should honor them. #### Proposal. * Use a key in Cromwell's configuration to locate the appropriate project metadata; * For example, perhaps: ; ```; backend {; providers {; PAPIv2 {; config {; backend.providers.PAPIv2.config.vpc {; name: ""terra-network""; subnetwork: ""terra-subnetwork""; }; }; }; }; }; ```; * When about to submit a job to PAPI, see whether the `name` field exists in the configuration.; * If so, check whether the specified label exists in the Google project, eg:; ```; $ gcloud projects describe my-fc-project. createTime: '2017-07-07T17:07:10.345Z'; labels:; terra-network: firecloud; terra-subnetwork: firecloud; lifecycleState: ACTIVE; name: my-fc-project; ```; * If so, deduce a `NETWORK_PATH` by combining the `GOOGLE_PROJECT_ID` (from workflow options) and `NETWORK_NAME` (the label value) as: `projects/GOOGLE_PROJECT_ID/global/networks/NETWORK_NAME`; * Include this in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; }; ```; * If both the `name` and `subnetwork` fields are defined in configuration, and both exist as project labels:; * The `SUBNETWORK` is the raw value of the `subnetwork` label in the google project; * We additionally provide the subnetwork in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; subnetwork: SUBNETWORK; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4806
https://github.com/broadinstitute/cromwell/issues/4806:250,Security,secur,security,250,"#### Background. For background information see [this design doc](https://docs.google.com/document/d/1QqXuURg1HlwAymaQkwCL6v9HA5NWsp1pbribvGafNt0/edit?ts=5c6c875c#heading=h.56f418pyjwq8). #### Concept. * Allow Cromwell to spin up PAPIv2 jobs on high security networks ; * At project-creation time, scripts may create a high-security network for that project and record the network name in the project metadata.; * If these fields exist, Cromwell should honor them. #### Proposal. * Use a key in Cromwell's configuration to locate the appropriate project metadata; * For example, perhaps: ; ```; backend {; providers {; PAPIv2 {; config {; backend.providers.PAPIv2.config.vpc {; name: ""terra-network""; subnetwork: ""terra-subnetwork""; }; }; }; }; }; ```; * When about to submit a job to PAPI, see whether the `name` field exists in the configuration.; * If so, check whether the specified label exists in the Google project, eg:; ```; $ gcloud projects describe my-fc-project. createTime: '2017-07-07T17:07:10.345Z'; labels:; terra-network: firecloud; terra-subnetwork: firecloud; lifecycleState: ACTIVE; name: my-fc-project; ```; * If so, deduce a `NETWORK_PATH` by combining the `GOOGLE_PROJECT_ID` (from workflow options) and `NETWORK_NAME` (the label value) as: `projects/GOOGLE_PROJECT_ID/global/networks/NETWORK_NAME`; * Include this in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; }; ```; * If both the `name` and `subnetwork` fields are defined in configuration, and both exist as project labels:; * The `SUBNETWORK` is the raw value of the `subnetwork` label in the google project; * We additionally provide the subnetwork in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; subnetwork: SUBNETWORK; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4806
https://github.com/broadinstitute/cromwell/issues/4806:324,Security,secur,security,324,"#### Background. For background information see [this design doc](https://docs.google.com/document/d/1QqXuURg1HlwAymaQkwCL6v9HA5NWsp1pbribvGafNt0/edit?ts=5c6c875c#heading=h.56f418pyjwq8). #### Concept. * Allow Cromwell to spin up PAPIv2 jobs on high security networks ; * At project-creation time, scripts may create a high-security network for that project and record the network name in the project metadata.; * If these fields exist, Cromwell should honor them. #### Proposal. * Use a key in Cromwell's configuration to locate the appropriate project metadata; * For example, perhaps: ; ```; backend {; providers {; PAPIv2 {; config {; backend.providers.PAPIv2.config.vpc {; name: ""terra-network""; subnetwork: ""terra-subnetwork""; }; }; }; }; }; ```; * When about to submit a job to PAPI, see whether the `name` field exists in the configuration.; * If so, check whether the specified label exists in the Google project, eg:; ```; $ gcloud projects describe my-fc-project. createTime: '2017-07-07T17:07:10.345Z'; labels:; terra-network: firecloud; terra-subnetwork: firecloud; lifecycleState: ACTIVE; name: my-fc-project; ```; * If so, deduce a `NETWORK_PATH` by combining the `GOOGLE_PROJECT_ID` (from workflow options) and `NETWORK_NAME` (the label value) as: `projects/GOOGLE_PROJECT_ID/global/networks/NETWORK_NAME`; * Include this in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; }; ```; * If both the `name` and `subnetwork` fields are defined in configuration, and both exist as project labels:; * The `SUBNETWORK` is the raw value of the `subnetwork` label in the google project; * We additionally provide the subnetwork in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; subnetwork: SUBNETWORK; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4806
https://github.com/broadinstitute/cromwell/pull/4807:22,Deployability,update,update,22,- [x] Necessitates an update to https://github.com/broadinstitute/firecloud-develop/pull/1598; - Closes #4737,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4807
https://github.com/broadinstitute/cromwell/issues/4809:138,Availability,error,error,138,"When using Pipelines API v2 on Cromwell v38 -- it seems like if localization fails, Cromwell tries to delocalize files and then prints an error message (printed below) that is essentially impossible to debug. AC:; 1. Upon localization failure, Cromwell shouldn't run any other actions.; 2. Upon localization failure, the error message returned should print the *file* that couldn't be localized and the error message it failed with. Along with a link to the log for this job. ```Task depthOfCoverageTest.depthOfCov:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: while running ""/bin/sh -c python -c 'import base64; print(base64.b64decode(\""IyEvYmluL2Jhc2gKCmZvciBpIGluICQoc2VxIDMpOyBkbwogICgKICAgIHJtIC1mICRIT01FLy5jb25maWcvZ2Nsb3VkL2djZSAmJiBnc3V0aWwgIGNwIGdzOi8vZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgL2Nyb213ZWxsX3Jvb3QvZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgPiBnc3V0aWxfb3V0cHV0LnR4dCAyPiYxCiMgUmVjb3JkIHRoZSBleGl0IGNvZGUgb2YgdGhlIGdzdXRpbCBjb21tYW5kIHdpdGhvdXQgcHJvamVjdCBmbGFnClJDX0dTVVRJTD0kPwppZiBbICIkUkNfR1NVVElMIiAhPSAiMCIgXTsgdGhlbgogIHByaW50ZiAnJXMgJXNcbicgIiQoZGF0ZSAtdSAnKyVZLyVtLyVkICVIOiVNOiVTJykiIHJtXCAtZlwgXCRIT01FLy5jb25maWcvZ2Nsb3VkL2djZVwgXCZcJlwgZ3N1dGlsXCBcIGNwXCBnczovL2ZjLWYxMTA3MTk5LTRkMDItNGNhNC1iZWRmLTkwNzA5MmQ1MTMzMi9jZ2FleHQvc3UyYy91a19nY3QvX0VHQVIwMDAwMTI4ODYzNF9FR0FTMDAwMDEwMDEwODRfTDY3NTJfZ2F0azIuYmFpXCAvY3JvbXdlbGxfcm9vdC9mYy1mMTEwNzE5OS00ZDAyLTRjYTQtYmVkZi05MDcwOTJkNTEzMzIvY2dhZXh0L3N1MmMvdWtfZ2N0L19FR0FSMDAwMDEyODg2MzRfRUdBUzAwMDAxMDAxMDg0X0w2NzUyX2dhdGsyLmJhaVwgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CgogICMgQ2hlY2sgaWYgaXQgbWF0Y2hlcyB0aGUgQnVja2V0SXNSZXF1ZXN0ZXJQYXlzRXJyb3JNZXNzYWdlCiAgaWYgZ3JlcCAtcSAiQnVja2V0IGlzIHJlcXVlc3RlciBwYXlzIGJ1Y2tldC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4809
https://github.com/broadinstitute/cromwell/issues/4809:235,Availability,failure,failure,235,"When using Pipelines API v2 on Cromwell v38 -- it seems like if localization fails, Cromwell tries to delocalize files and then prints an error message (printed below) that is essentially impossible to debug. AC:; 1. Upon localization failure, Cromwell shouldn't run any other actions.; 2. Upon localization failure, the error message returned should print the *file* that couldn't be localized and the error message it failed with. Along with a link to the log for this job. ```Task depthOfCoverageTest.depthOfCov:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: while running ""/bin/sh -c python -c 'import base64; print(base64.b64decode(\""IyEvYmluL2Jhc2gKCmZvciBpIGluICQoc2VxIDMpOyBkbwogICgKICAgIHJtIC1mICRIT01FLy5jb25maWcvZ2Nsb3VkL2djZSAmJiBnc3V0aWwgIGNwIGdzOi8vZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgL2Nyb213ZWxsX3Jvb3QvZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgPiBnc3V0aWxfb3V0cHV0LnR4dCAyPiYxCiMgUmVjb3JkIHRoZSBleGl0IGNvZGUgb2YgdGhlIGdzdXRpbCBjb21tYW5kIHdpdGhvdXQgcHJvamVjdCBmbGFnClJDX0dTVVRJTD0kPwppZiBbICIkUkNfR1NVVElMIiAhPSAiMCIgXTsgdGhlbgogIHByaW50ZiAnJXMgJXNcbicgIiQoZGF0ZSAtdSAnKyVZLyVtLyVkICVIOiVNOiVTJykiIHJtXCAtZlwgXCRIT01FLy5jb25maWcvZ2Nsb3VkL2djZVwgXCZcJlwgZ3N1dGlsXCBcIGNwXCBnczovL2ZjLWYxMTA3MTk5LTRkMDItNGNhNC1iZWRmLTkwNzA5MmQ1MTMzMi9jZ2FleHQvc3UyYy91a19nY3QvX0VHQVIwMDAwMTI4ODYzNF9FR0FTMDAwMDEwMDEwODRfTDY3NTJfZ2F0azIuYmFpXCAvY3JvbXdlbGxfcm9vdC9mYy1mMTEwNzE5OS00ZDAyLTRjYTQtYmVkZi05MDcwOTJkNTEzMzIvY2dhZXh0L3N1MmMvdWtfZ2N0L19FR0FSMDAwMDEyODg2MzRfRUdBUzAwMDAxMDAxMDg0X0w2NzUyX2dhdGsyLmJhaVwgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CgogICMgQ2hlY2sgaWYgaXQgbWF0Y2hlcyB0aGUgQnVja2V0SXNSZXF1ZXN0ZXJQYXlzRXJyb3JNZXNzYWdlCiAgaWYgZ3JlcCAtcSAiQnVja2V0IGlzIHJlcXVlc3RlciBwYXlzIGJ1Y2tldC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4809
https://github.com/broadinstitute/cromwell/issues/4809:308,Availability,failure,failure,308,"When using Pipelines API v2 on Cromwell v38 -- it seems like if localization fails, Cromwell tries to delocalize files and then prints an error message (printed below) that is essentially impossible to debug. AC:; 1. Upon localization failure, Cromwell shouldn't run any other actions.; 2. Upon localization failure, the error message returned should print the *file* that couldn't be localized and the error message it failed with. Along with a link to the log for this job. ```Task depthOfCoverageTest.depthOfCov:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: while running ""/bin/sh -c python -c 'import base64; print(base64.b64decode(\""IyEvYmluL2Jhc2gKCmZvciBpIGluICQoc2VxIDMpOyBkbwogICgKICAgIHJtIC1mICRIT01FLy5jb25maWcvZ2Nsb3VkL2djZSAmJiBnc3V0aWwgIGNwIGdzOi8vZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgL2Nyb213ZWxsX3Jvb3QvZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgPiBnc3V0aWxfb3V0cHV0LnR4dCAyPiYxCiMgUmVjb3JkIHRoZSBleGl0IGNvZGUgb2YgdGhlIGdzdXRpbCBjb21tYW5kIHdpdGhvdXQgcHJvamVjdCBmbGFnClJDX0dTVVRJTD0kPwppZiBbICIkUkNfR1NVVElMIiAhPSAiMCIgXTsgdGhlbgogIHByaW50ZiAnJXMgJXNcbicgIiQoZGF0ZSAtdSAnKyVZLyVtLyVkICVIOiVNOiVTJykiIHJtXCAtZlwgXCRIT01FLy5jb25maWcvZ2Nsb3VkL2djZVwgXCZcJlwgZ3N1dGlsXCBcIGNwXCBnczovL2ZjLWYxMTA3MTk5LTRkMDItNGNhNC1iZWRmLTkwNzA5MmQ1MTMzMi9jZ2FleHQvc3UyYy91a19nY3QvX0VHQVIwMDAwMTI4ODYzNF9FR0FTMDAwMDEwMDEwODRfTDY3NTJfZ2F0azIuYmFpXCAvY3JvbXdlbGxfcm9vdC9mYy1mMTEwNzE5OS00ZDAyLTRjYTQtYmVkZi05MDcwOTJkNTEzMzIvY2dhZXh0L3N1MmMvdWtfZ2N0L19FR0FSMDAwMDEyODg2MzRfRUdBUzAwMDAxMDAxMDg0X0w2NzUyX2dhdGsyLmJhaVwgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CgogICMgQ2hlY2sgaWYgaXQgbWF0Y2hlcyB0aGUgQnVja2V0SXNSZXF1ZXN0ZXJQYXlzRXJyb3JNZXNzYWdlCiAgaWYgZ3JlcCAtcSAiQnVja2V0IGlzIHJlcXVlc3RlciBwYXlzIGJ1Y2tldC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4809
https://github.com/broadinstitute/cromwell/issues/4809:321,Availability,error,error,321,"When using Pipelines API v2 on Cromwell v38 -- it seems like if localization fails, Cromwell tries to delocalize files and then prints an error message (printed below) that is essentially impossible to debug. AC:; 1. Upon localization failure, Cromwell shouldn't run any other actions.; 2. Upon localization failure, the error message returned should print the *file* that couldn't be localized and the error message it failed with. Along with a link to the log for this job. ```Task depthOfCoverageTest.depthOfCov:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: while running ""/bin/sh -c python -c 'import base64; print(base64.b64decode(\""IyEvYmluL2Jhc2gKCmZvciBpIGluICQoc2VxIDMpOyBkbwogICgKICAgIHJtIC1mICRIT01FLy5jb25maWcvZ2Nsb3VkL2djZSAmJiBnc3V0aWwgIGNwIGdzOi8vZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgL2Nyb213ZWxsX3Jvb3QvZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgPiBnc3V0aWxfb3V0cHV0LnR4dCAyPiYxCiMgUmVjb3JkIHRoZSBleGl0IGNvZGUgb2YgdGhlIGdzdXRpbCBjb21tYW5kIHdpdGhvdXQgcHJvamVjdCBmbGFnClJDX0dTVVRJTD0kPwppZiBbICIkUkNfR1NVVElMIiAhPSAiMCIgXTsgdGhlbgogIHByaW50ZiAnJXMgJXNcbicgIiQoZGF0ZSAtdSAnKyVZLyVtLyVkICVIOiVNOiVTJykiIHJtXCAtZlwgXCRIT01FLy5jb25maWcvZ2Nsb3VkL2djZVwgXCZcJlwgZ3N1dGlsXCBcIGNwXCBnczovL2ZjLWYxMTA3MTk5LTRkMDItNGNhNC1iZWRmLTkwNzA5MmQ1MTMzMi9jZ2FleHQvc3UyYy91a19nY3QvX0VHQVIwMDAwMTI4ODYzNF9FR0FTMDAwMDEwMDEwODRfTDY3NTJfZ2F0azIuYmFpXCAvY3JvbXdlbGxfcm9vdC9mYy1mMTEwNzE5OS00ZDAyLTRjYTQtYmVkZi05MDcwOTJkNTEzMzIvY2dhZXh0L3N1MmMvdWtfZ2N0L19FR0FSMDAwMDEyODg2MzRfRUdBUzAwMDAxMDAxMDg0X0w2NzUyX2dhdGsyLmJhaVwgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CgogICMgQ2hlY2sgaWYgaXQgbWF0Y2hlcyB0aGUgQnVja2V0SXNSZXF1ZXN0ZXJQYXlzRXJyb3JNZXNzYWdlCiAgaWYgZ3JlcCAtcSAiQnVja2V0IGlzIHJlcXVlc3RlciBwYXlzIGJ1Y2tldC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4809
https://github.com/broadinstitute/cromwell/issues/4809:403,Availability,error,error,403,"When using Pipelines API v2 on Cromwell v38 -- it seems like if localization fails, Cromwell tries to delocalize files and then prints an error message (printed below) that is essentially impossible to debug. AC:; 1. Upon localization failure, Cromwell shouldn't run any other actions.; 2. Upon localization failure, the error message returned should print the *file* that couldn't be localized and the error message it failed with. Along with a link to the log for this job. ```Task depthOfCoverageTest.depthOfCov:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: while running ""/bin/sh -c python -c 'import base64; print(base64.b64decode(\""IyEvYmluL2Jhc2gKCmZvciBpIGluICQoc2VxIDMpOyBkbwogICgKICAgIHJtIC1mICRIT01FLy5jb25maWcvZ2Nsb3VkL2djZSAmJiBnc3V0aWwgIGNwIGdzOi8vZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgL2Nyb213ZWxsX3Jvb3QvZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgPiBnc3V0aWxfb3V0cHV0LnR4dCAyPiYxCiMgUmVjb3JkIHRoZSBleGl0IGNvZGUgb2YgdGhlIGdzdXRpbCBjb21tYW5kIHdpdGhvdXQgcHJvamVjdCBmbGFnClJDX0dTVVRJTD0kPwppZiBbICIkUkNfR1NVVElMIiAhPSAiMCIgXTsgdGhlbgogIHByaW50ZiAnJXMgJXNcbicgIiQoZGF0ZSAtdSAnKyVZLyVtLyVkICVIOiVNOiVTJykiIHJtXCAtZlwgXCRIT01FLy5jb25maWcvZ2Nsb3VkL2djZVwgXCZcJlwgZ3N1dGlsXCBcIGNwXCBnczovL2ZjLWYxMTA3MTk5LTRkMDItNGNhNC1iZWRmLTkwNzA5MmQ1MTMzMi9jZ2FleHQvc3UyYy91a19nY3QvX0VHQVIwMDAwMTI4ODYzNF9FR0FTMDAwMDEwMDEwODRfTDY3NTJfZ2F0azIuYmFpXCAvY3JvbXdlbGxfcm9vdC9mYy1mMTEwNzE5OS00ZDAyLTRjYTQtYmVkZi05MDcwOTJkNTEzMzIvY2dhZXh0L3N1MmMvdWtfZ2N0L19FR0FSMDAwMDEyODg2MzRfRUdBUzAwMDAxMDAxMDg0X0w2NzUyX2dhdGsyLmJhaVwgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CgogICMgQ2hlY2sgaWYgaXQgbWF0Y2hlcyB0aGUgQnVja2V0SXNSZXF1ZXN0ZXJQYXlzRXJyb3JNZXNzYWdlCiAgaWYgZ3JlcCAtcSAiQnVja2V0IGlzIHJlcXVlc3RlciBwYXlzIGJ1Y2tldC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4809
https://github.com/broadinstitute/cromwell/issues/4809:582,Availability,error,error,582,"When using Pipelines API v2 on Cromwell v38 -- it seems like if localization fails, Cromwell tries to delocalize files and then prints an error message (printed below) that is essentially impossible to debug. AC:; 1. Upon localization failure, Cromwell shouldn't run any other actions.; 2. Upon localization failure, the error message returned should print the *file* that couldn't be localized and the error message it failed with. Along with a link to the log for this job. ```Task depthOfCoverageTest.depthOfCov:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: while running ""/bin/sh -c python -c 'import base64; print(base64.b64decode(\""IyEvYmluL2Jhc2gKCmZvciBpIGluICQoc2VxIDMpOyBkbwogICgKICAgIHJtIC1mICRIT01FLy5jb25maWcvZ2Nsb3VkL2djZSAmJiBnc3V0aWwgIGNwIGdzOi8vZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgL2Nyb213ZWxsX3Jvb3QvZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgPiBnc3V0aWxfb3V0cHV0LnR4dCAyPiYxCiMgUmVjb3JkIHRoZSBleGl0IGNvZGUgb2YgdGhlIGdzdXRpbCBjb21tYW5kIHdpdGhvdXQgcHJvamVjdCBmbGFnClJDX0dTVVRJTD0kPwppZiBbICIkUkNfR1NVVElMIiAhPSAiMCIgXTsgdGhlbgogIHByaW50ZiAnJXMgJXNcbicgIiQoZGF0ZSAtdSAnKyVZLyVtLyVkICVIOiVNOiVTJykiIHJtXCAtZlwgXCRIT01FLy5jb25maWcvZ2Nsb3VkL2djZVwgXCZcJlwgZ3N1dGlsXCBcIGNwXCBnczovL2ZjLWYxMTA3MTk5LTRkMDItNGNhNC1iZWRmLTkwNzA5MmQ1MTMzMi9jZ2FleHQvc3UyYy91a19nY3QvX0VHQVIwMDAwMTI4ODYzNF9FR0FTMDAwMDEwMDEwODRfTDY3NTJfZ2F0azIuYmFpXCAvY3JvbXdlbGxfcm9vdC9mYy1mMTEwNzE5OS00ZDAyLTRjYTQtYmVkZi05MDcwOTJkNTEzMzIvY2dhZXh0L3N1MmMvdWtfZ2N0L19FR0FSMDAwMDEyODg2MzRfRUdBUzAwMDAxMDAxMDg0X0w2NzUyX2dhdGsyLmJhaVwgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CgogICMgQ2hlY2sgaWYgaXQgbWF0Y2hlcyB0aGUgQnVja2V0SXNSZXF1ZXN0ZXJQYXlzRXJyb3JNZXNzYWdlCiAgaWYgZ3JlcCAtcSAiQnVja2V0IGlzIHJlcXVlc3RlciBwYXlzIGJ1Y2tldC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4809
https://github.com/broadinstitute/cromwell/issues/4809:11,Deployability,Pipeline,Pipelines,11,"When using Pipelines API v2 on Cromwell v38 -- it seems like if localization fails, Cromwell tries to delocalize files and then prints an error message (printed below) that is essentially impossible to debug. AC:; 1. Upon localization failure, Cromwell shouldn't run any other actions.; 2. Upon localization failure, the error message returned should print the *file* that couldn't be localized and the error message it failed with. Along with a link to the log for this job. ```Task depthOfCoverageTest.depthOfCov:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: while running ""/bin/sh -c python -c 'import base64; print(base64.b64decode(\""IyEvYmluL2Jhc2gKCmZvciBpIGluICQoc2VxIDMpOyBkbwogICgKICAgIHJtIC1mICRIT01FLy5jb25maWcvZ2Nsb3VkL2djZSAmJiBnc3V0aWwgIGNwIGdzOi8vZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgL2Nyb213ZWxsX3Jvb3QvZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgPiBnc3V0aWxfb3V0cHV0LnR4dCAyPiYxCiMgUmVjb3JkIHRoZSBleGl0IGNvZGUgb2YgdGhlIGdzdXRpbCBjb21tYW5kIHdpdGhvdXQgcHJvamVjdCBmbGFnClJDX0dTVVRJTD0kPwppZiBbICIkUkNfR1NVVElMIiAhPSAiMCIgXTsgdGhlbgogIHByaW50ZiAnJXMgJXNcbicgIiQoZGF0ZSAtdSAnKyVZLyVtLyVkICVIOiVNOiVTJykiIHJtXCAtZlwgXCRIT01FLy5jb25maWcvZ2Nsb3VkL2djZVwgXCZcJlwgZ3N1dGlsXCBcIGNwXCBnczovL2ZjLWYxMTA3MTk5LTRkMDItNGNhNC1iZWRmLTkwNzA5MmQ1MTMzMi9jZ2FleHQvc3UyYy91a19nY3QvX0VHQVIwMDAwMTI4ODYzNF9FR0FTMDAwMDEwMDEwODRfTDY3NTJfZ2F0azIuYmFpXCAvY3JvbXdlbGxfcm9vdC9mYy1mMTEwNzE5OS00ZDAyLTRjYTQtYmVkZi05MDcwOTJkNTEzMzIvY2dhZXh0L3N1MmMvdWtfZ2N0L19FR0FSMDAwMDEyODg2MzRfRUdBUzAwMDAxMDAxMDg0X0w2NzUyX2dhdGsyLmJhaVwgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CgogICMgQ2hlY2sgaWYgaXQgbWF0Y2hlcyB0aGUgQnVja2V0SXNSZXF1ZXN0ZXJQYXlzRXJyb3JNZXNzYWdlCiAgaWYgZ3JlcCAtcSAiQnVja2V0IGlzIHJlcXVlc3RlciBwYXlzIGJ1Y2tldC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4809
https://github.com/broadinstitute/cromwell/issues/4809:144,Integrability,message,message,144,"When using Pipelines API v2 on Cromwell v38 -- it seems like if localization fails, Cromwell tries to delocalize files and then prints an error message (printed below) that is essentially impossible to debug. AC:; 1. Upon localization failure, Cromwell shouldn't run any other actions.; 2. Upon localization failure, the error message returned should print the *file* that couldn't be localized and the error message it failed with. Along with a link to the log for this job. ```Task depthOfCoverageTest.depthOfCov:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: while running ""/bin/sh -c python -c 'import base64; print(base64.b64decode(\""IyEvYmluL2Jhc2gKCmZvciBpIGluICQoc2VxIDMpOyBkbwogICgKICAgIHJtIC1mICRIT01FLy5jb25maWcvZ2Nsb3VkL2djZSAmJiBnc3V0aWwgIGNwIGdzOi8vZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgL2Nyb213ZWxsX3Jvb3QvZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgPiBnc3V0aWxfb3V0cHV0LnR4dCAyPiYxCiMgUmVjb3JkIHRoZSBleGl0IGNvZGUgb2YgdGhlIGdzdXRpbCBjb21tYW5kIHdpdGhvdXQgcHJvamVjdCBmbGFnClJDX0dTVVRJTD0kPwppZiBbICIkUkNfR1NVVElMIiAhPSAiMCIgXTsgdGhlbgogIHByaW50ZiAnJXMgJXNcbicgIiQoZGF0ZSAtdSAnKyVZLyVtLyVkICVIOiVNOiVTJykiIHJtXCAtZlwgXCRIT01FLy5jb25maWcvZ2Nsb3VkL2djZVwgXCZcJlwgZ3N1dGlsXCBcIGNwXCBnczovL2ZjLWYxMTA3MTk5LTRkMDItNGNhNC1iZWRmLTkwNzA5MmQ1MTMzMi9jZ2FleHQvc3UyYy91a19nY3QvX0VHQVIwMDAwMTI4ODYzNF9FR0FTMDAwMDEwMDEwODRfTDY3NTJfZ2F0azIuYmFpXCAvY3JvbXdlbGxfcm9vdC9mYy1mMTEwNzE5OS00ZDAyLTRjYTQtYmVkZi05MDcwOTJkNTEzMzIvY2dhZXh0L3N1MmMvdWtfZ2N0L19FR0FSMDAwMDEyODg2MzRfRUdBUzAwMDAxMDAxMDg0X0w2NzUyX2dhdGsyLmJhaVwgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CgogICMgQ2hlY2sgaWYgaXQgbWF0Y2hlcyB0aGUgQnVja2V0SXNSZXF1ZXN0ZXJQYXlzRXJyb3JNZXNzYWdlCiAgaWYgZ3JlcCAtcSAiQnVja2V0IGlzIHJlcXVlc3RlciBwYXlzIGJ1Y2tldC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4809
https://github.com/broadinstitute/cromwell/issues/4809:327,Integrability,message,message,327,"When using Pipelines API v2 on Cromwell v38 -- it seems like if localization fails, Cromwell tries to delocalize files and then prints an error message (printed below) that is essentially impossible to debug. AC:; 1. Upon localization failure, Cromwell shouldn't run any other actions.; 2. Upon localization failure, the error message returned should print the *file* that couldn't be localized and the error message it failed with. Along with a link to the log for this job. ```Task depthOfCoverageTest.depthOfCov:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: while running ""/bin/sh -c python -c 'import base64; print(base64.b64decode(\""IyEvYmluL2Jhc2gKCmZvciBpIGluICQoc2VxIDMpOyBkbwogICgKICAgIHJtIC1mICRIT01FLy5jb25maWcvZ2Nsb3VkL2djZSAmJiBnc3V0aWwgIGNwIGdzOi8vZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgL2Nyb213ZWxsX3Jvb3QvZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgPiBnc3V0aWxfb3V0cHV0LnR4dCAyPiYxCiMgUmVjb3JkIHRoZSBleGl0IGNvZGUgb2YgdGhlIGdzdXRpbCBjb21tYW5kIHdpdGhvdXQgcHJvamVjdCBmbGFnClJDX0dTVVRJTD0kPwppZiBbICIkUkNfR1NVVElMIiAhPSAiMCIgXTsgdGhlbgogIHByaW50ZiAnJXMgJXNcbicgIiQoZGF0ZSAtdSAnKyVZLyVtLyVkICVIOiVNOiVTJykiIHJtXCAtZlwgXCRIT01FLy5jb25maWcvZ2Nsb3VkL2djZVwgXCZcJlwgZ3N1dGlsXCBcIGNwXCBnczovL2ZjLWYxMTA3MTk5LTRkMDItNGNhNC1iZWRmLTkwNzA5MmQ1MTMzMi9jZ2FleHQvc3UyYy91a19nY3QvX0VHQVIwMDAwMTI4ODYzNF9FR0FTMDAwMDEwMDEwODRfTDY3NTJfZ2F0azIuYmFpXCAvY3JvbXdlbGxfcm9vdC9mYy1mMTEwNzE5OS00ZDAyLTRjYTQtYmVkZi05MDcwOTJkNTEzMzIvY2dhZXh0L3N1MmMvdWtfZ2N0L19FR0FSMDAwMDEyODg2MzRfRUdBUzAwMDAxMDAxMDg0X0w2NzUyX2dhdGsyLmJhaVwgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CgogICMgQ2hlY2sgaWYgaXQgbWF0Y2hlcyB0aGUgQnVja2V0SXNSZXF1ZXN0ZXJQYXlzRXJyb3JNZXNzYWdlCiAgaWYgZ3JlcCAtcSAiQnVja2V0IGlzIHJlcXVlc3RlciBwYXlzIGJ1Y2tldC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4809
https://github.com/broadinstitute/cromwell/issues/4809:409,Integrability,message,message,409,"When using Pipelines API v2 on Cromwell v38 -- it seems like if localization fails, Cromwell tries to delocalize files and then prints an error message (printed below) that is essentially impossible to debug. AC:; 1. Upon localization failure, Cromwell shouldn't run any other actions.; 2. Upon localization failure, the error message returned should print the *file* that couldn't be localized and the error message it failed with. Along with a link to the log for this job. ```Task depthOfCoverageTest.depthOfCov:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: while running ""/bin/sh -c python -c 'import base64; print(base64.b64decode(\""IyEvYmluL2Jhc2gKCmZvciBpIGluICQoc2VxIDMpOyBkbwogICgKICAgIHJtIC1mICRIT01FLy5jb25maWcvZ2Nsb3VkL2djZSAmJiBnc3V0aWwgIGNwIGdzOi8vZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgL2Nyb213ZWxsX3Jvb3QvZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgPiBnc3V0aWxfb3V0cHV0LnR4dCAyPiYxCiMgUmVjb3JkIHRoZSBleGl0IGNvZGUgb2YgdGhlIGdzdXRpbCBjb21tYW5kIHdpdGhvdXQgcHJvamVjdCBmbGFnClJDX0dTVVRJTD0kPwppZiBbICIkUkNfR1NVVElMIiAhPSAiMCIgXTsgdGhlbgogIHByaW50ZiAnJXMgJXNcbicgIiQoZGF0ZSAtdSAnKyVZLyVtLyVkICVIOiVNOiVTJykiIHJtXCAtZlwgXCRIT01FLy5jb25maWcvZ2Nsb3VkL2djZVwgXCZcJlwgZ3N1dGlsXCBcIGNwXCBnczovL2ZjLWYxMTA3MTk5LTRkMDItNGNhNC1iZWRmLTkwNzA5MmQ1MTMzMi9jZ2FleHQvc3UyYy91a19nY3QvX0VHQVIwMDAwMTI4ODYzNF9FR0FTMDAwMDEwMDEwODRfTDY3NTJfZ2F0azIuYmFpXCAvY3JvbXdlbGxfcm9vdC9mYy1mMTEwNzE5OS00ZDAyLTRjYTQtYmVkZi05MDcwOTJkNTEzMzIvY2dhZXh0L3N1MmMvdWtfZ2N0L19FR0FSMDAwMDEyODg2MzRfRUdBUzAwMDAxMDAxMDg0X0w2NzUyX2dhdGsyLmJhaVwgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CgogICMgQ2hlY2sgaWYgaXQgbWF0Y2hlcyB0aGUgQnVja2V0SXNSZXF1ZXN0ZXJQYXlzRXJyb3JNZXNzYWdlCiAgaWYgZ3JlcCAtcSAiQnVja2V0IGlzIHJlcXVlc3RlciBwYXlzIGJ1Y2tldC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4809
https://github.com/broadinstitute/cromwell/issues/4809:458,Testability,log,log,458,"When using Pipelines API v2 on Cromwell v38 -- it seems like if localization fails, Cromwell tries to delocalize files and then prints an error message (printed below) that is essentially impossible to debug. AC:; 1. Upon localization failure, Cromwell shouldn't run any other actions.; 2. Upon localization failure, the error message returned should print the *file* that couldn't be localized and the error message it failed with. Along with a link to the log for this job. ```Task depthOfCoverageTest.depthOfCov:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: while running ""/bin/sh -c python -c 'import base64; print(base64.b64decode(\""IyEvYmluL2Jhc2gKCmZvciBpIGluICQoc2VxIDMpOyBkbwogICgKICAgIHJtIC1mICRIT01FLy5jb25maWcvZ2Nsb3VkL2djZSAmJiBnc3V0aWwgIGNwIGdzOi8vZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgL2Nyb213ZWxsX3Jvb3QvZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgPiBnc3V0aWxfb3V0cHV0LnR4dCAyPiYxCiMgUmVjb3JkIHRoZSBleGl0IGNvZGUgb2YgdGhlIGdzdXRpbCBjb21tYW5kIHdpdGhvdXQgcHJvamVjdCBmbGFnClJDX0dTVVRJTD0kPwppZiBbICIkUkNfR1NVVElMIiAhPSAiMCIgXTsgdGhlbgogIHByaW50ZiAnJXMgJXNcbicgIiQoZGF0ZSAtdSAnKyVZLyVtLyVkICVIOiVNOiVTJykiIHJtXCAtZlwgXCRIT01FLy5jb25maWcvZ2Nsb3VkL2djZVwgXCZcJlwgZ3N1dGlsXCBcIGNwXCBnczovL2ZjLWYxMTA3MTk5LTRkMDItNGNhNC1iZWRmLTkwNzA5MmQ1MTMzMi9jZ2FleHQvc3UyYy91a19nY3QvX0VHQVIwMDAwMTI4ODYzNF9FR0FTMDAwMDEwMDEwODRfTDY3NTJfZ2F0azIuYmFpXCAvY3JvbXdlbGxfcm9vdC9mYy1mMTEwNzE5OS00ZDAyLTRjYTQtYmVkZi05MDcwOTJkNTEzMzIvY2dhZXh0L3N1MmMvdWtfZ2N0L19FR0FSMDAwMDEyODg2MzRfRUdBUzAwMDAxMDAxMDg0X0w2NzUyX2dhdGsyLmJhaVwgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CgogICMgQ2hlY2sgaWYgaXQgbWF0Y2hlcyB0aGUgQnVja2V0SXNSZXF1ZXN0ZXJQYXlzRXJyb3JNZXNzYWdlCiAgaWYgZ3JlcCAtcSAiQnVja2V0IGlzIHJlcXVlc3RlciBwYXlzIGJ1Y2tldC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4809
https://github.com/broadinstitute/cromwell/issues/4810:890,Deployability,Configurat,Configuration,890,"The documentation for call caching could use some additional clarity - as a new user, it's fairly confusing and non-trivial to get right. Happy to break this out into multiple issues if necessary, but wanted to 'brain dump' them while it was fresh in my mind:. 1. In the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, the *Docker Lookup* section has detailed documentation on the modes but never mentions the actual config key used to set it, nor the config string values for the different modes.; 2. Hyperlinks from the call caching section to other sections (and possibly elsewhere in the docs) are broken as it would appear there has been a change in the URL format - for example, on that same [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, ; > Call Caching can be enabled in your [Cromwell Configuration](https://cromwell.readthedocs.io/en/stable/cromwell_features/Configuring#call-caching). links to `/en/stable/cromwell_features/Configuring#call-caching`; when the current link path is `/en/stable/Configuring/#call-caching`. This is also true for the MySQL link on the page.; 3. As a user, I would expect links within documentation for a stable release to link to only ""stable"" assets. However, the Runtime Attributes link on the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page links to the develop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:1248,Deployability,release,release,1248," it was fresh in my mind:. 1. In the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, the *Docker Lookup* section has detailed documentation on the modes but never mentions the actual config key used to set it, nor the config string values for the different modes.; 2. Hyperlinks from the call caching section to other sections (and possibly elsewhere in the docs) are broken as it would appear there has been a change in the URL format - for example, on that same [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, ; > Call Caching can be enabled in your [Cromwell Configuration](https://cromwell.readthedocs.io/en/stable/cromwell_features/Configuring#call-caching). links to `/en/stable/cromwell_features/Configuring#call-caching`; when the current link path is `/en/stable/Configuring/#call-caching`. This is also true for the MySQL link on the page.; 3. As a user, I would expect links within documentation for a stable release to link to only ""stable"" assets. However, the Runtime Attributes link on the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page links to the develop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing becau",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:1544,Deployability,Configurat,Configuration,1544,"fferent modes.; 2. Hyperlinks from the call caching section to other sections (and possibly elsewhere in the docs) are broken as it would appear there has been a change in the URL format - for example, on that same [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, ; > Call Caching can be enabled in your [Cromwell Configuration](https://cromwell.readthedocs.io/en/stable/cromwell_features/Configuring#call-caching). links to `/en/stable/cromwell_features/Configuring#call-caching`; when the current link path is `/en/stable/Configuring/#call-caching`. This is also true for the MySQL link on the page.; 3. As a user, I would expect links within documentation for a stable release to link to only ""stable"" assets. However, the Runtime Attributes link on the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page links to the develop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:1737,Deployability,Configurat,Configuration,1737,"here has been a change in the URL format - for example, on that same [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, ; > Call Caching can be enabled in your [Cromwell Configuration](https://cromwell.readthedocs.io/en/stable/cromwell_features/Configuring#call-caching). links to `/en/stable/cromwell_features/Configuring#call-caching`; when the current link path is `/en/stable/Configuring/#call-caching`. This is also true for the MySQL link on the page.; 3. As a user, I would expect links within documentation for a stable release to link to only ""stable"" assets. However, the Runtime Attributes link on the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page links to the develop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:464,Modifiability,config,config,464,"The documentation for call caching could use some additional clarity - as a new user, it's fairly confusing and non-trivial to get right. Happy to break this out into multiple issues if necessary, but wanted to 'brain dump' them while it was fresh in my mind:. 1. In the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, the *Docker Lookup* section has detailed documentation on the modes but never mentions the actual config key used to set it, nor the config string values for the different modes.; 2. Hyperlinks from the call caching section to other sections (and possibly elsewhere in the docs) are broken as it would appear there has been a change in the URL format - for example, on that same [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, ; > Call Caching can be enabled in your [Cromwell Configuration](https://cromwell.readthedocs.io/en/stable/cromwell_features/Configuring#call-caching). links to `/en/stable/cromwell_features/Configuring#call-caching`; when the current link path is `/en/stable/Configuring/#call-caching`. This is also true for the MySQL link on the page.; 3. As a user, I would expect links within documentation for a stable release to link to only ""stable"" assets. However, the Runtime Attributes link on the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page links to the develop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:499,Modifiability,config,config,499,"The documentation for call caching could use some additional clarity - as a new user, it's fairly confusing and non-trivial to get right. Happy to break this out into multiple issues if necessary, but wanted to 'brain dump' them while it was fresh in my mind:. 1. In the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, the *Docker Lookup* section has detailed documentation on the modes but never mentions the actual config key used to set it, nor the config string values for the different modes.; 2. Hyperlinks from the call caching section to other sections (and possibly elsewhere in the docs) are broken as it would appear there has been a change in the URL format - for example, on that same [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, ; > Call Caching can be enabled in your [Cromwell Configuration](https://cromwell.readthedocs.io/en/stable/cromwell_features/Configuring#call-caching). links to `/en/stable/cromwell_features/Configuring#call-caching`; when the current link path is `/en/stable/Configuring/#call-caching`. This is also true for the MySQL link on the page.; 3. As a user, I would expect links within documentation for a stable release to link to only ""stable"" assets. However, the Runtime Attributes link on the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page links to the develop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:890,Modifiability,Config,Configuration,890,"The documentation for call caching could use some additional clarity - as a new user, it's fairly confusing and non-trivial to get right. Happy to break this out into multiple issues if necessary, but wanted to 'brain dump' them while it was fresh in my mind:. 1. In the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, the *Docker Lookup* section has detailed documentation on the modes but never mentions the actual config key used to set it, nor the config string values for the different modes.; 2. Hyperlinks from the call caching section to other sections (and possibly elsewhere in the docs) are broken as it would appear there has been a change in the URL format - for example, on that same [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, ; > Call Caching can be enabled in your [Cromwell Configuration](https://cromwell.readthedocs.io/en/stable/cromwell_features/Configuring#call-caching). links to `/en/stable/cromwell_features/Configuring#call-caching`; when the current link path is `/en/stable/Configuring/#call-caching`. This is also true for the MySQL link on the page.; 3. As a user, I would expect links within documentation for a stable release to link to only ""stable"" assets. However, the Runtime Attributes link on the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page links to the develop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:965,Modifiability,Config,Configuring,965,"The documentation for call caching could use some additional clarity - as a new user, it's fairly confusing and non-trivial to get right. Happy to break this out into multiple issues if necessary, but wanted to 'brain dump' them while it was fresh in my mind:. 1. In the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, the *Docker Lookup* section has detailed documentation on the modes but never mentions the actual config key used to set it, nor the config string values for the different modes.; 2. Hyperlinks from the call caching section to other sections (and possibly elsewhere in the docs) are broken as it would appear there has been a change in the URL format - for example, on that same [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, ; > Call Caching can be enabled in your [Cromwell Configuration](https://cromwell.readthedocs.io/en/stable/cromwell_features/Configuring#call-caching). links to `/en/stable/cromwell_features/Configuring#call-caching`; when the current link path is `/en/stable/Configuring/#call-caching`. This is also true for the MySQL link on the page.; 3. As a user, I would expect links within documentation for a stable release to link to only ""stable"" assets. However, the Runtime Attributes link on the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page links to the develop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:1031,Modifiability,Config,Configuring,1031,"l clarity - as a new user, it's fairly confusing and non-trivial to get right. Happy to break this out into multiple issues if necessary, but wanted to 'brain dump' them while it was fresh in my mind:. 1. In the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, the *Docker Lookup* section has detailed documentation on the modes but never mentions the actual config key used to set it, nor the config string values for the different modes.; 2. Hyperlinks from the call caching section to other sections (and possibly elsewhere in the docs) are broken as it would appear there has been a change in the URL format - for example, on that same [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, ; > Call Caching can be enabled in your [Cromwell Configuration](https://cromwell.readthedocs.io/en/stable/cromwell_features/Configuring#call-caching). links to `/en/stable/cromwell_features/Configuring#call-caching`; when the current link path is `/en/stable/Configuring/#call-caching`. This is also true for the MySQL link on the page.; 3. As a user, I would expect links within documentation for a stable release to link to only ""stable"" assets. However, the Runtime Attributes link on the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page links to the develop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:1100,Modifiability,Config,Configuring,1100,"l clarity - as a new user, it's fairly confusing and non-trivial to get right. Happy to break this out into multiple issues if necessary, but wanted to 'brain dump' them while it was fresh in my mind:. 1. In the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, the *Docker Lookup* section has detailed documentation on the modes but never mentions the actual config key used to set it, nor the config string values for the different modes.; 2. Hyperlinks from the call caching section to other sections (and possibly elsewhere in the docs) are broken as it would appear there has been a change in the URL format - for example, on that same [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, ; > Call Caching can be enabled in your [Cromwell Configuration](https://cromwell.readthedocs.io/en/stable/cromwell_features/Configuring#call-caching). links to `/en/stable/cromwell_features/Configuring#call-caching`; when the current link path is `/en/stable/Configuring/#call-caching`. This is also true for the MySQL link on the page.; 3. As a user, I would expect links within documentation for a stable release to link to only ""stable"" assets. However, the Runtime Attributes link on the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page links to the develop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:1544,Modifiability,Config,Configuration,1544,"fferent modes.; 2. Hyperlinks from the call caching section to other sections (and possibly elsewhere in the docs) are broken as it would appear there has been a change in the URL format - for example, on that same [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, ; > Call Caching can be enabled in your [Cromwell Configuration](https://cromwell.readthedocs.io/en/stable/cromwell_features/Configuring#call-caching). links to `/en/stable/cromwell_features/Configuring#call-caching`; when the current link path is `/en/stable/Configuring/#call-caching`. This is also true for the MySQL link on the page.; 3. As a user, I would expect links within documentation for a stable release to link to only ""stable"" assets. However, the Runtime Attributes link on the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page links to the develop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:1601,Modifiability,Config,Configuring,1601,"here has been a change in the URL format - for example, on that same [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, ; > Call Caching can be enabled in your [Cromwell Configuration](https://cromwell.readthedocs.io/en/stable/cromwell_features/Configuring#call-caching). links to `/en/stable/cromwell_features/Configuring#call-caching`; when the current link path is `/en/stable/Configuring/#call-caching`. This is also true for the MySQL link on the page.; 3. As a user, I would expect links within documentation for a stable release to link to only ""stable"" assets. However, the Runtime Attributes link on the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page links to the develop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:1737,Modifiability,Config,Configuration,1737,"here has been a change in the URL format - for example, on that same [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, ; > Call Caching can be enabled in your [Cromwell Configuration](https://cromwell.readthedocs.io/en/stable/cromwell_features/Configuring#call-caching). links to `/en/stable/cromwell_features/Configuring#call-caching`; when the current link path is `/en/stable/Configuring/#call-caching`. This is also true for the MySQL link on the page.; 3. As a user, I would expect links within documentation for a stable release to link to only ""stable"" assets. However, the Runtime Attributes link on the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page links to the develop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:2261,Modifiability,config,configured,2261,"release to link to only ""stable"" assets. However, the Runtime Attributes link on the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page links to the develop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem (e.g., TES). This is not the case, and they are in fact tied to the actor factory implementation and the only actor factory with support is `ConfigBackendLifecycleActorFactory` (as far as I could tell).; - The documentation does make mention of mention:; > When running a job on the **Config (Shared Filesystem) backend**, Cromwell provides some additional options in the backend's config section. However neither a Config nor a Shared Filesystem backend are mentionend in the documentation, although SFS is found under [Filesystems](https://cromwell.readthedocs.io/en/stab",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:2318,Modifiability,config,config,2318,"link on the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page links to the develop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem (e.g., TES). This is not the case, and they are in fact tied to the actor factory implementation and the only actor factory with support is `ConfigBackendLifecycleActorFactory` (as far as I could tell).; - The documentation does make mention of mention:; > When running a job on the **Config (Shared Filesystem) backend**, Cromwell provides some additional options in the backend's config section. However neither a Config nor a Shared Filesystem backend are mentionend in the documentation, although SFS is found under [Filesystems](https://cromwell.readthedocs.io/en/stable/filesystems/Filesystems/). Therefore, I think the docs need additional",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:2325,Modifiability,Config,ConfigBackendLifecycleActorFactory,2325,"velop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem (e.g., TES). This is not the case, and they are in fact tied to the actor factory implementation and the only actor factory with support is `ConfigBackendLifecycleActorFactory` (as far as I could tell).; - The documentation does make mention of mention:; > When running a job on the **Config (Shared Filesystem) backend**, Cromwell provides some additional options in the backend's config section. However neither a Config nor a Shared Filesystem backend are mentionend in the documentation, although SFS is found under [Filesystems](https://cromwell.readthedocs.io/en/stable/filesystems/Filesystems/). Therefore, I think the docs need additional clarity on config concepts like provider and actor-factory vs what we are referring to at a high-level as a ""backend"" (",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:2534,Modifiability,config,configured,2534,"velop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem (e.g., TES). This is not the case, and they are in fact tied to the actor factory implementation and the only actor factory with support is `ConfigBackendLifecycleActorFactory` (as far as I could tell).; - The documentation does make mention of mention:; > When running a job on the **Config (Shared Filesystem) backend**, Cromwell provides some additional options in the backend's config section. However neither a Config nor a Shared Filesystem backend are mentionend in the documentation, although SFS is found under [Filesystems](https://cromwell.readthedocs.io/en/stable/filesystems/Filesystems/). Therefore, I think the docs need additional clarity on config concepts like provider and actor-factory vs what we are referring to at a high-level as a ""backend"" (",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:2552,Modifiability,config,config,2552,"velop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem (e.g., TES). This is not the case, and they are in fact tied to the actor factory implementation and the only actor factory with support is `ConfigBackendLifecycleActorFactory` (as far as I could tell).; - The documentation does make mention of mention:; > When running a job on the **Config (Shared Filesystem) backend**, Cromwell provides some additional options in the backend's config section. However neither a Config nor a Shared Filesystem backend are mentionend in the documentation, although SFS is found under [Filesystems](https://cromwell.readthedocs.io/en/stable/filesystems/Filesystems/). Therefore, I think the docs need additional clarity on config concepts like provider and actor-factory vs what we are referring to at a high-level as a ""backend"" (",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:2644,Modifiability,config,configured,2644," the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem (e.g., TES). This is not the case, and they are in fact tied to the actor factory implementation and the only actor factory with support is `ConfigBackendLifecycleActorFactory` (as far as I could tell).; - The documentation does make mention of mention:; > When running a job on the **Config (Shared Filesystem) backend**, Cromwell provides some additional options in the backend's config section. However neither a Config nor a Shared Filesystem backend are mentionend in the documentation, although SFS is found under [Filesystems](https://cromwell.readthedocs.io/en/stable/filesystems/Filesystems/). Therefore, I think the docs need additional clarity on config concepts like provider and actor-factory vs what we are referring to at a high-level as a ""backend"" (which is in fact just the `ConfigBackendLifecycleActorFactory` implementation with some config specific to that ""backend""). As well, I would suggest an implementation of the call caching",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:2817,Modifiability,Config,ConfigBackendLifecycleActorFactory,2817,"titute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem (e.g., TES). This is not the case, and they are in fact tied to the actor factory implementation and the only actor factory with support is `ConfigBackendLifecycleActorFactory` (as far as I could tell).; - The documentation does make mention of mention:; > When running a job on the **Config (Shared Filesystem) backend**, Cromwell provides some additional options in the backend's config section. However neither a Config nor a Shared Filesystem backend are mentionend in the documentation, although SFS is found under [Filesystems](https://cromwell.readthedocs.io/en/stable/filesystems/Filesystems/). Therefore, I think the docs need additional clarity on config concepts like provider and actor-factory vs what we are referring to at a high-level as a ""backend"" (which is in fact just the `ConfigBackendLifecycleActorFactory` implementation with some config specific to that ""backend""). As well, I would suggest an implementation of the call caching options should be made in the StandardLifecycleActorFactory so that all backends inherit the behavior, or else those keys should be moved to the backend-s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:2961,Modifiability,Config,Config,2961,"along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem (e.g., TES). This is not the case, and they are in fact tied to the actor factory implementation and the only actor factory with support is `ConfigBackendLifecycleActorFactory` (as far as I could tell).; - The documentation does make mention of mention:; > When running a job on the **Config (Shared Filesystem) backend**, Cromwell provides some additional options in the backend's config section. However neither a Config nor a Shared Filesystem backend are mentionend in the documentation, although SFS is found under [Filesystems](https://cromwell.readthedocs.io/en/stable/filesystems/Filesystems/). Therefore, I think the docs need additional clarity on config concepts like provider and actor-factory vs what we are referring to at a high-level as a ""backend"" (which is in fact just the `ConfigBackendLifecycleActorFactory` implementation with some config specific to that ""backend""). As well, I would suggest an implementation of the call caching options should be made in the StandardLifecycleActorFactory so that all backends inherit the behavior, or else those keys should be moved to the backend-specific config stanzas and not the local filesystem's.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:3058,Modifiability,config,config,3058,"along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem (e.g., TES). This is not the case, and they are in fact tied to the actor factory implementation and the only actor factory with support is `ConfigBackendLifecycleActorFactory` (as far as I could tell).; - The documentation does make mention of mention:; > When running a job on the **Config (Shared Filesystem) backend**, Cromwell provides some additional options in the backend's config section. However neither a Config nor a Shared Filesystem backend are mentionend in the documentation, although SFS is found under [Filesystems](https://cromwell.readthedocs.io/en/stable/filesystems/Filesystems/). Therefore, I think the docs need additional clarity on config concepts like provider and actor-factory vs what we are referring to at a high-level as a ""backend"" (which is in fact just the `ConfigBackendLifecycleActorFactory` implementation with some config specific to that ""backend""). As well, I would suggest an implementation of the call caching options should be made in the StandardLifecycleActorFactory so that all backends inherit the behavior, or else those keys should be moved to the backend-specific config stanzas and not the local filesystem's.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:3092,Modifiability,Config,Config,3092,"along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem (e.g., TES). This is not the case, and they are in fact tied to the actor factory implementation and the only actor factory with support is `ConfigBackendLifecycleActorFactory` (as far as I could tell).; - The documentation does make mention of mention:; > When running a job on the **Config (Shared Filesystem) backend**, Cromwell provides some additional options in the backend's config section. However neither a Config nor a Shared Filesystem backend are mentionend in the documentation, although SFS is found under [Filesystems](https://cromwell.readthedocs.io/en/stable/filesystems/Filesystems/). Therefore, I think the docs need additional clarity on config concepts like provider and actor-factory vs what we are referring to at a high-level as a ""backend"" (which is in fact just the `ConfigBackendLifecycleActorFactory` implementation with some config specific to that ""backend""). As well, I would suggest an implementation of the call caching options should be made in the StandardLifecycleActorFactory so that all backends inherit the behavior, or else those keys should be moved to the backend-specific config stanzas and not the local filesystem's.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:3334,Modifiability,config,config,3334,"along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem (e.g., TES). This is not the case, and they are in fact tied to the actor factory implementation and the only actor factory with support is `ConfigBackendLifecycleActorFactory` (as far as I could tell).; - The documentation does make mention of mention:; > When running a job on the **Config (Shared Filesystem) backend**, Cromwell provides some additional options in the backend's config section. However neither a Config nor a Shared Filesystem backend are mentionend in the documentation, although SFS is found under [Filesystems](https://cromwell.readthedocs.io/en/stable/filesystems/Filesystems/). Therefore, I think the docs need additional clarity on config concepts like provider and actor-factory vs what we are referring to at a high-level as a ""backend"" (which is in fact just the `ConfigBackendLifecycleActorFactory` implementation with some config specific to that ""backend""). As well, I would suggest an implementation of the call caching options should be made in the StandardLifecycleActorFactory so that all backends inherit the behavior, or else those keys should be moved to the backend-specific config stanzas and not the local filesystem's.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:3469,Modifiability,Config,ConfigBackendLifecycleActorFactory,3469,"along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem (e.g., TES). This is not the case, and they are in fact tied to the actor factory implementation and the only actor factory with support is `ConfigBackendLifecycleActorFactory` (as far as I could tell).; - The documentation does make mention of mention:; > When running a job on the **Config (Shared Filesystem) backend**, Cromwell provides some additional options in the backend's config section. However neither a Config nor a Shared Filesystem backend are mentionend in the documentation, although SFS is found under [Filesystems](https://cromwell.readthedocs.io/en/stable/filesystems/Filesystems/). Therefore, I think the docs need additional clarity on config concepts like provider and actor-factory vs what we are referring to at a high-level as a ""backend"" (which is in fact just the `ConfigBackendLifecycleActorFactory` implementation with some config specific to that ""backend""). As well, I would suggest an implementation of the call caching options should be made in the StandardLifecycleActorFactory so that all backends inherit the behavior, or else those keys should be moved to the backend-specific config stanzas and not the local filesystem's.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:3530,Modifiability,config,config,3530,"along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem (e.g., TES). This is not the case, and they are in fact tied to the actor factory implementation and the only actor factory with support is `ConfigBackendLifecycleActorFactory` (as far as I could tell).; - The documentation does make mention of mention:; > When running a job on the **Config (Shared Filesystem) backend**, Cromwell provides some additional options in the backend's config section. However neither a Config nor a Shared Filesystem backend are mentionend in the documentation, although SFS is found under [Filesystems](https://cromwell.readthedocs.io/en/stable/filesystems/Filesystems/). Therefore, I think the docs need additional clarity on config concepts like provider and actor-factory vs what we are referring to at a high-level as a ""backend"" (which is in fact just the `ConfigBackendLifecycleActorFactory` implementation with some config specific to that ""backend""). As well, I would suggest an implementation of the call caching options should be made in the StandardLifecycleActorFactory so that all backends inherit the behavior, or else those keys should be moved to the backend-specific config stanzas and not the local filesystem's.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:3710,Modifiability,inherit,inherit,3710,"along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem (e.g., TES). This is not the case, and they are in fact tied to the actor factory implementation and the only actor factory with support is `ConfigBackendLifecycleActorFactory` (as far as I could tell).; - The documentation does make mention of mention:; > When running a job on the **Config (Shared Filesystem) backend**, Cromwell provides some additional options in the backend's config section. However neither a Config nor a Shared Filesystem backend are mentionend in the documentation, although SFS is found under [Filesystems](https://cromwell.readthedocs.io/en/stable/filesystems/Filesystems/). Therefore, I think the docs need additional clarity on config concepts like provider and actor-factory vs what we are referring to at a high-level as a ""backend"" (which is in fact just the `ConfigBackendLifecycleActorFactory` implementation with some config specific to that ""backend""). As well, I would suggest an implementation of the call caching options should be made in the StandardLifecycleActorFactory so that all backends inherit the behavior, or else those keys should be moved to the backend-specific config stanzas and not the local filesystem's.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:3791,Modifiability,config,config,3791,"along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem (e.g., TES). This is not the case, and they are in fact tied to the actor factory implementation and the only actor factory with support is `ConfigBackendLifecycleActorFactory` (as far as I could tell).; - The documentation does make mention of mention:; > When running a job on the **Config (Shared Filesystem) backend**, Cromwell provides some additional options in the backend's config section. However neither a Config nor a Shared Filesystem backend are mentionend in the documentation, although SFS is found under [Filesystems](https://cromwell.readthedocs.io/en/stable/filesystems/Filesystems/). Therefore, I think the docs need additional clarity on config concepts like provider and actor-factory vs what we are referring to at a high-level as a ""backend"" (which is in fact just the `ConfigBackendLifecycleActorFactory` implementation with some config specific to that ""backend""). As well, I would suggest an implementation of the call caching options should be made in the StandardLifecycleActorFactory so that all backends inherit the behavior, or else those keys should be moved to the backend-specific config stanzas and not the local filesystem's.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:2029,Performance,bottleneck,bottleneck,2029,"le/Configuring/#call-caching`. This is also true for the MySQL link on the page.; 3. As a user, I would expect links within documentation for a stable release to link to only ""stable"" assets. However, the Runtime Attributes link on the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page links to the develop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem (e.g., TES). This is not the case, and they are in fact tied to the actor factory implementation and the only actor factory with support is `ConfigBackendLifecycleActorFactory` (as far as I could tell).; - The documentation does make mention of mention:; > When running a job on the **Config (Shared Filesystem) backend**, Cromwell provides some additional options in the backend's config section. However neither a Config",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:2053,Security,hash,hashing,2053,"le/Configuring/#call-caching`. This is also true for the MySQL link on the page.; 3. As a user, I would expect links within documentation for a stable release to link to only ""stable"" assets. However, the Runtime Attributes link on the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page links to the develop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem (e.g., TES). This is not the case, and they are in fact tied to the actor factory implementation and the only actor factory with support is `ConfigBackendLifecycleActorFactory` (as far as I could tell).; - The documentation does make mention of mention:; > When running a job on the **Config (Shared Filesystem) backend**, Cromwell provides some additional options in the backend's config section. However neither a Config",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:2181,Security,hash,hashes,2181,"le/Configuring/#call-caching`. This is also true for the MySQL link on the page.; 3. As a user, I would expect links within documentation for a stable release to link to only ""stable"" assets. However, the Runtime Attributes link on the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page links to the develop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem (e.g., TES). This is not the case, and they are in fact tied to the actor factory implementation and the only actor factory with support is `ConfigBackendLifecycleActorFactory` (as far as I could tell).; - The documentation does make mention of mention:; > When running a job on the **Config (Shared Filesystem) backend**, Cromwell provides some additional options in the backend's config section. However neither a Config",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:2431,Security,hash,hashing-strategy,2431,"velop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem (e.g., TES). This is not the case, and they are in fact tied to the actor factory implementation and the only actor factory with support is `ConfigBackendLifecycleActorFactory` (as far as I could tell).; - The documentation does make mention of mention:; > When running a job on the **Config (Shared Filesystem) backend**, Cromwell provides some additional options in the backend's config section. However neither a Config nor a Shared Filesystem backend are mentionend in the documentation, although SFS is found under [Filesystems](https://cromwell.readthedocs.io/en/stable/filesystems/Filesystems/). Therefore, I think the docs need additional clarity on config concepts like provider and actor-factory vs what we are referring to at a high-level as a ""backend"" (",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/issues/4810:2416,Usability,clear,clear,2416,"velop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem (e.g., TES). This is not the case, and they are in fact tied to the actor factory implementation and the only actor factory with support is `ConfigBackendLifecycleActorFactory` (as far as I could tell).; - The documentation does make mention of mention:; > When running a job on the **Config (Shared Filesystem) backend**, Cromwell provides some additional options in the backend's config section. However neither a Config nor a Shared Filesystem backend are mentionend in the documentation, although SFS is found under [Filesystems](https://cromwell.readthedocs.io/en/stable/filesystems/Filesystems/). Therefore, I think the docs need additional clarity on config concepts like provider and actor-factory vs what we are referring to at a high-level as a ""backend"" (",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810
https://github.com/broadinstitute/cromwell/pull/4812:6,Testability,log,logic,6,~This logic only runs on PR builds so I needed to make a PR but maybe don't look at it quite yet.~ Review away!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4812
https://github.com/broadinstitute/cromwell/issues/4813:152,Availability,failure,failures,152,centaur tests blocked by this (note - some seem to be using the behavior foro ther purposes and might still fail):. ```; continue_on_return_code; exit; failures.terminal_status; default_runtime_attributes; globbingBehavior; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4813
https://github.com/broadinstitute/cromwell/issues/4813:8,Testability,test,tests,8,centaur tests blocked by this (note - some seem to be using the behavior foro ther purposes and might still fail):. ```; continue_on_return_code; exit; failures.terminal_status; default_runtime_attributes; globbingBehavior; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4813
https://github.com/broadinstitute/cromwell/issues/4814:709,Deployability,configurat,configuration,709,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4814
https://github.com/broadinstitute/cromwell/issues/4814:709,Modifiability,config,configuration,709,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4814
https://github.com/broadinstitute/cromwell/issues/4814:754,Security,PASSWORD,PASSWORDS,754,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4814
https://github.com/broadinstitute/cromwell/issues/4814:47,Usability,feedback,feedback,47,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4814
https://github.com/broadinstitute/cromwell/pull/4815:594,Usability,feedback,feedback,594,Fixes #1641 . I have added a ~~flatten_workflow_outputs~~`use_relative_output_paths` option to the workflow options json. This will remove all the cromwell subdirectories. (Of which there can be a lot if there are sub workflows). And copies paths as they are relative to the execution directory. This may lead to file collisions. So I also built in a file collision check. . @kyleabeauchamp @antonkulaga @oneillkza @dinvlad @ktibbett is this what you had in mind?. To the cromwell developers: I solemnly swear I will maintain this feature once it gets implemented. I am looking forward to your feedback!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4815
https://github.com/broadinstitute/cromwell/issues/4816:511,Availability,Fault,FaultHandling,511,"Centaur testing appears to fail when too many tests run concurrently due to:; ```; java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); [...]; Caused by: software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: Batch, Status Code: 429, Request ID: 932e695f-5a4b-11e9-abf3-d5638efd51d6); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); [...]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4816
https://github.com/broadinstitute/cromwell/issues/4816:765,Deployability,pipeline,pipeline,765,"Centaur testing appears to fail when too many tests run concurrently due to:; ```; java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); [...]; Caused by: software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: Batch, Status Code: 429, Request ID: 932e695f-5a4b-11e9-abf3-d5638efd51d6); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); [...]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4816
https://github.com/broadinstitute/cromwell/issues/4816:898,Deployability,pipeline,pipeline,898,"Centaur testing appears to fail when too many tests run concurrently due to:; ```; java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); [...]; Caused by: software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: Batch, Status Code: 429, Request ID: 932e695f-5a4b-11e9-abf3-d5638efd51d6); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); [...]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4816
https://github.com/broadinstitute/cromwell/issues/4816:56,Performance,concurren,concurrently,56,"Centaur testing appears to fail when too many tests run concurrently due to:; ```; java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); [...]; Caused by: software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: Batch, Status Code: 429, Request ID: 932e695f-5a4b-11e9-abf3-d5638efd51d6); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); [...]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4816
https://github.com/broadinstitute/cromwell/issues/4816:8,Testability,test,testing,8,"Centaur testing appears to fail when too many tests run concurrently due to:; ```; java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); [...]; Caused by: software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: Batch, Status Code: 429, Request ID: 932e695f-5a4b-11e9-abf3-d5638efd51d6); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); [...]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4816
https://github.com/broadinstitute/cromwell/issues/4816:46,Testability,test,tests,46,"Centaur testing appears to fail when too many tests run concurrently due to:; ```; java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); [...]; Caused by: software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: Batch, Status Code: 429, Request ID: 932e695f-5a4b-11e9-abf3-d5638efd51d6); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); [...]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4816
https://github.com/broadinstitute/cromwell/pull/4817:206,Availability,resilien,resilience,206,"~~Might not make much difference until *all* our PRs are rebased to include this throttle~~. Actually this PR is more about allowing the AWS backend to hook into the existing poll retry logic to allow more resilience in the face of ""429 / Too Many Request"" exceptions during status polling.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4817
https://github.com/broadinstitute/cromwell/pull/4817:81,Performance,throttle,throttle,81,"~~Might not make much difference until *all* our PRs are rebased to include this throttle~~. Actually this PR is more about allowing the AWS backend to hook into the existing poll retry logic to allow more resilience in the face of ""429 / Too Many Request"" exceptions during status polling.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4817
https://github.com/broadinstitute/cromwell/pull/4817:186,Testability,log,logic,186,"~~Might not make much difference until *all* our PRs are rebased to include this throttle~~. Actually this PR is more about allowing the AWS backend to hook into the existing poll retry logic to allow more resilience in the face of ""429 / Too Many Request"" exceptions during status polling.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4817
https://github.com/broadinstitute/cromwell/issues/4818:217,Testability,test,tests,217,"It seems that with some version change, Cromwell started deforming certain dates [see below]. This is breaking Tableau's tool reporting. It would be great to investigate when and how this change happened, and why our tests didn't catch it. Need to confirm which version of Cromwell is seeing this change.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4818
https://github.com/broadinstitute/cromwell/pull/4819:325,Testability,test,test,325,"This change is necessary, but not sufficient, for us to pass conformance #135, i.e. packed CWL. We do not yet run #135 because it was added after our pinned commit. Once the other other necessary change, https://github.com/common-workflow-language/common-workflow-language/pull/849, lands I will make a PR to bump the pinned test cases. I should disclose that I am aware of [an old comment](https://github.com/broadinstitute/cromwell/pull/3468#discussion_r178306868) explaining why the conditional is necessary, but conformance passes on the branch and as we say the proof of the pudding is in the eating.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4819
https://github.com/broadinstitute/cromwell/issues/4820:674,Availability,echo,echo,674,"Ran a workflow (see below) with nested scatters on Cromwell v38. The metadata for the calls seems a bit odd... It would be helpful to find out:; 1. Why the name of the call is `ScatterAt9_13`; 2. Why aren't the shards numbered as they are for a single scatter?. <img width=""570"" alt=""Screen Shot 2019-04-10 at 2 44 12 PM"" src=""https://user-images.githubusercontent.com/14941133/55905173-75746200-5b9f-11e9-8388-4ca4d4e887b5.png"">. ```; version 1.0. workflow nested_scatter {. Array[Int] indices = [1,2,3]; Int y = 55. scatter(a in indices) {; scatter(b in indices) {; Int x = a + b; call add { ; 	input: foo = x; }; }; }; }. task add {; 	; 	input { Int foo }; 	. 	command { echo foo }. 	runtime {; 		docker: ""ubuntu:latest"". 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4820
https://github.com/broadinstitute/cromwell/issues/4826:17,Modifiability,config,configured,17,"When Cromwell is configured with `workflow-log-temporary: false` and workflow does not specify `final_workflow_log_dir`, Cromwell server does not close the log file. ; When `final_workflow_log_dir` is specified the original log file is correctly closed and copied. ; In my case, this prevents blobfuse (waits for the file to be closed) from transferring the log files to Azure storage. . Looking at the code, there is no call to `workflowLogger.close() `or `workflowLogger.close(andDelete = false)` unless the file is being copied. . Probably this line in WorkflowActor.scala:; `case None if WorkflowLogger.isTemporary => workflowLogger.close(andDelete = true)` ; needs to change to:; `case None => workflowLogger.close(andDelete = WorkflowLogger.isTemporary)`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826
https://github.com/broadinstitute/cromwell/issues/4826:43,Testability,log,log-temporary,43,"When Cromwell is configured with `workflow-log-temporary: false` and workflow does not specify `final_workflow_log_dir`, Cromwell server does not close the log file. ; When `final_workflow_log_dir` is specified the original log file is correctly closed and copied. ; In my case, this prevents blobfuse (waits for the file to be closed) from transferring the log files to Azure storage. . Looking at the code, there is no call to `workflowLogger.close() `or `workflowLogger.close(andDelete = false)` unless the file is being copied. . Probably this line in WorkflowActor.scala:; `case None if WorkflowLogger.isTemporary => workflowLogger.close(andDelete = true)` ; needs to change to:; `case None => workflowLogger.close(andDelete = WorkflowLogger.isTemporary)`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826
https://github.com/broadinstitute/cromwell/issues/4826:156,Testability,log,log,156,"When Cromwell is configured with `workflow-log-temporary: false` and workflow does not specify `final_workflow_log_dir`, Cromwell server does not close the log file. ; When `final_workflow_log_dir` is specified the original log file is correctly closed and copied. ; In my case, this prevents blobfuse (waits for the file to be closed) from transferring the log files to Azure storage. . Looking at the code, there is no call to `workflowLogger.close() `or `workflowLogger.close(andDelete = false)` unless the file is being copied. . Probably this line in WorkflowActor.scala:; `case None if WorkflowLogger.isTemporary => workflowLogger.close(andDelete = true)` ; needs to change to:; `case None => workflowLogger.close(andDelete = WorkflowLogger.isTemporary)`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826
https://github.com/broadinstitute/cromwell/issues/4826:224,Testability,log,log,224,"When Cromwell is configured with `workflow-log-temporary: false` and workflow does not specify `final_workflow_log_dir`, Cromwell server does not close the log file. ; When `final_workflow_log_dir` is specified the original log file is correctly closed and copied. ; In my case, this prevents blobfuse (waits for the file to be closed) from transferring the log files to Azure storage. . Looking at the code, there is no call to `workflowLogger.close() `or `workflowLogger.close(andDelete = false)` unless the file is being copied. . Probably this line in WorkflowActor.scala:; `case None if WorkflowLogger.isTemporary => workflowLogger.close(andDelete = true)` ; needs to change to:; `case None => workflowLogger.close(andDelete = WorkflowLogger.isTemporary)`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826
https://github.com/broadinstitute/cromwell/issues/4826:358,Testability,log,log,358,"When Cromwell is configured with `workflow-log-temporary: false` and workflow does not specify `final_workflow_log_dir`, Cromwell server does not close the log file. ; When `final_workflow_log_dir` is specified the original log file is correctly closed and copied. ; In my case, this prevents blobfuse (waits for the file to be closed) from transferring the log files to Azure storage. . Looking at the code, there is no call to `workflowLogger.close() `or `workflowLogger.close(andDelete = false)` unless the file is being copied. . Probably this line in WorkflowActor.scala:; `case None if WorkflowLogger.isTemporary => workflowLogger.close(andDelete = true)` ; needs to change to:; `case None => workflowLogger.close(andDelete = WorkflowLogger.isTemporary)`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826
https://github.com/broadinstitute/cromwell/pull/4827:88,Deployability,update,updated,88,While working on #4815 I had some issues getting the testing going with centaur. I have updated the documentation so it will be (hopefully) a bit easier or the next person who reads it. There were some things that I did find in the code and in the centaur tests that I could not find information on. These are marked with `TODO`. I hope the cromwell devs will fill in the blanks.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4827
https://github.com/broadinstitute/cromwell/pull/4827:53,Testability,test,testing,53,While working on #4815 I had some issues getting the testing going with centaur. I have updated the documentation so it will be (hopefully) a bit easier or the next person who reads it. There were some things that I did find in the code and in the centaur tests that I could not find information on. These are marked with `TODO`. I hope the cromwell devs will fill in the blanks.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4827
https://github.com/broadinstitute/cromwell/pull/4827:256,Testability,test,tests,256,While working on #4815 I had some issues getting the testing going with centaur. I have updated the documentation so it will be (hopefully) a bit easier or the next person who reads it. There were some things that I did find in the code and in the centaur tests that I could not find information on. These are marked with `TODO`. I hope the cromwell devs will fill in the blanks.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4827
https://github.com/broadinstitute/cromwell/issues/4828:119,Performance,cache,cache,119,"In at least one example (I have the workflow & broad accessible inputs if someone needs them) the following leads to a cache miss:. Workflow A, Task A: Output is multipart uploaded to S3 and has an etag to match; Workflow A, Task B: Consumes this file as an input. Workflow B, Task A: Cache hit. Copies that file, but receives an md5 etag; Workflow B, Task B: Cache miss - the two etags are now mismatched. It seems highly likely that a solution to this would also solve #4805",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828
https://github.com/broadinstitute/cromwell/issues/4828:285,Performance,Cache,Cache,285,"In at least one example (I have the workflow & broad accessible inputs if someone needs them) the following leads to a cache miss:. Workflow A, Task A: Output is multipart uploaded to S3 and has an etag to match; Workflow A, Task B: Consumes this file as an input. Workflow B, Task A: Cache hit. Copies that file, but receives an md5 etag; Workflow B, Task B: Cache miss - the two etags are now mismatched. It seems highly likely that a solution to this would also solve #4805",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828
https://github.com/broadinstitute/cromwell/issues/4828:360,Performance,Cache,Cache,360,"In at least one example (I have the workflow & broad accessible inputs if someone needs them) the following leads to a cache miss:. Workflow A, Task A: Output is multipart uploaded to S3 and has an etag to match; Workflow A, Task B: Consumes this file as an input. Workflow B, Task A: Cache hit. Copies that file, but receives an md5 etag; Workflow B, Task B: Cache miss - the two etags are now mismatched. It seems highly likely that a solution to this would also solve #4805",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828
https://github.com/broadinstitute/cromwell/issues/4828:53,Security,access,accessible,53,"In at least one example (I have the workflow & broad accessible inputs if someone needs them) the following leads to a cache miss:. Workflow A, Task A: Output is multipart uploaded to S3 and has an etag to match; Workflow A, Task B: Consumes this file as an input. Workflow B, Task A: Cache hit. Copies that file, but receives an md5 etag; Workflow B, Task B: Cache miss - the two etags are now mismatched. It seems highly likely that a solution to this would also solve #4805",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828
https://github.com/broadinstitute/cromwell/issues/4832:136,Performance,queue,queue,136,"Idea: remove a potential window of doom around Cromwell restart by ensuring no workflow metadata is still waiting in the metadata write queue *before* deleting the Workflow Store entry. Problem:. * A workflow finishes; * It submits a bunch of ""finishing out"" metadata to the MetadataWriteActor; * It removes its entry from the workflow store; * == Cromwell Restarts ==; * The finishing out metadata (eg ""Status => Complete"") is never persisted. To help decide how important this is: ; * How long is our current metadata write queue (in total metadata entries, and in time in queue); * How big a deal is it if we occasionally lose Status Complete metadata entries?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4832
https://github.com/broadinstitute/cromwell/issues/4832:526,Performance,queue,queue,526,"Idea: remove a potential window of doom around Cromwell restart by ensuring no workflow metadata is still waiting in the metadata write queue *before* deleting the Workflow Store entry. Problem:. * A workflow finishes; * It submits a bunch of ""finishing out"" metadata to the MetadataWriteActor; * It removes its entry from the workflow store; * == Cromwell Restarts ==; * The finishing out metadata (eg ""Status => Complete"") is never persisted. To help decide how important this is: ; * How long is our current metadata write queue (in total metadata entries, and in time in queue); * How big a deal is it if we occasionally lose Status Complete metadata entries?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4832
https://github.com/broadinstitute/cromwell/issues/4832:575,Performance,queue,queue,575,"Idea: remove a potential window of doom around Cromwell restart by ensuring no workflow metadata is still waiting in the metadata write queue *before* deleting the Workflow Store entry. Problem:. * A workflow finishes; * It submits a bunch of ""finishing out"" metadata to the MetadataWriteActor; * It removes its entry from the workflow store; * == Cromwell Restarts ==; * The finishing out metadata (eg ""Status => Complete"") is never persisted. To help decide how important this is: ; * How long is our current metadata write queue (in total metadata entries, and in time in queue); * How big a deal is it if we occasionally lose Status Complete metadata entries?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4832
https://github.com/broadinstitute/cromwell/issues/4833:37,Testability,test,tests,37,"Looks like the relative output paths tests are breaking our AWS CI tests:; ```; should successfully run relative_output_paths *** FAILED *** (28 minutes, 35 seconds); centaur.test.CentaurTestException: Expected to find 1 item(s) at /tmp/outputs/relative_output_paths/greatpress.txt but got 0; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4833
https://github.com/broadinstitute/cromwell/issues/4833:67,Testability,test,tests,67,"Looks like the relative output paths tests are breaking our AWS CI tests:; ```; should successfully run relative_output_paths *** FAILED *** (28 minutes, 35 seconds); centaur.test.CentaurTestException: Expected to find 1 item(s) at /tmp/outputs/relative_output_paths/greatpress.txt but got 0; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4833
https://github.com/broadinstitute/cromwell/issues/4833:175,Testability,test,test,175,"Looks like the relative output paths tests are breaking our AWS CI tests:; ```; should successfully run relative_output_paths *** FAILED *** (28 minutes, 35 seconds); centaur.test.CentaurTestException: Expected to find 1 item(s) at /tmp/outputs/relative_output_paths/greatpress.txt but got 0; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4833
https://github.com/broadinstitute/cromwell/issues/4834:669,Availability,echo,echo,669,"I used cromwell-38.jar for testing. It looks like outputs from tasks with the same task-name and shard index in different scatter blocks share the same scratch directory in a container. Output files are written on this shared directory and globbed at the end of each task and then moved to S3 bucket together with all text/log files. This can result in globbing of wrong output files (from tasks with the same task-name but different alias).; ```; workflow cromwell_aws_glob_test {; scatter(i in range(2)) {; call t1 { input: i=i, alias = ""t1"" }; }. scatter(i in range(2)) {; call t1 as t2 { input: i=i, alias = ""t2"" }; }; }. task t1 {; Int i; String alias; command {; echo ""${alias},${i}"" > ${alias}-${i}.txt; }; output {; Array[File] outs = glob(""*.txt""); }; }; ```; On the execution directory for task t1 on S3. Everything looks okay. We don't have any t2 things.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/; PRE glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:30:54 0; 2019-04-11 20:32:48 30 glob-ef5df339533c1334f081dc8cc75ee4f3.list; 2019-04-11 20:30:54 2238 script; 2019-04-11 20:32:49 2 t1-0-rc.txt; 2019-04-11 20:32:51 0 t1-0-stderr.log; 2019-04-11 20:32:50 0 t1-0-stdout.log. But the glob directory has outputs from task t2.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:32:47 277 cromwell_glob_control_file; 2019-04-11 20:32:47 5 t1-0.txt; 2019-04-11 20:32:47 2 t2-0-rc.txt; 2019-04-11 20:32:47 5 t2-0.txt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4834
https://github.com/broadinstitute/cromwell/issues/4834:929,Deployability,pipeline,pipeline-test-runs,929,"I used cromwell-38.jar for testing. It looks like outputs from tasks with the same task-name and shard index in different scatter blocks share the same scratch directory in a container. Output files are written on this shared directory and globbed at the end of each task and then moved to S3 bucket together with all text/log files. This can result in globbing of wrong output files (from tasks with the same task-name but different alias).; ```; workflow cromwell_aws_glob_test {; scatter(i in range(2)) {; call t1 { input: i=i, alias = ""t1"" }; }. scatter(i in range(2)) {; call t1 as t2 { input: i=i, alias = ""t2"" }; }; }. task t1 {; Int i; String alias; command {; echo ""${alias},${i}"" > ${alias}-${i}.txt; }; output {; Array[File] outs = glob(""*.txt""); }; }; ```; On the execution directory for task t1 on S3. Everything looks okay. We don't have any t2 things.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/; PRE glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:30:54 0; 2019-04-11 20:32:48 30 glob-ef5df339533c1334f081dc8cc75ee4f3.list; 2019-04-11 20:30:54 2238 script; 2019-04-11 20:32:49 2 t1-0-rc.txt; 2019-04-11 20:32:51 0 t1-0-stderr.log; 2019-04-11 20:32:50 0 t1-0-stdout.log. But the glob directory has outputs from task t2.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:32:47 277 cromwell_glob_control_file; 2019-04-11 20:32:47 5 t1-0.txt; 2019-04-11 20:32:47 2 t2-0-rc.txt; 2019-04-11 20:32:47 5 t2-0.txt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4834
https://github.com/broadinstitute/cromwell/issues/4834:1423,Deployability,pipeline,pipeline-test-runs,1423,"I used cromwell-38.jar for testing. It looks like outputs from tasks with the same task-name and shard index in different scatter blocks share the same scratch directory in a container. Output files are written on this shared directory and globbed at the end of each task and then moved to S3 bucket together with all text/log files. This can result in globbing of wrong output files (from tasks with the same task-name but different alias).; ```; workflow cromwell_aws_glob_test {; scatter(i in range(2)) {; call t1 { input: i=i, alias = ""t1"" }; }. scatter(i in range(2)) {; call t1 as t2 { input: i=i, alias = ""t2"" }; }; }. task t1 {; Int i; String alias; command {; echo ""${alias},${i}"" > ${alias}-${i}.txt; }; output {; Array[File] outs = glob(""*.txt""); }; }; ```; On the execution directory for task t1 on S3. Everything looks okay. We don't have any t2 things.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/; PRE glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:30:54 0; 2019-04-11 20:32:48 30 glob-ef5df339533c1334f081dc8cc75ee4f3.list; 2019-04-11 20:30:54 2238 script; 2019-04-11 20:32:49 2 t1-0-rc.txt; 2019-04-11 20:32:51 0 t1-0-stderr.log; 2019-04-11 20:32:50 0 t1-0-stdout.log. But the glob directory has outputs from task t2.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:32:47 277 cromwell_glob_control_file; 2019-04-11 20:32:47 5 t1-0.txt; 2019-04-11 20:32:47 2 t2-0-rc.txt; 2019-04-11 20:32:47 5 t2-0.txt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4834
https://github.com/broadinstitute/cromwell/issues/4834:27,Testability,test,testing,27,"I used cromwell-38.jar for testing. It looks like outputs from tasks with the same task-name and shard index in different scatter blocks share the same scratch directory in a container. Output files are written on this shared directory and globbed at the end of each task and then moved to S3 bucket together with all text/log files. This can result in globbing of wrong output files (from tasks with the same task-name but different alias).; ```; workflow cromwell_aws_glob_test {; scatter(i in range(2)) {; call t1 { input: i=i, alias = ""t1"" }; }. scatter(i in range(2)) {; call t1 as t2 { input: i=i, alias = ""t2"" }; }; }. task t1 {; Int i; String alias; command {; echo ""${alias},${i}"" > ${alias}-${i}.txt; }; output {; Array[File] outs = glob(""*.txt""); }; }; ```; On the execution directory for task t1 on S3. Everything looks okay. We don't have any t2 things.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/; PRE glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:30:54 0; 2019-04-11 20:32:48 30 glob-ef5df339533c1334f081dc8cc75ee4f3.list; 2019-04-11 20:30:54 2238 script; 2019-04-11 20:32:49 2 t1-0-rc.txt; 2019-04-11 20:32:51 0 t1-0-stderr.log; 2019-04-11 20:32:50 0 t1-0-stdout.log. But the glob directory has outputs from task t2.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:32:47 277 cromwell_glob_control_file; 2019-04-11 20:32:47 5 t1-0.txt; 2019-04-11 20:32:47 2 t2-0-rc.txt; 2019-04-11 20:32:47 5 t2-0.txt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4834
https://github.com/broadinstitute/cromwell/issues/4834:323,Testability,log,log,323,"I used cromwell-38.jar for testing. It looks like outputs from tasks with the same task-name and shard index in different scatter blocks share the same scratch directory in a container. Output files are written on this shared directory and globbed at the end of each task and then moved to S3 bucket together with all text/log files. This can result in globbing of wrong output files (from tasks with the same task-name but different alias).; ```; workflow cromwell_aws_glob_test {; scatter(i in range(2)) {; call t1 { input: i=i, alias = ""t1"" }; }. scatter(i in range(2)) {; call t1 as t2 { input: i=i, alias = ""t2"" }; }; }. task t1 {; Int i; String alias; command {; echo ""${alias},${i}"" > ${alias}-${i}.txt; }; output {; Array[File] outs = glob(""*.txt""); }; }; ```; On the execution directory for task t1 on S3. Everything looks okay. We don't have any t2 things.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/; PRE glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:30:54 0; 2019-04-11 20:32:48 30 glob-ef5df339533c1334f081dc8cc75ee4f3.list; 2019-04-11 20:30:54 2238 script; 2019-04-11 20:32:49 2 t1-0-rc.txt; 2019-04-11 20:32:51 0 t1-0-stderr.log; 2019-04-11 20:32:50 0 t1-0-stdout.log. But the glob directory has outputs from task t2.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:32:47 277 cromwell_glob_control_file; 2019-04-11 20:32:47 5 t1-0.txt; 2019-04-11 20:32:47 2 t2-0-rc.txt; 2019-04-11 20:32:47 5 t2-0.txt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4834
https://github.com/broadinstitute/cromwell/issues/4834:938,Testability,test,test-runs,938,"I used cromwell-38.jar for testing. It looks like outputs from tasks with the same task-name and shard index in different scatter blocks share the same scratch directory in a container. Output files are written on this shared directory and globbed at the end of each task and then moved to S3 bucket together with all text/log files. This can result in globbing of wrong output files (from tasks with the same task-name but different alias).; ```; workflow cromwell_aws_glob_test {; scatter(i in range(2)) {; call t1 { input: i=i, alias = ""t1"" }; }. scatter(i in range(2)) {; call t1 as t2 { input: i=i, alias = ""t2"" }; }; }. task t1 {; Int i; String alias; command {; echo ""${alias},${i}"" > ${alias}-${i}.txt; }; output {; Array[File] outs = glob(""*.txt""); }; }; ```; On the execution directory for task t1 on S3. Everything looks okay. We don't have any t2 things.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/; PRE glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:30:54 0; 2019-04-11 20:32:48 30 glob-ef5df339533c1334f081dc8cc75ee4f3.list; 2019-04-11 20:30:54 2238 script; 2019-04-11 20:32:49 2 t1-0-rc.txt; 2019-04-11 20:32:51 0 t1-0-stderr.log; 2019-04-11 20:32:50 0 t1-0-stdout.log. But the glob directory has outputs from task t2.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:32:47 277 cromwell_glob_control_file; 2019-04-11 20:32:47 5 t1-0.txt; 2019-04-11 20:32:47 2 t2-0-rc.txt; 2019-04-11 20:32:47 5 t2-0.txt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4834
https://github.com/broadinstitute/cromwell/issues/4834:1268,Testability,log,log,1268,"I used cromwell-38.jar for testing. It looks like outputs from tasks with the same task-name and shard index in different scatter blocks share the same scratch directory in a container. Output files are written on this shared directory and globbed at the end of each task and then moved to S3 bucket together with all text/log files. This can result in globbing of wrong output files (from tasks with the same task-name but different alias).; ```; workflow cromwell_aws_glob_test {; scatter(i in range(2)) {; call t1 { input: i=i, alias = ""t1"" }; }. scatter(i in range(2)) {; call t1 as t2 { input: i=i, alias = ""t2"" }; }; }. task t1 {; Int i; String alias; command {; echo ""${alias},${i}"" > ${alias}-${i}.txt; }; output {; Array[File] outs = glob(""*.txt""); }; }; ```; On the execution directory for task t1 on S3. Everything looks okay. We don't have any t2 things.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/; PRE glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:30:54 0; 2019-04-11 20:32:48 30 glob-ef5df339533c1334f081dc8cc75ee4f3.list; 2019-04-11 20:30:54 2238 script; 2019-04-11 20:32:49 2 t1-0-rc.txt; 2019-04-11 20:32:51 0 t1-0-stderr.log; 2019-04-11 20:32:50 0 t1-0-stdout.log. But the glob directory has outputs from task t2.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:32:47 277 cromwell_glob_control_file; 2019-04-11 20:32:47 5 t1-0.txt; 2019-04-11 20:32:47 2 t2-0-rc.txt; 2019-04-11 20:32:47 5 t2-0.txt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4834
https://github.com/broadinstitute/cromwell/issues/4834:1307,Testability,log,log,1307,"I used cromwell-38.jar for testing. It looks like outputs from tasks with the same task-name and shard index in different scatter blocks share the same scratch directory in a container. Output files are written on this shared directory and globbed at the end of each task and then moved to S3 bucket together with all text/log files. This can result in globbing of wrong output files (from tasks with the same task-name but different alias).; ```; workflow cromwell_aws_glob_test {; scatter(i in range(2)) {; call t1 { input: i=i, alias = ""t1"" }; }. scatter(i in range(2)) {; call t1 as t2 { input: i=i, alias = ""t2"" }; }; }. task t1 {; Int i; String alias; command {; echo ""${alias},${i}"" > ${alias}-${i}.txt; }; output {; Array[File] outs = glob(""*.txt""); }; }; ```; On the execution directory for task t1 on S3. Everything looks okay. We don't have any t2 things.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/; PRE glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:30:54 0; 2019-04-11 20:32:48 30 glob-ef5df339533c1334f081dc8cc75ee4f3.list; 2019-04-11 20:30:54 2238 script; 2019-04-11 20:32:49 2 t1-0-rc.txt; 2019-04-11 20:32:51 0 t1-0-stderr.log; 2019-04-11 20:32:50 0 t1-0-stdout.log. But the glob directory has outputs from task t2.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:32:47 277 cromwell_glob_control_file; 2019-04-11 20:32:47 5 t1-0.txt; 2019-04-11 20:32:47 2 t2-0-rc.txt; 2019-04-11 20:32:47 5 t2-0.txt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4834
https://github.com/broadinstitute/cromwell/issues/4834:1432,Testability,test,test-runs,1432,"I used cromwell-38.jar for testing. It looks like outputs from tasks with the same task-name and shard index in different scatter blocks share the same scratch directory in a container. Output files are written on this shared directory and globbed at the end of each task and then moved to S3 bucket together with all text/log files. This can result in globbing of wrong output files (from tasks with the same task-name but different alias).; ```; workflow cromwell_aws_glob_test {; scatter(i in range(2)) {; call t1 { input: i=i, alias = ""t1"" }; }. scatter(i in range(2)) {; call t1 as t2 { input: i=i, alias = ""t2"" }; }; }. task t1 {; Int i; String alias; command {; echo ""${alias},${i}"" > ${alias}-${i}.txt; }; output {; Array[File] outs = glob(""*.txt""); }; }; ```; On the execution directory for task t1 on S3. Everything looks okay. We don't have any t2 things.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/; PRE glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:30:54 0; 2019-04-11 20:32:48 30 glob-ef5df339533c1334f081dc8cc75ee4f3.list; 2019-04-11 20:30:54 2238 script; 2019-04-11 20:32:49 2 t1-0-rc.txt; 2019-04-11 20:32:51 0 t1-0-stderr.log; 2019-04-11 20:32:50 0 t1-0-stdout.log. But the glob directory has outputs from task t2.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:32:47 277 cromwell_glob_control_file; 2019-04-11 20:32:47 5 t1-0.txt; 2019-04-11 20:32:47 2 t2-0-rc.txt; 2019-04-11 20:32:47 5 t2-0.txt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4834
https://github.com/broadinstitute/cromwell/pull/4837:55,Testability,log,log,55,"I'm still hoping to add some metrics around this, but ""log if metadata is dropped"" seems like it could be a valuable ""really quick win""?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4837
https://github.com/broadinstitute/cromwell/pull/4838:100,Testability,log,logging,100,Alternative to #4837 - this version will retry up to 10 times to write metadata before failing (and logging along the way),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4838
https://github.com/broadinstitute/cromwell/pull/4839:90,Energy Efficiency,green,green,90,"See the various commit test timings to see whether tests did or did not run. Idea:; * One green light is as good as two since in any real situation the 'push' tests would be restarted if the 'PR' tests are green.; * We can still test on push if we really want to, using `FORCETEST` in the commit message; * We won't have so many ""background"" tests running at any given time, hopefully reducing the queue-time for PRs. In other words:; * Should significantly *increase* cycle time for PRs and *reduce* developer time shepherding PRs through two sets of identical tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4839
https://github.com/broadinstitute/cromwell/pull/4839:206,Energy Efficiency,green,green,206,"See the various commit test timings to see whether tests did or did not run. Idea:; * One green light is as good as two since in any real situation the 'push' tests would be restarted if the 'PR' tests are green.; * We can still test on push if we really want to, using `FORCETEST` in the commit message; * We won't have so many ""background"" tests running at any given time, hopefully reducing the queue-time for PRs. In other words:; * Should significantly *increase* cycle time for PRs and *reduce* developer time shepherding PRs through two sets of identical tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4839
https://github.com/broadinstitute/cromwell/pull/4839:493,Energy Efficiency,reduce,reduce,493,"See the various commit test timings to see whether tests did or did not run. Idea:; * One green light is as good as two since in any real situation the 'push' tests would be restarted if the 'PR' tests are green.; * We can still test on push if we really want to, using `FORCETEST` in the commit message; * We won't have so many ""background"" tests running at any given time, hopefully reducing the queue-time for PRs. In other words:; * Should significantly *increase* cycle time for PRs and *reduce* developer time shepherding PRs through two sets of identical tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4839
https://github.com/broadinstitute/cromwell/pull/4839:296,Integrability,message,message,296,"See the various commit test timings to see whether tests did or did not run. Idea:; * One green light is as good as two since in any real situation the 'push' tests would be restarted if the 'PR' tests are green.; * We can still test on push if we really want to, using `FORCETEST` in the commit message; * We won't have so many ""background"" tests running at any given time, hopefully reducing the queue-time for PRs. In other words:; * Should significantly *increase* cycle time for PRs and *reduce* developer time shepherding PRs through two sets of identical tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4839
https://github.com/broadinstitute/cromwell/pull/4839:398,Performance,queue,queue-time,398,"See the various commit test timings to see whether tests did or did not run. Idea:; * One green light is as good as two since in any real situation the 'push' tests would be restarted if the 'PR' tests are green.; * We can still test on push if we really want to, using `FORCETEST` in the commit message; * We won't have so many ""background"" tests running at any given time, hopefully reducing the queue-time for PRs. In other words:; * Should significantly *increase* cycle time for PRs and *reduce* developer time shepherding PRs through two sets of identical tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4839
https://github.com/broadinstitute/cromwell/pull/4839:23,Testability,test,test,23,"See the various commit test timings to see whether tests did or did not run. Idea:; * One green light is as good as two since in any real situation the 'push' tests would be restarted if the 'PR' tests are green.; * We can still test on push if we really want to, using `FORCETEST` in the commit message; * We won't have so many ""background"" tests running at any given time, hopefully reducing the queue-time for PRs. In other words:; * Should significantly *increase* cycle time for PRs and *reduce* developer time shepherding PRs through two sets of identical tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4839
https://github.com/broadinstitute/cromwell/pull/4839:51,Testability,test,tests,51,"See the various commit test timings to see whether tests did or did not run. Idea:; * One green light is as good as two since in any real situation the 'push' tests would be restarted if the 'PR' tests are green.; * We can still test on push if we really want to, using `FORCETEST` in the commit message; * We won't have so many ""background"" tests running at any given time, hopefully reducing the queue-time for PRs. In other words:; * Should significantly *increase* cycle time for PRs and *reduce* developer time shepherding PRs through two sets of identical tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4839
https://github.com/broadinstitute/cromwell/pull/4839:159,Testability,test,tests,159,"See the various commit test timings to see whether tests did or did not run. Idea:; * One green light is as good as two since in any real situation the 'push' tests would be restarted if the 'PR' tests are green.; * We can still test on push if we really want to, using `FORCETEST` in the commit message; * We won't have so many ""background"" tests running at any given time, hopefully reducing the queue-time for PRs. In other words:; * Should significantly *increase* cycle time for PRs and *reduce* developer time shepherding PRs through two sets of identical tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4839
https://github.com/broadinstitute/cromwell/pull/4839:196,Testability,test,tests,196,"See the various commit test timings to see whether tests did or did not run. Idea:; * One green light is as good as two since in any real situation the 'push' tests would be restarted if the 'PR' tests are green.; * We can still test on push if we really want to, using `FORCETEST` in the commit message; * We won't have so many ""background"" tests running at any given time, hopefully reducing the queue-time for PRs. In other words:; * Should significantly *increase* cycle time for PRs and *reduce* developer time shepherding PRs through two sets of identical tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4839
https://github.com/broadinstitute/cromwell/pull/4839:229,Testability,test,test,229,"See the various commit test timings to see whether tests did or did not run. Idea:; * One green light is as good as two since in any real situation the 'push' tests would be restarted if the 'PR' tests are green.; * We can still test on push if we really want to, using `FORCETEST` in the commit message; * We won't have so many ""background"" tests running at any given time, hopefully reducing the queue-time for PRs. In other words:; * Should significantly *increase* cycle time for PRs and *reduce* developer time shepherding PRs through two sets of identical tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4839
https://github.com/broadinstitute/cromwell/pull/4839:342,Testability,test,tests,342,"See the various commit test timings to see whether tests did or did not run. Idea:; * One green light is as good as two since in any real situation the 'push' tests would be restarted if the 'PR' tests are green.; * We can still test on push if we really want to, using `FORCETEST` in the commit message; * We won't have so many ""background"" tests running at any given time, hopefully reducing the queue-time for PRs. In other words:; * Should significantly *increase* cycle time for PRs and *reduce* developer time shepherding PRs through two sets of identical tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4839
https://github.com/broadinstitute/cromwell/pull/4839:562,Testability,test,tests,562,"See the various commit test timings to see whether tests did or did not run. Idea:; * One green light is as good as two since in any real situation the 'push' tests would be restarted if the 'PR' tests are green.; * We can still test on push if we really want to, using `FORCETEST` in the commit message; * We won't have so many ""background"" tests running at any given time, hopefully reducing the queue-time for PRs. In other words:; * Should significantly *increase* cycle time for PRs and *reduce* developer time shepherding PRs through two sets of identical tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4839
https://github.com/broadinstitute/cromwell/issues/4842:74,Deployability,Deploy,Deployment,74,"AC: Multiple cromwells are used to execute a performance test. ## Desired Deployment. Utilize single instances for summarizer & reader. Bring up a instance ""group"" (a.k.a. cluster) for workers. ### Worker Cromwells [Instance Group](https://cloud.google.com/compute/docs/instance-groups/). Instantiate a GCP instance group to bring up a cluster of 3 nodes. ### TODO. - [ ] Write an instance template to represent a cromwell worker.; - [ ] Edit Jenkins script to bring up instances as described above.; - [ ] Edit instance startup script to remove shutdown part, move that work to end of Jenkins script.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4842
https://github.com/broadinstitute/cromwell/issues/4842:45,Performance,perform,performance,45,"AC: Multiple cromwells are used to execute a performance test. ## Desired Deployment. Utilize single instances for summarizer & reader. Bring up a instance ""group"" (a.k.a. cluster) for workers. ### Worker Cromwells [Instance Group](https://cloud.google.com/compute/docs/instance-groups/). Instantiate a GCP instance group to bring up a cluster of 3 nodes. ### TODO. - [ ] Write an instance template to represent a cromwell worker.; - [ ] Edit Jenkins script to bring up instances as described above.; - [ ] Edit instance startup script to remove shutdown part, move that work to end of Jenkins script.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4842
https://github.com/broadinstitute/cromwell/issues/4842:57,Testability,test,test,57,"AC: Multiple cromwells are used to execute a performance test. ## Desired Deployment. Utilize single instances for summarizer & reader. Bring up a instance ""group"" (a.k.a. cluster) for workers. ### Worker Cromwells [Instance Group](https://cloud.google.com/compute/docs/instance-groups/). Instantiate a GCP instance group to bring up a cluster of 3 nodes. ### TODO. - [ ] Write an instance template to represent a cromwell worker.; - [ ] Edit Jenkins script to bring up instances as described above.; - [ ] Edit instance startup script to remove shutdown part, move that work to end of Jenkins script.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4842
https://github.com/broadinstitute/cromwell/issues/4843:4,Deployability,Release,Release,4,AC: Release is running in both FireCloud and CaaS Prod,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4843
https://github.com/broadinstitute/cromwell/issues/4844:162,Testability,test,test,162,"In the horizontal world, there will be many Cromwell nodes brought up in the perf script. . ## What we have today ; One node is brought up and that node runs the test in its startup script. ## What we want; The nodes are brought up and do nothing other than run Cromwell. Something else (probably Jenkins) runs the test script against them. --. Probably a generalization of #4251.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4844
https://github.com/broadinstitute/cromwell/issues/4844:315,Testability,test,test,315,"In the horizontal world, there will be many Cromwell nodes brought up in the perf script. . ## What we have today ; One node is brought up and that node runs the test in its startup script. ## What we want; The nodes are brought up and do nothing other than run Cromwell. Something else (probably Jenkins) runs the test script against them. --. Probably a generalization of #4251.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4844
https://github.com/broadinstitute/cromwell/issues/4848:374,Availability,reliab,reliably,374,"A problem (#4364) existed with aliased tasks not being correctly separated on the AWS instance. . The test case relied on a file on the EC2 instance being written to by both tasks and failed if this happened. Unfortunately, the test case only works (ie fails correctly) if the two aliased tasks *happen to* run on the same EC2 instance. Come up with a way to make this test reliably fail if the aliasing is broken. Maybe by forcing centaur tests to always run on exactly one instance?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4848
https://github.com/broadinstitute/cromwell/issues/4848:102,Testability,test,test,102,"A problem (#4364) existed with aliased tasks not being correctly separated on the AWS instance. . The test case relied on a file on the EC2 instance being written to by both tasks and failed if this happened. Unfortunately, the test case only works (ie fails correctly) if the two aliased tasks *happen to* run on the same EC2 instance. Come up with a way to make this test reliably fail if the aliasing is broken. Maybe by forcing centaur tests to always run on exactly one instance?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4848
https://github.com/broadinstitute/cromwell/issues/4848:228,Testability,test,test,228,"A problem (#4364) existed with aliased tasks not being correctly separated on the AWS instance. . The test case relied on a file on the EC2 instance being written to by both tasks and failed if this happened. Unfortunately, the test case only works (ie fails correctly) if the two aliased tasks *happen to* run on the same EC2 instance. Come up with a way to make this test reliably fail if the aliasing is broken. Maybe by forcing centaur tests to always run on exactly one instance?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4848
https://github.com/broadinstitute/cromwell/issues/4848:369,Testability,test,test,369,"A problem (#4364) existed with aliased tasks not being correctly separated on the AWS instance. . The test case relied on a file on the EC2 instance being written to by both tasks and failed if this happened. Unfortunately, the test case only works (ie fails correctly) if the two aliased tasks *happen to* run on the same EC2 instance. Come up with a way to make this test reliably fail if the aliasing is broken. Maybe by forcing centaur tests to always run on exactly one instance?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4848
https://github.com/broadinstitute/cromwell/issues/4848:440,Testability,test,tests,440,"A problem (#4364) existed with aliased tasks not being correctly separated on the AWS instance. . The test case relied on a file on the EC2 instance being written to by both tasks and failed if this happened. Unfortunately, the test case only works (ie fails correctly) if the two aliased tasks *happen to* run on the same EC2 instance. Come up with a way to make this test reliably fail if the aliasing is broken. Maybe by forcing centaur tests to always run on exactly one instance?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4848
https://github.com/broadinstitute/cromwell/issues/4856:264,Availability,failure,failures,264,"When a step in a workflow fails, you can go read the `stderr` file for logs which are very informative. Cromwell also logs useful information to stdout. But checking the `/metadata` API endpoint always gives the same message: ; ```; ""executionStatus"": ""Failed"",; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Job rna.kallisto:0:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.""; }; ],; ```. It only tells you that the return code was not registered, not a clue into what might have caused the error. Could this be improved?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4856
https://github.com/broadinstitute/cromwell/issues/4856:588,Availability,error,error,588,"When a step in a workflow fails, you can go read the `stderr` file for logs which are very informative. Cromwell also logs useful information to stdout. But checking the `/metadata` API endpoint always gives the same message: ; ```; ""executionStatus"": ""Failed"",; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Job rna.kallisto:0:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.""; }; ],; ```. It only tells you that the return code was not registered, not a clue into what might have caused the error. Could this be improved?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4856
https://github.com/broadinstitute/cromwell/issues/4856:217,Integrability,message,message,217,"When a step in a workflow fails, you can go read the `stderr` file for logs which are very informative. Cromwell also logs useful information to stdout. But checking the `/metadata` API endpoint always gives the same message: ; ```; ""executionStatus"": ""Failed"",; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Job rna.kallisto:0:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.""; }; ],; ```. It only tells you that the return code was not registered, not a clue into what might have caused the error. Could this be improved?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4856
https://github.com/broadinstitute/cromwell/issues/4856:299,Integrability,message,message,299,"When a step in a workflow fails, you can go read the `stderr` file for logs which are very informative. Cromwell also logs useful information to stdout. But checking the `/metadata` API endpoint always gives the same message: ; ```; ""executionStatus"": ""Failed"",; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Job rna.kallisto:0:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.""; }; ],; ```. It only tells you that the return code was not registered, not a clue into what might have caused the error. Could this be improved?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4856
https://github.com/broadinstitute/cromwell/issues/4856:71,Testability,log,logs,71,"When a step in a workflow fails, you can go read the `stderr` file for logs which are very informative. Cromwell also logs useful information to stdout. But checking the `/metadata` API endpoint always gives the same message: ; ```; ""executionStatus"": ""Failed"",; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Job rna.kallisto:0:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.""; }; ],; ```. It only tells you that the return code was not registered, not a clue into what might have caused the error. Could this be improved?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4856
https://github.com/broadinstitute/cromwell/issues/4856:118,Testability,log,logs,118,"When a step in a workflow fails, you can go read the `stderr` file for logs which are very informative. Cromwell also logs useful information to stdout. But checking the `/metadata` API endpoint always gives the same message: ; ```; ""executionStatus"": ""Failed"",; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Job rna.kallisto:0:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.""; }; ],; ```. It only tells you that the return code was not registered, not a clue into what might have caused the error. Could this be improved?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4856
https://github.com/broadinstitute/cromwell/issues/4857:208,Availability,recover,recover,208,Cromwell throws `java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor` exception when it tries to recover a running job. Stacktrace:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:249,Availability,ERROR,ERROR,249,Cromwell throws `java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor` exception when it tries to recover a running job. Stacktrace:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:350,Availability,Error,Error,350,Cromwell throws `java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor` exception when it tries to recover a running job. Stacktrace:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:370,Availability,Recover,Recover,370,Cromwell throws `java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor` exception when it tries to recover a running job. Stacktrace:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:995,Availability,recover,recover,995,ws `java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor` exception when it tries to recover a running job. Stacktrace:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.sc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:1101,Availability,recover,recover,1101,mwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor` exception when it tries to recover a running job. Stacktrace:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyn,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:1218,Availability,recover,recover,1218,:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:1342,Availability,recover,recoverAsync,1342,empting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:1497,Availability,recover,recoverAsync,1497,emented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:1608,Availability,recover,recoverAsync,1608,StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65);,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:1730,Availability,recover,recoverAsync,1730,onActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:2223,Availability,robust,robustExecuteOrRecover,2223,JobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:2566,Availability,robust,robustExecuteOrRecover,2566,ckend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.jav,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:3812,Availability,ERROR,ERROR,3812,"e.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-18 13:22:35,882 cromwell-system-akka.dispatchers.engine-dispatcher-42 ERROR - WorkflowManagerActor Workflow 4057b0c6-0019-4a00-b8af-e392fbf89697 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.core.CromwellFatalException$.apply(core.scala:22); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:39); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run$$$capture(Promise.scala:60); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:4390,Availability,recover,recoverWith,4390,"exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-18 13:22:35,882 cromwell-system-akka.dispatchers.engine-dispatcher-42 ERROR - WorkflowManagerActor Workflow 4057b0c6-0019-4a00-b8af-e392fbf89697 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.core.CromwellFatalException$.apply(core.scala:22); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:39); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run$$$capture(Promise.scala:60); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:6117,Availability,recover,recover,6117,spatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.sc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:6223,Availability,recover,recover,6223,.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyn,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:6340,Availability,recover,recover,6340,orkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:6464,Availability,recover,recoverAsync,6464,orkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:6619,Availability,recover,recoverAsync,6619,emented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:6730,Availability,recover,recoverAsync,6730,StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65);,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:6852,Availability,recover,recoverAsync,6852,onActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:7345,Availability,robust,robustExecuteOrRecover,7345,JobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:7688,Availability,robust,robustExecuteOrRecover,7688,; 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	... 4 more; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:4363,Performance,concurren,concurrent,4363,"; 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-18 13:22:35,882 cromwell-system-akka.dispatchers.engine-dispatcher-42 ERROR - WorkflowManagerActor Workflow 4057b0c6-0019-4a00-b8af-e392fbf89697 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.core.CromwellFatalException$.apply(core.scala:22); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:39); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run$$$capture(Promise.scala:60); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:4433,Performance,concurren,concurrent,4433,".forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-18 13:22:35,882 cromwell-system-akka.dispatchers.engine-dispatcher-42 ERROR - WorkflowManagerActor Workflow 4057b0c6-0019-4a00-b8af-e392fbf89697 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.core.CromwellFatalException$.apply(core.scala:22); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:39); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run$$$capture(Promise.scala:60); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(Fo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:4511,Performance,concurren,concurrent,4511,"in.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-18 13:22:35,882 cromwell-system-akka.dispatchers.engine-dispatcher-42 ERROR - WorkflowManagerActor Workflow 4057b0c6-0019-4a00-b8af-e392fbf89697 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.core.CromwellFatalException$.apply(core.scala:22); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:39); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run$$$capture(Promise.scala:60); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(For",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:4587,Performance,concurren,concurrent,4587,".forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-18 13:22:35,882 cromwell-system-akka.dispatchers.engine-dispatcher-42 ERROR - WorkflowManagerActor Workflow 4057b0c6-0019-4a00-b8af-e392fbf89697 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.core.CromwellFatalException$.apply(core.scala:22); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:39); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run$$$capture(Promise.scala:60); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationExcept",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:4909,Performance,concurren,concurrent,4909,kflowState): cromwell.core.CromwellFatalException: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.core.CromwellFatalException$.apply(core.scala:22); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:39); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run$$$capture(Promise.scala:60); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutio,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:208,Safety,recover,recover,208,Cromwell throws `java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor` exception when it tries to recover a running job. Stacktrace:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:370,Safety,Recover,Recover,370,Cromwell throws `java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor` exception when it tries to recover a running job. Stacktrace:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:995,Safety,recover,recover,995,ws `java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor` exception when it tries to recover a running job. Stacktrace:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.sc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:1101,Safety,recover,recover,1101,mwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor` exception when it tries to recover a running job. Stacktrace:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyn,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:1218,Safety,recover,recover,1218,:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:1342,Safety,recover,recoverAsync,1342,empting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:1497,Safety,recover,recoverAsync,1497,emented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:1608,Safety,recover,recoverAsync,1608,StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65);,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:1730,Safety,recover,recoverAsync,1730,onActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:4390,Safety,recover,recoverWith,4390,"exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-18 13:22:35,882 cromwell-system-akka.dispatchers.engine-dispatcher-42 ERROR - WorkflowManagerActor Workflow 4057b0c6-0019-4a00-b8af-e392fbf89697 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.core.CromwellFatalException$.apply(core.scala:22); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:39); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run$$$capture(Promise.scala:60); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:6117,Safety,recover,recover,6117,spatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.sc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:6223,Safety,recover,recover,6223,.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyn,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:6340,Safety,recover,recover,6340,orkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:6464,Safety,recover,recoverAsync,6464,orkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:6619,Safety,recover,recoverAsync,6619,emented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:6730,Safety,recover,recoverAsync,6730,StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65);,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/issues/4857:6852,Safety,recover,recoverAsync,6852,onActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857
https://github.com/broadinstitute/cromwell/pull/4865:0,Deployability,Update,Update,0,Update what docker repos are compatible with AWS Batch,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4865
https://github.com/broadinstitute/cromwell/issues/4869:4,Deployability,release,release,4,"The release WDL makes some overly bold presumptions with respect to the platform and tools of its environment. `curl` seems pretty universal, but compatible versions of `python` and `jq` might not be. The release WDL's current ""upsert a jq"" logic might be especially troublesome for the wired ethernet set:. ```; which jq || brew install jq; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4869
https://github.com/broadinstitute/cromwell/issues/4869:205,Deployability,release,release,205,"The release WDL makes some overly bold presumptions with respect to the platform and tools of its environment. `curl` seems pretty universal, but compatible versions of `python` and `jq` might not be. The release WDL's current ""upsert a jq"" logic might be especially troublesome for the wired ethernet set:. ```; which jq || brew install jq; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4869
https://github.com/broadinstitute/cromwell/issues/4869:330,Deployability,install,install,330,"The release WDL makes some overly bold presumptions with respect to the platform and tools of its environment. `curl` seems pretty universal, but compatible versions of `python` and `jq` might not be. The release WDL's current ""upsert a jq"" logic might be especially troublesome for the wired ethernet set:. ```; which jq || brew install jq; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4869
https://github.com/broadinstitute/cromwell/issues/4869:241,Testability,log,logic,241,"The release WDL makes some overly bold presumptions with respect to the platform and tools of its environment. `curl` seems pretty universal, but compatible versions of `python` and `jq` might not be. The release WDL's current ""upsert a jq"" logic might be especially troublesome for the wired ethernet set:. ```; which jq || brew install jq; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4869
https://github.com/broadinstitute/cromwell/issues/4870:4,Deployability,release,release,4,"The release WDL relies on a GitHub access token for some of its operations and ""environmental"" auth for others. Trying to run the release WDL in an environment that does not have an account auth'd that can `git push` to the `broadinstitute/cromwell` repository will end badly. Ideally all GitHub operations in the release WDL should use the access token.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4870
https://github.com/broadinstitute/cromwell/issues/4870:130,Deployability,release,release,130,"The release WDL relies on a GitHub access token for some of its operations and ""environmental"" auth for others. Trying to run the release WDL in an environment that does not have an account auth'd that can `git push` to the `broadinstitute/cromwell` repository will end badly. Ideally all GitHub operations in the release WDL should use the access token.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4870
https://github.com/broadinstitute/cromwell/issues/4870:314,Deployability,release,release,314,"The release WDL relies on a GitHub access token for some of its operations and ""environmental"" auth for others. Trying to run the release WDL in an environment that does not have an account auth'd that can `git push` to the `broadinstitute/cromwell` repository will end badly. Ideally all GitHub operations in the release WDL should use the access token.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4870
https://github.com/broadinstitute/cromwell/issues/4870:35,Security,access,access,35,"The release WDL relies on a GitHub access token for some of its operations and ""environmental"" auth for others. Trying to run the release WDL in an environment that does not have an account auth'd that can `git push` to the `broadinstitute/cromwell` repository will end badly. Ideally all GitHub operations in the release WDL should use the access token.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4870
https://github.com/broadinstitute/cromwell/issues/4870:341,Security,access,access,341,"The release WDL relies on a GitHub access token for some of its operations and ""environmental"" auth for others. Trying to run the release WDL in an environment that does not have an account auth'd that can `git push` to the `broadinstitute/cromwell` repository will end badly. Ideally all GitHub operations in the release WDL should use the access token.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4870
https://github.com/broadinstitute/cromwell/pull/4872:230,Integrability,message,message,230,"This was inspired by the surprise discovery of several suspiciously long running PAPI jobs. We can find these by running ad-hoc database queries but I personally like the idea of a more proactive warning being emitted. Sample log message: `2019-04-19 15:26:56,383 cromwell-system-akka.dispatchers.backend-dispatcher-53 WARN - Job with ID 'projects/broad-dsde-cromwell-dev/operations/17748569219664621060' has been running since 2019-04-19T15:26:23.283-04:00 and might not be making progress`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4872
https://github.com/broadinstitute/cromwell/pull/4872:226,Testability,log,log,226,"This was inspired by the surprise discovery of several suspiciously long running PAPI jobs. We can find these by running ad-hoc database queries but I personally like the idea of a more proactive warning being emitted. Sample log message: `2019-04-19 15:26:56,383 cromwell-system-akka.dispatchers.backend-dispatcher-53 WARN - Job with ID 'projects/broad-dsde-cromwell-dev/operations/17748569219664621060' has been running since 2019-04-19T15:26:23.283-04:00 and might not be making progress`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4872
https://github.com/broadinstitute/cromwell/issues/4873:129,Integrability,message,message,129,"@mwalker174 originally reported:. > Hi all, I’ve got a critical problem where call caching times out on a particular WDL task (`“message”: “Hashing request timed out for gs://...“’). This makes some sense since the task is checking ~200 files on each of ~200 shards. This is on cromwell v36/papiv2. I thinking bundling the files would probably fix this, but is there any way to increase the timeout limit in the server settings? Would upgrading to v38 help? Thanks!. There's currently a non-configurable 5 minute timeout per GCS hash request. Assuming no batching (which I didn't come across) for ~40K individual requests that's about 100 requests/second to GCS. I'm pretty sure w/ GCS request throttling & internal cromwell backoffs at least one of those requests would fail to return in 5 minutes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4873
https://github.com/broadinstitute/cromwell/issues/4873:491,Modifiability,config,configurable,491,"@mwalker174 originally reported:. > Hi all, I’ve got a critical problem where call caching times out on a particular WDL task (`“message”: “Hashing request timed out for gs://...“’). This makes some sense since the task is checking ~200 files on each of ~200 shards. This is on cromwell v36/papiv2. I thinking bundling the files would probably fix this, but is there any way to increase the timeout limit in the server settings? Would upgrading to v38 help? Thanks!. There's currently a non-configurable 5 minute timeout per GCS hash request. Assuming no batching (which I didn't come across) for ~40K individual requests that's about 100 requests/second to GCS. I'm pretty sure w/ GCS request throttling & internal cromwell backoffs at least one of those requests would fail to return in 5 minutes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4873
https://github.com/broadinstitute/cromwell/issues/4873:391,Safety,timeout,timeout,391,"@mwalker174 originally reported:. > Hi all, I’ve got a critical problem where call caching times out on a particular WDL task (`“message”: “Hashing request timed out for gs://...“’). This makes some sense since the task is checking ~200 files on each of ~200 shards. This is on cromwell v36/papiv2. I thinking bundling the files would probably fix this, but is there any way to increase the timeout limit in the server settings? Would upgrading to v38 help? Thanks!. There's currently a non-configurable 5 minute timeout per GCS hash request. Assuming no batching (which I didn't come across) for ~40K individual requests that's about 100 requests/second to GCS. I'm pretty sure w/ GCS request throttling & internal cromwell backoffs at least one of those requests would fail to return in 5 minutes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4873
https://github.com/broadinstitute/cromwell/issues/4873:513,Safety,timeout,timeout,513,"@mwalker174 originally reported:. > Hi all, I’ve got a critical problem where call caching times out on a particular WDL task (`“message”: “Hashing request timed out for gs://...“’). This makes some sense since the task is checking ~200 files on each of ~200 shards. This is on cromwell v36/papiv2. I thinking bundling the files would probably fix this, but is there any way to increase the timeout limit in the server settings? Would upgrading to v38 help? Thanks!. There's currently a non-configurable 5 minute timeout per GCS hash request. Assuming no batching (which I didn't come across) for ~40K individual requests that's about 100 requests/second to GCS. I'm pretty sure w/ GCS request throttling & internal cromwell backoffs at least one of those requests would fail to return in 5 minutes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4873
https://github.com/broadinstitute/cromwell/issues/4873:140,Security,Hash,Hashing,140,"@mwalker174 originally reported:. > Hi all, I’ve got a critical problem where call caching times out on a particular WDL task (`“message”: “Hashing request timed out for gs://...“’). This makes some sense since the task is checking ~200 files on each of ~200 shards. This is on cromwell v36/papiv2. I thinking bundling the files would probably fix this, but is there any way to increase the timeout limit in the server settings? Would upgrading to v38 help? Thanks!. There's currently a non-configurable 5 minute timeout per GCS hash request. Assuming no batching (which I didn't come across) for ~40K individual requests that's about 100 requests/second to GCS. I'm pretty sure w/ GCS request throttling & internal cromwell backoffs at least one of those requests would fail to return in 5 minutes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4873
https://github.com/broadinstitute/cromwell/issues/4873:529,Security,hash,hash,529,"@mwalker174 originally reported:. > Hi all, I’ve got a critical problem where call caching times out on a particular WDL task (`“message”: “Hashing request timed out for gs://...“’). This makes some sense since the task is checking ~200 files on each of ~200 shards. This is on cromwell v36/papiv2. I thinking bundling the files would probably fix this, but is there any way to increase the timeout limit in the server settings? Would upgrading to v38 help? Thanks!. There's currently a non-configurable 5 minute timeout per GCS hash request. Assuming no batching (which I didn't come across) for ~40K individual requests that's about 100 requests/second to GCS. I'm pretty sure w/ GCS request throttling & internal cromwell backoffs at least one of those requests would fail to return in 5 minutes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4873
https://github.com/broadinstitute/cromwell/pull/4876:72,Testability,test,test,72,"Make some values lazy, move a one-use function inside its caller, add a test case",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4876
https://github.com/broadinstitute/cromwell/issues/4877:49,Availability,alive,alive,49,"The documentation for the new HPC backend 'check-alive' feature is a bit scattered an incomplete. https://cromwell.readthedocs.io/en/stable/backends/HPC/; > This option will implicitly enable polling with the check-alive option. https://github.com/broadinstitute/cromwell/releases:; > When the value exit-code-timeout-seconds is set, check-alive command is now only called once every timeout interval instead of each poll. https://github.com/broadinstitute/cromwell/blob/73ad264b4c7919d0bbd344fecbc903f819f5e16c/cromwell.example.backends/SGE.conf#L27; > # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). I had a look at the code and PRs implementing the feature, but am not confident I can even see how the feature is implemented, so cannot write the docs as a PR myself!. I had assumed, when you specify `exit-code-timeout-seconds`, cromwell would poll `check-alive` at that interval, and after 2 failed `check-alive`s with no rc file, would mark the job as failed. Now I think there is some other polling mechanism, perhaps to cap polling load per backend instead of scaling it with number of jobs. . Is it possible to revisit that documentation and actually explain what is happening? Alternatively just a link here to the `unrelated to this timeout` documentation?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877
https://github.com/broadinstitute/cromwell/issues/4877:215,Availability,alive,alive,215,"The documentation for the new HPC backend 'check-alive' feature is a bit scattered an incomplete. https://cromwell.readthedocs.io/en/stable/backends/HPC/; > This option will implicitly enable polling with the check-alive option. https://github.com/broadinstitute/cromwell/releases:; > When the value exit-code-timeout-seconds is set, check-alive command is now only called once every timeout interval instead of each poll. https://github.com/broadinstitute/cromwell/blob/73ad264b4c7919d0bbd344fecbc903f819f5e16c/cromwell.example.backends/SGE.conf#L27; > # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). I had a look at the code and PRs implementing the feature, but am not confident I can even see how the feature is implemented, so cannot write the docs as a PR myself!. I had assumed, when you specify `exit-code-timeout-seconds`, cromwell would poll `check-alive` at that interval, and after 2 failed `check-alive`s with no rc file, would mark the job as failed. Now I think there is some other polling mechanism, perhaps to cap polling load per backend instead of scaling it with number of jobs. . Is it possible to revisit that documentation and actually explain what is happening? Alternatively just a link here to the `unrelated to this timeout` documentation?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877
https://github.com/broadinstitute/cromwell/issues/4877:340,Availability,alive,alive,340,"The documentation for the new HPC backend 'check-alive' feature is a bit scattered an incomplete. https://cromwell.readthedocs.io/en/stable/backends/HPC/; > This option will implicitly enable polling with the check-alive option. https://github.com/broadinstitute/cromwell/releases:; > When the value exit-code-timeout-seconds is set, check-alive command is now only called once every timeout interval instead of each poll. https://github.com/broadinstitute/cromwell/blob/73ad264b4c7919d0bbd344fecbc903f819f5e16c/cromwell.example.backends/SGE.conf#L27; > # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). I had a look at the code and PRs implementing the feature, but am not confident I can even see how the feature is implemented, so cannot write the docs as a PR myself!. I had assumed, when you specify `exit-code-timeout-seconds`, cromwell would poll `check-alive` at that interval, and after 2 failed `check-alive`s with no rc file, would mark the job as failed. Now I think there is some other polling mechanism, perhaps to cap polling load per backend instead of scaling it with number of jobs. . Is it possible to revisit that documentation and actually explain what is happening? Alternatively just a link here to the `unrelated to this timeout` documentation?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877
https://github.com/broadinstitute/cromwell/issues/4877:600,Availability,alive,alive,600,"The documentation for the new HPC backend 'check-alive' feature is a bit scattered an incomplete. https://cromwell.readthedocs.io/en/stable/backends/HPC/; > This option will implicitly enable polling with the check-alive option. https://github.com/broadinstitute/cromwell/releases:; > When the value exit-code-timeout-seconds is set, check-alive command is now only called once every timeout interval instead of each poll. https://github.com/broadinstitute/cromwell/blob/73ad264b4c7919d0bbd344fecbc903f819f5e16c/cromwell.example.backends/SGE.conf#L27; > # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). I had a look at the code and PRs implementing the feature, but am not confident I can even see how the feature is implemented, so cannot write the docs as a PR myself!. I had assumed, when you specify `exit-code-timeout-seconds`, cromwell would poll `check-alive` at that interval, and after 2 failed `check-alive`s with no rc file, would mark the job as failed. Now I think there is some other polling mechanism, perhaps to cap polling load per backend instead of scaling it with number of jobs. . Is it possible to revisit that documentation and actually explain what is happening? Alternatively just a link here to the `unrelated to this timeout` documentation?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877
https://github.com/broadinstitute/cromwell/issues/4877:928,Availability,alive,alive,928,"The documentation for the new HPC backend 'check-alive' feature is a bit scattered an incomplete. https://cromwell.readthedocs.io/en/stable/backends/HPC/; > This option will implicitly enable polling with the check-alive option. https://github.com/broadinstitute/cromwell/releases:; > When the value exit-code-timeout-seconds is set, check-alive command is now only called once every timeout interval instead of each poll. https://github.com/broadinstitute/cromwell/blob/73ad264b4c7919d0bbd344fecbc903f819f5e16c/cromwell.example.backends/SGE.conf#L27; > # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). I had a look at the code and PRs implementing the feature, but am not confident I can even see how the feature is implemented, so cannot write the docs as a PR myself!. I had assumed, when you specify `exit-code-timeout-seconds`, cromwell would poll `check-alive` at that interval, and after 2 failed `check-alive`s with no rc file, would mark the job as failed. Now I think there is some other polling mechanism, perhaps to cap polling load per backend instead of scaling it with number of jobs. . Is it possible to revisit that documentation and actually explain what is happening? Alternatively just a link here to the `unrelated to this timeout` documentation?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877
https://github.com/broadinstitute/cromwell/issues/4877:979,Availability,alive,alive,979,"The documentation for the new HPC backend 'check-alive' feature is a bit scattered an incomplete. https://cromwell.readthedocs.io/en/stable/backends/HPC/; > This option will implicitly enable polling with the check-alive option. https://github.com/broadinstitute/cromwell/releases:; > When the value exit-code-timeout-seconds is set, check-alive command is now only called once every timeout interval instead of each poll. https://github.com/broadinstitute/cromwell/blob/73ad264b4c7919d0bbd344fecbc903f819f5e16c/cromwell.example.backends/SGE.conf#L27; > # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). I had a look at the code and PRs implementing the feature, but am not confident I can even see how the feature is implemented, so cannot write the docs as a PR myself!. I had assumed, when you specify `exit-code-timeout-seconds`, cromwell would poll `check-alive` at that interval, and after 2 failed `check-alive`s with no rc file, would mark the job as failed. Now I think there is some other polling mechanism, perhaps to cap polling load per backend instead of scaling it with number of jobs. . Is it possible to revisit that documentation and actually explain what is happening? Alternatively just a link here to the `unrelated to this timeout` documentation?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877
https://github.com/broadinstitute/cromwell/issues/4877:272,Deployability,release,releases,272,"The documentation for the new HPC backend 'check-alive' feature is a bit scattered an incomplete. https://cromwell.readthedocs.io/en/stable/backends/HPC/; > This option will implicitly enable polling with the check-alive option. https://github.com/broadinstitute/cromwell/releases:; > When the value exit-code-timeout-seconds is set, check-alive command is now only called once every timeout interval instead of each poll. https://github.com/broadinstitute/cromwell/blob/73ad264b4c7919d0bbd344fecbc903f819f5e16c/cromwell.example.backends/SGE.conf#L27; > # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). I had a look at the code and PRs implementing the feature, but am not confident I can even see how the feature is implemented, so cannot write the docs as a PR myself!. I had assumed, when you specify `exit-code-timeout-seconds`, cromwell would poll `check-alive` at that interval, and after 2 failed `check-alive`s with no rc file, would mark the job as failed. Now I think there is some other polling mechanism, perhaps to cap polling load per backend instead of scaling it with number of jobs. . Is it possible to revisit that documentation and actually explain what is happening? Alternatively just a link here to the `unrelated to this timeout` documentation?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877
https://github.com/broadinstitute/cromwell/issues/4877:1108,Performance,load,load,1108,"The documentation for the new HPC backend 'check-alive' feature is a bit scattered an incomplete. https://cromwell.readthedocs.io/en/stable/backends/HPC/; > This option will implicitly enable polling with the check-alive option. https://github.com/broadinstitute/cromwell/releases:; > When the value exit-code-timeout-seconds is set, check-alive command is now only called once every timeout interval instead of each poll. https://github.com/broadinstitute/cromwell/blob/73ad264b4c7919d0bbd344fecbc903f819f5e16c/cromwell.example.backends/SGE.conf#L27; > # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). I had a look at the code and PRs implementing the feature, but am not confident I can even see how the feature is implemented, so cannot write the docs as a PR myself!. I had assumed, when you specify `exit-code-timeout-seconds`, cromwell would poll `check-alive` at that interval, and after 2 failed `check-alive`s with no rc file, would mark the job as failed. Now I think there is some other polling mechanism, perhaps to cap polling load per backend instead of scaling it with number of jobs. . Is it possible to revisit that documentation and actually explain what is happening? Alternatively just a link here to the `unrelated to this timeout` documentation?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877
https://github.com/broadinstitute/cromwell/issues/4877:310,Safety,timeout,timeout-seconds,310,"The documentation for the new HPC backend 'check-alive' feature is a bit scattered an incomplete. https://cromwell.readthedocs.io/en/stable/backends/HPC/; > This option will implicitly enable polling with the check-alive option. https://github.com/broadinstitute/cromwell/releases:; > When the value exit-code-timeout-seconds is set, check-alive command is now only called once every timeout interval instead of each poll. https://github.com/broadinstitute/cromwell/blob/73ad264b4c7919d0bbd344fecbc903f819f5e16c/cromwell.example.backends/SGE.conf#L27; > # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). I had a look at the code and PRs implementing the feature, but am not confident I can even see how the feature is implemented, so cannot write the docs as a PR myself!. I had assumed, when you specify `exit-code-timeout-seconds`, cromwell would poll `check-alive` at that interval, and after 2 failed `check-alive`s with no rc file, would mark the job as failed. Now I think there is some other polling mechanism, perhaps to cap polling load per backend instead of scaling it with number of jobs. . Is it possible to revisit that documentation and actually explain what is happening? Alternatively just a link here to the `unrelated to this timeout` documentation?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877
https://github.com/broadinstitute/cromwell/issues/4877:384,Safety,timeout,timeout,384,"The documentation for the new HPC backend 'check-alive' feature is a bit scattered an incomplete. https://cromwell.readthedocs.io/en/stable/backends/HPC/; > This option will implicitly enable polling with the check-alive option. https://github.com/broadinstitute/cromwell/releases:; > When the value exit-code-timeout-seconds is set, check-alive command is now only called once every timeout interval instead of each poll. https://github.com/broadinstitute/cromwell/blob/73ad264b4c7919d0bbd344fecbc903f819f5e16c/cromwell.example.backends/SGE.conf#L27; > # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). I had a look at the code and PRs implementing the feature, but am not confident I can even see how the feature is implemented, so cannot write the docs as a PR myself!. I had assumed, when you specify `exit-code-timeout-seconds`, cromwell would poll `check-alive` at that interval, and after 2 failed `check-alive`s with no rc file, would mark the job as failed. Now I think there is some other polling mechanism, perhaps to cap polling load per backend instead of scaling it with number of jobs. . Is it possible to revisit that documentation and actually explain what is happening? Alternatively just a link here to the `unrelated to this timeout` documentation?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877
https://github.com/broadinstitute/cromwell/issues/4877:661,Safety,timeout,timeout,661,"The documentation for the new HPC backend 'check-alive' feature is a bit scattered an incomplete. https://cromwell.readthedocs.io/en/stable/backends/HPC/; > This option will implicitly enable polling with the check-alive option. https://github.com/broadinstitute/cromwell/releases:; > When the value exit-code-timeout-seconds is set, check-alive command is now only called once every timeout interval instead of each poll. https://github.com/broadinstitute/cromwell/blob/73ad264b4c7919d0bbd344fecbc903f819f5e16c/cromwell.example.backends/SGE.conf#L27; > # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). I had a look at the code and PRs implementing the feature, but am not confident I can even see how the feature is implemented, so cannot write the docs as a PR myself!. I had assumed, when you specify `exit-code-timeout-seconds`, cromwell would poll `check-alive` at that interval, and after 2 failed `check-alive`s with no rc file, would mark the job as failed. Now I think there is some other polling mechanism, perhaps to cap polling load per backend instead of scaling it with number of jobs. . Is it possible to revisit that documentation and actually explain what is happening? Alternatively just a link here to the `unrelated to this timeout` documentation?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877
https://github.com/broadinstitute/cromwell/issues/4877:883,Safety,timeout,timeout-seconds,883,"The documentation for the new HPC backend 'check-alive' feature is a bit scattered an incomplete. https://cromwell.readthedocs.io/en/stable/backends/HPC/; > This option will implicitly enable polling with the check-alive option. https://github.com/broadinstitute/cromwell/releases:; > When the value exit-code-timeout-seconds is set, check-alive command is now only called once every timeout interval instead of each poll. https://github.com/broadinstitute/cromwell/blob/73ad264b4c7919d0bbd344fecbc903f819f5e16c/cromwell.example.backends/SGE.conf#L27; > # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). I had a look at the code and PRs implementing the feature, but am not confident I can even see how the feature is implemented, so cannot write the docs as a PR myself!. I had assumed, when you specify `exit-code-timeout-seconds`, cromwell would poll `check-alive` at that interval, and after 2 failed `check-alive`s with no rc file, would mark the job as failed. Now I think there is some other polling mechanism, perhaps to cap polling load per backend instead of scaling it with number of jobs. . Is it possible to revisit that documentation and actually explain what is happening? Alternatively just a link here to the `unrelated to this timeout` documentation?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877
https://github.com/broadinstitute/cromwell/issues/4877:1312,Safety,timeout,timeout,1312,"The documentation for the new HPC backend 'check-alive' feature is a bit scattered an incomplete. https://cromwell.readthedocs.io/en/stable/backends/HPC/; > This option will implicitly enable polling with the check-alive option. https://github.com/broadinstitute/cromwell/releases:; > When the value exit-code-timeout-seconds is set, check-alive command is now only called once every timeout interval instead of each poll. https://github.com/broadinstitute/cromwell/blob/73ad264b4c7919d0bbd344fecbc903f819f5e16c/cromwell.example.backends/SGE.conf#L27; > # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). I had a look at the code and PRs implementing the feature, but am not confident I can even see how the feature is implemented, so cannot write the docs as a PR myself!. I had assumed, when you specify `exit-code-timeout-seconds`, cromwell would poll `check-alive` at that interval, and after 2 failed `check-alive`s with no rc file, would mark the job as failed. Now I think there is some other polling mechanism, perhaps to cap polling load per backend instead of scaling it with number of jobs. . Is it possible to revisit that documentation and actually explain what is happening? Alternatively just a link here to the `unrelated to this timeout` documentation?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877
https://github.com/broadinstitute/cromwell/pull/4878:128,Deployability,release,releases,128,Hi. I forgot to add this to the changelog when making the pr #4815 . Can this be retroactively be added to the changelog on the releases page as well? Thanks!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4878
https://github.com/broadinstitute/cromwell/issues/4880:10,Availability,recover,recover,10,Trying to recover space to prevent us hitting the 10 TB limit.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4880
https://github.com/broadinstitute/cromwell/issues/4880:10,Safety,recover,recover,10,Trying to recover space to prevent us hitting the 10 TB limit.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4880
https://github.com/broadinstitute/cromwell/pull/4882:30,Deployability,upgrade,upgrade,30,Exists to run the horicromtal upgrade tests - which only occur on PRs. But this isn't ready for review yet.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4882
https://github.com/broadinstitute/cromwell/pull/4882:38,Testability,test,tests,38,Exists to run the horicromtal upgrade tests - which only occur on PRs. But this isn't ready for review yet.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4882
https://github.com/broadinstitute/cromwell/pull/4883:27,Deployability,upgrade,upgrade,27,Fix the horicromtal engine upgrade test.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4883
https://github.com/broadinstitute/cromwell/pull/4883:35,Testability,test,test,35,Fix the horicromtal engine upgrade test.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4883
https://github.com/broadinstitute/cromwell/issues/4885:110,Availability,error,error,110,"When Cromiam tries to create a Sam resource that already exists, Sam will return a `HTTP 409 Conflict`. This ""error"" is expected, and should not be forwarded back to the client. Instead, the 409 response should be followed up with a request from Cromiam to Sam to add the user to the existing Sam resource. This functionality was previously implemented, but in #4624 **all** Sam errors were passed on, even the expected `409`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4885
https://github.com/broadinstitute/cromwell/issues/4885:379,Availability,error,errors,379,"When Cromiam tries to create a Sam resource that already exists, Sam will return a `HTTP 409 Conflict`. This ""error"" is expected, and should not be forwarded back to the client. Instead, the 409 response should be followed up with a request from Cromiam to Sam to add the user to the existing Sam resource. This functionality was previously implemented, but in #4624 **all** Sam errors were passed on, even the expected `409`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4885
https://github.com/broadinstitute/cromwell/issues/4891:62,Testability,test,test,62,"Over the last 7 days, the `invalidate_bad_caches_jes_no_copy` test has failed over twice as often as the next flakiest test, and nearly 6 times as often as the next one after that.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4891
https://github.com/broadinstitute/cromwell/issues/4891:119,Testability,test,test,119,"Over the last 7 days, the `invalidate_bad_caches_jes_no_copy` test has failed over twice as often as the next flakiest test, and nearly 6 times as often as the next one after that.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4891
https://github.com/broadinstitute/cromwell/issues/4892:327,Integrability,depend,depending,327,"Examples could include:; - If a WDL is invalid, don't return the keys for I/O... because they can be seen as falsely indicating that the WDL has no inputs or outputs. (DMohs); - Move away from form data to make hand-written curling easier; - Find a way to provide an unauthenticated version (this may be very big or very small depending on the path chosen)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4892
https://github.com/broadinstitute/cromwell/issues/4894:73,Availability,failure,failures,73,"Job Manager would like to distinguish between ""normal"" and ""exceptional"" failures, e.g. preemption versus something truly wrong. One implementation idea is to use the `causedBy` field but we have not vetted this at all. A/C distinguish between preemption and all other failures in the metadata response for an attempt - i.e. why did this particular attempt for a call fail?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4894
https://github.com/broadinstitute/cromwell/issues/4894:269,Availability,failure,failures,269,"Job Manager would like to distinguish between ""normal"" and ""exceptional"" failures, e.g. preemption versus something truly wrong. One implementation idea is to use the `causedBy` field but we have not vetted this at all. A/C distinguish between preemption and all other failures in the metadata response for an attempt - i.e. why did this particular attempt for a call fail?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4894
https://github.com/broadinstitute/cromwell/pull/4898:166,Energy Efficiency,schedul,scheduled,166,"Or, ""unflakify `WriteMetadataSpec`"". This test's assertions that metadata write batching and retrying would happen in the way it was expecting them to was upset if a scheduled metadata flush occurred before the first batch was full. This was leading to a non-trivial rate of sbt tests failing with messages like ""expected 50 database writes but got 52""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4898
https://github.com/broadinstitute/cromwell/pull/4898:298,Integrability,message,messages,298,"Or, ""unflakify `WriteMetadataSpec`"". This test's assertions that metadata write batching and retrying would happen in the way it was expecting them to was upset if a scheduled metadata flush occurred before the first batch was full. This was leading to a non-trivial rate of sbt tests failing with messages like ""expected 50 database writes but got 52""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4898
https://github.com/broadinstitute/cromwell/pull/4898:42,Testability,test,test,42,"Or, ""unflakify `WriteMetadataSpec`"". This test's assertions that metadata write batching and retrying would happen in the way it was expecting them to was upset if a scheduled metadata flush occurred before the first batch was full. This was leading to a non-trivial rate of sbt tests failing with messages like ""expected 50 database writes but got 52""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4898
https://github.com/broadinstitute/cromwell/pull/4898:49,Testability,assert,assertions,49,"Or, ""unflakify `WriteMetadataSpec`"". This test's assertions that metadata write batching and retrying would happen in the way it was expecting them to was upset if a scheduled metadata flush occurred before the first batch was full. This was leading to a non-trivial rate of sbt tests failing with messages like ""expected 50 database writes but got 52""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4898
https://github.com/broadinstitute/cromwell/pull/4898:279,Testability,test,tests,279,"Or, ""unflakify `WriteMetadataSpec`"". This test's assertions that metadata write batching and retrying would happen in the way it was expecting them to was upset if a scheduled metadata flush occurred before the first batch was full. This was leading to a non-trivial rate of sbt tests failing with messages like ""expected 50 database writes but got 52""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4898
https://github.com/broadinstitute/cromwell/pull/4900:3542,Deployability,integrat,integrate,3542,"Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, and requires quite some expertise to navigate. I could not find very quickly what I needed, and the `synchronization` primitive works fine. It is also **just 2 lines of extra code**. So if the akka solution is quite elegant as well I would like to learn about that. If not, well, it is not too bad having 2 lines of understandable commented code that is not ""the proper way of doing things(TM)"".~~. * I used the SFS scalatests to make sure everything worked correctly. However this did not test whether the thread safety was working correctly. I have added a test wdl in centaur: `standardTestCases/cached_copy/cached_copy.wdl`. This workflow creates 10 jobs that read the same input file. This workflow will crash if the `cached-inputs` cache is not used in a thread-safe way. I tested this manually with `java -Dbackend.providers.Local.config.filesystems.local.localization.0=""cached-copy"" -jar server/target/scala-2.12/cromwell-41-*-SNAP.jar run centaur/src/main/resources/standardTestCases/cached_copy/cached_copy.wdl` . Is there a way to integrate such a test in scalatest file? I have tried the `.par` method, but that did not quite work. I hope you will consider this PR as it solves an important issue for us. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:2220,Integrability,synchroniz,synchronized,2220,"hard-linked to `<call_dir>/inputs`. When the same file is needed again it will check if the file already exists in the `cached-inputs` and then hard-link it. This saves all the time needed for copying. ### Technical notes; File collisions are prevented by appending the last modified time epoch millisecond to the name of the cached file. If the input file is changed between jobs it will create a new cache entry. Md5sums are not used because it is extremely compute intensive for large files. Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, and requires quite some expertise to navigate. I could not find very quickly what I needed, and the `synchronization` primitive works fine. It is also **just 2 lines of extra code**. So if the akka solution is quite elegant as well I would like to learn about that. If not, well, it is not too bad having 2 lines of understandable commented code that is not ""the proper way of doing things(TM)"".~~. * I used the SFS scalatests to make sure everything worked correctly. However this did not test whether the thread safety was working correctly. I have added a test wdl in centaur: `standardTestCases/cached_copy/cached_copy.wdl`. This workflow creates 10 jobs that read the same input file. This workflow will crash if the `cached-i",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:2600,Integrability,synchroniz,synchronization,2600,"anged between jobs it will create a new cache entry. Md5sums are not used because it is extremely compute intensive for large files. Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, and requires quite some expertise to navigate. I could not find very quickly what I needed, and the `synchronization` primitive works fine. It is also **just 2 lines of extra code**. So if the akka solution is quite elegant as well I would like to learn about that. If not, well, it is not too bad having 2 lines of understandable commented code that is not ""the proper way of doing things(TM)"".~~. * I used the SFS scalatests to make sure everything worked correctly. However this did not test whether the thread safety was working correctly. I have added a test wdl in centaur: `standardTestCases/cached_copy/cached_copy.wdl`. This workflow creates 10 jobs that read the same input file. This workflow will crash if the `cached-inputs` cache is not used in a thread-safe way. I tested this manually with `java -Dbackend.providers.Local.config.filesystems.local.localization.0=""cached-copy"" -jar server/target/scala-2.12/cromwell-41-*-SNAP.jar run centaur/src/main/resources/standardTestCases/cached_copy/cached_copy.wdl` . Is there a way to integrate such a test in scalatest file? I have tr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:3542,Integrability,integrat,integrate,3542,"Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, and requires quite some expertise to navigate. I could not find very quickly what I needed, and the `synchronization` primitive works fine. It is also **just 2 lines of extra code**. So if the akka solution is quite elegant as well I would like to learn about that. If not, well, it is not too bad having 2 lines of understandable commented code that is not ""the proper way of doing things(TM)"".~~. * I used the SFS scalatests to make sure everything worked correctly. However this did not test whether the thread safety was working correctly. I have added a test wdl in centaur: `standardTestCases/cached_copy/cached_copy.wdl`. This workflow creates 10 jobs that read the same input file. This workflow will crash if the `cached-inputs` cache is not used in a thread-safe way. I tested this manually with `java -Dbackend.providers.Local.config.filesystems.local.localization.0=""cached-copy"" -jar server/target/scala-2.12/cromwell-41-*-SNAP.jar run centaur/src/main/resources/standardTestCases/cached_copy/cached_copy.wdl` . Is there a way to integrate such a test in scalatest file? I have tried the `.par` method, but that did not quite work. I hope you will consider this PR as it solves an important issue for us. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:3337,Modifiability,config,config,3337,"Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, and requires quite some expertise to navigate. I could not find very quickly what I needed, and the `synchronization` primitive works fine. It is also **just 2 lines of extra code**. So if the akka solution is quite elegant as well I would like to learn about that. If not, well, it is not too bad having 2 lines of understandable commented code that is not ""the proper way of doing things(TM)"".~~. * I used the SFS scalatests to make sure everything worked correctly. However this did not test whether the thread safety was working correctly. I have added a test wdl in centaur: `standardTestCases/cached_copy/cached_copy.wdl`. This workflow creates 10 jobs that read the same input file. This workflow will crash if the `cached-inputs` cache is not used in a thread-safe way. I tested this manually with `java -Dbackend.providers.Local.config.filesystems.local.localization.0=""cached-copy"" -jar server/target/scala-2.12/cromwell-41-*-SNAP.jar run centaur/src/main/resources/standardTestCases/cached_copy/cached_copy.wdl` . Is there a way to integrate such a test in scalatest file? I have tried the `.par` method, but that did not quite work. I hope you will consider this PR as it solves an important issue for us. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:7,Performance,cache,cached-copy,7,"## Add cached-copy localization strategy. ### The problem. Containers are great, but soft-links cannot be used. Hard-links in the execution folder are fine, but these cannot be used across physical disks. Most HPC have a filesystem consisting of multiple disks. This means only `copy` can be used as a localization strategy. Say you want to align multiple samples to the reference genome. And the reference genome is on another physical disk. That means the reference genome is copied once for each sample. That takes a lot of IO time and a lot of physical disk space. This makes cromwell practically unusable for use with containers on a HPC. Multiple people have the same problem and proposed a solution in #2620, #3447 and #4711. . ### This solution. This PR aims to tackle the problem by adding a new strategy called `cached-copy`. Since hard-links do not work across physical disks and soft-links do not work, copying is unavoidable. However, instead of copying the file directly into `<call_dir>/inputs` the file is first copied to a cache in `<cromwell_root>/cached-inputs`. This should ensure that `cached-inputs` is on the same physical disk as `<call_dir>` (which is also in `<cromwell_root>`). The cached file is then hard-linked to `<call_dir>/inputs`. When the same file is needed again it will check if the file already exists in the `cached-inputs` and then hard-link it. This saves all the time needed for copying. ### Technical notes; File collisions are prevented by appending the last modified time epoch millisecond to the name of the cached file. If the input file is changed between jobs it will create a new cache entry. Md5sums are not used because it is extremely compute intensive for large files. Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. T",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:822,Performance,cache,cached-copy,822,"## Add cached-copy localization strategy. ### The problem. Containers are great, but soft-links cannot be used. Hard-links in the execution folder are fine, but these cannot be used across physical disks. Most HPC have a filesystem consisting of multiple disks. This means only `copy` can be used as a localization strategy. Say you want to align multiple samples to the reference genome. And the reference genome is on another physical disk. That means the reference genome is copied once for each sample. That takes a lot of IO time and a lot of physical disk space. This makes cromwell practically unusable for use with containers on a HPC. Multiple people have the same problem and proposed a solution in #2620, #3447 and #4711. . ### This solution. This PR aims to tackle the problem by adding a new strategy called `cached-copy`. Since hard-links do not work across physical disks and soft-links do not work, copying is unavoidable. However, instead of copying the file directly into `<call_dir>/inputs` the file is first copied to a cache in `<cromwell_root>/cached-inputs`. This should ensure that `cached-inputs` is on the same physical disk as `<call_dir>` (which is also in `<cromwell_root>`). The cached file is then hard-linked to `<call_dir>/inputs`. When the same file is needed again it will check if the file already exists in the `cached-inputs` and then hard-link it. This saves all the time needed for copying. ### Technical notes; File collisions are prevented by appending the last modified time epoch millisecond to the name of the cached file. If the input file is changed between jobs it will create a new cache entry. Md5sums are not used because it is extremely compute intensive for large files. Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. T",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:1040,Performance,cache,cache,1040,"ched-copy localization strategy. ### The problem. Containers are great, but soft-links cannot be used. Hard-links in the execution folder are fine, but these cannot be used across physical disks. Most HPC have a filesystem consisting of multiple disks. This means only `copy` can be used as a localization strategy. Say you want to align multiple samples to the reference genome. And the reference genome is on another physical disk. That means the reference genome is copied once for each sample. That takes a lot of IO time and a lot of physical disk space. This makes cromwell practically unusable for use with containers on a HPC. Multiple people have the same problem and proposed a solution in #2620, #3447 and #4711. . ### This solution. This PR aims to tackle the problem by adding a new strategy called `cached-copy`. Since hard-links do not work across physical disks and soft-links do not work, copying is unavoidable. However, instead of copying the file directly into `<call_dir>/inputs` the file is first copied to a cache in `<cromwell_root>/cached-inputs`. This should ensure that `cached-inputs` is on the same physical disk as `<call_dir>` (which is also in `<cromwell_root>`). The cached file is then hard-linked to `<call_dir>/inputs`. When the same file is needed again it will check if the file already exists in the `cached-inputs` and then hard-link it. This saves all the time needed for copying. ### Technical notes; File collisions are prevented by appending the last modified time epoch millisecond to the name of the cached file. If the input file is changed between jobs it will create a new cache entry. Md5sums are not used because it is extremely compute intensive for large files. Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:1066,Performance,cache,cached-inputs,1066,"ched-copy localization strategy. ### The problem. Containers are great, but soft-links cannot be used. Hard-links in the execution folder are fine, but these cannot be used across physical disks. Most HPC have a filesystem consisting of multiple disks. This means only `copy` can be used as a localization strategy. Say you want to align multiple samples to the reference genome. And the reference genome is on another physical disk. That means the reference genome is copied once for each sample. That takes a lot of IO time and a lot of physical disk space. This makes cromwell practically unusable for use with containers on a HPC. Multiple people have the same problem and proposed a solution in #2620, #3447 and #4711. . ### This solution. This PR aims to tackle the problem by adding a new strategy called `cached-copy`. Since hard-links do not work across physical disks and soft-links do not work, copying is unavoidable. However, instead of copying the file directly into `<call_dir>/inputs` the file is first copied to a cache in `<cromwell_root>/cached-inputs`. This should ensure that `cached-inputs` is on the same physical disk as `<call_dir>` (which is also in `<cromwell_root>`). The cached file is then hard-linked to `<call_dir>/inputs`. When the same file is needed again it will check if the file already exists in the `cached-inputs` and then hard-link it. This saves all the time needed for copying. ### Technical notes; File collisions are prevented by appending the last modified time epoch millisecond to the name of the cached file. If the input file is changed between jobs it will create a new cache entry. Md5sums are not used because it is extremely compute intensive for large files. Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:1107,Performance,cache,cached-inputs,1107,"lder are fine, but these cannot be used across physical disks. Most HPC have a filesystem consisting of multiple disks. This means only `copy` can be used as a localization strategy. Say you want to align multiple samples to the reference genome. And the reference genome is on another physical disk. That means the reference genome is copied once for each sample. That takes a lot of IO time and a lot of physical disk space. This makes cromwell practically unusable for use with containers on a HPC. Multiple people have the same problem and proposed a solution in #2620, #3447 and #4711. . ### This solution. This PR aims to tackle the problem by adding a new strategy called `cached-copy`. Since hard-links do not work across physical disks and soft-links do not work, copying is unavoidable. However, instead of copying the file directly into `<call_dir>/inputs` the file is first copied to a cache in `<cromwell_root>/cached-inputs`. This should ensure that `cached-inputs` is on the same physical disk as `<call_dir>` (which is also in `<cromwell_root>`). The cached file is then hard-linked to `<call_dir>/inputs`. When the same file is needed again it will check if the file already exists in the `cached-inputs` and then hard-link it. This saves all the time needed for copying. ### Technical notes; File collisions are prevented by appending the last modified time epoch millisecond to the name of the cached file. If the input file is changed between jobs it will create a new cache entry. Md5sums are not used because it is extremely compute intensive for large files. Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:1209,Performance,cache,cached,1209,"nsisting of multiple disks. This means only `copy` can be used as a localization strategy. Say you want to align multiple samples to the reference genome. And the reference genome is on another physical disk. That means the reference genome is copied once for each sample. That takes a lot of IO time and a lot of physical disk space. This makes cromwell practically unusable for use with containers on a HPC. Multiple people have the same problem and proposed a solution in #2620, #3447 and #4711. . ### This solution. This PR aims to tackle the problem by adding a new strategy called `cached-copy`. Since hard-links do not work across physical disks and soft-links do not work, copying is unavoidable. However, instead of copying the file directly into `<call_dir>/inputs` the file is first copied to a cache in `<cromwell_root>/cached-inputs`. This should ensure that `cached-inputs` is on the same physical disk as `<call_dir>` (which is also in `<cromwell_root>`). The cached file is then hard-linked to `<call_dir>/inputs`. When the same file is needed again it will check if the file already exists in the `cached-inputs` and then hard-link it. This saves all the time needed for copying. ### Technical notes; File collisions are prevented by appending the last modified time epoch millisecond to the name of the cached file. If the input file is changed between jobs it will create a new cache entry. Md5sums are not used because it is extremely compute intensive for large files. Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:1349,Performance,cache,cached-inputs,1349,"Say you want to align multiple samples to the reference genome. And the reference genome is on another physical disk. That means the reference genome is copied once for each sample. That takes a lot of IO time and a lot of physical disk space. This makes cromwell practically unusable for use with containers on a HPC. Multiple people have the same problem and proposed a solution in #2620, #3447 and #4711. . ### This solution. This PR aims to tackle the problem by adding a new strategy called `cached-copy`. Since hard-links do not work across physical disks and soft-links do not work, copying is unavoidable. However, instead of copying the file directly into `<call_dir>/inputs` the file is first copied to a cache in `<cromwell_root>/cached-inputs`. This should ensure that `cached-inputs` is on the same physical disk as `<call_dir>` (which is also in `<cromwell_root>`). The cached file is then hard-linked to `<call_dir>/inputs`. When the same file is needed again it will check if the file already exists in the `cached-inputs` and then hard-link it. This saves all the time needed for copying. ### Technical notes; File collisions are prevented by appending the last modified time epoch millisecond to the name of the cached file. If the input file is changed between jobs it will create a new cache entry. Md5sums are not used because it is extremely compute intensive for large files. Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:1555,Performance,cache,cached,1555," sample. That takes a lot of IO time and a lot of physical disk space. This makes cromwell practically unusable for use with containers on a HPC. Multiple people have the same problem and proposed a solution in #2620, #3447 and #4711. . ### This solution. This PR aims to tackle the problem by adding a new strategy called `cached-copy`. Since hard-links do not work across physical disks and soft-links do not work, copying is unavoidable. However, instead of copying the file directly into `<call_dir>/inputs` the file is first copied to a cache in `<cromwell_root>/cached-inputs`. This should ensure that `cached-inputs` is on the same physical disk as `<call_dir>` (which is also in `<cromwell_root>`). The cached file is then hard-linked to `<call_dir>/inputs`. When the same file is needed again it will check if the file already exists in the `cached-inputs` and then hard-link it. This saves all the time needed for copying. ### Technical notes; File collisions are prevented by appending the last modified time epoch millisecond to the name of the cached file. If the input file is changed between jobs it will create a new cache entry. Md5sums are not used because it is extremely compute intensive for large files. Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:1631,Performance,cache,cache,1631,"able for use with containers on a HPC. Multiple people have the same problem and proposed a solution in #2620, #3447 and #4711. . ### This solution. This PR aims to tackle the problem by adding a new strategy called `cached-copy`. Since hard-links do not work across physical disks and soft-links do not work, copying is unavoidable. However, instead of copying the file directly into `<call_dir>/inputs` the file is first copied to a cache in `<cromwell_root>/cached-inputs`. This should ensure that `cached-inputs` is on the same physical disk as `<call_dir>` (which is also in `<cromwell_root>`). The cached file is then hard-linked to `<call_dir>/inputs`. When the same file is needed again it will check if the file already exists in the `cached-inputs` and then hard-link it. This saves all the time needed for copying. ### Technical notes; File collisions are prevented by appending the last modified time epoch millisecond to the name of the cached file. If the input file is changed between jobs it will create a new cache entry. Md5sums are not used because it is extremely compute intensive for large files. Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, and requires quite some expertise to navigate. I could not find very quickly what I needed, and the `synch",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:1851,Performance,cache,cached-inputs,1851,"d-copy`. Since hard-links do not work across physical disks and soft-links do not work, copying is unavoidable. However, instead of copying the file directly into `<call_dir>/inputs` the file is first copied to a cache in `<cromwell_root>/cached-inputs`. This should ensure that `cached-inputs` is on the same physical disk as `<call_dir>` (which is also in `<cromwell_root>`). The cached file is then hard-linked to `<call_dir>/inputs`. When the same file is needed again it will check if the file already exists in the `cached-inputs` and then hard-link it. This saves all the time needed for copying. ### Technical notes; File collisions are prevented by appending the last modified time epoch millisecond to the name of the cached file. If the input file is changed between jobs it will create a new cache entry. Md5sums are not used because it is extremely compute intensive for large files. Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, and requires quite some expertise to navigate. I could not find very quickly what I needed, and the `synchronization` primitive works fine. It is also **just 2 lines of extra code**. So if the akka solution is quite elegant as well I would like to learn about that. If not, well, it is not too bad having 2 lines of understandabl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:2140,Performance,cache,cache,2140,"cal disk as `<call_dir>` (which is also in `<cromwell_root>`). The cached file is then hard-linked to `<call_dir>/inputs`. When the same file is needed again it will check if the file already exists in the `cached-inputs` and then hard-link it. This saves all the time needed for copying. ### Technical notes; File collisions are prevented by appending the last modified time epoch millisecond to the name of the cached file. If the input file is changed between jobs it will create a new cache entry. Md5sums are not used because it is extremely compute intensive for large files. Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, and requires quite some expertise to navigate. I could not find very quickly what I needed, and the `synchronization` primitive works fine. It is also **just 2 lines of extra code**. So if the akka solution is quite elegant as well I would like to learn about that. If not, well, it is not too bad having 2 lines of understandable commented code that is not ""the proper way of doing things(TM)"".~~. * I used the SFS scalatests to make sure everything worked correctly. However this did not test whether the thread safety was working correctly. I have added a test wdl in centaur: `standardTestCases/cached_copy/cached_copy.wdl`. This workflow ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:3222,Performance,cache,cached-inputs,3222,"Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, and requires quite some expertise to navigate. I could not find very quickly what I needed, and the `synchronization` primitive works fine. It is also **just 2 lines of extra code**. So if the akka solution is quite elegant as well I would like to learn about that. If not, well, it is not too bad having 2 lines of understandable commented code that is not ""the proper way of doing things(TM)"".~~. * I used the SFS scalatests to make sure everything worked correctly. However this did not test whether the thread safety was working correctly. I have added a test wdl in centaur: `standardTestCases/cached_copy/cached_copy.wdl`. This workflow creates 10 jobs that read the same input file. This workflow will crash if the `cached-inputs` cache is not used in a thread-safe way. I tested this manually with `java -Dbackend.providers.Local.config.filesystems.local.localization.0=""cached-copy"" -jar server/target/scala-2.12/cromwell-41-*-SNAP.jar run centaur/src/main/resources/standardTestCases/cached_copy/cached_copy.wdl` . Is there a way to integrate such a test in scalatest file? I have tried the `.par` method, but that did not quite work. I hope you will consider this PR as it solves an important issue for us. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:3237,Performance,cache,cache,3237,"Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, and requires quite some expertise to navigate. I could not find very quickly what I needed, and the `synchronization` primitive works fine. It is also **just 2 lines of extra code**. So if the akka solution is quite elegant as well I would like to learn about that. If not, well, it is not too bad having 2 lines of understandable commented code that is not ""the proper way of doing things(TM)"".~~. * I used the SFS scalatests to make sure everything worked correctly. However this did not test whether the thread safety was working correctly. I have added a test wdl in centaur: `standardTestCases/cached_copy/cached_copy.wdl`. This workflow creates 10 jobs that read the same input file. This workflow will crash if the `cached-inputs` cache is not used in a thread-safe way. I tested this manually with `java -Dbackend.providers.Local.config.filesystems.local.localization.0=""cached-copy"" -jar server/target/scala-2.12/cromwell-41-*-SNAP.jar run centaur/src/main/resources/standardTestCases/cached_copy/cached_copy.wdl` . Is there a way to integrate such a test in scalatest file? I have tried the `.par` method, but that did not quite work. I hope you will consider this PR as it solves an important issue for us. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:3378,Performance,cache,cached-copy,3378,"Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, and requires quite some expertise to navigate. I could not find very quickly what I needed, and the `synchronization` primitive works fine. It is also **just 2 lines of extra code**. So if the akka solution is quite elegant as well I would like to learn about that. If not, well, it is not too bad having 2 lines of understandable commented code that is not ""the proper way of doing things(TM)"".~~. * I used the SFS scalatests to make sure everything worked correctly. However this did not test whether the thread safety was working correctly. I have added a test wdl in centaur: `standardTestCases/cached_copy/cached_copy.wdl`. This workflow creates 10 jobs that read the same input file. This workflow will crash if the `cached-inputs` cache is not used in a thread-safe way. I tested this manually with `java -Dbackend.providers.Local.config.filesystems.local.localization.0=""cached-copy"" -jar server/target/scala-2.12/cromwell-41-*-SNAP.jar run centaur/src/main/resources/standardTestCases/cached_copy/cached_copy.wdl` . Is there a way to integrate such a test in scalatest file? I have tried the `.par` method, but that did not quite work. I hope you will consider this PR as it solves an important issue for us. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:3013,Safety,safe,safety,3013,"Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, and requires quite some expertise to navigate. I could not find very quickly what I needed, and the `synchronization` primitive works fine. It is also **just 2 lines of extra code**. So if the akka solution is quite elegant as well I would like to learn about that. If not, well, it is not too bad having 2 lines of understandable commented code that is not ""the proper way of doing things(TM)"".~~. * I used the SFS scalatests to make sure everything worked correctly. However this did not test whether the thread safety was working correctly. I have added a test wdl in centaur: `standardTestCases/cached_copy/cached_copy.wdl`. This workflow creates 10 jobs that read the same input file. This workflow will crash if the `cached-inputs` cache is not used in a thread-safe way. I tested this manually with `java -Dbackend.providers.Local.config.filesystems.local.localization.0=""cached-copy"" -jar server/target/scala-2.12/cromwell-41-*-SNAP.jar run centaur/src/main/resources/standardTestCases/cached_copy/cached_copy.wdl` . Is there a way to integrate such a test in scalatest file? I have tried the `.par` method, but that did not quite work. I hope you will consider this PR as it solves an important issue for us. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:3267,Safety,safe,safe,3267,"Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, and requires quite some expertise to navigate. I could not find very quickly what I needed, and the `synchronization` primitive works fine. It is also **just 2 lines of extra code**. So if the akka solution is quite elegant as well I would like to learn about that. If not, well, it is not too bad having 2 lines of understandable commented code that is not ""the proper way of doing things(TM)"".~~. * I used the SFS scalatests to make sure everything worked correctly. However this did not test whether the thread safety was working correctly. I have added a test wdl in centaur: `standardTestCases/cached_copy/cached_copy.wdl`. This workflow creates 10 jobs that read the same input file. This workflow will crash if the `cached-inputs` cache is not used in a thread-safe way. I tested this manually with `java -Dbackend.providers.Local.config.filesystems.local.localization.0=""cached-copy"" -jar server/target/scala-2.12/cromwell-41-*-SNAP.jar run centaur/src/main/resources/standardTestCases/cached_copy/cached_copy.wdl` . Is there a way to integrate such a test in scalatest file? I have tried the `.par` method, but that did not quite work. I hope you will consider this PR as it solves an important issue for us. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:1805,Testability,test,test,1805,"d-copy`. Since hard-links do not work across physical disks and soft-links do not work, copying is unavoidable. However, instead of copying the file directly into `<call_dir>/inputs` the file is first copied to a cache in `<cromwell_root>/cached-inputs`. This should ensure that `cached-inputs` is on the same physical disk as `<call_dir>` (which is also in `<cromwell_root>`). The cached file is then hard-linked to `<call_dir>/inputs`. When the same file is needed again it will check if the file already exists in the `cached-inputs` and then hard-link it. This saves all the time needed for copying. ### Technical notes; File collisions are prevented by appending the last modified time epoch millisecond to the name of the cached file. If the input file is changed between jobs it will create a new cache entry. Md5sums are not used because it is extremely compute intensive for large files. Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, and requires quite some expertise to navigate. I could not find very quickly what I needed, and the `synchronization` primitive works fine. It is also **just 2 lines of extra code**. So if the akka solution is quite elegant as well I would like to learn about that. If not, well, it is not too bad having 2 lines of understandabl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:1836,Testability,test,tests,1836,"d-copy`. Since hard-links do not work across physical disks and soft-links do not work, copying is unavoidable. However, instead of copying the file directly into `<call_dir>/inputs` the file is first copied to a cache in `<cromwell_root>/cached-inputs`. This should ensure that `cached-inputs` is on the same physical disk as `<call_dir>` (which is also in `<cromwell_root>`). The cached file is then hard-linked to `<call_dir>/inputs`. When the same file is needed again it will check if the file already exists in the `cached-inputs` and then hard-link it. This saves all the time needed for copying. ### Technical notes; File collisions are prevented by appending the last modified time epoch millisecond to the name of the cached file. If the input file is changed between jobs it will create a new cache entry. Md5sums are not used because it is extremely compute intensive for large files. Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, and requires quite some expertise to navigate. I could not find very quickly what I needed, and the `synchronization` primitive works fine. It is also **just 2 lines of extra code**. So if the akka solution is quite elegant as well I would like to learn about that. If not, well, it is not too bad having 2 lines of understandabl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:2989,Testability,test,test,2989,"Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, and requires quite some expertise to navigate. I could not find very quickly what I needed, and the `synchronization` primitive works fine. It is also **just 2 lines of extra code**. So if the akka solution is quite elegant as well I would like to learn about that. If not, well, it is not too bad having 2 lines of understandable commented code that is not ""the proper way of doing things(TM)"".~~. * I used the SFS scalatests to make sure everything worked correctly. However this did not test whether the thread safety was working correctly. I have added a test wdl in centaur: `standardTestCases/cached_copy/cached_copy.wdl`. This workflow creates 10 jobs that read the same input file. This workflow will crash if the `cached-inputs` cache is not used in a thread-safe way. I tested this manually with `java -Dbackend.providers.Local.config.filesystems.local.localization.0=""cached-copy"" -jar server/target/scala-2.12/cromwell-41-*-SNAP.jar run centaur/src/main/resources/standardTestCases/cached_copy/cached_copy.wdl` . Is there a way to integrate such a test in scalatest file? I have tried the `.par` method, but that did not quite work. I hope you will consider this PR as it solves an important issue for us. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:3058,Testability,test,test,3058,"Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, and requires quite some expertise to navigate. I could not find very quickly what I needed, and the `synchronization` primitive works fine. It is also **just 2 lines of extra code**. So if the akka solution is quite elegant as well I would like to learn about that. If not, well, it is not too bad having 2 lines of understandable commented code that is not ""the proper way of doing things(TM)"".~~. * I used the SFS scalatests to make sure everything worked correctly. However this did not test whether the thread safety was working correctly. I have added a test wdl in centaur: `standardTestCases/cached_copy/cached_copy.wdl`. This workflow creates 10 jobs that read the same input file. This workflow will crash if the `cached-inputs` cache is not used in a thread-safe way. I tested this manually with `java -Dbackend.providers.Local.config.filesystems.local.localization.0=""cached-copy"" -jar server/target/scala-2.12/cromwell-41-*-SNAP.jar run centaur/src/main/resources/standardTestCases/cached_copy/cached_copy.wdl` . Is there a way to integrate such a test in scalatest file? I have tried the `.par` method, but that did not quite work. I hope you will consider this PR as it solves an important issue for us. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:3279,Testability,test,tested,3279,"Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, and requires quite some expertise to navigate. I could not find very quickly what I needed, and the `synchronization` primitive works fine. It is also **just 2 lines of extra code**. So if the akka solution is quite elegant as well I would like to learn about that. If not, well, it is not too bad having 2 lines of understandable commented code that is not ""the proper way of doing things(TM)"".~~. * I used the SFS scalatests to make sure everything worked correctly. However this did not test whether the thread safety was working correctly. I have added a test wdl in centaur: `standardTestCases/cached_copy/cached_copy.wdl`. This workflow creates 10 jobs that read the same input file. This workflow will crash if the `cached-inputs` cache is not used in a thread-safe way. I tested this manually with `java -Dbackend.providers.Local.config.filesystems.local.localization.0=""cached-copy"" -jar server/target/scala-2.12/cromwell-41-*-SNAP.jar run centaur/src/main/resources/standardTestCases/cached_copy/cached_copy.wdl` . Is there a way to integrate such a test in scalatest file? I have tried the `.par` method, but that did not quite work. I hope you will consider this PR as it solves an important issue for us. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:3559,Testability,test,test,3559,"Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, and requires quite some expertise to navigate. I could not find very quickly what I needed, and the `synchronization` primitive works fine. It is also **just 2 lines of extra code**. So if the akka solution is quite elegant as well I would like to learn about that. If not, well, it is not too bad having 2 lines of understandable commented code that is not ""the proper way of doing things(TM)"".~~. * I used the SFS scalatests to make sure everything worked correctly. However this did not test whether the thread safety was working correctly. I have added a test wdl in centaur: `standardTestCases/cached_copy/cached_copy.wdl`. This workflow creates 10 jobs that read the same input file. This workflow will crash if the `cached-inputs` cache is not used in a thread-safe way. I tested this manually with `java -Dbackend.providers.Local.config.filesystems.local.localization.0=""cached-copy"" -jar server/target/scala-2.12/cromwell-41-*-SNAP.jar run centaur/src/main/resources/standardTestCases/cached_copy/cached_copy.wdl` . Is there a way to integrate such a test in scalatest file? I have tried the `.par` method, but that did not quite work. I hope you will consider this PR as it solves an important issue for us. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/pull/4900:2747,Usability,learn,learn,2747,". Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, and requires quite some expertise to navigate. I could not find very quickly what I needed, and the `synchronization` primitive works fine. It is also **just 2 lines of extra code**. So if the akka solution is quite elegant as well I would like to learn about that. If not, well, it is not too bad having 2 lines of understandable commented code that is not ""the proper way of doing things(TM)"".~~. * I used the SFS scalatests to make sure everything worked correctly. However this did not test whether the thread safety was working correctly. I have added a test wdl in centaur: `standardTestCases/cached_copy/cached_copy.wdl`. This workflow creates 10 jobs that read the same input file. This workflow will crash if the `cached-inputs` cache is not used in a thread-safe way. I tested this manually with `java -Dbackend.providers.Local.config.filesystems.local.localization.0=""cached-copy"" -jar server/target/scala-2.12/cromwell-41-*-SNAP.jar run centaur/src/main/resources/standardTestCases/cached_copy/cached_copy.wdl` . Is there a way to integrate such a test in scalatest file? I have tried the `.par` method, but that did not quite work. I hope you will consider this PR as it solves an important issue for us. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900
https://github.com/broadinstitute/cromwell/issues/4901:477,Availability,echo,echo,477,"**Backend**: PAPIv2; **Cromwell version**: 38-6725312. When a task output is referenced by a variable, `read_*` functions fail to delocalize a file if the file name is referenced by a variable, instead of as a literal string. This might be related to https://github.com/broadinstitute/cromwell/issues/3698. Example:; ```wdl; version 1.0. workflow TestFailureDelocalize {; call Test. output {; String test = Test.out; }; }. task Test {; String testFile = ""test.txt"". command {; echo OK > ~{testFile}; }. output {; String out = read_string(testFile); }. runtime {; docker: ""debian:stable-slim""; }; }; ```. If I change the syntax to; ```wdl; String out = read_string(""~{testFile}""); ```; then the file is delocalized successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4901
https://github.com/broadinstitute/cromwell/issues/4901:93,Modifiability,variab,variable,93,"**Backend**: PAPIv2; **Cromwell version**: 38-6725312. When a task output is referenced by a variable, `read_*` functions fail to delocalize a file if the file name is referenced by a variable, instead of as a literal string. This might be related to https://github.com/broadinstitute/cromwell/issues/3698. Example:; ```wdl; version 1.0. workflow TestFailureDelocalize {; call Test. output {; String test = Test.out; }; }. task Test {; String testFile = ""test.txt"". command {; echo OK > ~{testFile}; }. output {; String out = read_string(testFile); }. runtime {; docker: ""debian:stable-slim""; }; }; ```. If I change the syntax to; ```wdl; String out = read_string(""~{testFile}""); ```; then the file is delocalized successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4901
https://github.com/broadinstitute/cromwell/issues/4901:184,Modifiability,variab,variable,184,"**Backend**: PAPIv2; **Cromwell version**: 38-6725312. When a task output is referenced by a variable, `read_*` functions fail to delocalize a file if the file name is referenced by a variable, instead of as a literal string. This might be related to https://github.com/broadinstitute/cromwell/issues/3698. Example:; ```wdl; version 1.0. workflow TestFailureDelocalize {; call Test. output {; String test = Test.out; }; }. task Test {; String testFile = ""test.txt"". command {; echo OK > ~{testFile}; }. output {; String out = read_string(testFile); }. runtime {; docker: ""debian:stable-slim""; }; }; ```. If I change the syntax to; ```wdl; String out = read_string(""~{testFile}""); ```; then the file is delocalized successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4901
https://github.com/broadinstitute/cromwell/issues/4901:347,Testability,Test,TestFailureDelocalize,347,"**Backend**: PAPIv2; **Cromwell version**: 38-6725312. When a task output is referenced by a variable, `read_*` functions fail to delocalize a file if the file name is referenced by a variable, instead of as a literal string. This might be related to https://github.com/broadinstitute/cromwell/issues/3698. Example:; ```wdl; version 1.0. workflow TestFailureDelocalize {; call Test. output {; String test = Test.out; }; }. task Test {; String testFile = ""test.txt"". command {; echo OK > ~{testFile}; }. output {; String out = read_string(testFile); }. runtime {; docker: ""debian:stable-slim""; }; }; ```. If I change the syntax to; ```wdl; String out = read_string(""~{testFile}""); ```; then the file is delocalized successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4901
https://github.com/broadinstitute/cromwell/issues/4901:377,Testability,Test,Test,377,"**Backend**: PAPIv2; **Cromwell version**: 38-6725312. When a task output is referenced by a variable, `read_*` functions fail to delocalize a file if the file name is referenced by a variable, instead of as a literal string. This might be related to https://github.com/broadinstitute/cromwell/issues/3698. Example:; ```wdl; version 1.0. workflow TestFailureDelocalize {; call Test. output {; String test = Test.out; }; }. task Test {; String testFile = ""test.txt"". command {; echo OK > ~{testFile}; }. output {; String out = read_string(testFile); }. runtime {; docker: ""debian:stable-slim""; }; }; ```. If I change the syntax to; ```wdl; String out = read_string(""~{testFile}""); ```; then the file is delocalized successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4901
https://github.com/broadinstitute/cromwell/issues/4901:400,Testability,test,test,400,"**Backend**: PAPIv2; **Cromwell version**: 38-6725312. When a task output is referenced by a variable, `read_*` functions fail to delocalize a file if the file name is referenced by a variable, instead of as a literal string. This might be related to https://github.com/broadinstitute/cromwell/issues/3698. Example:; ```wdl; version 1.0. workflow TestFailureDelocalize {; call Test. output {; String test = Test.out; }; }. task Test {; String testFile = ""test.txt"". command {; echo OK > ~{testFile}; }. output {; String out = read_string(testFile); }. runtime {; docker: ""debian:stable-slim""; }; }; ```. If I change the syntax to; ```wdl; String out = read_string(""~{testFile}""); ```; then the file is delocalized successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4901
https://github.com/broadinstitute/cromwell/issues/4901:407,Testability,Test,Test,407,"**Backend**: PAPIv2; **Cromwell version**: 38-6725312. When a task output is referenced by a variable, `read_*` functions fail to delocalize a file if the file name is referenced by a variable, instead of as a literal string. This might be related to https://github.com/broadinstitute/cromwell/issues/3698. Example:; ```wdl; version 1.0. workflow TestFailureDelocalize {; call Test. output {; String test = Test.out; }; }. task Test {; String testFile = ""test.txt"". command {; echo OK > ~{testFile}; }. output {; String out = read_string(testFile); }. runtime {; docker: ""debian:stable-slim""; }; }; ```. If I change the syntax to; ```wdl; String out = read_string(""~{testFile}""); ```; then the file is delocalized successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4901
https://github.com/broadinstitute/cromwell/issues/4901:428,Testability,Test,Test,428,"**Backend**: PAPIv2; **Cromwell version**: 38-6725312. When a task output is referenced by a variable, `read_*` functions fail to delocalize a file if the file name is referenced by a variable, instead of as a literal string. This might be related to https://github.com/broadinstitute/cromwell/issues/3698. Example:; ```wdl; version 1.0. workflow TestFailureDelocalize {; call Test. output {; String test = Test.out; }; }. task Test {; String testFile = ""test.txt"". command {; echo OK > ~{testFile}; }. output {; String out = read_string(testFile); }. runtime {; docker: ""debian:stable-slim""; }; }; ```. If I change the syntax to; ```wdl; String out = read_string(""~{testFile}""); ```; then the file is delocalized successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4901
https://github.com/broadinstitute/cromwell/issues/4901:443,Testability,test,testFile,443,"**Backend**: PAPIv2; **Cromwell version**: 38-6725312. When a task output is referenced by a variable, `read_*` functions fail to delocalize a file if the file name is referenced by a variable, instead of as a literal string. This might be related to https://github.com/broadinstitute/cromwell/issues/3698. Example:; ```wdl; version 1.0. workflow TestFailureDelocalize {; call Test. output {; String test = Test.out; }; }. task Test {; String testFile = ""test.txt"". command {; echo OK > ~{testFile}; }. output {; String out = read_string(testFile); }. runtime {; docker: ""debian:stable-slim""; }; }; ```. If I change the syntax to; ```wdl; String out = read_string(""~{testFile}""); ```; then the file is delocalized successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4901
https://github.com/broadinstitute/cromwell/issues/4901:455,Testability,test,test,455,"**Backend**: PAPIv2; **Cromwell version**: 38-6725312. When a task output is referenced by a variable, `read_*` functions fail to delocalize a file if the file name is referenced by a variable, instead of as a literal string. This might be related to https://github.com/broadinstitute/cromwell/issues/3698. Example:; ```wdl; version 1.0. workflow TestFailureDelocalize {; call Test. output {; String test = Test.out; }; }. task Test {; String testFile = ""test.txt"". command {; echo OK > ~{testFile}; }. output {; String out = read_string(testFile); }. runtime {; docker: ""debian:stable-slim""; }; }; ```. If I change the syntax to; ```wdl; String out = read_string(""~{testFile}""); ```; then the file is delocalized successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4901
https://github.com/broadinstitute/cromwell/issues/4901:489,Testability,test,testFile,489,"**Backend**: PAPIv2; **Cromwell version**: 38-6725312. When a task output is referenced by a variable, `read_*` functions fail to delocalize a file if the file name is referenced by a variable, instead of as a literal string. This might be related to https://github.com/broadinstitute/cromwell/issues/3698. Example:; ```wdl; version 1.0. workflow TestFailureDelocalize {; call Test. output {; String test = Test.out; }; }. task Test {; String testFile = ""test.txt"". command {; echo OK > ~{testFile}; }. output {; String out = read_string(testFile); }. runtime {; docker: ""debian:stable-slim""; }; }; ```. If I change the syntax to; ```wdl; String out = read_string(""~{testFile}""); ```; then the file is delocalized successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4901
https://github.com/broadinstitute/cromwell/issues/4901:538,Testability,test,testFile,538,"**Backend**: PAPIv2; **Cromwell version**: 38-6725312. When a task output is referenced by a variable, `read_*` functions fail to delocalize a file if the file name is referenced by a variable, instead of as a literal string. This might be related to https://github.com/broadinstitute/cromwell/issues/3698. Example:; ```wdl; version 1.0. workflow TestFailureDelocalize {; call Test. output {; String test = Test.out; }; }. task Test {; String testFile = ""test.txt"". command {; echo OK > ~{testFile}; }. output {; String out = read_string(testFile); }. runtime {; docker: ""debian:stable-slim""; }; }; ```. If I change the syntax to; ```wdl; String out = read_string(""~{testFile}""); ```; then the file is delocalized successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4901
https://github.com/broadinstitute/cromwell/issues/4901:667,Testability,test,testFile,667,"**Backend**: PAPIv2; **Cromwell version**: 38-6725312. When a task output is referenced by a variable, `read_*` functions fail to delocalize a file if the file name is referenced by a variable, instead of as a literal string. This might be related to https://github.com/broadinstitute/cromwell/issues/3698. Example:; ```wdl; version 1.0. workflow TestFailureDelocalize {; call Test. output {; String test = Test.out; }; }. task Test {; String testFile = ""test.txt"". command {; echo OK > ~{testFile}; }. output {; String out = read_string(testFile); }. runtime {; docker: ""debian:stable-slim""; }; }; ```. If I change the syntax to; ```wdl; String out = read_string(""~{testFile}""); ```; then the file is delocalized successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4901
https://github.com/broadinstitute/cromwell/issues/4904:8,Deployability,integrat,integration,8,"For the integration tests Cromwell runs, it would be good for the test definition to assert the hash of the expected output and actual output. This particular case is to ensure that there are no changes to the engine that could cause the contents of the outputs to change over time, and alert the team when such a case happens.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4904
https://github.com/broadinstitute/cromwell/issues/4904:8,Integrability,integrat,integration,8,"For the integration tests Cromwell runs, it would be good for the test definition to assert the hash of the expected output and actual output. This particular case is to ensure that there are no changes to the engine that could cause the contents of the outputs to change over time, and alert the team when such a case happens.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4904
https://github.com/broadinstitute/cromwell/issues/4904:96,Security,hash,hash,96,"For the integration tests Cromwell runs, it would be good for the test definition to assert the hash of the expected output and actual output. This particular case is to ensure that there are no changes to the engine that could cause the contents of the outputs to change over time, and alert the team when such a case happens.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4904
https://github.com/broadinstitute/cromwell/issues/4904:20,Testability,test,tests,20,"For the integration tests Cromwell runs, it would be good for the test definition to assert the hash of the expected output and actual output. This particular case is to ensure that there are no changes to the engine that could cause the contents of the outputs to change over time, and alert the team when such a case happens.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4904
https://github.com/broadinstitute/cromwell/issues/4904:66,Testability,test,test,66,"For the integration tests Cromwell runs, it would be good for the test definition to assert the hash of the expected output and actual output. This particular case is to ensure that there are no changes to the engine that could cause the contents of the outputs to change over time, and alert the team when such a case happens.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4904
https://github.com/broadinstitute/cromwell/issues/4904:85,Testability,assert,assert,85,"For the integration tests Cromwell runs, it would be good for the test definition to assert the hash of the expected output and actual output. This particular case is to ensure that there are no changes to the engine that could cause the contents of the outputs to change over time, and alert the team when such a case happens.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4904
https://github.com/broadinstitute/cromwell/pull/4905:211,Energy Efficiency,reduce,reduces,211,https://github.com/broadinstitute/cromwell/issues/4877. I also have this (only barely tested) script that may be useful to other HPC users:. https://gist.github.com/EvanTheB/8f9e07746af0c84831fc17f94ac4672d. It reduces the load on qstat of having thousands of jobs running. Let me know if it can be improved or incorporated here somewhere,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4905
https://github.com/broadinstitute/cromwell/pull/4905:223,Performance,load,load,223,https://github.com/broadinstitute/cromwell/issues/4877. I also have this (only barely tested) script that may be useful to other HPC users:. https://gist.github.com/EvanTheB/8f9e07746af0c84831fc17f94ac4672d. It reduces the load on qstat of having thousands of jobs running. Let me know if it can be improved or incorporated here somewhere,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4905
https://github.com/broadinstitute/cromwell/pull/4905:86,Testability,test,tested,86,https://github.com/broadinstitute/cromwell/issues/4877. I also have this (only barely tested) script that may be useful to other HPC users:. https://gist.github.com/EvanTheB/8f9e07746af0c84831fc17f94ac4672d. It reduces the load on qstat of having thousands of jobs running. Let me know if it can be improved or incorporated here somewhere,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4905
https://github.com/broadinstitute/cromwell/issues/4908:350,Availability,error,error,350,"The singleton `JobExecutionTokenDispenserActor` seems to crash quite a bit with no cause currently known. The default Akka supervision strategy causes JETD to be restarted, but the reincarnated JETD knows nothing of its predecessors' token allocations. This will cause tokens to be dispensed as if none are outstanding and returned tokens to produce error messages like:. ```; 2019-04-26 19:40:20 [cromwell-system-akka.actor.default-dispatcher-1109] ERROR c.e.w.t.JobExecutionTokenDispenserActor - Job execution token returned from incorrect actor: 202ffd24-a696-4edc-b832-a75f1127fdb4-EngineJobExecutionActor-Gatk3BamToGVCF.PrintReads:1:1; ```. JETD restarts in production can be found by looking for `""log token queue events""` in Kibana where there is no corresponding Cromwell restart.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4908
https://github.com/broadinstitute/cromwell/issues/4908:450,Availability,ERROR,ERROR,450,"The singleton `JobExecutionTokenDispenserActor` seems to crash quite a bit with no cause currently known. The default Akka supervision strategy causes JETD to be restarted, but the reincarnated JETD knows nothing of its predecessors' token allocations. This will cause tokens to be dispensed as if none are outstanding and returned tokens to produce error messages like:. ```; 2019-04-26 19:40:20 [cromwell-system-akka.actor.default-dispatcher-1109] ERROR c.e.w.t.JobExecutionTokenDispenserActor - Job execution token returned from incorrect actor: 202ffd24-a696-4edc-b832-a75f1127fdb4-EngineJobExecutionActor-Gatk3BamToGVCF.PrintReads:1:1; ```. JETD restarts in production can be found by looking for `""log token queue events""` in Kibana where there is no corresponding Cromwell restart.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4908
https://github.com/broadinstitute/cromwell/issues/4908:356,Integrability,message,messages,356,"The singleton `JobExecutionTokenDispenserActor` seems to crash quite a bit with no cause currently known. The default Akka supervision strategy causes JETD to be restarted, but the reincarnated JETD knows nothing of its predecessors' token allocations. This will cause tokens to be dispensed as if none are outstanding and returned tokens to produce error messages like:. ```; 2019-04-26 19:40:20 [cromwell-system-akka.actor.default-dispatcher-1109] ERROR c.e.w.t.JobExecutionTokenDispenserActor - Job execution token returned from incorrect actor: 202ffd24-a696-4edc-b832-a75f1127fdb4-EngineJobExecutionActor-Gatk3BamToGVCF.PrintReads:1:1; ```. JETD restarts in production can be found by looking for `""log token queue events""` in Kibana where there is no corresponding Cromwell restart.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4908
https://github.com/broadinstitute/cromwell/issues/4908:714,Performance,queue,queue,714,"The singleton `JobExecutionTokenDispenserActor` seems to crash quite a bit with no cause currently known. The default Akka supervision strategy causes JETD to be restarted, but the reincarnated JETD knows nothing of its predecessors' token allocations. This will cause tokens to be dispensed as if none are outstanding and returned tokens to produce error messages like:. ```; 2019-04-26 19:40:20 [cromwell-system-akka.actor.default-dispatcher-1109] ERROR c.e.w.t.JobExecutionTokenDispenserActor - Job execution token returned from incorrect actor: 202ffd24-a696-4edc-b832-a75f1127fdb4-EngineJobExecutionActor-Gatk3BamToGVCF.PrintReads:1:1; ```. JETD restarts in production can be found by looking for `""log token queue events""` in Kibana where there is no corresponding Cromwell restart.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4908
https://github.com/broadinstitute/cromwell/issues/4908:704,Testability,log,log,704,"The singleton `JobExecutionTokenDispenserActor` seems to crash quite a bit with no cause currently known. The default Akka supervision strategy causes JETD to be restarted, but the reincarnated JETD knows nothing of its predecessors' token allocations. This will cause tokens to be dispensed as if none are outstanding and returned tokens to produce error messages like:. ```; 2019-04-26 19:40:20 [cromwell-system-akka.actor.default-dispatcher-1109] ERROR c.e.w.t.JobExecutionTokenDispenserActor - Job execution token returned from incorrect actor: 202ffd24-a696-4edc-b832-a75f1127fdb4-EngineJobExecutionActor-Gatk3BamToGVCF.PrintReads:1:1; ```. JETD restarts in production can be found by looking for `""log token queue events""` in Kibana where there is no corresponding Cromwell restart.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4908
https://github.com/broadinstitute/cromwell/pull/4909:124,Availability,recover,recover,124,Potential hotfix candidate but would be nice to know why these empty queues are appearing in the first place. An attempt to recover from (though probably not fix the underlying cause of) tokens going missing due to stack traces like:; ```; [cromwell-system-akka.actor.default-dispatcher-1158] ERROR akka.actor.OneForOneStrategy - dequeue on empty queue; java.util.NoSuchElementException: dequeue on empty queue; 	at scala.collection.immutable.Queue.dequeue(Queue.scala:155); 	at cromwell.engine.workflow.tokens.TokenQueue.recursingDequeue(TokenQueue.scala:63); 	at cromwell.engine.workflow.tokens.TokenQueue.dequeue(TokenQueue.scala:50); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1(RoundRobinQueueIterator.scala:46); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1$adapted(RoundRobinQueueIterator.scala:46); 	at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:415); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1169); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1159); 	at scala.collection.immutable.StreamIterator.$anonfun$next$1(Stream.scala:1058); 	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1052); 	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:144); 	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:132); 	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:104); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.findFirst(RoundRobinQueueIterator.scala:48); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:32); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:10); 	at scala.collection.Iterator$SliceIterator.next(Itera,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909
https://github.com/broadinstitute/cromwell/pull/4909:293,Availability,ERROR,ERROR,293,Potential hotfix candidate but would be nice to know why these empty queues are appearing in the first place. An attempt to recover from (though probably not fix the underlying cause of) tokens going missing due to stack traces like:; ```; [cromwell-system-akka.actor.default-dispatcher-1158] ERROR akka.actor.OneForOneStrategy - dequeue on empty queue; java.util.NoSuchElementException: dequeue on empty queue; 	at scala.collection.immutable.Queue.dequeue(Queue.scala:155); 	at cromwell.engine.workflow.tokens.TokenQueue.recursingDequeue(TokenQueue.scala:63); 	at cromwell.engine.workflow.tokens.TokenQueue.dequeue(TokenQueue.scala:50); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1(RoundRobinQueueIterator.scala:46); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1$adapted(RoundRobinQueueIterator.scala:46); 	at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:415); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1169); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1159); 	at scala.collection.immutable.StreamIterator.$anonfun$next$1(Stream.scala:1058); 	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1052); 	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:144); 	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:132); 	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:104); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.findFirst(RoundRobinQueueIterator.scala:48); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:32); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:10); 	at scala.collection.Iterator$SliceIterator.next(Itera,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909
https://github.com/broadinstitute/cromwell/pull/4909:10,Deployability,hotfix,hotfix,10,Potential hotfix candidate but would be nice to know why these empty queues are appearing in the first place. An attempt to recover from (though probably not fix the underlying cause of) tokens going missing due to stack traces like:; ```; [cromwell-system-akka.actor.default-dispatcher-1158] ERROR akka.actor.OneForOneStrategy - dequeue on empty queue; java.util.NoSuchElementException: dequeue on empty queue; 	at scala.collection.immutable.Queue.dequeue(Queue.scala:155); 	at cromwell.engine.workflow.tokens.TokenQueue.recursingDequeue(TokenQueue.scala:63); 	at cromwell.engine.workflow.tokens.TokenQueue.dequeue(TokenQueue.scala:50); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1(RoundRobinQueueIterator.scala:46); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1$adapted(RoundRobinQueueIterator.scala:46); 	at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:415); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1169); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1159); 	at scala.collection.immutable.StreamIterator.$anonfun$next$1(Stream.scala:1058); 	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1052); 	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:144); 	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:132); 	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:104); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.findFirst(RoundRobinQueueIterator.scala:48); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:32); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:10); 	at scala.collection.Iterator$SliceIterator.next(Itera,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909
https://github.com/broadinstitute/cromwell/pull/4909:835,Energy Efficiency,adapt,adapted,835,Potential hotfix candidate but would be nice to know why these empty queues are appearing in the first place. An attempt to recover from (though probably not fix the underlying cause of) tokens going missing due to stack traces like:; ```; [cromwell-system-akka.actor.default-dispatcher-1158] ERROR akka.actor.OneForOneStrategy - dequeue on empty queue; java.util.NoSuchElementException: dequeue on empty queue; 	at scala.collection.immutable.Queue.dequeue(Queue.scala:155); 	at cromwell.engine.workflow.tokens.TokenQueue.recursingDequeue(TokenQueue.scala:63); 	at cromwell.engine.workflow.tokens.TokenQueue.dequeue(TokenQueue.scala:50); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1(RoundRobinQueueIterator.scala:46); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1$adapted(RoundRobinQueueIterator.scala:46); 	at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:415); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1169); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1159); 	at scala.collection.immutable.StreamIterator.$anonfun$next$1(Stream.scala:1058); 	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1052); 	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:144); 	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:132); 	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:104); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.findFirst(RoundRobinQueueIterator.scala:48); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:32); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:10); 	at scala.collection.Iterator$SliceIterator.next(Itera,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909
https://github.com/broadinstitute/cromwell/pull/4909:835,Modifiability,adapt,adapted,835,Potential hotfix candidate but would be nice to know why these empty queues are appearing in the first place. An attempt to recover from (though probably not fix the underlying cause of) tokens going missing due to stack traces like:; ```; [cromwell-system-akka.actor.default-dispatcher-1158] ERROR akka.actor.OneForOneStrategy - dequeue on empty queue; java.util.NoSuchElementException: dequeue on empty queue; 	at scala.collection.immutable.Queue.dequeue(Queue.scala:155); 	at cromwell.engine.workflow.tokens.TokenQueue.recursingDequeue(TokenQueue.scala:63); 	at cromwell.engine.workflow.tokens.TokenQueue.dequeue(TokenQueue.scala:50); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1(RoundRobinQueueIterator.scala:46); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1$adapted(RoundRobinQueueIterator.scala:46); 	at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:415); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1169); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1159); 	at scala.collection.immutable.StreamIterator.$anonfun$next$1(Stream.scala:1058); 	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1052); 	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:144); 	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:132); 	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:104); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.findFirst(RoundRobinQueueIterator.scala:48); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:32); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:10); 	at scala.collection.Iterator$SliceIterator.next(Itera,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909
https://github.com/broadinstitute/cromwell/pull/4909:69,Performance,queue,queues,69,Potential hotfix candidate but would be nice to know why these empty queues are appearing in the first place. An attempt to recover from (though probably not fix the underlying cause of) tokens going missing due to stack traces like:; ```; [cromwell-system-akka.actor.default-dispatcher-1158] ERROR akka.actor.OneForOneStrategy - dequeue on empty queue; java.util.NoSuchElementException: dequeue on empty queue; 	at scala.collection.immutable.Queue.dequeue(Queue.scala:155); 	at cromwell.engine.workflow.tokens.TokenQueue.recursingDequeue(TokenQueue.scala:63); 	at cromwell.engine.workflow.tokens.TokenQueue.dequeue(TokenQueue.scala:50); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1(RoundRobinQueueIterator.scala:46); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1$adapted(RoundRobinQueueIterator.scala:46); 	at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:415); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1169); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1159); 	at scala.collection.immutable.StreamIterator.$anonfun$next$1(Stream.scala:1058); 	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1052); 	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:144); 	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:132); 	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:104); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.findFirst(RoundRobinQueueIterator.scala:48); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:32); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:10); 	at scala.collection.Iterator$SliceIterator.next(Itera,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909
https://github.com/broadinstitute/cromwell/pull/4909:347,Performance,queue,queue,347,Potential hotfix candidate but would be nice to know why these empty queues are appearing in the first place. An attempt to recover from (though probably not fix the underlying cause of) tokens going missing due to stack traces like:; ```; [cromwell-system-akka.actor.default-dispatcher-1158] ERROR akka.actor.OneForOneStrategy - dequeue on empty queue; java.util.NoSuchElementException: dequeue on empty queue; 	at scala.collection.immutable.Queue.dequeue(Queue.scala:155); 	at cromwell.engine.workflow.tokens.TokenQueue.recursingDequeue(TokenQueue.scala:63); 	at cromwell.engine.workflow.tokens.TokenQueue.dequeue(TokenQueue.scala:50); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1(RoundRobinQueueIterator.scala:46); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1$adapted(RoundRobinQueueIterator.scala:46); 	at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:415); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1169); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1159); 	at scala.collection.immutable.StreamIterator.$anonfun$next$1(Stream.scala:1058); 	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1052); 	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:144); 	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:132); 	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:104); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.findFirst(RoundRobinQueueIterator.scala:48); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:32); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:10); 	at scala.collection.Iterator$SliceIterator.next(Itera,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909
https://github.com/broadinstitute/cromwell/pull/4909:405,Performance,queue,queue,405,Potential hotfix candidate but would be nice to know why these empty queues are appearing in the first place. An attempt to recover from (though probably not fix the underlying cause of) tokens going missing due to stack traces like:; ```; [cromwell-system-akka.actor.default-dispatcher-1158] ERROR akka.actor.OneForOneStrategy - dequeue on empty queue; java.util.NoSuchElementException: dequeue on empty queue; 	at scala.collection.immutable.Queue.dequeue(Queue.scala:155); 	at cromwell.engine.workflow.tokens.TokenQueue.recursingDequeue(TokenQueue.scala:63); 	at cromwell.engine.workflow.tokens.TokenQueue.dequeue(TokenQueue.scala:50); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1(RoundRobinQueueIterator.scala:46); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1$adapted(RoundRobinQueueIterator.scala:46); 	at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:415); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1169); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1159); 	at scala.collection.immutable.StreamIterator.$anonfun$next$1(Stream.scala:1058); 	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1052); 	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:144); 	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:132); 	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:104); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.findFirst(RoundRobinQueueIterator.scala:48); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:32); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:10); 	at scala.collection.Iterator$SliceIterator.next(Itera,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909
https://github.com/broadinstitute/cromwell/pull/4909:443,Performance,Queue,Queue,443,Potential hotfix candidate but would be nice to know why these empty queues are appearing in the first place. An attempt to recover from (though probably not fix the underlying cause of) tokens going missing due to stack traces like:; ```; [cromwell-system-akka.actor.default-dispatcher-1158] ERROR akka.actor.OneForOneStrategy - dequeue on empty queue; java.util.NoSuchElementException: dequeue on empty queue; 	at scala.collection.immutable.Queue.dequeue(Queue.scala:155); 	at cromwell.engine.workflow.tokens.TokenQueue.recursingDequeue(TokenQueue.scala:63); 	at cromwell.engine.workflow.tokens.TokenQueue.dequeue(TokenQueue.scala:50); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1(RoundRobinQueueIterator.scala:46); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1$adapted(RoundRobinQueueIterator.scala:46); 	at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:415); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1169); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1159); 	at scala.collection.immutable.StreamIterator.$anonfun$next$1(Stream.scala:1058); 	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1052); 	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:144); 	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:132); 	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:104); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.findFirst(RoundRobinQueueIterator.scala:48); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:32); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:10); 	at scala.collection.Iterator$SliceIterator.next(Itera,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909
https://github.com/broadinstitute/cromwell/pull/4909:457,Performance,Queue,Queue,457,Potential hotfix candidate but would be nice to know why these empty queues are appearing in the first place. An attempt to recover from (though probably not fix the underlying cause of) tokens going missing due to stack traces like:; ```; [cromwell-system-akka.actor.default-dispatcher-1158] ERROR akka.actor.OneForOneStrategy - dequeue on empty queue; java.util.NoSuchElementException: dequeue on empty queue; 	at scala.collection.immutable.Queue.dequeue(Queue.scala:155); 	at cromwell.engine.workflow.tokens.TokenQueue.recursingDequeue(TokenQueue.scala:63); 	at cromwell.engine.workflow.tokens.TokenQueue.dequeue(TokenQueue.scala:50); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1(RoundRobinQueueIterator.scala:46); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1$adapted(RoundRobinQueueIterator.scala:46); 	at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:415); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1169); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1159); 	at scala.collection.immutable.StreamIterator.$anonfun$next$1(Stream.scala:1058); 	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1052); 	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:144); 	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:132); 	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:104); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.findFirst(RoundRobinQueueIterator.scala:48); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:32); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:10); 	at scala.collection.Iterator$SliceIterator.next(Itera,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909
https://github.com/broadinstitute/cromwell/pull/4909:124,Safety,recover,recover,124,Potential hotfix candidate but would be nice to know why these empty queues are appearing in the first place. An attempt to recover from (though probably not fix the underlying cause of) tokens going missing due to stack traces like:; ```; [cromwell-system-akka.actor.default-dispatcher-1158] ERROR akka.actor.OneForOneStrategy - dequeue on empty queue; java.util.NoSuchElementException: dequeue on empty queue; 	at scala.collection.immutable.Queue.dequeue(Queue.scala:155); 	at cromwell.engine.workflow.tokens.TokenQueue.recursingDequeue(TokenQueue.scala:63); 	at cromwell.engine.workflow.tokens.TokenQueue.dequeue(TokenQueue.scala:50); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1(RoundRobinQueueIterator.scala:46); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1$adapted(RoundRobinQueueIterator.scala:46); 	at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:415); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1169); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1159); 	at scala.collection.immutable.StreamIterator.$anonfun$next$1(Stream.scala:1058); 	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1052); 	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:144); 	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:132); 	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:104); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.findFirst(RoundRobinQueueIterator.scala:48); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:32); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:10); 	at scala.collection.Iterator$SliceIterator.next(Itera,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909
https://github.com/broadinstitute/cromwell/issues/4912:75,Security,hash,hash,75,"When the 'docker' runtime attribute of a task is specified with the sha256 hash, as in; runtime {; docker: ""quay.io/broadinstitute/viral-ngs:1.22.0@sha256:be458fe91e176102c9c8e2933c9ff430d899a0ec8f919939cbdb26f2d4c7d4be""; }; Cromwell reports exceptions such as; 2019-04-28 04:40:08,146 cromwell-system-akka.dispatchers.engine-dispatcher-39 WARN - BackendPreparationActor_for_d5800039:assemble_de\; novo_with_deplete.deplete_taxa:-1:1 [UUID(d5800039)]: Docker lookup failed; java.lang.Exception: Docker image quay.io/broadinstitute/viral-ngs@sha256:be458fe91e176102c9c8e2933c9ff430d899a0ec8f919939cbdb26f2d4c7\; d4be not found; at cromwell.engine.workflow.WorkflowDockerLookupActor.cromwell$engine$workflow$WorkflowDockerLookupActor$$handleLookupFailure(\; WorkflowDockerLookupActor.scala:194); at cromwell.engine.workflow.WorkflowDockerLookupActor$$anonfun$3.applyOrElse(WorkflowDockerLookupActor.scala:86); at cromwell.engine.workflow.WorkflowDockerLookupActor$$anonfun$3.applyOrElse(WorkflowDockerLookupActor.scala:70); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:684); at akka.actor.FSM.processEvent$(FSM.scala:681); at cromwell.engine.workflow.WorkflowDockerLookupActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowDockerLookupActor.scal\; a:37); at akka.actor.LoggingFSM.processEvent(FSM.scala:820); at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); at cromwell.engine.workflow.WorkflowDockerLookupActor.processEvent(WorkflowDockerLookupActor.scala:37); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at cromwell.docker.DockerClientHelper$$anonfun$dockerResponseReceive$1.applyOrElse(DockerClientHelper.scala:16); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4912
https://github.com/broadinstitute/cromwell/issues/4912:1264,Testability,Log,LoggingFSM,1264,romwell-system-akka.dispatchers.engine-dispatcher-39 WARN - BackendPreparationActor_for_d5800039:assemble_de\; novo_with_deplete.deplete_taxa:-1:1 [UUID(d5800039)]: Docker lookup failed; java.lang.Exception: Docker image quay.io/broadinstitute/viral-ngs@sha256:be458fe91e176102c9c8e2933c9ff430d899a0ec8f919939cbdb26f2d4c7\; d4be not found; at cromwell.engine.workflow.WorkflowDockerLookupActor.cromwell$engine$workflow$WorkflowDockerLookupActor$$handleLookupFailure(\; WorkflowDockerLookupActor.scala:194); at cromwell.engine.workflow.WorkflowDockerLookupActor$$anonfun$3.applyOrElse(WorkflowDockerLookupActor.scala:86); at cromwell.engine.workflow.WorkflowDockerLookupActor$$anonfun$3.applyOrElse(WorkflowDockerLookupActor.scala:70); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:684); at akka.actor.FSM.processEvent$(FSM.scala:681); at cromwell.engine.workflow.WorkflowDockerLookupActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowDockerLookupActor.scal\; a:37); at akka.actor.LoggingFSM.processEvent(FSM.scala:820); at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); at cromwell.engine.workflow.WorkflowDockerLookupActor.processEvent(WorkflowDockerLookupActor.scala:37); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at cromwell.docker.DockerClientHelper$$anonfun$dockerResponseReceive$1.applyOrElse(DockerClientHelper.scala:16); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); - at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.engine.workflow.WorkflowDockerLookupActor.aroundReceive(WorkflowDockerLookupActor.scala:37); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4912
https://github.com/broadinstitute/cromwell/issues/4912:1349,Testability,Log,LoggingFSM,1349,PreparationActor_for_d5800039:assemble_de\; novo_with_deplete.deplete_taxa:-1:1 [UUID(d5800039)]: Docker lookup failed; java.lang.Exception: Docker image quay.io/broadinstitute/viral-ngs@sha256:be458fe91e176102c9c8e2933c9ff430d899a0ec8f919939cbdb26f2d4c7\; d4be not found; at cromwell.engine.workflow.WorkflowDockerLookupActor.cromwell$engine$workflow$WorkflowDockerLookupActor$$handleLookupFailure(\; WorkflowDockerLookupActor.scala:194); at cromwell.engine.workflow.WorkflowDockerLookupActor$$anonfun$3.applyOrElse(WorkflowDockerLookupActor.scala:86); at cromwell.engine.workflow.WorkflowDockerLookupActor$$anonfun$3.applyOrElse(WorkflowDockerLookupActor.scala:70); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:684); at akka.actor.FSM.processEvent$(FSM.scala:681); at cromwell.engine.workflow.WorkflowDockerLookupActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowDockerLookupActor.scal\; a:37); at akka.actor.LoggingFSM.processEvent(FSM.scala:820); at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); at cromwell.engine.workflow.WorkflowDockerLookupActor.processEvent(WorkflowDockerLookupActor.scala:37); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at cromwell.docker.DockerClientHelper$$anonfun$dockerResponseReceive$1.applyOrElse(DockerClientHelper.scala:16); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); - at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.engine.workflow.WorkflowDockerLookupActor.aroundReceive(WorkflowDockerLookupActor.scala:37); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4912
https://github.com/broadinstitute/cromwell/issues/4912:1403,Testability,Log,LoggingFSM,1403,deplete.deplete_taxa:-1:1 [UUID(d5800039)]: Docker lookup failed; java.lang.Exception: Docker image quay.io/broadinstitute/viral-ngs@sha256:be458fe91e176102c9c8e2933c9ff430d899a0ec8f919939cbdb26f2d4c7\; d4be not found; at cromwell.engine.workflow.WorkflowDockerLookupActor.cromwell$engine$workflow$WorkflowDockerLookupActor$$handleLookupFailure(\; WorkflowDockerLookupActor.scala:194); at cromwell.engine.workflow.WorkflowDockerLookupActor$$anonfun$3.applyOrElse(WorkflowDockerLookupActor.scala:86); at cromwell.engine.workflow.WorkflowDockerLookupActor$$anonfun$3.applyOrElse(WorkflowDockerLookupActor.scala:70); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:684); at akka.actor.FSM.processEvent$(FSM.scala:681); at cromwell.engine.workflow.WorkflowDockerLookupActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowDockerLookupActor.scal\; a:37); at akka.actor.LoggingFSM.processEvent(FSM.scala:820); at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); at cromwell.engine.workflow.WorkflowDockerLookupActor.processEvent(WorkflowDockerLookupActor.scala:37); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at cromwell.docker.DockerClientHelper$$anonfun$dockerResponseReceive$1.applyOrElse(DockerClientHelper.scala:16); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); - at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.engine.workflow.WorkflowDockerLookupActor.aroundReceive(WorkflowDockerLookupActor.scala:37); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.di,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4912
https://github.com/broadinstitute/cromwell/issues/4913:1520,Availability,failure,failure-mode,1520,"e a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; The [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies the default type version.; #workflow-type-version: ""draft-2"". # To set a default hog group rather than defaulting to workflow ID:; #hogGroup: ""static""; }; }; ```; However, most are set with the equals sign:; ```; # Google configuration; google {. #application-name = ""cromwell"". # Default: just application default; #auths = [. # Application default; #{; # name = ""application-default""; # scheme = ""application_default""; #},. # Use a refresh token; #{; # name = ""user-via-refresh""; # scheme = ""refresh_token""; # client-id = ""secret_id""; # client-secr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913
https://github.com/broadinstitute/cromwell/issues/4913:1745,Availability,failure,failure-mode,1745,"O OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; The [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies the default type version.; #workflow-type-version: ""draft-2"". # To set a default hog group rather than defaulting to workflow ID:; #hogGroup: ""static""; }; }; ```; However, most are set with the equals sign:; ```; # Google configuration; google {. #application-name = ""cromwell"". # Default: just application default; #auths = [. # Application default; #{; # name = ""application-default""; # scheme = ""application_default""; #},. # Use a refresh token; #{; # name = ""user-via-refresh""; # scheme = ""refresh_token""; # client-id = ""secret_id""; # client-secret = ""secret_secret""; #},; (etc); ```. Do both of these work? If so, is one format preferred to minimize confusion?. What prompted me to write this issue is that setting some defaults isn't yet worki",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913
https://github.com/broadinstitute/cromwell/issues/4913:709,Deployability,configurat,configuration,709,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; The [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913
https://github.com/broadinstitute/cromwell/issues/4913:2221,Deployability,configurat,configuration,2221,"les.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies the default type version.; #workflow-type-version: ""draft-2"". # To set a default hog group rather than defaulting to workflow ID:; #hogGroup: ""static""; }; }; ```; However, most are set with the equals sign:; ```; # Google configuration; google {. #application-name = ""cromwell"". # Default: just application default; #auths = [. # Application default; #{; # name = ""application-default""; # scheme = ""application_default""; #},. # Use a refresh token; #{; # name = ""user-via-refresh""; # scheme = ""refresh_token""; # client-id = ""secret_id""; # client-secret = ""secret_secret""; #},; (etc); ```. Do both of these work? If so, is one format preferred to minimize confusion?. What prompted me to write this issue is that setting some defaults isn't yet working for me. This is probably an issue on my end, but the lack of consistency makes me wonder whether (e.g.) parameters set with colons don't actually take effect and the docs are wrong.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913
https://github.com/broadinstitute/cromwell/issues/4913:709,Modifiability,config,configuration,709,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; The [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913
https://github.com/broadinstitute/cromwell/issues/4913:2221,Modifiability,config,configuration,2221,"les.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies the default type version.; #workflow-type-version: ""draft-2"". # To set a default hog group rather than defaulting to workflow ID:; #hogGroup: ""static""; }; }; ```; However, most are set with the equals sign:; ```; # Google configuration; google {. #application-name = ""cromwell"". # Default: just application default; #auths = [. # Application default; #{; # name = ""application-default""; # scheme = ""application_default""; #},. # Use a refresh token; #{; # name = ""user-via-refresh""; # scheme = ""refresh_token""; # client-id = ""secret_id""; # client-secret = ""secret_secret""; #},; (etc); ```. Do both of these work? If so, is one format preferred to minimize confusion?. What prompted me to write this issue is that setting some defaults isn't yet working for me. This is probably an issue on my end, but the lack of consistency makes me wonder whether (e.g.) parameters set with colons don't actually take effect and the docs are wrong.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913
https://github.com/broadinstitute/cromwell/issues/4913:754,Security,PASSWORD,PASSWORDS,754,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; The [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913
https://github.com/broadinstitute/cromwell/issues/4913:1125,Security,encrypt,encrypted,1125,"lready answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; The [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies the default type version.; #workflow-type-version: ""draft-2"". # To set a default hog group rather than defaulting to workflow I",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913
https://github.com/broadinstitute/cromwell/issues/4913:1165,Security,encrypt,encrypted-fields,1165,"lready answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; The [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies the default type version.; #workflow-type-version: ""draft-2"". # To set a default hog group rather than defaulting to workflow I",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913
https://github.com/broadinstitute/cromwell/issues/4913:1211,Security,encrypt,encrypt,1211,"ll.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; The [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies the default type version.; #workflow-type-version: ""draft-2"". # To set a default hog group rather than defaulting to workflow ID:; #hogGroup: ""static""; }; }; ```; However, most are set with the equals sign:; ```; # Google configuration; google {. #applicat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913
https://github.com/broadinstitute/cromwell/issues/4913:1234,Security,encrypt,encrypted-fields,1234,"ll.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; The [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies the default type version.; #workflow-type-version: ""draft-2"". # To set a default hog group rather than defaulting to workflow ID:; #hogGroup: ""static""; }; }; ```; However, most are set with the equals sign:; ```; # Google configuration; google {. #applicat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913
https://github.com/broadinstitute/cromwell/issues/4913:1261,Security,encrypt,encryption-key,1261,"ll.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; The [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies the default type version.; #workflow-type-version: ""draft-2"". # To set a default hog group rather than defaulting to workflow ID:; #hogGroup: ""static""; }; }; ```; However, most are set with the equals sign:; ```; # Google configuration; google {. #applicat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913
https://github.com/broadinstitute/cromwell/issues/4913:1365,Testability,log,logs,1365,"/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; The [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies the default type version.; #workflow-type-version: ""draft-2"". # To set a default hog group rather than defaulting to workflow ID:; #hogGroup: ""static""; }; }; ```; However, most are set with the equals sign:; ```; # Google configuration; google {. #application-name = ""cromwell"". # Default: just application default; #auths = [. # Application default; #{; # name = ""applic",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913
https://github.com/broadinstitute/cromwell/issues/4913:1381,Testability,log,log-dir,1381,"/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; The [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies the default type version.; #workflow-type-version: ""draft-2"". # To set a default hog group rather than defaulting to workflow ID:; #hogGroup: ""static""; }; }; ```; However, most are set with the equals sign:; ```; # Google configuration; google {. #application-name = ""cromwell"". # Default: just application default; #auths = [. # Application default; #{; # name = ""applic",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913
https://github.com/broadinstitute/cromwell/issues/4913:1409,Testability,log,logs,1409,"/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; The [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies the default type version.; #workflow-type-version: ""draft-2"". # To set a default hog group rather than defaulting to workflow ID:; #hogGroup: ""static""; }; }; ```; However, most are set with the equals sign:; ```; # Google configuration; google {. #application-name = ""cromwell"". # Default: just application default; #auths = [. # Application default; #{; # name = ""applic",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913
https://github.com/broadinstitute/cromwell/issues/4913:1442,Testability,log,logs,1442,"l-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; The [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies the default type version.; #workflow-type-version: ""draft-2"". # To set a default hog group rather than defaulting to workflow ID:; #hogGroup: ""static""; }; }; ```; However, most are set with the equals sign:; ```; # Google configuration; google {. #application-name = ""cromwell"". # Default: just application default; #auths = [. # Application default; #{; # name = ""application-default""; # scheme = ""application_default""; #},. # Use a refresh token; #{; # name = """,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913
https://github.com/broadinstitute/cromwell/issues/4913:1488,Testability,log,log-temporary,1488,"l-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; The [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies the default type version.; #workflow-type-version: ""draft-2"". # To set a default hog group rather than defaulting to workflow ID:; #hogGroup: ""static""; }; }; ```; However, most are set with the equals sign:; ```; # Google configuration; google {. #application-name = ""cromwell"". # Default: just application default; #auths = [. # Application default; #{; # name = ""application-default""; # scheme = ""application_default""; #},. # Use a refresh token; #{; # name = """,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913
https://github.com/broadinstitute/cromwell/issues/4913:47,Usability,feedback,feedback,47,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; The [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913
https://github.com/broadinstitute/cromwell/issues/4914:917,Availability,error,error,917,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I am running Cromwell on GCP, launching a workflow that shards into ~5,000 pieces. I am getting the following error: `cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out`. ```; 2019-04-29 00:02:13,419 cromwell-system-akka.dispatchers.backend-dispatcher-139 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(95b34a77)vcf2bigquery.convertVCF:2058:1]: Status chang; e from Running to Success; 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:171); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStre",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:981,Availability,ERROR,ERROR,981,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I am running Cromwell on GCP, launching a workflow that shards into ~5,000 pieces. I am getting the following error: `cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out`. ```; 2019-04-29 00:02:13,419 cromwell-system-akka.dispatchers.backend-dispatcher-139 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(95b34a77)vcf2bigquery.convertVCF:2058:1]: Status chang; e from Running to Success; 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:171); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStre",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:1309,Availability,ERROR,ERROR,1309,"ttps://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I am running Cromwell on GCP, launching a workflow that shards into ~5,000 pieces. I am getting the following error: `cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out`. ```; 2019-04-29 00:02:13,419 cromwell-system-akka.dispatchers.backend-dispatcher-139 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(95b34a77)vcf2bigquery.convertVCF:2058:1]: Status chang; e from Running to Success; 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:171); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:735); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678); at su",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:5213,Availability,error,error,5213,"$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryWorker-aaa95e49-59b4-4de6-864d-22920eac6164 terminated. 99 run creation requests, 1 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; ```. Of note, I am running Cromwell 40 with the following `java -Xmx100g -Dconfig.file=google.conf -jar cromwell-40.jar server` on a 16-core highmem system that has 102g of RAM. Of those 102G, only 30G are in use per `htop` (including both active and cache). Cromwell does continue, but the concern, as noted in the error, is that 99 jobs might now be duplicated. If I run with just 1 or 2 jobs, I don't get this message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:709,Deployability,configurat,configuration,709,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I am running Cromwell on GCP, launching a workflow that shards into ~5,000 pieces. I am getting the following error: `cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out`. ```; 2019-04-29 00:02:13,419 cromwell-system-akka.dispatchers.backend-dispatcher-139 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(95b34a77)vcf2bigquery.convertVCF:2058:1]: Status chang; e from Running to Success; 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:171); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStre",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:1098,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1098,"ion is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I am running Cromwell on GCP, launching a workflow that shards into ~5,000 pieces. I am getting the following error: `cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out`. ```; 2019-04-29 00:02:13,419 cromwell-system-akka.dispatchers.backend-dispatcher-139 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(95b34a77)vcf2bigquery.convertVCF:2058:1]: Status chang; e from Running to Success; 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:171); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:3118,Deployability,pipeline,pipelines,3118,eredInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:735); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1587); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:3139,Deployability,Pipeline,PipelinesApiRequestWorker,3139,InputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:735); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1587); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:3174,Deployability,Pipeline,PipelinesApiRequestWorker,3174,net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:735); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1587); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:3238,Deployability,pipeline,pipelines,3238, at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1587); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.r,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:3259,Deployability,Pipeline,PipelinesApiRequestWorker,3259,nt.parseHTTP(HttpClient.java:678); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1587); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:19,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:3309,Deployability,pipeline,pipelines,3309,URLConnection.getInputStream0(HttpURLConnection.java:1587); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWork,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:3330,Deployability,Pipeline,PipelinesApiRequestWorker,3330,URLConnection.getInputStream0(HttpURLConnection.java:1587); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWork,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:3369,Deployability,Pipeline,PipelinesApiRequestWorker,3369,URLConnection.getInputStream0(HttpURLConnection.java:1587); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWork,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:3433,Deployability,pipeline,pipelines,3433,"nnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-13",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:3454,Deployability,Pipeline,PipelinesApiRequestWorker,3454,"ion.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryW",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:3511,Deployability,Pipeline,PipelinesApiRequestWorker,3511,"ion.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryWorker-aaa95e49-59b4-4de6-864d-22920eac616",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:3680,Deployability,pipeline,pipelines,3680,"pi.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryWorker-aaa95e49-59b4-4de6-864d-22920eac6164 terminated. 99 run creation requests, 1 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch requ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:3701,Deployability,Pipeline,PipelinesApiRequestWorker,3701,"pResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryWorker-aaa95e49-59b4-4de6-864d-22920eac6164 terminated. 99 run creation requests, 1 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:3741,Deployability,Pipeline,PipelinesApiRequestWorker,3741,"java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryWorker-aaa95e49-59b4-4de6-864d-22920eac6164 terminated. 99 run creation requests, 1 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:4765,Deployability,pipeline,pipelines,4765,"$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryWorker-aaa95e49-59b4-4de6-864d-22920eac6164 terminated. 99 run creation requests, 1 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; ```. Of note, I am running Cromwell 40 with the following `java -Xmx100g -Dconfig.file=google.conf -jar cromwell-40.jar server` on a 16-core highmem system that has 102g of RAM. Of those 102G, only 30G are in use per `htop` (including both active and cache). Cromwell does continue, but the concern, as noted in the error, is that 99 jobs might now be duplicated. If I run with just 1 or 2 jobs, I don't get this message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:4786,Deployability,Pipeline,PipelinesApiRequestManager,4786,"$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryWorker-aaa95e49-59b4-4de6-864d-22920eac6164 terminated. 99 run creation requests, 1 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; ```. Of note, I am running Cromwell 40 with the following `java -Xmx100g -Dconfig.file=google.conf -jar cromwell-40.jar server` on a 16-core highmem system that has 102g of RAM. Of those 102G, only 30G are in use per `htop` (including both active and cache). Cromwell does continue, but the concern, as noted in the error, is that 99 jobs might now be duplicated. If I run with just 1 or 2 jobs, I don't get this message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:4833,Deployability,Pipeline,PipelinesApiRequestHandler,4833,"$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryWorker-aaa95e49-59b4-4de6-864d-22920eac6164 terminated. 99 run creation requests, 1 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; ```. Of note, I am running Cromwell 40 with the following `java -Xmx100g -Dconfig.file=google.conf -jar cromwell-40.jar server` on a 16-core highmem system that has 102g of RAM. Of those 102G, only 30G are in use per `htop` (including both active and cache). Cromwell does continue, but the concern, as noted in the error, is that 99 jobs might now be duplicated. If I run with just 1 or 2 jobs, I don't get this message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:2322,Integrability,protocol,protocol,2322,d out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:171); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:735); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1587); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:2415,Integrability,protocol,protocol,2415,ead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:171); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:735); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1587); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.ba,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:2582,Integrability,protocol,protocol,2582,SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:735); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1587); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:5310,Integrability,message,message,5310,"$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryWorker-aaa95e49-59b4-4de6-864d-22920eac6164 terminated. 99 run creation requests, 1 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; ```. Of note, I am running Cromwell 40 with the following `java -Xmx100g -Dconfig.file=google.conf -jar cromwell-40.jar server` on a 16-core highmem system that has 102g of RAM. Of those 102G, only 30G are in use per `htop` (including both active and cache). Cromwell does continue, but the concern, as noted in the error, is that 99 jobs might now be duplicated. If I run with just 1 or 2 jobs, I don't get this message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:709,Modifiability,config,configuration,709,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I am running Cromwell on GCP, launching a workflow that shards into ~5,000 pieces. I am getting the following error: `cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out`. ```; 2019-04-29 00:02:13,419 cromwell-system-akka.dispatchers.backend-dispatcher-139 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(95b34a77)vcf2bigquery.convertVCF:2058:1]: Status chang; e from Running to Success; 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:171); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStre",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:5148,Performance,cache,cache,5148,"$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryWorker-aaa95e49-59b4-4de6-864d-22920eac6164 terminated. 99 run creation requests, 1 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; ```. Of note, I am running Cromwell 40 with the following `java -Xmx100g -Dconfig.file=google.conf -jar cromwell-40.jar server` on a 16-core highmem system that has 102g of RAM. Of those 102G, only 30G are in use per `htop` (including both active and cache). Cromwell does continue, but the concern, as noted in the error, is that 99 jobs might now be duplicated. If I run with just 1 or 2 jobs, I don't get this message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:4588,Safety,abort,abort,4588,"$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryWorker-aaa95e49-59b4-4de6-864d-22920eac6164 terminated. 99 run creation requests, 1 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; ```. Of note, I am running Cromwell 40 with the following `java -Xmx100g -Dconfig.file=google.conf -jar cromwell-40.jar server` on a 16-core highmem system that has 102g of RAM. Of those 102G, only 30G are in use per `htop` (including both active and cache). Cromwell does continue, but the concern, as noted in the error, is that 99 jobs might now be duplicated. If I run with just 1 or 2 jobs, I don't get this message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:754,Security,PASSWORD,PASSWORDS,754,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I am running Cromwell on GCP, launching a workflow that shards into ~5,000 pieces. I am getting the following error: `cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out`. ```; 2019-04-29 00:02:13,419 cromwell-system-akka.dispatchers.backend-dispatcher-139 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(95b34a77)vcf2bigquery.convertVCF:2058:1]: Status chang; e from Running to Success; 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:171); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStre",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:1645,Security,secur,security,1645,"aste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I am running Cromwell on GCP, launching a workflow that shards into ~5,000 pieces. I am getting the following error: `cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out`. ```; 2019-04-29 00:02:13,419 cromwell-system-akka.dispatchers.backend-dispatcher-139 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(95b34a77)vcf2bigquery.convertVCF:2058:1]: Status chang; e from Running to Success; 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:171); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:735); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1587); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:1710,Security,secur,security,1710,"guration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I am running Cromwell on GCP, launching a workflow that shards into ~5,000 pieces. I am getting the following error: `cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out`. ```; 2019-04-29 00:02:13,419 cromwell-system-akka.dispatchers.backend-dispatcher-139 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(95b34a77)vcf2bigquery.convertVCF:2058:1]: Status chang; e from Running to Success; 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:171); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:735); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1587); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:1770,Security,secur,security,1770,"D OTHER SENSITIVE MATERIAL: -->; I am running Cromwell on GCP, launching a workflow that shards into ~5,000 pieces. I am getting the following error: `cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out`. ```; 2019-04-29 00:02:13,419 cromwell-system-akka.dispatchers.backend-dispatcher-139 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(95b34a77)vcf2bigquery.convertVCF:2058:1]: Status chang; e from Running to Success; 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:171); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:735); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1587); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:1840,Security,secur,security,1840,"ng a workflow that shards into ~5,000 pieces. I am getting the following error: `cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out`. ```; 2019-04-29 00:02:13,419 cromwell-system-akka.dispatchers.backend-dispatcher-139 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(95b34a77)vcf2bigquery.convertVCF:2058:1]: Status chang; e from Running to Success; 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:171); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:735); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1587); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
