id,quality_attribute,keyword,matched_word,match_idx,sentence,source,filename,author,repo,version,wiki,url
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp:26,Availability,error,error,26,"//===- Error.cpp - tblgen error handling helper routines --------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains error handling helper routines to pretty-print diagnostic; // messages from tblgen.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Error.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp:402,Availability,error,error,402,"//===- Error.cpp - tblgen error handling helper routines --------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains error handling helper routines to pretty-print diagnostic; // messages from tblgen.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Error.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp:48,Integrability,rout,routines,48,"//===- Error.cpp - tblgen error handling helper routines --------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains error handling helper routines to pretty-print diagnostic; // messages from tblgen.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Error.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp:424,Integrability,rout,routines,424,"//===- Error.cpp - tblgen error handling helper routines --------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains error handling helper routines to pretty-print diagnostic; // messages from tblgen.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Error.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp:464,Integrability,message,messages,464,"//===- Error.cpp - tblgen error handling helper routines --------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains error handling helper routines to pretty-print diagnostic; // messages from tblgen.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Error.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp:29,Availability,error,errors,29,// Count the total number of errors printed.; // This is used to exit with an error code if there were any errors.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Error.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp:78,Availability,error,error,78,// Count the total number of errors printed.; // This is used to exit with an error code if there were any errors.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Error.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp:107,Availability,error,errors,107,// Count the total number of errors printed.; // This is used to exit with an error code if there were any errors.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Error.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp:22,Availability,error,errors,22,// Functions to print errors.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Error.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp:28,Availability,error,errors,28,// Functions to print fatal errors.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Error.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp:103,Availability,error,error,103,"// Check an assertion: Obtain the condition value and be sure it is true.; // If not, print a nonfatal error along with the message.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Error.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp:124,Integrability,message,message,124,"// Check an assertion: Obtain the condition value and be sure it is true.; // If not, print a nonfatal error along with the message.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Error.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp:12,Testability,assert,assertion,12,"// Check an assertion: Obtain the condition value and be sure it is true.; // If not, print a nonfatal error along with the message.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Error.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp:10,Integrability,message,message,10,// Dump a message to stderr.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Error.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Error.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/JSONBackend.cpp:51,Usability,simpl,simply,51,"// Final fallback: anything that gets past here is simply given a; // kind field of 'complex', and the only other field is the standard; // 'printable' representation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/JSONBackend.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/JSONBackend.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Main.cpp:13,Integrability,depend,dependency,13,/// Create a dependency file for `-d` option.; ///; /// This functionality is really only for the benefit of the build system.; /// It is similar to GCC's `-M*` family of options.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Main.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Main.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Main.cpp:8,Deployability,update,updates,8,// Only updates the real output file if there are any differences.; // This prevents recompilation of all the files depending on it if there; // aren't any.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Main.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Main.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Main.cpp:116,Integrability,depend,depending,116,// Only updates the real output file if there are any differences.; // This prevents recompilation of all the files depending on it if there; // aren't any.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Main.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Main.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp:190,Integrability,depend,dependencies,190,"/// This class represents the internal implementation of the RecordKeeper.; /// It contains all of the contextual static state of the Record classes. It is; /// kept out-of-line to simplify dependencies, and also make it easier for; /// internal classes to access the uniquer state of the keeper.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Record.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp:257,Security,access,access,257,"/// This class represents the internal implementation of the RecordKeeper.; /// It contains all of the contextual static state of the Record classes. It is; /// kept out-of-line to simplify dependencies, and also make it easier for; /// internal classes to access the uniquer state of the keeper.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Record.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp:181,Usability,simpl,simplify,181,"/// This class represents the internal implementation of the RecordKeeper.; /// It contains all of the contextual static state of the Record classes. It is; /// kept out-of-line to simplify dependencies, and also make it easier for; /// internal classes to access the uniquer state of the keeper.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Record.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp:13,Safety,redund,redundancy,13,// Check for redundancy.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Record.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp:37,Modifiability,variab,variable,37,"// Otherwise, print the value of the variable.; //; // NOTE: we could recursively !repr the elements of a list,; // but that could produce a lot of output when printing a; // defset.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Record.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp:3,Security,Access,Accessor,3,// Accessor by index,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Record.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp:3,Security,Access,Accessor,3,// Accessor by name,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Record.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp:59,Modifiability,variab,variable,59,"// Applies RHS to all elements of MHS, using LHS as a temp variable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Record.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp:62,Modifiability,variab,variable,62,"// Evaluates RHS for all elements of MHS, using LHS as a temp variable.; // Creates a new list with the elements that evaluated to true.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Record.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp:31,Modifiability,variab,variable,31,// Cannot subscript a non-bits variable.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Record.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp:8,Testability,assert,assertions,8,// Copy assertions from class to instance.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Record.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp:13,Testability,assert,assertions,13,// Check the assertions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Record.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp:13,Testability,assert,assertions,13,// Check the assertions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Record.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp:15,Testability,assert,assertion,15,// Resolve the assertion expressions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Record.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp:76,Integrability,message,message,76,"// Check all record assertions: For each one, resolve the condition; // and message, then call CheckAssert().; // Note: The condition and message are probably already resolved,; // but resolving again allows calls before records are resolved.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Record.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp:138,Integrability,message,message,138,"// Check all record assertions: For each one, resolve the condition; // and message, then call CheckAssert().; // Note: The condition and message are probably already resolved,; // but resolving again allows calls before records are resolved.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Record.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp:20,Testability,assert,assertions,20,"// Check all record assertions: For each one, resolve the condition; // and message, then call CheckAssert().; // Note: The condition and message are probably already resolved,; // but resolving again allows calls before records are resolved.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Record.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp:6,Performance,cache,cache,6,// We cache the record vectors for single classes. Many backends request; // the same vectors multiple times.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Record.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp:46,Modifiability,variab,variables,46,"// Resolve mutual references among the mapped variables, but prevent; // infinite recursion.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Record.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp:169,Modifiability,variab,variables,169,"// Do not recurse into the resolved initializer, as that would change; // the behavior of the resolver we're delegating, but do check to see; // if there are unresolved variables remaining.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/Record.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/Record.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/SetTheory.cpp:9,Usability,simpl,simply,9,// Lists simply expand.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/SetTheory.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/SetTheory.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TableGenBackendSkeleton.cpp:22,Modifiability,variab,variable,22,// To suppress unused variable warning; remove on use.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TableGenBackendSkeleton.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TableGenBackendSkeleton.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp:26,Availability,error,error,26,/// ReturnError - Set the error to the specified string at the specified; /// location. This is defined to always return tgtok::Error.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp:128,Availability,Error,Error,128,/// ReturnError - Set the error to the specified string at the specified; /// location. This is defined to always return tgtok::Error.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp:24,Safety,detect,detects,24,"// If prepExitInclude() detects a problem with the preprocessing; // control stack, it will return false. Pretend that we reached; // the final EOF and stop lexing more tokens by returning false; // to LexToken().",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp:82,Availability,error,error,82,"// Pretend that we exit the ""top-level"" include file.; // Note that in case of an error (e.g. control stack imbalance); // the routine will issue a fatal error.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp:154,Availability,error,error,154,"// Pretend that we exit the ""top-level"" include file.; // Note that in case of an error (e.g. control stack imbalance); // the routine will issue a fatal error.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp:127,Integrability,rout,routine,127,"// Pretend that we exit the ""top-level"" include file.; // Note that in case of an error (e.g. control stack imbalance); // the routine will issue a fatal error.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp:30,Availability,error,error,30,"// Unknown character, emit an error.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp:25,Availability,error,error,25,"// Otherwise, this is an error.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp:46,Availability,error,error,46,"// If we hit the end of the buffer, report an error.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp:46,Availability,error,error,46,"// If we hit the end of the buffer, report an error.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp:13,Availability,error,error,13,"// Report an error, if preprocessor control stack for the current; // file is not empty.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp:80,Availability,error,error,80,// New line and EOF may follow only #else/#endif. It will be reported; // as an error for #ifdef/#define after the call to prepLexMacroName().,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp:39,Availability,error,error,39,"// If lexPreprocessor() encountered an error during lexing this; // preprocessor idiom, then return false to the calling lexPreprocessor().; // This will force tgtok::Error to be returned to the tokens processing.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp:167,Availability,Error,Error,167,"// If lexPreprocessor() encountered an error during lexing this; // preprocessor idiom, then return false to the calling lexPreprocessor().; // This will force tgtok::Error to be returned to the tokens processing.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp:502,Availability,error,error,502,"// Skip C-style comment.; // Note that we do not care about skipping the C++-style comments.; // If the line contains ""//"", it may not contain any processable; // preprocessing directive. Just return CurPtr pointing to; // the first '/' in this case. We also do not care about; // incorrect symbols after the first '/' - we are in lines-skipping; // mode, so incorrect code is allowed to some extent.; // Set TokStart to the beginning of the comment to enable proper; // diagnostic printing in case of error in SkipCComment().",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp:105,Usability,simpl,simplify,105,"// Skip C++-style comment.; // We may just return true now, but let's skip to the line/buffer end; // to simplify the method specification.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp:741,Availability,error,error,741,"// When we are skipping C-style comment at the end of a preprocessing; // directive, we can skip several lines. If any meaningful TD token; // follows the end of the C-style comment on the same line, it will; // be considered as an invalid usage of TD token.; // For example, we want to forbid usages like this one:; // #define MACRO class Class {}; // But with C-style comments we also disallow the following:; // #define MACRO /* This macro is used; // to ... */ class Class {}; // One can argue that this should be allowed, but it does not seem; // to be worth of the complication. Moreover, this matches; // the C preprocessor behavior.; // Set TokStart to the beginning of the comment to enable proper; // diagnostic printer in case of error in SkipCComment().",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h:4,Integrability,Depend,Dependencies,4,/// Dependencies - This is the list of all included files.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h:101,Deployability,update,update,101,"// Process EOF encountered in LexToken().; // If EOF is met in an include file, then the method will update; // CurPtr, CurBuf and preprocessing include stack, and return true.; // If EOF is met in the top-level file, then the method will; // update and check the preprocessing include stack, and return false.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h:243,Deployability,update,update,243,"// Process EOF encountered in LexToken().; // If EOF is met in an include file, then the method will update; // CurPtr, CurBuf and preprocessing include stack, and return true.; // If EOF is met in the top-level file, then the method will; // update and check the preprocessing include stack, and return false.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h:330,Deployability,update,updated,330,"// Each of #ifdef and #else directives has a descriptor associated; // with it.; //; // An ordered list of preprocessing controls defined by #ifdef/#else; // directives that are in effect currently is called preprocessing; // control stack. It is represented as a vector of PreprocessorControlDesc's.; //; // The control stack is updated according to the following rules:; //; // For each #ifdef we add an element to the control stack.; // For each #else we replace the top element with a descriptor; // with an inverted IsDefined value.; // For each #endif we pop the top element from the control stack.; //; // When CurPtr reaches the current buffer's end, the control stack; // must be empty, i.e. #ifdef and the corresponding #endif; // must be located in the same file.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h:173,Usability,clear,clear,173,"// We want to disallow code like this:; // file1.td:; // #define NAME; // #ifdef NAME; // include ""file2.td""; // EOF; // file2.td:; // #endif; // EOF; //; // To do this, we clear the preprocessing control stack on entry; // to each of the included file. PrepIncludeStack is used to store; // preprocessing control stacks for the current file and all its; // parent files. The back() element is the preprocessing control; // stack for the current file.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h:3,Security,Validat,Validate,3,"// Validate that the current preprocessing control stack is empty,; // since we are about to exit a file, and pop the include stack.; //; // If IncludeStackMustBeEmpty is true, the include stack must be empty; // after the popping, otherwise, the include stack must not be empty; // after the popping. Basically, the include stack must be empty; // only if we exit the ""top-level"" file (i.e. finish lexing).; //; // The method returns false, if the current preprocessing control stack; // is not empty (e.g. there is an unterminated #ifdef/#else),; // true - otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h:255,Safety,avoid,avoid,255,"// Given a preprocessing token kind, adjusts CurPtr to the end; // of the preprocessing directive word. Returns true, unless; // an unsupported token kind is passed in.; //; // We use look-ahead prepIsDirective() and prepEatPreprocessorDirective(); // to avoid adjusting CurPtr before we are sure that '#' is followed; // by a preprocessing directive. If it is not, then we fall back to; // tgtok::paste interpretation of '#'.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h:1521,Availability,error,errors,1521,"// The main ""exit"" point from the token parsing to preprocessor.; //; // The method is called for CurPtr, when prepIsDirective() returns; // true. The first parameter matches the result of prepIsDirective(),; // denoting the actual preprocessor directive to be processed.; //; // If the preprocessing directive disables the tokens processing, e.g.:; // #ifdef NAME // NAME is undefined; // then lexPreprocessor() enters the lines-skipping mode.; // In this mode, it does not parse any tokens, because the code under; // the #ifdef may not even be a correct tablegen code. The preprocessor; // looks for lines containing other preprocessing directives, which; // may be prepended with whitespaces and C-style comments. If the line; // does not contain a preprocessing directive, it is skipped completely.; // Otherwise, the preprocessing directive is processed by recursively; // calling lexPreprocessor(). The processing of the encountered; // preprocessing directives includes updating preprocessing control stack; // and adding new macros into DefinedMacros set.; //; // The second parameter controls whether lexPreprocessor() is called from; // LexToken() (true) or recursively from lexPreprocessor() (false).; //; // If ReturnNextLiveToken is true, the method returns the next; // LEX token following the current directive or following the end; // of the disabled preprocessing region corresponding to this directive.; // If ReturnNextLiveToken is false, the method returns the first parameter,; // unless there were errors encountered in the disabled preprocessing; // region - in this case, it returns tgtok::Error.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h:1615,Availability,Error,Error,1615,"// The main ""exit"" point from the token parsing to preprocessor.; //; // The method is called for CurPtr, when prepIsDirective() returns; // true. The first parameter matches the result of prepIsDirective(),; // denoting the actual preprocessor directive to be processed.; //; // If the preprocessing directive disables the tokens processing, e.g.:; // #ifdef NAME // NAME is undefined; // then lexPreprocessor() enters the lines-skipping mode.; // In this mode, it does not parse any tokens, because the code under; // the #ifdef may not even be a correct tablegen code. The preprocessor; // looks for lines containing other preprocessing directives, which; // may be prepended with whitespaces and C-style comments. If the line; // does not contain a preprocessing directive, it is skipped completely.; // Otherwise, the preprocessing directive is processed by recursively; // calling lexPreprocessor(). The processing of the encountered; // preprocessing directives includes updating preprocessing control stack; // and adding new macros into DefinedMacros set.; //; // The second parameter controls whether lexPreprocessor() is called from; // LexToken() (true) or recursively from lexPreprocessor() (false).; //; // If ReturnNextLiveToken is true, the method returns the next; // LEX token following the current directive or following the end; // of the disabled preprocessing region corresponding to this directive.; // If ReturnNextLiveToken is false, the method returns the first parameter,; // unless there were errors encountered in the disabled preprocessing; // region - in this case, it returns tgtok::Error.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h:297,Availability,error,error,297,"// Worker method for lexPreprocessor() to skip lines after some; // preprocessing directive up to the buffer end or to the directive; // that re-enables token processing. The method returns true; // upon processing the next directive that re-enables tokens; // processing. False is returned if an error was encountered.; //; // Note that prepSkipRegion() calls lexPreprocessor() to process; // encountered preprocessing directives. In this case, the second; // parameter to lexPreprocessor() is set to false. Being passed; // false ReturnNextLiveToken, lexPreprocessor() must never call; // prepSkipRegion(). We assert this by passing ReturnNextLiveToken; // to prepSkipRegion() and checking that it is never set to false.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h:612,Testability,assert,assert,612,"// Worker method for lexPreprocessor() to skip lines after some; // preprocessing directive up to the buffer end or to the directive; // that re-enables token processing. The method returns true; // upon processing the next directive that re-enables tokens; // processing. False is returned if an error was encountered.; //; // Note that prepSkipRegion() calls lexPreprocessor() to process; // encountered preprocessing directives. In this case, the second; // parameter to lexPreprocessor() is set to false. Being passed; // false ReturnNextLiveToken, lexPreprocessor() must never call; // prepSkipRegion(). We assert this by passing ReturnNextLiveToken; // to prepSkipRegion() and checking that it is never set to false.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h:189,Availability,error,errors,189,"// Lex name of the macro after either #ifdef or #define. We could have used; // LexIdentifier(), but it has special handling of ""include"" word, which; // could result in awkward diagnostic errors. Consider:; // ----; // #ifdef include; // class ...; // ----; // LexIdentifier() will engage LexInclude(), which will complain about; // missing file with name ""class"". Instead, prepLexMacroName() will treat; // ""include"" as a normal macro name.; //; // On entry, CurPtr points to the end of a preprocessing directive word.; // The method allows for whitespaces between the preprocessing directive; // and the macro name. The allowed whitespaces are ' ' and '\t'.; //; // If the first non-whitespace symbol after the preprocessing directive; // is a valid start symbol for an identifier (i.e. [a-zA-Z_]), then; // the method updates TokStart to the position of the first non-whitespace; // symbol, sets CurPtr to the position of the macro name's last symbol,; // and returns a string reference to the macro name. Otherwise,; // TokStart is set to the first non-whitespace symbol after the preprocessing; // directive, and the method returns an empty string reference.; //; // In all cases, TokStart may be used to point to the word following; // the preprocessing directive.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h:822,Deployability,update,updates,822,"// Lex name of the macro after either #ifdef or #define. We could have used; // LexIdentifier(), but it has special handling of ""include"" word, which; // could result in awkward diagnostic errors. Consider:; // ----; // #ifdef include; // class ...; // ----; // LexIdentifier() will engage LexInclude(), which will complain about; // missing file with name ""class"". Instead, prepLexMacroName() will treat; // ""include"" as a normal macro name.; //; // On entry, CurPtr points to the end of a preprocessing directive word.; // The method allows for whitespaces between the preprocessing directive; // and the macro name. The allowed whitespaces are ' ' and '\t'.; //; // If the first non-whitespace symbol after the preprocessing directive; // is a valid start symbol for an identifier (i.e. [a-zA-Z_]), then; // the method updates TokStart to the position of the first non-whitespace; // symbol, sets CurPtr to the position of the macro name's last symbol,; // and returns a string reference to the macro name. Otherwise,; // TokStart is set to the first non-whitespace symbol after the preprocessing; // directive, and the method returns an empty string reference.; //; // In all cases, TokStart may be used to point to the word following; // the preprocessing directive.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h:847,Availability,error,error,847,"// Skip any whitespaces starting from CurPtr. The method is used; // only in the lines-skipping mode to find the first non-whitespace; // symbol after or at CurPtr. Allowed whitespaces are ' ', '\t', '\n'; // and '\r'. The method skips C-style comments as well, because; // it is used to find the beginning of the preprocessing directive.; // If we do not handle C-style comments the following code would; // result in incorrect detection of a preprocessing directive:; // /*; // #ifdef NAME; // */; // As long as we skip C-style comments, the following code is correctly; // recognized as a preprocessing directive:; // /* first line comment; // second line comment */ #ifdef NAME; //; // The method returns true upon reaching the first non-whitespace symbol; // or EOF, CurPtr is set to point to this symbol. The method returns false,; // if an error occurred during skipping of a C-style comment.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h:429,Safety,detect,detection,429,"// Skip any whitespaces starting from CurPtr. The method is used; // only in the lines-skipping mode to find the first non-whitespace; // symbol after or at CurPtr. Allowed whitespaces are ' ', '\t', '\n'; // and '\r'. The method skips C-style comments as well, because; // it is used to find the beginning of the preprocessing directive.; // If we do not handle C-style comments the following code would; // result in incorrect detection of a preprocessing directive:; // /*; // #ifdef NAME; // */; // As long as we skip C-style comments, the following code is correctly; // recognized as a preprocessing directive:; // /* first line comment; // second line comment */ #ifdef NAME; //; // The method returns true upon reaching the first non-whitespace symbol; // or EOF, CurPtr is set to point to this symbol. The method returns false,; // if an error occurred during skipping of a C-style comment.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h:363,Availability,error,error,363,"// Skip any whitespaces or comments after a preprocessing directive.; // The method returns true upon reaching either end of the line; // or end of the file. If there is a multiline C-style comment; // after the preprocessing directive, the method skips; // the comment, so the final CurPtr may point to one of the next lines.; // The method returns false, if an error occurred during skipping; // C- or C++-style comment, or a non-whitespace symbol appears; // after the preprocessing directive.; //; // The method maybe called both during lines-skipping and tokens; // processing. It actually verifies that only whitespaces or/and; // comments follow a preprocessing directive.; //; // After the execution of this mehod, CurPtr points either to new line; // symbol, buffer end or non-whitespace symbol following the preprocesing; // directive.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h:13,Availability,error,error,13,"// Report an error, if we reach EOF with non-empty preprocessing control; // stack. This means there is no matching #endif for the previous; // #ifdef/#else.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGLexer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:32,Modifiability,variab,variables,32,"// HACK: Disable this check for variables declared with 'field'. This is; // done merely because existing targets have legitimate cases of; // non-concrete variables in helper defs. Ideally, we'd introduce a; // 'maybe' or 'optional' modifier instead of this.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:156,Modifiability,variab,variables,156,"// HACK: Disable this check for variables declared with 'field'. This is; // done merely because existing targets have legitimate cases of; // non-concrete variables in helper defs. Ideally, we'd introduce a; // 'maybe' or 'optional' modifier instead of this.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:29,Modifiability,variab,variables,29,"// First, we search in local variables.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:36,Modifiability,variab,variable,36,"// If not found, we try to find the variable in additional variables like; // arguments, loop iterator, etc.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:59,Modifiability,variab,variables,59,"// If not found, we try to find the variable in additional variables like; // arguments, loop iterator, etc.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:7,Modifiability,variab,variable,7,// The variable is a record field?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:7,Modifiability,variab,variable,7,// The variable is a class template argument?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:7,Modifiability,variab,variable,7,// The variable is a loop iterator?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:7,Modifiability,variab,variable,7,// The variable is a multiclass template argument?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:35,Availability,error,error,35,"/// SetValue -; /// Return true on error, false on success.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:30,Testability,assert,assertions,30,// Copy the subclass record's assertions to the new record.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:35,Testability,assert,assertion,35,"/// Add a record, foreach loop, or assertion to the current context.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:37,Performance,perform,perform,37,"// If it is a loop, then resolve and perform the loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:15,Testability,assert,assertion,15,"// If it is an assertion, then it's a top-level one, so check it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:169,Modifiability,variab,variable,169,"// For if-then-else blocks, we lower to a foreach loop whose list is a; // ternary selection between lists of different length. Since we don't; // have a means to track variable length record lists, we *must* resolve; // the condition here. We want to defer final resolution of the arms; // until the resulting records are finalized.; // e.g. !if(!exists<SchedWrite>(""__does_not_exist__""), [1], [])",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:13,Testability,assert,assertions,13,// Check the assertions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:49,Availability,error,error,49,"// If ObjectBody has template arguments, it's an error.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:159,Availability,error,error,159,"/// ParseObjectName - If a valid object name is specified, return it. If no; /// name is specified, return the unset initializer. Return nullptr on parse; /// error.; /// ObjectName ::= Value [ '#' Value ]*; /// ObjectName ::= /*empty*/; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:92,Availability,error,error,92,/// ParseClassID - Parse and resolve a reference to a class name. This returns; /// null on error.; ///; /// ClassID ::= ID; ///,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:102,Availability,error,error,102,/// ParseMultiClassID - Parse and resolve a reference to a multiclass name.; /// This returns null on error.; ///; /// MultiClassID ::= ID; ///,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:135,Availability,error,error,135,/// ParseSubClassReference - Parse a reference to a subclass or a; /// multiclass. This returns a SubClassRefTy with a null Record* on error.; ///; /// SubClassRef ::= ClassID; /// SubClassRef ::= ClassID '<' ArgValueList '>'; ///,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:3,Availability,Error,Error,3,// Error parsing value list.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:3,Availability,Error,Error,3,// Error checking value list.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:166,Availability,error,error,166,/// ParseSubMultiClassReference - Parse a reference to a subclass or to a; /// templated submulticlass. This returns a SubMultiClassRefTy with a null; /// Record* on error.; ///; /// SubMultiClassRef ::= MultiClassID; /// SubMultiClassRef ::= MultiClassID '<' ArgValueList '>'; ///,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:3,Availability,Error,Error,3,// Error parsing value list.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:69,Availability,error,error,69,/// ParseType - Parse and return a tblgen type. This returns null on error.; ///; /// Type ::= STRING // string type; /// Type ::= CODE // code type; /// Type ::= BIT // bit type; /// Type ::= BITS '<' INTVAL '>' // bits<x> type; /// Type ::= INT // int type; /// Type ::= LIST '<' Type '>' // list<x> type; /// Type ::= DAG // dag type; /// Type ::= ClassID // Record Type; ///,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:61,Availability,error,error,61,/// ParseOperation - Parse an operator. This returns null on error.; ///; /// Operation ::= XOperator ['<' Type '>'] '(' Args ')'; ///,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:10,Availability,error,error,10,// Detect error if 2nd arg were present.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:3,Safety,Detect,Detect,3,// Detect error if 2nd arg were present.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:91,Modifiability,variab,variables,91,"// eat the ','; // We need to create a temporary record to provide a scope for the; // two variables.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:80,Availability,error,error,80,/// ParseOperatorType - Parse a type for an operator. This returns; /// null on error.; ///; /// OperatorType ::= '<' Type '>'; ///,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:48,Availability,error,error,48,"/// Parse the !substr operation. Return null on error.; ///; /// Substr ::= !substr(string, start-int [, length-int]) => string",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:46,Availability,error,error,46,"/// Parse the !find operation. Return null on error.; ///; /// Substr ::= !find(string, string [, start-int]) => int",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:62,Availability,error,error,62,"/// Parse the !foreach and !filter operations. Return null on error.; ///; /// ForEach ::= !foreach(ID, list-or-dag, expr) => list<expr type>; /// Filter ::= !foreach(ID, list, predicate) ==> list<list type>",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:81,Modifiability,variab,variable,81,// We need to create a temporary record to provide a scope for the; // iteration variable.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:66,Availability,error,error,66,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:83,Usability,Simpl,SimpleValue,83,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:112,Usability,Simpl,SimpleValue,112,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:140,Usability,Simpl,SimpleValue,140,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:169,Usability,Simpl,SimpleValue,169,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:203,Usability,Simpl,SimpleValue,203,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:228,Usability,Simpl,SimpleValue,228,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:267,Usability,Simpl,SimpleValue,267,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:311,Usability,Simpl,SimpleValue,311,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:350,Usability,Simpl,SimpleValue,350,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:398,Usability,Simpl,SimpleValue,398,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:453,Usability,Simpl,SimpleValue,453,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:505,Usability,Simpl,SimpleValue,505,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:557,Usability,Simpl,SimpleValue,557,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:609,Usability,Simpl,SimpleValue,609,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:661,Usability,Simpl,SimpleValue,661,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:713,Usability,Simpl,SimpleValue,713,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:765,Usability,Simpl,SimpleValue,765,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:824,Usability,Simpl,SimpleValue,824,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:882,Usability,Simpl,SimpleValue,882,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:941,Usability,Simpl,SimpleValue,941,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:982,Usability,Simpl,SimpleValue,982,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:1033,Usability,Simpl,SimpleValue,1033,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:1094,Usability,Simpl,SimpleValue,1094,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:1152,Usability,Simpl,SimpleValue,1152,"/// ParseSimpleValue - Parse a tblgen value. This returns null on error.; ///; /// SimpleValue ::= IDValue; /// SimpleValue ::= INTVAL; /// SimpleValue ::= STRVAL+; /// SimpleValue ::= CODEFRAGMENT; /// SimpleValue ::= '?'; /// SimpleValue ::= '{' ValueList '}'; /// SimpleValue ::= ID '<' ValueListNE '>'; /// SimpleValue ::= '[' ValueList ']'; /// SimpleValue ::= '(' IDValue DagArgList ')'; /// SimpleValue ::= CONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= ADDTOK '(' Value ',' Value ')'; /// SimpleValue ::= DIVTOK '(' Value ',' Value ')'; /// SimpleValue ::= SUBTOK '(' Value ',' Value ')'; /// SimpleValue ::= SHLTOK '(' Value ',' Value ')'; /// SimpleValue ::= SRATOK '(' Value ',' Value ')'; /// SimpleValue ::= SRLTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTSPLATTOK '(' Value ',' Value ')'; /// SimpleValue ::= LISTREMOVETOK '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ')'; /// SimpleValue ::= RANGE '(' Value ',' Value ',' Value ')'; /// SimpleValue ::= STRCONCATTOK '(' Value ',' Value ')'; /// SimpleValue ::= COND '(' [Value ':' Value,]+ ')'; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:3,Availability,Error,Error,3,// Error parsing value list.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:3,Availability,Error,Error,3,// Error checking template argument values.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:30,Modifiability,variab,variable,30,// bits<n> can also come from variable initializers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:62,Availability,error,error,62,/// ParseValue - Parse a TableGen value. This returns null on error.; ///; /// Value ::= SimpleValue ValueSuffix*; /// ValueSuffix ::= '{' BitList '}'; /// ValueSuffix ::= '[' SliceElements ']'; /// ValueSuffix ::= '.' ID; ///,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:89,Usability,Simpl,SimpleValue,89,/// ParseValue - Parse a TableGen value. This returns null on error.; ///; /// Value ::= SimpleValue ValueSuffix*; /// ValueSuffix ::= '{' BitList '}'; /// ValueSuffix ::= '[' SliceElements ']'; /// ValueSuffix ::= '.' ID; ///,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:10,Modifiability,variab,variable,10,"// If the variable name is present, add it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:229,Availability,error,error,229,"// ParseTemplateArgValueList - Parse a template argument list with the syntax; // shown, filling in the Result vector. The open angle has been consumed.; // An empty argument list is allowed. Return false if okay, true if an; // error was detected.; //; // ArgValueList ::= '<' PostionalArgValueList [','] NamedArgValueList '>'; // PostionalArgValueList ::= [Value {',' Value}*]; // NamedArgValueList ::= [NameValue '=' Value {',' NameValue '=' Value}*]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:239,Safety,detect,detected,239,"// ParseTemplateArgValueList - Parse a template argument list with the syntax; // shown, filling in the Result vector. The open angle has been consumed.; // An empty argument list is allowed. Return false if okay, true if an; // error was detected.; //; // ArgValueList ::= '<' PostionalArgValueList [','] NamedArgValueList '>'; // PostionalArgValueList ::= [Value {',' Value}*]; // NamedArgValueList ::= [NameValue '=' Value {',' NameValue '=' Value}*]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:102,Availability,error,error,102,"/// ParseDeclaration - Read a declaration, returning the name of field ID, or an; /// empty string on error. This can happen in a number of different contexts,; /// including within a def or in the template args for a class (in which case; /// CurRec will be non-null) and within the template args for a multiclass (in; /// which case CurRec will be null, but CurMultiClass will be set). This can; /// also happen within a def that is within a multiclass, which will set both; /// CurRec and CurMultiClass.; ///; /// Declaration ::= FIELD? Type ID ('=' Value)?; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:31,Availability,error,error,31,"// Return the name, even if an error is thrown. This is so that we can; // continue to make some progress, even without the value having been; // initialized.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:123,Availability,error,error,123,"/// ParseForeachDeclaration - Read a foreach declaration, returning; /// the name of the declared object or a NULL Init on error. Return; /// the name of the parsed initializer list through ForeachListName.; ///; /// ForeachDeclaration ::= ID '=' '{' RangeList '}'; /// ForeachDeclaration ::= ID '=' RangePiece; /// ForeachDeclaration ::= ID '=' Value; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:235,Testability,Assert,Assert,235,/// ParseBodyItem - Parse a single item within the body of a def or class.; ///; /// BodyItem ::= Declaration ';'; /// BodyItem ::= LET ID OptionalBitList '=' Value ';'; /// BodyItem ::= Defvar; /// BodyItem ::= Dump; /// BodyItem ::= Assert; ///,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:64,Availability,error,error,64,"/// ParseBody - Read the body of a class or def. Return true on error, false on; /// success.; ///; /// Body ::= ';'; /// Body ::= '{' BodyList '}'; /// BodyList BodyItem*; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:42,Availability,error,error,42,"// If we have a semicolon, print a gentle error.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:71,Availability,error,error,71,"/// Apply the current let bindings to \a CurRec.; /// \returns true on error, false otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:35,Testability,assert,assertions,35,// Let bindings are not applied to assertions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:51,Modifiability,variab,variables,51,// An object body introduces a new scope for local variables.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:13,Availability,error,error,13,// Check for error.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:112,Availability,error,error,112,"/// ParseDef - Parse and return a top level or multiclass record definition.; /// Return false if okay, true if error.; ///; /// DefInst ::= DEF ObjectName ObjectBody; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:107,Availability,error,error,107,/// ParseForeach - Parse a for statement. Return the record corresponding; /// to it. This returns true on error.; ///; /// Foreach ::= FOREACH Declaration IN '{ ObjectList '}'; /// Foreach ::= FOREACH Declaration IN Object; ///,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:51,Modifiability,variab,variables,51,// A foreach loop introduces a new scope for local variables.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:265,Integrability,depend,depending,265,"// We have to be able to save if statements to execute later, and they have; // to live on the same stack as foreach loops. The simplest implementation; // technique is to convert each 'then' or 'else' clause *into* a foreach; // loop, over a list of length 0 or 1 depending on the condition, and with no; // iteration variable being assigned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:319,Modifiability,variab,variable,319,"// We have to be able to save if statements to execute later, and they have; // to live on the same stack as foreach loops. The simplest implementation; // technique is to convert each 'then' or 'else' clause *into* a foreach; // loop, over a list of length 0 or 1 depending on the condition, and with no; // iteration variable being assigned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:128,Usability,simpl,simplest,128,"// We have to be able to save if statements to execute later, and they have; // to live on the same stack as foreach loops. The simplest implementation; // technique is to convert each 'then' or 'else' clause *into* a foreach; // loop, over a list of length 0 or 1 depending on the condition, and with no; // iteration variable being assigned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:52,Modifiability,variab,variables,52,// An if-statement introduces a new scope for local variables.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:85,Integrability,message,message,85,"/// ParseAssert - Parse an assert statement.; ///; /// Assert ::= ASSERT condition , message ;",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:27,Testability,assert,assert,27,"/// ParseAssert - Parse an assert statement.; ///; /// Assert ::= ASSERT condition , message ;",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:55,Testability,Assert,Assert,55,"/// ParseAssert - Parse an assert statement.; ///; /// Assert ::= ASSERT condition , message ;",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:66,Testability,ASSERT,ASSERT,66,"/// ParseAssert - Parse an assert statement.; ///; /// Assert ::= ASSERT condition , message ;",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:12,Testability,assert,assert,12,// Eat the 'assert' token.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:50,Availability,error,error,50,"// If the body was previously defined, this is an error.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:65,Modifiability,variab,variables,65,// eat the '{'.; // A group let introduces a new scope for local variables.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:193,Testability,Assert,Assert,193,/// ParseMultiClass - Parse a multiclass definition.; ///; /// MultiClassInst ::= MULTICLASS ID TemplateArgList?; /// ':' BaseMultiClassList '{' MultiClassObject+ '}'; /// MultiClassObject ::= Assert; /// MultiClassObject ::= DefInst; /// MultiClassObject ::= DefMInst; /// MultiClassObject ::= Defvar; /// MultiClassObject ::= Foreach; /// MultiClassObject ::= If; /// MultiClassObject ::= LETCommand '{' ObjectList '}'; /// MultiClassObject ::= LETCommand Object; ///,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:78,Modifiability,variab,variables,78,// Eat the identifier.; // A multiclass body introduces a new scope for local variables.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:13,Availability,error,error,13,// Check for error.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:59,Availability,error,error,59,"// eat the '}'.; // If we have a semicolon, print a gentle error.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:20,Modifiability,inherit,inherits,20,// This record also inherits from a regular class (non-multiclass)?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:14,Modifiability,inherit,inherit,14,// A defm can inherit from regular classes (non-multiclasses) as; // long as they come in the end of the inheritance list.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:105,Modifiability,inherit,inheritance,105,// A defm can inherit from regular classes (non-multiclasses) as; // long as they come in the end of the inheritance list.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:30,Modifiability,inherit,inherit,30,// Process all the classes to inherit as if they were part of a; // regular 'def' and inherit all record values.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:86,Modifiability,inherit,inherit,86,// Process all the classes to inherit as if they were part of a; // regular 'def' and inherit all record values.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:13,Availability,error,error,13,// Check for error.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:106,Modifiability,inherit,inherit,106,// Get the expanded definition prototypes and teach them about; // the record values the current class to inherit has,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:264,Testability,Assert,Assert,264,/// ParseObject; /// Object ::= ClassInst; /// Object ::= DefInst; /// Object ::= MultiClassInst; /// Object ::= DefMInst; /// Object ::= LETCommand '{' ObjectList '}'; /// Object ::= LETCommand Object; /// Object ::= Defset; /// Object ::= Defvar; /// Object ::= Assert; /// Object ::= Dump,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:67,Modifiability,inherit,inheritance,67,"// Check the types of the template argument values for a class; // inheritance, multiclass invocation, or anonymous class invocation.; // If necessary, replace an argument with a cast to the required type.; // The argument count has already been checked.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:56,Integrability,wrap,wrapping,56,"// Allow to use dump directly on `defvar` and `def`, by wrapping; // them with a `!repl`.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.h:71,Testability,Assert,AssertionInfo,71,"/// RecordsEntry - Holds exactly one of a Record, ForeachLoop, or; /// AssertionInfo.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.h:198,Modifiability,variab,variable,198,"/// ForeachLoop - Record the iteration state associated with a for loop.; /// This is used to instantiate items in the loop body.; ///; /// IterVar is allowed to be null, in which case no iteration variable is; /// defined in the loop at all. (This happens when a ForeachLoop is; /// constructed by desugaring an if statement.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.h:19,Modifiability,variab,variable,19,// A scope to hold variable definitions from defvar.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.h:92,Availability,error,error,92,"// When we check whether a variable is already defined, for the purpose of; // reporting an error on redefinition, we don't look up to the parent; // scope, because it's all right to shadow an outer definition with an; // inner one.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.h:27,Modifiability,variab,variable,27,"// When we check whether a variable is already defined, for the purpose of; // reporting an error on redefinition, we don't look up to the parent; // scope, because it's all right to shadow an outer definition with an; // inner one.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.h:67,Modifiability,variab,variables,67,/// CurScope - Innermost of the current nested scopes for 'defvar' variables.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.h:303,Availability,error,errors,303,"// A ""named boolean"" indicating how to parse identifiers. Usually; // identifiers map to some existing object but in special cases; // (e.g. parsing def names) no such object exists yet because we are; // in the middle of creating in. For those situations, allow the; // parser to ignore missing object errors.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.h:101,Availability,error,error,101,"/// ParseFile - Main entrypoint for parsing a tblgen file. These parser; /// routines return true on error, or false on success.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.h:77,Integrability,rout,routines,77,"/// ParseFile - Main entrypoint for parsing a tblgen file. These parser; /// routines return true on error, or false on success.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.h:111,Testability,assert,assertion,111,"// Returns a pointer to the new scope, so that the caller can pass it back; // to PopScope which will check by assertion that the pushes and pops; // match up properly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/TableGen/TGParser.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Target.cpp:3,Safety,Avoid,Avoid,3,"// Avoid including ""llvm-c/Core.h"" for compile time, fwd-declare this instead.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/Target.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Target.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp:114,Modifiability,variab,variable,114,"/// getKindForGlobal - This is a top-level target-independent classifier for; /// a global object. Given a global variable and information from the TM, this; /// function classifies the global in a target independent manner. This function; /// may be overridden by the target implementation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp:10,Modifiability,variab,variables,10,// Global variables require more detailed analysis.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp:24,Modifiability,variab,variables,24,// Zero-initialized TLS variables with local linkage always get classified; // as ThreadBSSLocal.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp:3,Modifiability,Variab,Variables,3,// Variables with common linkage always get classified as common.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp:10,Modifiability,variab,variables,10,// Global variables with '!exclude' should get the exclude section kind if; // they have an explicit section and no other metadata.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp:160,Availability,avail,available,160,/// This method computes the appropriate section to emit the specified global; /// variable or function definition. This should not be passed external (or; /// available externally) globals.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp:83,Modifiability,variab,variable,83,/// This method computes the appropriate section to emit the specified global; /// variable or function definition. This should not be passed external (or; /// available externally) globals.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp:23,Integrability,depend,depending,23,// Use default section depending on the 'type' of global,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp:160,Availability,avail,available,160,/// This method computes the appropriate section to emit the specified global; /// variable or function definition. This should not be passed external (or; /// available externally) globals.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp:83,Modifiability,variab,variable,83,/// This method computes the appropriate section to emit the specified global; /// variable or function definition. This should not be passed external (or; /// available externally) globals.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp:99,Modifiability,variab,variable,99,/// getTTypeGlobalReference - Return an MCExpr to use for a; /// reference to the specified global variable from exception; /// handling information.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp:19,Usability,clear,clear,19,"// FIXME: It's not clear what, if any, default this should have - perhaps a; // null return could mean 'no location' & we should just do that here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetLoweringObjectFile.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp:128,Energy Efficiency,reduce,reduces,128,"// Treat all globals in explicit sections as small, except for the standard; // large sections of .lbss, .ldata, .lrodata. This reduces the risk of linking; // together small and large sections, resulting in small references to large; // data sections. The code model attribute overrides this above.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp:140,Safety,risk,risk,140,"// Treat all globals in explicit sections as small, except for the standard; // large sections of .lbss, .ldata, .lrodata. This reduces the risk of linking; // together small and large sections, resulting in small references to large; // data sections. The code model attribute overrides this above.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp:333,Testability,log,logic,333,"// According to the llvm language reference, we should be able to; // just return false in here if we have a GV, as we know it is; // dso_preemptable. At this point in time, the various IR producers; // have not been transitioned to always produce a dso_local when it; // is possible to do so.; //; // As a result we still have some logic in here to improve the quality of the; // generated code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp:13,Modifiability,variab,variables,13,"// On MinGW, variables that haven't been declared with DLLImport may still; // end up automatically imported by the linker. To make this feasible,; // don't assume the variables to be DSO local unless we actually know; // that for sure. This only has to be done for variables; for functions; // the linker can insert thunks for calling functions from another DLL.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp:168,Modifiability,variab,variables,168,"// On MinGW, variables that haven't been declared with DLLImport may still; // end up automatically imported by the linker. To make this feasible,; // don't assume the variables to be DSO local unless we actually know; // that for sure. This only has to be done for variables; for functions; // the linker can insert thunks for calling functions from another DLL.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp:266,Modifiability,variab,variables,266,"// On MinGW, variables that haven't been declared with DLLImport may still; // end up automatically imported by the linker. To make this feasible,; // don't assume the variables to be DSO local unless we actually know; // that for sure. This only has to be done for variables; for functions; // the linker can insert thunks for calling functions from another DLL.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp:16,Performance,optimiz,optimization,16,"/// Returns the optimization level: None, Less, Default, or Aggressive.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp:3,Usability,Simpl,Simple,3,"// Simple case: If GV is not private, it is not important to find out if; // private labels are legal in this case or not.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp:24,Integrability,depend,depend,24,"// Since Analysis can't depend on Target, use a std::function to invert the; // dependency.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp:80,Integrability,depend,dependency,80,"// Since Analysis can't depend on Target, use a std::function to invert the; // dependency.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64.h:29,Integrability,interface,interface,29,"//==-- AArch64.h - Top-level interface for AArch64 --------------*- C++ -*-==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains the entry points for global functions defined in the LLVM; // AArch64 back-end.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A53Fix835769.cpp:706,Performance,load,loads,706,"//===-- AArch64A53Fix835769.cpp -------------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // This pass changes code to work around Cortex-A53 erratum 835769.; // It works around it by inserting a nop instruction in code sequences that; // in some circumstances may trigger the erratum.; // It inserts a nop instruction between a sequence of the following 2 classes; // of instructions:; // instr 1: mem-instr (including loads, stores and prefetches).; // instr 2: non-SIMD integer multiply-accumulate writing 64-bit X registers.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A53Fix835769.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A53Fix835769.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A53Fix835769.cpp:45,Performance,load,load,45,"// Must return true if this instruction is a load, a store or a prefetch.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A53Fix835769.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A53Fix835769.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A53Fix835769.cpp:8,Deployability,update,update,8,"// Then update the basic block, inserting nops between the detected sequences.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A53Fix835769.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A53Fix835769.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A53Fix835769.cpp:59,Safety,detect,detected,59,"// Then update the basic block, inserting nops between the detected sequences.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A53Fix835769.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A53Fix835769.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:906,Energy Efficiency,allocate,allocate,906,"//===-- AArch64A57FPLoadBalancing.cpp - Balance FP ops statically on A57---===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // For best-case performance on Cortex-A57, we should try to use a balanced; // mix of odd and even D-registers when performing a critical sequence of; // independent, non-quadword FP/ASIMD floating-point multiply or; // multiply-accumulate operations.; //; // This pass attempts to detect situations where the register allocation may; // adversely affect this load balancing and to change the registers used so as; // to better utilize the CPU.; //; // Ideally we'd just take each multiply or multiply-accumulate in turn and; // allocate it alternating even or odd registers. However, multiply-accumulates; // are most efficiently performed in the same functional unit as their; // accumulation operand. Therefore this pass tries to find maximal sequences; // (""Chains"") of multiply-accumulates linked via their accumulation operand,; // and assign them all the same ""color"" (oddness/evenness).; //; // This optimization affects S-register and D-register floating point; // multiplies and FMADD/FMAs, as well as vector (floating point only) muls and; // FMADD/FMA. Q register instructions (and 128-bit vector instructions) are; // not affected.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:996,Energy Efficiency,efficient,efficiently,996,"//===-- AArch64A57FPLoadBalancing.cpp - Balance FP ops statically on A57---===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // For best-case performance on Cortex-A57, we should try to use a balanced; // mix of odd and even D-registers when performing a critical sequence of; // independent, non-quadword FP/ASIMD floating-point multiply or; // multiply-accumulate operations.; //; // This pass attempts to detect situations where the register allocation may; // adversely affect this load balancing and to change the registers used so as; // to better utilize the CPU.; //; // Ideally we'd just take each multiply or multiply-accumulate in turn and; // allocate it alternating even or odd registers. However, multiply-accumulates; // are most efficiently performed in the same functional unit as their; // accumulation operand. Therefore this pass tries to find maximal sequences; // (""Chains"") of multiply-accumulates linked via their accumulation operand,; // and assign them all the same ""color"" (oddness/evenness).; //; // This optimization affects S-register and D-register floating point; // multiplies and FMADD/FMAs, as well as vector (floating point only) muls and; // FMADD/FMA. Q register instructions (and 128-bit vector instructions) are; // not affected.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:393,Performance,perform,performance,393,"//===-- AArch64A57FPLoadBalancing.cpp - Balance FP ops statically on A57---===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // For best-case performance on Cortex-A57, we should try to use a balanced; // mix of odd and even D-registers when performing a critical sequence of; // independent, non-quadword FP/ASIMD floating-point multiply or; // multiply-accumulate operations.; //; // This pass attempts to detect situations where the register allocation may; // adversely affect this load balancing and to change the registers used so as; // to better utilize the CPU.; //; // Ideally we'd just take each multiply or multiply-accumulate in turn and; // allocate it alternating even or odd registers. However, multiply-accumulates; // are most efficiently performed in the same functional unit as their; // accumulation operand. Therefore this pass tries to find maximal sequences; // (""Chains"") of multiply-accumulates linked via their accumulation operand,; // and assign them all the same ""color"" (oddness/evenness).; //; // This optimization affects S-register and D-register floating point; // multiplies and FMADD/FMAs, as well as vector (floating point only) muls and; // FMADD/FMA. Q register instructions (and 128-bit vector instructions) are; // not affected.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:493,Performance,perform,performing,493,"//===-- AArch64A57FPLoadBalancing.cpp - Balance FP ops statically on A57---===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // For best-case performance on Cortex-A57, we should try to use a balanced; // mix of odd and even D-registers when performing a critical sequence of; // independent, non-quadword FP/ASIMD floating-point multiply or; // multiply-accumulate operations.; //; // This pass attempts to detect situations where the register allocation may; // adversely affect this load balancing and to change the registers used so as; // to better utilize the CPU.; //; // Ideally we'd just take each multiply or multiply-accumulate in turn and; // allocate it alternating even or odd registers. However, multiply-accumulates; // are most efficiently performed in the same functional unit as their; // accumulation operand. Therefore this pass tries to find maximal sequences; // (""Chains"") of multiply-accumulates linked via their accumulation operand,; // and assign them all the same ""color"" (oddness/evenness).; //; // This optimization affects S-register and D-register floating point; // multiplies and FMADD/FMAs, as well as vector (floating point only) muls and; // FMADD/FMA. Q register instructions (and 128-bit vector instructions) are; // not affected.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:737,Performance,load,load,737,"//===-- AArch64A57FPLoadBalancing.cpp - Balance FP ops statically on A57---===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // For best-case performance on Cortex-A57, we should try to use a balanced; // mix of odd and even D-registers when performing a critical sequence of; // independent, non-quadword FP/ASIMD floating-point multiply or; // multiply-accumulate operations.; //; // This pass attempts to detect situations where the register allocation may; // adversely affect this load balancing and to change the registers used so as; // to better utilize the CPU.; //; // Ideally we'd just take each multiply or multiply-accumulate in turn and; // allocate it alternating even or odd registers. However, multiply-accumulates; // are most efficiently performed in the same functional unit as their; // accumulation operand. Therefore this pass tries to find maximal sequences; // (""Chains"") of multiply-accumulates linked via their accumulation operand,; // and assign them all the same ""color"" (oddness/evenness).; //; // This optimization affects S-register and D-register floating point; // multiplies and FMADD/FMAs, as well as vector (floating point only) muls and; // FMADD/FMA. Q register instructions (and 128-bit vector instructions) are; // not affected.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:1008,Performance,perform,performed,1008,"//===-- AArch64A57FPLoadBalancing.cpp - Balance FP ops statically on A57---===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // For best-case performance on Cortex-A57, we should try to use a balanced; // mix of odd and even D-registers when performing a critical sequence of; // independent, non-quadword FP/ASIMD floating-point multiply or; // multiply-accumulate operations.; //; // This pass attempts to detect situations where the register allocation may; // adversely affect this load balancing and to change the registers used so as; // to better utilize the CPU.; //; // Ideally we'd just take each multiply or multiply-accumulate in turn and; // allocate it alternating even or odd registers. However, multiply-accumulates; // are most efficiently performed in the same functional unit as their; // accumulation operand. Therefore this pass tries to find maximal sequences; // (""Chains"") of multiply-accumulates linked via their accumulation operand,; // and assign them all the same ""color"" (oddness/evenness).; //; // This optimization affects S-register and D-register floating point; // multiplies and FMADD/FMAs, as well as vector (floating point only) muls and; // FMADD/FMA. Q register instructions (and 128-bit vector instructions) are; // not affected.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:1285,Performance,optimiz,optimization,1285,"//===-- AArch64A57FPLoadBalancing.cpp - Balance FP ops statically on A57---===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // For best-case performance on Cortex-A57, we should try to use a balanced; // mix of odd and even D-registers when performing a critical sequence of; // independent, non-quadword FP/ASIMD floating-point multiply or; // multiply-accumulate operations.; //; // This pass attempts to detect situations where the register allocation may; // adversely affect this load balancing and to change the registers used so as; // to better utilize the CPU.; //; // Ideally we'd just take each multiply or multiply-accumulate in turn and; // allocate it alternating even or odd registers. However, multiply-accumulates; // are most efficiently performed in the same functional unit as their; // accumulation operand. Therefore this pass tries to find maximal sequences; // (""Chains"") of multiply-accumulates linked via their accumulation operand,; // and assign them all the same ""color"" (oddness/evenness).; //; // This optimization affects S-register and D-register floating point; // multiplies and FMADD/FMAs, as well as vector (floating point only) muls and; // FMADD/FMA. Q register instructions (and 128-bit vector instructions) are; // not affected.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:659,Safety,detect,detect,659,"//===-- AArch64A57FPLoadBalancing.cpp - Balance FP ops statically on A57---===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // For best-case performance on Cortex-A57, we should try to use a balanced; // mix of odd and even D-registers when performing a critical sequence of; // independent, non-quadword FP/ASIMD floating-point multiply or; // multiply-accumulate operations.; //; // This pass attempts to detect situations where the register allocation may; // adversely affect this load balancing and to change the registers used so as; // to better utilize the CPU.; //; // Ideally we'd just take each multiply or multiply-accumulate in turn and; // allocate it alternating even or odd registers. However, multiply-accumulates; // are most efficiently performed in the same functional unit as their; // accumulation operand. Therefore this pass tries to find maximal sequences; // (""Chains"") of multiply-accumulates linked via their accumulation operand,; // and assign them all the same ""color"" (oddness/evenness).; //; // This optimization affects S-register and D-register floating point; // multiplies and FMADD/FMAs, as well as vector (floating point only) muls and; // FMADD/FMA. Q register instructions (and 128-bit vector instructions) are; // not affected.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:134,Testability,test,testing,134,// Enforce the algorithm to use the scavenged register even when the original; // destination register is the correct color. Used for testing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:105,Testability,test,testing,105,// Never use the balance information obtained from chains - return a specific; // color always. Used for testing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:963,Safety,safe,safely,963,"/// A Chain is a sequence of instructions that are linked together by; /// an accumulation operand. For example:; ///; /// fmul def d0, ?; /// fmla def d1, ?, ?, killed d0; /// fmla def d2, ?, ?, killed d1; ///; /// There may be other instructions interleaved in the sequence that; /// do not belong to the chain. These other instructions must not use; /// the ""chain"" register at any point.; ///; /// We currently only support chains where the ""chain"" operand is killed; /// at each link in the chain for simplicity.; /// A chain has three important instructions - Start, Last and Kill.; /// * The start instruction is the first instruction in the chain.; /// * Last is the final instruction in the chain.; /// * Kill may or may not be defined. If defined, Kill is the instruction; /// where the outgoing value of the Last instruction is killed.; /// This information is important as if we know the outgoing value is; /// killed with no intervening uses, we can safely change its register.; ///; /// Without a kill instruction, we must assume the outgoing value escapes; /// beyond our model and either must not change its register or must; /// create a fixup FMOV to keep the old register value consistent.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:506,Usability,simpl,simplicity,506,"/// A Chain is a sequence of instructions that are linked together by; /// an accumulation operand. For example:; ///; /// fmul def d0, ?; /// fmla def d1, ?, ?, killed d0; /// fmla def d2, ?, ?, killed d1; ///; /// There may be other instructions interleaved in the sequence that; /// do not belong to the chain. These other instructions must not use; /// the ""chain"" register at any point.; ///; /// We currently only support chains where the ""chain"" operand is killed; /// at each link in the chain for simplicity.; /// A chain has three important instructions - Start, Last and Kill.; /// * The start instruction is the first instruction in the chain.; /// * Last is the final instruction in the chain.; /// * Kill may or may not be defined. If defined, Kill is the instruction; /// where the outgoing value of the Last instruction is killed.; /// This information is important as if we know the outgoing value is; /// killed with no intervening uses, we can safely change its register.; ///; /// Without a kill instruction, we must assume the outgoing value escapes; /// beyond our model and either must not change its register or must; /// create a fixup FMOV to keep the old register value consistent.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:126,Testability,test,tests,126,"/// The index, from the start of the basic block, that each marker; /// appears. These are stored so we can do quick interval tests.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:13,Usability,simpl,simple,13,/// Return a simple string representation of the chain.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:172,Performance,perform,perform,172,"// Group the chains into disjoint sets based on their liveness range. This is; // a poor-man's version of graph coloring. Ideally we'd create an interference; // graph and perform full-on graph coloring on that, but;; // (a) That's rather heavyweight for only two colors.; // (b) We expect multiple disjoint interference regions - in practice the live; // range of chains is quite small and they are clustered between loads; // and stores.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:418,Performance,load,loads,418,"// Group the chains into disjoint sets based on their liveness range. This is; // a poor-man's version of graph coloring. Ideally we'd create an interference; // graph and perform full-on graph coloring on that, but;; // (a) That's rather heavyweight for only two colors.; // (b) We expect multiple disjoint interference regions - in practice the live; // range of chains is quite small and they are clustered between loads; // and stores.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:191,Usability,simpl,simpler,191,"// Now we assume that every member of an equivalence class interferes; // with every other member of that class, and with no members of other classes.; // Convert the EquivalenceClasses to a simpler set of sets.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:512,Usability,simpl,simplify,512,"// As we only have two colors, we can track the global (BB-level) balance of; // odds versus evens. We aim to keep this near zero to keep both execution; // units fed.; // Positive means we're even-heavy, negative we're odd-heavy.; //; // FIXME: If chains have interdependencies, for example:; // mul r0, r1, r2; // mul r3, r0, r1; // We do not model this and may color each one differently, assuming we'll; // get ILP when we obviously can't. This hasn't been seen to be a problem; // in practice so far, so we simplify the algorithm by ignoring it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:469,Modifiability,config,configurable,469,"// We try and get the best candidate from L to color next, given that our; // preferred color is ""PreferredColor"". L is ordered from larger to smaller; // chains. It is beneficial to color the large chains before the small chains,; // but if we can't find a chain of the maximum length with the preferred color,; // we fuzz the size and look for slightly smaller chains before giving up and; // returning a chain that must be recolored.; // FIXME: Does this need to be configurable?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:326,Deployability,update,updated,326,"// Sort by descending size order so that we allocate the most important; // sets first.; // Tie-break equivalent sizes by sorting chains requiring fixups before; // those without fixups. The logic here is that we should look at the; // chains that we cannot change before we look at those we can,; // so the parity counter is updated and we know what color we should; // change them to!; // Final tie-break with instruction order so pass output is stable (i.e. not; // dependent on malloc'd pointer values).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:44,Energy Efficiency,allocate,allocate,44,"// Sort by descending size order so that we allocate the most important; // sets first.; // Tie-break equivalent sizes by sorting chains requiring fixups before; // those without fixups. The logic here is that we should look at the; // chains that we cannot change before we look at those we can,; // so the parity counter is updated and we know what color we should; // change them to!; // Final tie-break with instruction order so pass output is stable (i.e. not; // dependent on malloc'd pointer values).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:469,Integrability,depend,dependent,469,"// Sort by descending size order so that we allocate the most important; // sets first.; // Tie-break equivalent sizes by sorting chains requiring fixups before; // those without fixups. The logic here is that we should look at the; // chains that we cannot change before we look at those we can,; // so the parity counter is updated and we know what color we should; // change them to!; // Final tie-break with instruction order so pass output is stable (i.e. not; // dependent on malloc'd pointer values).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:191,Testability,log,logic,191,"// Sort by descending size order so that we allocate the most important; // sets first.; // Tie-break equivalent sizes by sorting chains requiring fixups before; // those without fixups. The logic here is that we should look at the; // chains that we cannot change before we look at those we can,; // so the parity counter is updated and we know what color we should; // change them to!; // Final tie-break with instruction order so pass output is stable (i.e. not; // dependent on malloc'd pointer values).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:164,Availability,down,down,164,"// If we'll need a fixup FMOV, don't bother. Testing has shown that this; // happens infrequently and when it does it has at least a 50% chance of; // slowing code down instead of speeding it up.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:45,Testability,Test,Testing,45,"// If we'll need a fixup FMOV, don't bother. Testing has shown that this; // happens infrequently and when it does it has at least a 50% chance of; // slowing code down instead of speeding it up.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:47,Availability,avail,available,47,// Can we find an appropriate register that is available throughout the life; // of the chain? Simulate liveness backwards until the end of the chain.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:34,Availability,alive,alive,34,// Check which register units are alive throughout the chain.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:16,Energy Efficiency,allocate,allocate,16,"// Make sure we allocate in-order, to get the cheapest registers first.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:12,Safety,safe,safe,12,// Now it's safe to remove the substs identified earlier.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:207,Modifiability,rewrite,rewrite,207,// For simplicity we only chain together sequences of MULs/MLAs where the; // accumulator register is killed on each instruction. This means we don't; // need to track other uses of the registers we want to rewrite.; //; // FIXME: We could extend to handle the non-kill cases for more coverage.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:240,Modifiability,extend,extend,240,// For simplicity we only chain together sequences of MULs/MLAs where the; // accumulator register is killed on each instruction. This means we don't; // need to track other uses of the registers we want to rewrite.; //; // FIXME: We could extend to handle the non-kill cases for more coverage.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:7,Usability,simpl,simplicity,7,// For simplicity we only chain together sequences of MULs/MLAs where the; // accumulator register is killed on each instruction. This means we don't; // need to track other uses of the registers we want to rewrite.; //; // FIXME: We could extend to handle the non-kill cases for more coverage.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp:964,Performance,optimiz,optimization,964,"//===-- AArch64AdvSIMDScalar.cpp - Replace dead defs w/ zero reg --===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // When profitable, replace GPR targeting i64 instructions with their; // AdvSIMD scalar equivalents. Generally speaking, ""profitable"" is defined; // as minimizing the number of cross-class register copies.; //===----------------------------------------------------------------------===//; //===----------------------------------------------------------------------===//; // TODO: Graph based predicate heuristics.; // Walking the instruction list linearly will get many, perhaps most, of; // the cases, but to do a truly thorough job of this, we need a more; // wholistic approach.; //; // This optimization is very similar in spirit to the register allocator's; // spill placement, only here we're determining where to place cross-class; // register copies rather than spills. As such, a similar approach is; // called for.; //; // We want to build up a set of graphs of all instructions which are candidates; // for transformation along with instructions which generate their inputs and; // consume their outputs. For each edge in the graph, we assign a weight; // based on whether there is a copy required there (weight zero if not) and; // the block frequency of the block containing the defining or using; // instruction, whichever is less. Our optimization is then a graph problem; // to minimize the total weight of all the graphs, then transform instructions; // and add or remove copy instructions as called for to implement the; // solution.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp:1619,Performance,optimiz,optimization,1619,"//===-- AArch64AdvSIMDScalar.cpp - Replace dead defs w/ zero reg --===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // When profitable, replace GPR targeting i64 instructions with their; // AdvSIMD scalar equivalents. Generally speaking, ""profitable"" is defined; // as minimizing the number of cross-class register copies.; //===----------------------------------------------------------------------===//; //===----------------------------------------------------------------------===//; // TODO: Graph based predicate heuristics.; // Walking the instruction list linearly will get many, perhaps most, of; // the cases, but to do a truly thorough job of this, we need a more; // wholistic approach.; //; // This optimization is very similar in spirit to the register allocator's; // spill placement, only here we're determining where to place cross-class; // register copies rather than spills. As such, a similar approach is; // called for.; //; // We want to build up a set of graphs of all instructions which are candidates; // for transformation along with instructions which generate their inputs and; // consume their outputs. For each edge in the graph, we assign a weight; // based on whether there is a copy required there (weight zero if not) and; // the block frequency of the block containing the defining or using; // instruction, whichever is less. Our optimization is then a graph problem; // to minimize the total weight of all the graphs, then transform instructions; // and add or remove copy instructions as called for to implement the; // solution.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp:98,Testability,test,testing,98,// Allow forcing all i64 operations with equivalent SIMD instructions to use; // them. For stress-testing the transformation function.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp:121,Deployability,Update,Update,121,"// transformInstruction - Perform the transformation of an instruction; // to its equivalant AdvSIMD scalar instruction. Update inputs and outputs; // to be the correct register class, minimizing cross-class copies.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp:26,Performance,Perform,Perform,26,"// transformInstruction - Perform the transformation of an instruction; // to its equivalant AdvSIMD scalar instruction. Update inputs and outputs; // to be the correct register class, minimizing cross-class copies.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp:121,Deployability,Update,Update,121,"// transformInstruction - Perform the transformation of an instruction; // to its equivalant AdvSIMD scalar instruction. Update inputs and outputs; // to be the correct register class, minimizing cross-class copies.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp:26,Performance,Perform,Perform,26,"// transformInstruction - Perform the transformation of an instruction; // to its equivalant AdvSIMD scalar instruction. Update inputs and outputs; // to be the correct register class, minimizing cross-class copies.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp:130,Safety,avoid,avoid,130,// Create a vreg for the destination.; // FIXME: No need to do this if the ultimate user expects an FPR64.; // Check for that and avoid the copy if possible.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp:54,Usability,simpl,simple,54,"// For now, all of the new instructions have the same simple three-register; // form, so no need to special case based on what instruction we're; // building.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp:60,Safety,avoid,avoid,60,// Now copy the result back out to a GPR.; // FIXME: Try to avoid this if all uses could actually just use the FPR64; // directly.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AdvSIMDScalarPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp:120,Availability,down,down,120,"// The first argument to a thunk is the called function, stored in x9.; // For exit thunks, we pass the called function down to the emulator;; // for entry/guest exit thunks, we just call the Arm64 function directly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp:268,Usability,simpl,simplify,268,"// sret+inreg indicates a call that returns a C++ class value. This is; // actually equivalent to just passing and returning a void* pointer; // as the first argument. Translate it that way, instead of trying; // to model ""inreg"" in the thunk's calling convention, to simplify; // the rest of the code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp:194,Safety,avoid,avoids,194,"// Only copy sret from the first argument. For C++ instance methods, clang can; // stick an sret marking on a later argument, but it doesn't actually affect; // the ABI, so we can omit it. This avoids triggering a verifier assertion.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp:223,Testability,assert,assertion,223,"// Only copy sret from the first argument. For C++ instance methods, clang can; // stick an sret marking on a later argument, but it doesn't actually affect; // the ABI, so we can omit it. This avoids triggering a verifier assertion.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp:98,Usability,simpl,simple,98,"// Translate arguments from AArch64 calling convention to x86 calling; // convention.; //; // For simple types, we don't need to do any translation: they're; // represented the same way. (Implicit sign extension is not part of; // either convention.); //; // The big thing we have to worry about is struct types... but; // fortunately AArch64 clang is pretty friendly here: the cases that need; // translation are always passed as a struct or array. (If we run into; // some cases where this doesn't work, we can teach clang to mark it up; // with an attribute.); //; // The first argument is the called function, stored in x9.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp:263,Integrability,rout,routine,263,"// Builds the ""guest exit thunk"", a helper to call a function which may or may; // not be an exit thunk. (We optimistically assume non-dllimport function; // declarations refer to functions defined in AArch64 code; if the linker; // can't prove that, we use this routine instead.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp:3,Performance,Load,Load,3,// Load the global symbol as a pointer to the check function.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp:3,Performance,Load,Load,3,// Load the global symbol as a pointer to the check function.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Arm64ECCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:474,Integrability,depend,dependent,474,"//===- AArch64AsmPrinter.cpp - AArch64 LLVM assembly writer ---------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a printer that converts from our internal representation; // of machine-dependent LLVM code to the AArch64 assembly language.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:4,Integrability,Wrap,Wrapper,4,/// Wrapper for MCInstLowering.lowerOperand() for the; /// tblgen'erated pseudo lowering.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:47,Usability,simpl,simple,47,/// tblgen'erated driver function for lowering simple MI->MC; /// pseudo instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:205,Deployability,patch,patching,205,"// We want to emit the following pattern:; //; // .Lxray_sled_N:; // ALIGN; // B #32; // ; 7 NOP instructions (28 bytes); // .tmpN; //; // We need the 28 bytes (7 instructions) because at runtime, we'd be patching; // over the full 32 bytes (8 instructions) with the following pattern:; //; // STP X0, X30, [SP, #-16]! ; push X0 and the link register to the stack; // LDR W17, #12 ; W17 := function ID; // LDR X16,#12 ; X16 := addr of __xray_FunctionEntry or __xray_FunctionExit; // BLR X16 ; call the tracing trampoline; // ;DATA: 32 bits of function ID; // ;DATA: lower 32 bits of the address of the trampoline; // ;DATA: higher 32 bits of the address of the trampoline; // LDP X0, X30, [SP], #16 ; pop X0 and the link register from the stack; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:549,Deployability,patch,patching,549,"// Emit the following code for Intrinsic::{xray_customevent,xray_typedevent}; // (built-in functions __xray_customevent/__xray_typedevent).; //; // .Lxray_event_sled_N:; // b 1f; // save x0 and x1 (and also x2 for TYPED_EVENT_CALL); // set up x0 and x1 (and also x2 for TYPED_EVENT_CALL); // bl __xray_CustomEvent or __xray_TypedEvent; // restore x0 and x1 (and also x2 for TYPED_EVENT_CALL); // 1:; //; // There are 6 instructions for EVENT_CALL and 9 for TYPED_EVENT_CALL.; //; // Then record a sled of kind CUSTOM_EVENT or TYPED_EVENT.; // After patching, b .+N will become a nop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:87,Security,hash,hashes,87,// Default to using the intra-procedure-call temporary registers for; // comparing the hashes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:54,Performance,load,load,54,"// Checking XZR makes no sense. Instead of emitting a load, zero; // ScratchRegs[0] and use it for the ESR AddrIndex below.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:25,Deployability,patch,patchable-function-prefix,25,// Adjust the offset for patchable-function-prefix. This assumes that; // patchable-function-prefix is the same for all functions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:74,Deployability,patch,patchable-function-prefix,74,// Adjust the offset for patchable-function-prefix. This assumes that; // patchable-function-prefix is the same for all functions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:3,Performance,Load,Load,3,// Load the target function type hash.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:33,Security,hash,hash,33,// Load the target function type hash.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:3,Performance,Load,Load,3,// Load the expected type hash.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:26,Security,hash,hash,26,// Load the expected type hash.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:15,Security,hash,hashes,15,// Compare the hashes and trap if there's a mismatch.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:221,Security,hash,hash,221,"// The base ESR is 0x8000 and the register information is encoded in bits; // 0-9 as follows:; // - 0-4: n, where the register Xn contains the target address; // - 5-9: m, where the register Wm contains the expected type hash; // Where n, m are in [0, 30].",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:30,Performance,load,loader,30,"// The Linux kernel's dynamic loader doesn't support GOT relative; // relocations, but it doesn't support late binding either, so just call; // the function directly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:17,Performance,load,load,17,"// Intentionally load the GOT entry and branch to it, rather than possibly; // late binding the function, which may clobber the registers before we; // have a chance to save them.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:245,Performance,perform,perform,245,"// Funny Darwin hack: This flag tells the linker that no global symbols; // contain code that falls through to other global symbols (e.g. the obvious; // implementation of multiple entry points). If this doesn't occur, the; // linker can safely perform dead code stripping. Since LLVM never; // generates code that does this, it is always safe to set.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:238,Safety,safe,safely,238,"// Funny Darwin hack: This flag tells the linker that no global symbols; // contain code that falls through to other global symbols (e.g. the obvious; // implementation of multiple entry points). If this doesn't occur, the; // linker can safely perform dead code stripping. Since LLVM never; // generates code that does this, it is always safe to set.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:339,Safety,safe,safe,339,"// Funny Darwin hack: This flag tells the linker that no global symbols; // contain code that falls through to other global symbols (e.g. the obvious; // implementation of multiple entry points). If this doesn't occur, the; // linker can safely perform dead code stripping. Since LLVM never; // generates code that does this, it is always safe to set.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:18,Availability,fault,fault,18,// Emit stack and fault map information.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:71,Safety,avoid,avoid,71,"// Darwin uses a linker-private symbol name for constant-pools (to; // avoid addends on the relocation?), ELF has no such concept and; // uses a normal private symbol.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:3,Performance,Load,Load,3,// Load the number of instruction-steps to offset from the label.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:11,Deployability,patch,patchpoint,11,"// Lower a patchpoint of the form:; // [<def>], <id>, <numBytes>, <target>, <numArgs>",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:3,Usability,Simpl,Simple,3,// Simple pseudo-instructions have their lowering (with expansion to real; // instructions) auto-generated.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:94,Energy Efficiency,efficient,efficient,94,"// It is generally beneficial to rewrite ""fmov s0, wzr"" to ""movi d0, #0"".; // as movi is more efficient across all cores. Newer cores can eliminate; // fmovs early and there is no difference with movi, but this not true for; // all implementations.; //; // The floating-point version doesn't quite work in rare cases on older; // CPUs, so on those targets we lower this instruction to movi.16b instead.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:33,Modifiability,rewrite,rewrite,33,"// It is generally beneficial to rewrite ""fmov s0, wzr"" to ""movi d0, #0"".; // as movi is more efficient across all cores. Newer cores can eliminate; // fmovs early and there is no difference with movi, but this not true for; // all implementations.; //; // The floating-point version doesn't quite work in rare cases on older; // CPUs, so on those targets we lower this instruction to movi.16b instead.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:63,Performance,optimiz,optimizing,63,"// These stub helpers are only ever called once, so here we're optimizing for; // minimum size by using the pre-indexed store variants, which saves a few; // bytes of instructions to bump & restore sp.; // _ifunc.stub_helper:; // stp	fp, lr, [sp, #-16]!; // mov	fp, sp; // stp	x1, x0, [sp, #-16]!; // stp	x3, x2, [sp, #-16]!; // stp	x5, x4, [sp, #-16]!; // stp	x7, x6, [sp, #-16]!; // stp	d1, d0, [sp, #-16]!; // stp	d3, d2, [sp, #-16]!; // stp	d5, d4, [sp, #-16]!; // stp	d7, d6, [sp, #-16]!; // bl	_resolver; // adrp	x16, lazy_pointer@GOTPAGE; // ldr	x16, [x16, lazy_pointer@GOTPAGEOFF]; // str	x0, [x16]; // mov	x16, x0; // ldp	d7, d6, [sp], #16; // ldp	d5, d4, [sp], #16; // ldp	d3, d2, [sp], #16; // ldp	d1, d0, [sp], #16; // ldp	x7, x6, [sp], #16; // ldp	x5, x4, [sp], #16; // ldp	x3, x2, [sp], #16; // ldp	x1, x0, [sp], #16; // ldp	fp, lr, [sp], #16; // br	x16",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:9,Testability,stub,stub,9,"// These stub helpers are only ever called once, so here we're optimizing for; // minimum size by using the pre-indexed store variants, which saves a few; // bytes of instructions to bump & restore sp.; // _ifunc.stub_helper:; // stp	fp, lr, [sp, #-16]!; // mov	fp, sp; // stp	x1, x0, [sp, #-16]!; // stp	x3, x2, [sp, #-16]!; // stp	x5, x4, [sp, #-16]!; // stp	x7, x6, [sp, #-16]!; // stp	d1, d0, [sp, #-16]!; // stp	d3, d2, [sp, #-16]!; // stp	d5, d4, [sp, #-16]!; // stp	d7, d6, [sp, #-16]!; // bl	_resolver; // adrp	x16, lazy_pointer@GOTPAGE; // ldr	x16, [x16, lazy_pointer@GOTPAGEOFF]; // str	x0, [x16]; // mov	x16, x0; // ldp	d7, d6, [sp], #16; // ldp	d5, d4, [sp], #16; // ldp	d3, d2, [sp], #16; // ldp	d1, d0, [sp], #16; // ldp	x7, x6, [sp], #16; // ldp	x5, x4, [sp], #16; // ldp	x3, x2, [sp], #16; // ldp	x1, x0, [sp], #16; // ldp	fp, lr, [sp], #16; // br	x16",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64BranchTargets.cpp:715,Security,attack,attacks,715,"//===-- AArch64BranchTargets.cpp -- Harden code using v8.5-A BTI extension -==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass inserts BTI instructions at the start of every function and basic; // block which could be indirectly called. The hardware will (when enabled); // trap when an indirect branch or call instruction targets an instruction; // which is not a valid BTI instruction. This is intended to guard against; // control-flow hijacking attacks. Note that this does not do anything for RET; // instructions, as they can be more precisely protected by return address; // signing.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64BranchTargets.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64BranchTargets.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp:433,Integrability,rout,routines,433,"//=== AArch64CallingConvention.cpp - AArch64 CC impl ------------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains the table-generated and custom routines for the AArch64; // Calling Convention.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp:88,Energy Efficiency,allocate,allocate,88,"// The calling convention for passing SVE tuples states that in the event; // we cannot allocate enough registers for the tuple we should still leave; // any remaining registers unallocated. However, when we call the; // CCAssignFn again we want it to behave as if all remaining registers are; // allocated. This will force the code to pass the tuple indirectly in; // accordance with the PCS.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp:297,Energy Efficiency,allocate,allocated,297,"// The calling convention for passing SVE tuples states that in the event; // we cannot allocate enough registers for the tuple we should still leave; // any remaining registers unallocated. However, when we call the; // CCAssignFn again we want it to behave as if all remaining registers are; // allocated. This will force the code to pass the tuple indirectly in; // accordance with the PCS.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp:94,Availability,avail,available,94,"// Return the register state back to how it was before, leaving any; // unallocated registers available for other smaller types.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp:37,Energy Efficiency,allocate,allocated,37,// All pending members have now been allocated,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp:37,Energy Efficiency,allocate,allocated,37,// All pending members have now been allocated,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp:38,Energy Efficiency,allocate,allocated,38,// Add the argument to the list to be allocated once we know the size of the; // block.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp:117,Availability,avail,available,117,"/// Given an [N x Ty] block, it should be passed in a consecutive sequence of; /// registers. If no such sequence is available, mark the rest of the registers; /// of that type as used and place the argument on the stack.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp:10,Energy Efficiency,allocate,allocate,10,"// Try to allocate a contiguous block of registers, each of the correct; // size to hold one member.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp:38,Energy Efficiency,allocate,allocated,38,// Add the argument to the list to be allocated once we know the size of the; // block.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CallingConvention.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp:420,Modifiability,variab,variables,420,"//===-- AArch64CleanupLocalDynamicTLSPass.cpp ---------------------*- C++ -*-=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // Local-dynamic access to thread-local variables proceeds in three stages.; //; // 1. The offset of this Module's thread-local area from TPIDR_EL0 is calculated; // in much the same way as a general-dynamic TLS-descriptor access against; // the special symbol _TLS_MODULE_BASE.; // 2. The variable's offset from _TLS_MODULE_BASE_ is calculated using; // instructions with ""dtprel"" modifiers.; // 3. These two are added, together with TPIDR_EL0, to obtain the variable's; // true address.; //; // This is only better than general-dynamic access to the variable if two or; // more of the first stage TLS-descriptor calculations can be combined. This; // pass looks through a function and performs such combinations.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp:670,Modifiability,variab,variable,670,"//===-- AArch64CleanupLocalDynamicTLSPass.cpp ---------------------*- C++ -*-=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // Local-dynamic access to thread-local variables proceeds in three stages.; //; // 1. The offset of this Module's thread-local area from TPIDR_EL0 is calculated; // in much the same way as a general-dynamic TLS-descriptor access against; // the special symbol _TLS_MODULE_BASE.; // 2. The variable's offset from _TLS_MODULE_BASE_ is calculated using; // instructions with ""dtprel"" modifiers.; // 3. These two are added, together with TPIDR_EL0, to obtain the variable's; // true address.; //; // This is only better than general-dynamic access to the variable if two or; // more of the first stage TLS-descriptor calculations can be combined. This; // pass looks through a function and performs such combinations.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp:840,Modifiability,variab,variable,840,"//===-- AArch64CleanupLocalDynamicTLSPass.cpp ---------------------*- C++ -*-=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // Local-dynamic access to thread-local variables proceeds in three stages.; //; // 1. The offset of this Module's thread-local area from TPIDR_EL0 is calculated; // in much the same way as a general-dynamic TLS-descriptor access against; // the special symbol _TLS_MODULE_BASE.; // 2. The variable's offset from _TLS_MODULE_BASE_ is calculated using; // instructions with ""dtprel"" modifiers.; // 3. These two are added, together with TPIDR_EL0, to obtain the variable's; // true address.; //; // This is only better than general-dynamic access to the variable if two or; // more of the first stage TLS-descriptor calculations can be combined. This; // pass looks through a function and performs such combinations.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp:932,Modifiability,variab,variable,932,"//===-- AArch64CleanupLocalDynamicTLSPass.cpp ---------------------*- C++ -*-=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // Local-dynamic access to thread-local variables proceeds in three stages.; //; // 1. The offset of this Module's thread-local area from TPIDR_EL0 is calculated; // in much the same way as a general-dynamic TLS-descriptor access against; // the special symbol _TLS_MODULE_BASE.; // 2. The variable's offset from _TLS_MODULE_BASE_ is calculated using; // instructions with ""dtprel"" modifiers.; // 3. These two are added, together with TPIDR_EL0, to obtain the variable's; // true address.; //; // This is only better than general-dynamic access to the variable if two or; // more of the first stage TLS-descriptor calculations can be combined. This; // pass looks through a function and performs such combinations.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp:1067,Performance,perform,performs,1067,"//===-- AArch64CleanupLocalDynamicTLSPass.cpp ---------------------*- C++ -*-=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // Local-dynamic access to thread-local variables proceeds in three stages.; //; // 1. The offset of this Module's thread-local area from TPIDR_EL0 is calculated; // in much the same way as a general-dynamic TLS-descriptor access against; // the special symbol _TLS_MODULE_BASE.; // 2. The variable's offset from _TLS_MODULE_BASE_ is calculated using; // instructions with ""dtprel"" modifiers.; // 3. These two are added, together with TPIDR_EL0, to obtain the variable's; // true address.; //; // This is only better than general-dynamic access to the variable if two or; // more of the first stage TLS-descriptor calculations can be combined. This; // pass looks through a function and performs such combinations.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp:397,Security,access,access,397,"//===-- AArch64CleanupLocalDynamicTLSPass.cpp ---------------------*- C++ -*-=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // Local-dynamic access to thread-local variables proceeds in three stages.; //; // 1. The offset of this Module's thread-local area from TPIDR_EL0 is calculated; // in much the same way as a general-dynamic TLS-descriptor access against; // the special symbol _TLS_MODULE_BASE.; // 2. The variable's offset from _TLS_MODULE_BASE_ is calculated using; // instructions with ""dtprel"" modifiers.; // 3. These two are added, together with TPIDR_EL0, to obtain the variable's; // true address.; //; // This is only better than general-dynamic access to the variable if two or; // more of the first stage TLS-descriptor calculations can be combined. This; // pass looks through a function and performs such combinations.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp:603,Security,access,access,603,"//===-- AArch64CleanupLocalDynamicTLSPass.cpp ---------------------*- C++ -*-=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // Local-dynamic access to thread-local variables proceeds in three stages.; //; // 1. The offset of this Module's thread-local area from TPIDR_EL0 is calculated; // in much the same way as a general-dynamic TLS-descriptor access against; // the special symbol _TLS_MODULE_BASE.; // 2. The variable's offset from _TLS_MODULE_BASE_ is calculated using; // instructions with ""dtprel"" modifiers.; // 3. These two are added, together with TPIDR_EL0, to obtain the variable's; // true address.; //; // This is only better than general-dynamic access to the variable if two or; // more of the first stage TLS-descriptor calculations can be combined. This; // pass looks through a function and performs such combinations.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp:918,Security,access,access,918,"//===-- AArch64CleanupLocalDynamicTLSPass.cpp ---------------------*- C++ -*-=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // Local-dynamic access to thread-local variables proceeds in three stages.; //; // 1. The offset of this Module's thread-local area from TPIDR_EL0 is calculated; // in much the same way as a general-dynamic TLS-descriptor access against; // the special symbol _TLS_MODULE_BASE.; // 2. The variable's offset from _TLS_MODULE_BASE_ is calculated using; // instructions with ""dtprel"" modifiers.; // 3. These two are added, together with TPIDR_EL0, to obtain the variable's; // true address.; //; // This is only better than general-dynamic access to the variable if two or; // more of the first stage TLS-descriptor calculations can be combined. This; // pass looks through a function and performs such combinations.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp:20,Security,access,accesses,20,// No point folding accesses if there isn't at least two.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp:34,Security,access,access,34,// Make sure it's a local dynamic access.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp:3,Deployability,Update,Update,3,// Update the call site info.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CleanupLocalDynamicTLSPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:3440,Availability,redundant,redundant,3440,"upported LOHs are:; // * So called non-ADRP-related:; // - .loh AdrpAddLdr L1, L2, L3:; // L1: adrp xA, sym@PAGE; // L2: add xB, xA, sym@PAGEOFF; // L3: ldr xC, [xB, #imm]; // - .loh AdrpLdrGotLdr L1, L2, L3:; // L1: adrp xA, sym@GOTPAGE; // L2: ldr xB, [xA, sym@GOTPAGEOFF]; // L3: ldr xC, [xB, #imm]; // - .loh AdrpLdr L1, L3:; // L1: adrp xA, sym@PAGE; // L3: ldr xC, [xA, sym@PAGEOFF]; // - .loh AdrpAddStr L1, L2, L3:; // L1: adrp xA, sym@PAGE; // L2: add xB, xA, sym@PAGEOFF; // L3: str xC, [xB, #imm]; // - .loh AdrpLdrGotStr L1, L2, L3:; // L1: adrp xA, sym@GOTPAGE; // L2: ldr xB, [xA, sym@GOTPAGEOFF]; // L3: str xC, [xB, #imm]; // - .loh AdrpAdd L1, L2:; // L1: adrp xA, sym@PAGE; // L2: add xB, xA, sym@PAGEOFF; // For all these LOHs, L1, L2, L3 form a simple chain:; // L1 result is used only by L2 and L2 result by L3.; // L3 LOH-related argument is defined only by L2 and L2 LOH-related argument; // by L1.; // All these LOHs aim at using more efficient load/store patterns by folding; // some instructions used to compute the address directly into the load/store.; //; // * So called ADRP-related:; // - .loh AdrpAdrp L2, L1:; // L2: ADRP xA, sym1@PAGE; // L1: ADRP xA, sym2@PAGE; // L2 dominates L1 and xA is not redifined between L2 and L1; // This LOH aims at getting rid of redundant ADRP instructions.; //; // The overall design for emitting the LOHs is:; // 1. AArch64CollectLOH (this pass) records the LOHs in the AArch64FunctionInfo.; // 2. AArch64AsmPrinter reads the LOHs from AArch64FunctionInfo and it:; // 1. Associates them a label.; // 2. Emits them in a MCStreamer (EmitLOHDirective).; // - The MCMachOStreamer records them into the MCAssembler.; // - The MCAsmStreamer prints them.; // - Other MCStreamers ignore them.; // 3. Closes the MCStreamer:; // - The MachObjectWriter gets them from the MCAssembler and writes; // them in the object file.; // - Other ObjectWriters ignore them.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:1189,Energy Efficiency,efficient,efficient,1189,"-*- C++ -*-=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass that collect the Linker Optimization Hint (LOH).; // This pass should be run at the very end of the compilation flow, just before; // assembly printer.; // To be useful for the linker, the LOH must be printed into the assembly file.; //; // A LOH describes a sequence of instructions that may be optimized by the; // linker.; // This same sequence cannot be optimized by the compiler because some of; // the information will be known at link time.; // For instance, consider the following sequence:; // L1: adrp xA, sym@PAGE; // L2: add xB, xA, sym@PAGEOFF; // L3: ldr xC, [xB, #imm]; // This sequence can be turned into:; // A literal load if sym@PAGE + sym@PAGEOFF + #imm - address(L3) is < 1MB:; // L3: ldr xC, sym+#imm; // It may also be turned into either the following more efficient; // code sequences:; // - If sym@PAGEOFF + #imm fits the encoding space of L3.; // L1: adrp xA, sym@PAGE; // L3: ldr xC, [xB, sym@PAGEOFF + #imm]; // - If sym@PAGE + sym@PAGEOFF - address(L1) < 1MB:; // L1: adr xA, sym; // L3: ldr xC, [xB, #imm]; //; // To be valid a LOH must meet all the requirements needed by all the related; // possible linker transformations.; // For instance, using the running example, the constraints to emit; // "".loh AdrpAddLdr"" are:; // - L1, L2, and L3 instructions are of the expected type, i.e.,; // respectively ADRP, ADD (immediate), and LD.; // - The result of L1 is used only by L2.; // - The register argument (xA) used in the ADD instruction is defined; // only by L1.; // - The result of L2 is used only by L3.; // - The base address (xB) in L3 is defined only L2.; // - The ADRP in L1 and the ADD in L2 must reference the same s",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:3105,Energy Efficiency,efficient,efficient,3105,"upported LOHs are:; // * So called non-ADRP-related:; // - .loh AdrpAddLdr L1, L2, L3:; // L1: adrp xA, sym@PAGE; // L2: add xB, xA, sym@PAGEOFF; // L3: ldr xC, [xB, #imm]; // - .loh AdrpLdrGotLdr L1, L2, L3:; // L1: adrp xA, sym@GOTPAGE; // L2: ldr xB, [xA, sym@GOTPAGEOFF]; // L3: ldr xC, [xB, #imm]; // - .loh AdrpLdr L1, L3:; // L1: adrp xA, sym@PAGE; // L3: ldr xC, [xA, sym@PAGEOFF]; // - .loh AdrpAddStr L1, L2, L3:; // L1: adrp xA, sym@PAGE; // L2: add xB, xA, sym@PAGEOFF; // L3: str xC, [xB, #imm]; // - .loh AdrpLdrGotStr L1, L2, L3:; // L1: adrp xA, sym@GOTPAGE; // L2: ldr xB, [xA, sym@GOTPAGEOFF]; // L3: str xC, [xB, #imm]; // - .loh AdrpAdd L1, L2:; // L1: adrp xA, sym@PAGE; // L2: add xB, xA, sym@PAGEOFF; // For all these LOHs, L1, L2, L3 form a simple chain:; // L1 result is used only by L2 and L2 result by L3.; // L3 LOH-related argument is defined only by L2 and L2 LOH-related argument; // by L1.; // All these LOHs aim at using more efficient load/store patterns by folding; // some instructions used to compute the address directly into the load/store.; //; // * So called ADRP-related:; // - .loh AdrpAdrp L2, L1:; // L2: ADRP xA, sym1@PAGE; // L1: ADRP xA, sym2@PAGE; // L2 dominates L1 and xA is not redifined between L2 and L1; // This LOH aims at getting rid of redundant ADRP instructions.; //; // The overall design for emitting the LOHs is:; // 1. AArch64CollectLOH (this pass) records the LOHs in the AArch64FunctionInfo.; // 2. AArch64AsmPrinter reads the LOHs from AArch64FunctionInfo and it:; // 1. Associates them a label.; // 2. Emits them in a MCStreamer (EmitLOHDirective).; // - The MCMachOStreamer records them into the MCAssembler.; // - The MCAsmStreamer prints them.; // - Other MCStreamers ignore them.; // 3. Closes the MCStreamer:; // - The MachObjectWriter gets them from the MCAssembler and writes; // them in the object file.; // - Other ObjectWriters ignore them.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:433,Performance,Optimiz,Optimization,433,"//===---------- AArch64CollectLOH.cpp - AArch64 collect LOH pass --*- C++ -*-=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass that collect the Linker Optimization Hint (LOH).; // This pass should be run at the very end of the compilation flow, just before; // assembly printer.; // To be useful for the linker, the LOH must be printed into the assembly file.; //; // A LOH describes a sequence of instructions that may be optimized by the; // linker.; // This same sequence cannot be optimized by the compiler because some of; // the information will be known at link time.; // For instance, consider the following sequence:; // L1: adrp xA, sym@PAGE; // L2: add xB, xA, sym@PAGEOFF; // L3: ldr xC, [xB, #imm]; // This sequence can be turned into:; // A literal load if sym@PAGE + sym@PAGEOFF + #imm - address(L3) is < 1MB:; // L3: ldr xC, sym+#imm; // It may also be turned into either the following more efficient; // code sequences:; // - If sym@PAGEOFF + #imm fits the encoding space of L3.; // L1: adrp xA, sym@PAGE; // L3: ldr xC, [xB, sym@PAGEOFF + #imm]; // - If sym@PAGE + sym@PAGEOFF - address(L1) < 1MB:; // L1: adr xA, sym; // L3: ldr xC, [xB, #imm]; //; // To be valid a LOH must meet all the requirements needed by all the related; // possible linker transformations.; // For instance, using the running example, the constraints to emit; // "".loh AdrpAddLdr"" are:; // - L1, L2, and L3 instructions are of the expected type, i.e.,; // respectively ADRP, ADD (immediate), and LD.; // - The result of L1 is used only by L2.; // - The register argument (xA) used in the ADD instruction is defined; // only by L1.; // - The result of L2 is used only by L3.; // - The base address (xB) in L3 is defined only L2",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:705,Performance,optimiz,optimized,705,"//===---------- AArch64CollectLOH.cpp - AArch64 collect LOH pass --*- C++ -*-=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass that collect the Linker Optimization Hint (LOH).; // This pass should be run at the very end of the compilation flow, just before; // assembly printer.; // To be useful for the linker, the LOH must be printed into the assembly file.; //; // A LOH describes a sequence of instructions that may be optimized by the; // linker.; // This same sequence cannot be optimized by the compiler because some of; // the information will be known at link time.; // For instance, consider the following sequence:; // L1: adrp xA, sym@PAGE; // L2: add xB, xA, sym@PAGEOFF; // L3: ldr xC, [xB, #imm]; // This sequence can be turned into:; // A literal load if sym@PAGE + sym@PAGEOFF + #imm - address(L3) is < 1MB:; // L3: ldr xC, sym+#imm; // It may also be turned into either the following more efficient; // code sequences:; // - If sym@PAGEOFF + #imm fits the encoding space of L3.; // L1: adrp xA, sym@PAGE; // L3: ldr xC, [xB, sym@PAGEOFF + #imm]; // - If sym@PAGE + sym@PAGEOFF - address(L1) < 1MB:; // L1: adr xA, sym; // L3: ldr xC, [xB, #imm]; //; // To be valid a LOH must meet all the requirements needed by all the related; // possible linker transformations.; // For instance, using the running example, the constraints to emit; // "".loh AdrpAddLdr"" are:; // - L1, L2, and L3 instructions are of the expected type, i.e.,; // respectively ADRP, ADD (immediate), and LD.; // - The result of L1 is used only by L2.; // - The register argument (xA) used in the ADD instruction is defined; // only by L1.; // - The result of L2 is used only by L3.; // - The base address (xB) in L3 is defined only L2",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:767,Performance,optimiz,optimized,767,"//===---------- AArch64CollectLOH.cpp - AArch64 collect LOH pass --*- C++ -*-=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass that collect the Linker Optimization Hint (LOH).; // This pass should be run at the very end of the compilation flow, just before; // assembly printer.; // To be useful for the linker, the LOH must be printed into the assembly file.; //; // A LOH describes a sequence of instructions that may be optimized by the; // linker.; // This same sequence cannot be optimized by the compiler because some of; // the information will be known at link time.; // For instance, consider the following sequence:; // L1: adrp xA, sym@PAGE; // L2: add xB, xA, sym@PAGEOFF; // L3: ldr xC, [xB, #imm]; // This sequence can be turned into:; // A literal load if sym@PAGE + sym@PAGEOFF + #imm - address(L3) is < 1MB:; // L3: ldr xC, sym+#imm; // It may also be turned into either the following more efficient; // code sequences:; // - If sym@PAGEOFF + #imm fits the encoding space of L3.; // L1: adrp xA, sym@PAGE; // L3: ldr xC, [xB, sym@PAGEOFF + #imm]; // - If sym@PAGE + sym@PAGEOFF - address(L1) < 1MB:; // L1: adr xA, sym; // L3: ldr xC, [xB, #imm]; //; // To be valid a LOH must meet all the requirements needed by all the related; // possible linker transformations.; // For instance, using the running example, the constraints to emit; // "".loh AdrpAddLdr"" are:; // - L1, L2, and L3 instructions are of the expected type, i.e.,; // respectively ADRP, ADD (immediate), and LD.; // - The result of L1 is used only by L2.; // - The register argument (xA) used in the ADD instruction is defined; // only by L1.; // - The result of L2 is used only by L3.; // - The base address (xB) in L3 is defined only L2",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:1045,Performance,load,load,1045,"-*- C++ -*-=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass that collect the Linker Optimization Hint (LOH).; // This pass should be run at the very end of the compilation flow, just before; // assembly printer.; // To be useful for the linker, the LOH must be printed into the assembly file.; //; // A LOH describes a sequence of instructions that may be optimized by the; // linker.; // This same sequence cannot be optimized by the compiler because some of; // the information will be known at link time.; // For instance, consider the following sequence:; // L1: adrp xA, sym@PAGE; // L2: add xB, xA, sym@PAGEOFF; // L3: ldr xC, [xB, #imm]; // This sequence can be turned into:; // A literal load if sym@PAGE + sym@PAGEOFF + #imm - address(L3) is < 1MB:; // L3: ldr xC, sym+#imm; // It may also be turned into either the following more efficient; // code sequences:; // - If sym@PAGEOFF + #imm fits the encoding space of L3.; // L1: adrp xA, sym@PAGE; // L3: ldr xC, [xB, sym@PAGEOFF + #imm]; // - If sym@PAGE + sym@PAGEOFF - address(L1) < 1MB:; // L1: adr xA, sym; // L3: ldr xC, [xB, #imm]; //; // To be valid a LOH must meet all the requirements needed by all the related; // possible linker transformations.; // For instance, using the running example, the constraints to emit; // "".loh AdrpAddLdr"" are:; // - L1, L2, and L3 instructions are of the expected type, i.e.,; // respectively ADRP, ADD (immediate), and LD.; // - The result of L1 is used only by L2.; // - The register argument (xA) used in the ADD instruction is defined; // only by L1.; // - The result of L2 is used only by L3.; // - The base address (xB) in L3 is defined only L2.; // - The ADRP in L1 and the ADD in L2 must reference the same s",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:3115,Performance,load,load,3115,"upported LOHs are:; // * So called non-ADRP-related:; // - .loh AdrpAddLdr L1, L2, L3:; // L1: adrp xA, sym@PAGE; // L2: add xB, xA, sym@PAGEOFF; // L3: ldr xC, [xB, #imm]; // - .loh AdrpLdrGotLdr L1, L2, L3:; // L1: adrp xA, sym@GOTPAGE; // L2: ldr xB, [xA, sym@GOTPAGEOFF]; // L3: ldr xC, [xB, #imm]; // - .loh AdrpLdr L1, L3:; // L1: adrp xA, sym@PAGE; // L3: ldr xC, [xA, sym@PAGEOFF]; // - .loh AdrpAddStr L1, L2, L3:; // L1: adrp xA, sym@PAGE; // L2: add xB, xA, sym@PAGEOFF; // L3: str xC, [xB, #imm]; // - .loh AdrpLdrGotStr L1, L2, L3:; // L1: adrp xA, sym@GOTPAGE; // L2: ldr xB, [xA, sym@GOTPAGEOFF]; // L3: str xC, [xB, #imm]; // - .loh AdrpAdd L1, L2:; // L1: adrp xA, sym@PAGE; // L2: add xB, xA, sym@PAGEOFF; // For all these LOHs, L1, L2, L3 form a simple chain:; // L1 result is used only by L2 and L2 result by L3.; // L3 LOH-related argument is defined only by L2 and L2 LOH-related argument; // by L1.; // All these LOHs aim at using more efficient load/store patterns by folding; // some instructions used to compute the address directly into the load/store.; //; // * So called ADRP-related:; // - .loh AdrpAdrp L2, L1:; // L2: ADRP xA, sym1@PAGE; // L1: ADRP xA, sym2@PAGE; // L2 dominates L1 and xA is not redifined between L2 and L1; // This LOH aims at getting rid of redundant ADRP instructions.; //; // The overall design for emitting the LOHs is:; // 1. AArch64CollectLOH (this pass) records the LOHs in the AArch64FunctionInfo.; // 2. AArch64AsmPrinter reads the LOHs from AArch64FunctionInfo and it:; // 1. Associates them a label.; // 2. Emits them in a MCStreamer (EmitLOHDirective).; // - The MCMachOStreamer records them into the MCAssembler.; // - The MCAsmStreamer prints them.; // - Other MCStreamers ignore them.; // 3. Closes the MCStreamer:; // - The MachObjectWriter gets them from the MCAssembler and writes; // them in the object file.; // - Other ObjectWriters ignore them.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:3214,Performance,load,load,3214,"upported LOHs are:; // * So called non-ADRP-related:; // - .loh AdrpAddLdr L1, L2, L3:; // L1: adrp xA, sym@PAGE; // L2: add xB, xA, sym@PAGEOFF; // L3: ldr xC, [xB, #imm]; // - .loh AdrpLdrGotLdr L1, L2, L3:; // L1: adrp xA, sym@GOTPAGE; // L2: ldr xB, [xA, sym@GOTPAGEOFF]; // L3: ldr xC, [xB, #imm]; // - .loh AdrpLdr L1, L3:; // L1: adrp xA, sym@PAGE; // L3: ldr xC, [xA, sym@PAGEOFF]; // - .loh AdrpAddStr L1, L2, L3:; // L1: adrp xA, sym@PAGE; // L2: add xB, xA, sym@PAGEOFF; // L3: str xC, [xB, #imm]; // - .loh AdrpLdrGotStr L1, L2, L3:; // L1: adrp xA, sym@GOTPAGE; // L2: ldr xB, [xA, sym@GOTPAGEOFF]; // L3: str xC, [xB, #imm]; // - .loh AdrpAdd L1, L2:; // L1: adrp xA, sym@PAGE; // L2: add xB, xA, sym@PAGEOFF; // For all these LOHs, L1, L2, L3 form a simple chain:; // L1 result is used only by L2 and L2 result by L3.; // L3 LOH-related argument is defined only by L2 and L2 LOH-related argument; // by L1.; // All these LOHs aim at using more efficient load/store patterns by folding; // some instructions used to compute the address directly into the load/store.; //; // * So called ADRP-related:; // - .loh AdrpAdrp L2, L1:; // L2: ADRP xA, sym1@PAGE; // L1: ADRP xA, sym2@PAGE; // L2 dominates L1 and xA is not redifined between L2 and L1; // This LOH aims at getting rid of redundant ADRP instructions.; //; // The overall design for emitting the LOHs is:; // 1. AArch64CollectLOH (this pass) records the LOHs in the AArch64FunctionInfo.; // 2. AArch64AsmPrinter reads the LOHs from AArch64FunctionInfo and it:; // 1. Associates them a label.; // 2. Emits them in a MCStreamer (EmitLOHDirective).; // - The MCMachOStreamer records them into the MCAssembler.; // - The MCAsmStreamer prints them.; // - Other MCStreamers ignore them.; // 3. Closes the MCStreamer:; // - The MachObjectWriter gets them from the MCAssembler and writes; // them in the object file.; // - Other ObjectWriters ignore them.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:3440,Safety,redund,redundant,3440,"upported LOHs are:; // * So called non-ADRP-related:; // - .loh AdrpAddLdr L1, L2, L3:; // L1: adrp xA, sym@PAGE; // L2: add xB, xA, sym@PAGEOFF; // L3: ldr xC, [xB, #imm]; // - .loh AdrpLdrGotLdr L1, L2, L3:; // L1: adrp xA, sym@GOTPAGE; // L2: ldr xB, [xA, sym@GOTPAGEOFF]; // L3: ldr xC, [xB, #imm]; // - .loh AdrpLdr L1, L3:; // L1: adrp xA, sym@PAGE; // L3: ldr xC, [xA, sym@PAGEOFF]; // - .loh AdrpAddStr L1, L2, L3:; // L1: adrp xA, sym@PAGE; // L2: add xB, xA, sym@PAGEOFF; // L3: str xC, [xB, #imm]; // - .loh AdrpLdrGotStr L1, L2, L3:; // L1: adrp xA, sym@GOTPAGE; // L2: ldr xB, [xA, sym@GOTPAGEOFF]; // L3: str xC, [xB, #imm]; // - .loh AdrpAdd L1, L2:; // L1: adrp xA, sym@PAGE; // L2: add xB, xA, sym@PAGEOFF; // For all these LOHs, L1, L2, L3 form a simple chain:; // L1 result is used only by L2 and L2 result by L3.; // L3 LOH-related argument is defined only by L2 and L2 LOH-related argument; // by L1.; // All these LOHs aim at using more efficient load/store patterns by folding; // some instructions used to compute the address directly into the load/store.; //; // * So called ADRP-related:; // - .loh AdrpAdrp L2, L1:; // L2: ADRP xA, sym1@PAGE; // L1: ADRP xA, sym2@PAGE; // L2 dominates L1 and xA is not redifined between L2 and L1; // This LOH aims at getting rid of redundant ADRP instructions.; //; // The overall design for emitting the LOHs is:; // 1. AArch64CollectLOH (this pass) records the LOHs in the AArch64FunctionInfo.; // 2. AArch64AsmPrinter reads the LOHs from AArch64FunctionInfo and it:; // 1. Associates them a label.; // 2. Emits them in a MCStreamer (EmitLOHDirective).; // - The MCMachOStreamer records them into the MCAssembler.; // - The MCAsmStreamer prints them.; // - Other MCStreamers ignore them.; // 3. Closes the MCStreamer:; // - The MachObjectWriter gets them from the MCAssembler and writes; // them in the object file.; // - Other ObjectWriters ignore them.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:2911,Usability,simpl,simple,2911,"fined; // only by L1.; // - The result of L2 is used only by L3.; // - The base address (xB) in L3 is defined only L2.; // - The ADRP in L1 and the ADD in L2 must reference the same symbol using; // @PAGE/@PAGEOFF with no additional constants; //; // Currently supported LOHs are:; // * So called non-ADRP-related:; // - .loh AdrpAddLdr L1, L2, L3:; // L1: adrp xA, sym@PAGE; // L2: add xB, xA, sym@PAGEOFF; // L3: ldr xC, [xB, #imm]; // - .loh AdrpLdrGotLdr L1, L2, L3:; // L1: adrp xA, sym@GOTPAGE; // L2: ldr xB, [xA, sym@GOTPAGEOFF]; // L3: ldr xC, [xB, #imm]; // - .loh AdrpLdr L1, L3:; // L1: adrp xA, sym@PAGE; // L3: ldr xC, [xA, sym@PAGEOFF]; // - .loh AdrpAddStr L1, L2, L3:; // L1: adrp xA, sym@PAGE; // L2: add xB, xA, sym@PAGEOFF; // L3: str xC, [xB, #imm]; // - .loh AdrpLdrGotStr L1, L2, L3:; // L1: adrp xA, sym@GOTPAGE; // L2: ldr xB, [xA, sym@GOTPAGEOFF]; // L3: str xC, [xB, #imm]; // - .loh AdrpAdd L1, L2:; // L1: adrp xA, sym@PAGE; // L2: add xB, xA, sym@PAGEOFF; // For all these LOHs, L1, L2, L3 form a simple chain:; // L1 result is used only by L2 and L2 result by L3.; // L3 LOH-related argument is defined only by L2 and L2 LOH-related argument; // by L1.; // All these LOHs aim at using more efficient load/store patterns by folding; // some instructions used to compute the address directly into the load/store.; //; // * So called ADRP-related:; // - .loh AdrpAdrp L2, L1:; // L2: ADRP xA, sym1@PAGE; // L1: ADRP xA, sym2@PAGE; // L2 dominates L1 and xA is not redifined between L2 and L1; // This LOH aims at getting rid of redundant ADRP instructions.; //; // The overall design for emitting the LOHs is:; // 1. AArch64CollectLOH (this pass) records the LOHs in the AArch64FunctionInfo.; // 2. AArch64AsmPrinter reads the LOHs from AArch64FunctionInfo and it:; // 1. Associates them a label.; // 2. Emits them in a MCStreamer (EmitLOHDirective).; // - The MCMachOStreamer records them into the MCAssembler.; // - The MCAsmStreamer prints them.; // - Other MCStreamers ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:27,Performance,LOAD,LOADGot,27,"// Accept ADRP, ADDLow and LOADGot.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:15,Performance,optimiz,optimize,15,"// We can only optimize the index operand.; // In case we have str xA, [xA, #imm], this is two different uses; // of xA and we cannot fold, otherwise the xA stored may be wrong,; // even if #imm == 0.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:87,Performance,load,load,87,/// Check whether the given instruction can be the end of a LOH chain; /// involving a load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:44,Performance,load,load,44,/// Check whether the given instruction can load a litteral.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:4,Deployability,Update,Update,4,/// Update state \p Info given \p MI uses the tracked register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:4,Deployability,Update,Update,4,/// Update state \p Info given the tracked register is clobbered.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:4,Deployability,Update,Update,4,/// Update state \p Info given that \p MI is possibly the middle instruction; /// of an LOH involving 3 instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:4,Deployability,Update,Update,4,/// Update state when seeing and ADRP instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:239,Modifiability,rewrite,rewrite,239,"// ADRPs and ADDs for this candidate may be split apart if using; // GlobalISel instead of pseudo-expanded. If that happens, the; // def register of the ADD may have a use in between. Adding an LOH in; // this case can cause the linker to rewrite the ADRP to write to that; // register, clobbering the use.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:53,Modifiability,rewrite,rewrite,53,"// There is a possibility that the linker may try to rewrite:; // adrp x0, @sym@PAGE; // add x1, x0, @sym@PAGEOFF; // [x0 = some other def]; // ldr x2, [x1]; // ...into...; // adrp x0, @sym; // nop; // [x0 = some other def]; // ldr x2, [x0]; // ...if the offset to the symbol won't fit within a literal load.; // This causes the load to use the result of the adrp, which in this; // case has already been clobbered.; // FIXME: Implement proper liveness tracking for all registers. For now,; // don't emit the LOH if there are any instructions between the add and; // the ldr.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:303,Performance,load,load,303,"// There is a possibility that the linker may try to rewrite:; // adrp x0, @sym@PAGE; // add x1, x0, @sym@PAGEOFF; // [x0 = some other def]; // ldr x2, [x1]; // ...into...; // adrp x0, @sym; // nop; // [x0 = some other def]; // ldr x2, [x0]; // ...if the offset to the symbol won't fit within a literal load.; // This causes the load to use the result of the adrp, which in this; // case has already been clobbered.; // FIXME: Implement proper liveness tracking for all registers. For now,; // don't emit the LOH if there are any instructions between the add and; // the ldr.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:329,Performance,load,load,329,"// There is a possibility that the linker may try to rewrite:; // adrp x0, @sym@PAGE; // add x1, x0, @sym@PAGEOFF; // [x0 = some other def]; // ldr x2, [x1]; // ...into...; // adrp x0, @sym; // nop; // [x0 = some other def]; // ldr x2, [x0]; // ...if the offset to the symbol won't fit within a literal load.; // This causes the load to use the result of the adrp, which in this; // case has already been clobbered.; // FIXME: Implement proper liveness tracking for all registers. For now,; // don't emit the LOH if there are any instructions between the add and; // the ldr.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:104,Performance,optimiz,optimization,104,"// Multiple uses of the same register within a single instruction don't; // count as MultiUser or block optimization. This is especially important on; // arm64_32, where any memory operation is likely to be an explicit use of; // xN and an implicit use of wN (the base address register).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp:38,Deployability,update,update,38,// Walk the basic block backwards and update the per register state machine; // in the process.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CollectLOH.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CompressJumpTables.cpp:100,Safety,safe,safe,100,"/// Returns the size of instructions in the block \p MBB, or std::nullopt if; /// we couldn't get a safe upper bound.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CompressJumpTables.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CompressJumpTables.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CompressJumpTables.cpp:69,Performance,perform,perform,69,"/// Gather information about the function, returns false if we can't perform; /// this optimization for some reason.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CompressJumpTables.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CompressJumpTables.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CompressJumpTables.cpp:87,Performance,optimiz,optimization,87,"/// Gather information about the function, returns false if we can't perform; /// this optimization for some reason.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CompressJumpTables.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CompressJumpTables.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CompressJumpTables.cpp:127,Safety,safe,safe,127,"// Inline asm may contain some directives like .bytes which we don't; // currently have the ability to parse accurately. To be safe, just avoid; // computing a size and bail out.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CompressJumpTables.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CompressJumpTables.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CompressJumpTables.cpp:138,Safety,avoid,avoid,138,"// Inline asm may contain some directives like .bytes which we don't; // currently have the ability to parse accurately. To be safe, just avoid; // computing a size and bail out.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CompressJumpTables.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CompressJumpTables.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CompressJumpTables.cpp:34,Performance,optimiz,optimized,34,// The jump-table might have been optimized away.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CompressJumpTables.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CompressJumpTables.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CondBrTuning.cpp:619,Energy Efficiency,schedul,scheduling,619,"//===-- AArch64CondBrTuning.cpp --- Conditional branch tuning for AArch64 -===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; /// \file; /// This file contains a pass that transforms CBZ/CBNZ/TBZ/TBNZ instructions; /// into a conditional branch (B.cond), when the NZCV flags can be set for; /// ""free"". This is preferred on targets that have more flexibility when; /// scheduling B.cond instructions as compared to CBZ/CBNZ/TBZ/TBNZ (assuming; /// all other variables are equal). This can also reduce register pressure.; ///; /// A few examples:; ///; /// 1) add w8, w0, w1 -> cmn w0, w1 ; CMN is an alias of ADDS.; /// cbz w8, .LBB_2 -> b.eq .LBB0_2; ///; /// 2) add w8, w0, w1 -> adds w8, w0, w1 ; w8 has multiple uses.; /// cbz w8, .LBB1_2 -> b.eq .LBB1_2; ///; /// 3) sub w8, w0, w1 -> subs w8, w0, w1 ; w8 has multiple uses.; /// tbz w8, #31, .LBB6_2 -> b.pl .LBB6_2; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CondBrTuning.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CondBrTuning.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CondBrTuning.cpp:744,Energy Efficiency,reduce,reduce,744,"//===-- AArch64CondBrTuning.cpp --- Conditional branch tuning for AArch64 -===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; /// \file; /// This file contains a pass that transforms CBZ/CBNZ/TBZ/TBNZ instructions; /// into a conditional branch (B.cond), when the NZCV flags can be set for; /// ""free"". This is preferred on targets that have more flexibility when; /// scheduling B.cond instructions as compared to CBZ/CBNZ/TBZ/TBNZ (assuming; /// all other variables are equal). This can also reduce register pressure.; ///; /// A few examples:; ///; /// 1) add w8, w0, w1 -> cmn w0, w1 ; CMN is an alias of ADDS.; /// cbz w8, .LBB_2 -> b.eq .LBB0_2; ///; /// 2) add w8, w0, w1 -> adds w8, w0, w1 ; w8 has multiple uses.; /// cbz w8, .LBB1_2 -> b.eq .LBB1_2; ///; /// 3) sub w8, w0, w1 -> subs w8, w0, w1 ; w8 has multiple uses.; /// tbz w8, #31, .LBB6_2 -> b.pl .LBB6_2; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CondBrTuning.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CondBrTuning.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CondBrTuning.cpp:708,Modifiability,variab,variables,708,"//===-- AArch64CondBrTuning.cpp --- Conditional branch tuning for AArch64 -===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; /// \file; /// This file contains a pass that transforms CBZ/CBNZ/TBZ/TBNZ instructions; /// into a conditional branch (B.cond), when the NZCV flags can be set for; /// ""free"". This is preferred on targets that have more flexibility when; /// scheduling B.cond instructions as compared to CBZ/CBNZ/TBZ/TBNZ (assuming; /// all other variables are equal). This can also reduce register pressure.; ///; /// A few examples:; ///; /// 1) add w8, w0, w1 -> cmn w0, w1 ; CMN is an alias of ADDS.; /// cbz w8, .LBB_2 -> b.eq .LBB0_2; ///; /// 2) add w8, w0, w1 -> adds w8, w0, w1 ; w8 has multiple uses.; /// cbz w8, .LBB1_2 -> b.eq .LBB1_2; ///; /// 3) sub w8, w0, w1 -> subs w8, w0, w1 ; w8 has multiple uses.; /// tbz w8, #31, .LBB6_2 -> b.pl .LBB6_2; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CondBrTuning.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CondBrTuning.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CondBrTuning.cpp:102,Usability,clear,clearing,102,"// If this was a flag setting version of the instruction, we use the original; // instruction by just clearing the dead marked on the implicit-def of NCZV.; // Therefore, we should not erase this instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CondBrTuning.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CondBrTuning.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CondBrTuning.cpp:10,Performance,optimiz,optimization,10,"// If the optimization was successful, we can't optimize any other; // branches because doing so would clobber the NZCV flags.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CondBrTuning.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CondBrTuning.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CondBrTuning.cpp:48,Performance,optimiz,optimize,48,"// If the optimization was successful, we can't optimize any other; // branches because doing so would clobber the NZCV flags.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CondBrTuning.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CondBrTuning.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp:446,Energy Efficiency,reduce,reduces,446,"//===-- AArch64ConditionalCompares.cpp --- CCMP formation for AArch64 -----===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file implements the AArch64ConditionalCompares pass which reduces; // branching and code size by using the conditional compare instructions CCMP,; // CCMN, and FCMP.; //; // The CFG transformations for forming conditional compares are very similar to; // if-conversion, and this pass should run immediately before the early; // if-conversion pass.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp:10,Testability,test,testing,10,// Stress testing mode - disable heuristics.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp:209,Performance,perform,performs,209,"//===----------------------------------------------------------------------===//; // SSACCmpConv; //===----------------------------------------------------------------------===//; //; // The SSACCmpConv class performs ccmp-conversion on SSA form machine code; // after determining if it is possible. The class contains no heuristics;; // external code should be used to determine when ccmp-conversion is a good; // idea.; //; // CCmp-formation works on a CFG representing chained conditions, typically; // from C's short-circuit || and && operators:; //; // From: Head To: Head; // / | CmpBB; // / | / |; // | CmpBB / |; // | / | Tail |; // | / | | |; // Tail | | |; // | | | |; // ... ... ... ...; //; // The Head block is terminated by a br.cond instruction, and the CmpBB block; // contains compare + br.cond. Tail must be a successor of both.; //; // The cmp-conversion turns the compare instruction in CmpBB into a conditional; // compare, and merges CmpBB into Head, speculatively executing its; // instructions. The AArch64 conditional compare instructions have an immediate; // operand that specifies the NZCV flag values when the condition is false and; // the compare isn't executed. This makes it possible to chain compares with; // different condition codes.; //; // Example:; //; // if (a == 5 || b == 17); // foo();; //; // Head:; // cmp w0, #5; // b.eq Tail; // CmpBB:; // cmp w1, #17; // b.eq Tail; // ...; // Tail:; // bl _foo; //; // Becomes:; //; // Head:; // cmp w0, #5; // ccmp w1, #17, 4, ne ; 4 = nZcv; // b.eq Tail; // ...; // Tail:; // bl _foo; //; // The ccmp condition code is the one that would cause the Head terminator to; // branch to CmpBB.; //; // FIXME: It should also be possible to speculate a block on the critical edge; // between Head and Tail, just like if-converting a diamond.; //; // FIXME: Handle PHIs in Tail by turning them into selects (if-conversion).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp:65,Safety,safe,safely,65,/// Return true if all non-terminator instructions in MBB can be safely; /// speculated.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp:93,Performance,perform,performed,93,/// Return the expected code size delta if the conversion into a; /// conditional compare is performed.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp:44,Deployability,update,update,44,"// Assuming that trivialTailPHIs() is true, update the Tail PHIs by simply; // removing the CmpBB operands. The Head operands will be identical.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp:68,Usability,simpl,simply,68,"// Assuming that trivialTailPHIs() is true, update the Tail PHIs by simply; // removing the CmpBB operands. The Head operands will be identical.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp:20,Usability,simpl,simply,20,// A normal br.cond simply has the condition code.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp:49,Safety,safe,safely,49,/// Determine if all the instructions in MBB can safely; /// be speculated. The terminators are not considered.; ///; /// Only CmpMI is allowed to clobber the flags.; ///,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp:19,Performance,load,loads,19,"// Don't speculate loads. Note that it may be possible and desirable to; // speculate GOT or constant pool loads that are guaranteed not to trap,; // but we don't support that for now.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp:107,Performance,load,loads,107,"// Don't speculate loads. Note that it may be possible and desirable to; // speculate GOT or constant pool loads that are guaranteed not to trap,; // but we don't support that for now.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp:269,Availability,down,down,269,"// Tail is allowed to have many predecessors, but we can't handle PHIs yet.; //; // FIXME: Real PHIs could be if-converted as long as the CmpBB values are; // defined before The CmpBB cmp clobbers the flags. Alternatively, it should; // always be safe to sink the ccmp down to immediately before the CmpBB; // terminators.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp:247,Safety,safe,safe,247,"// Tail is allowed to have many predecessors, but we can't handle PHIs yet.; //; // FIXME: Real PHIs could be if-converted as long as the CmpBB values are; // defined before The CmpBB cmp clobbers the flags. Alternatively, it should; // always be safe to sink the ccmp down to immediately before the CmpBB; // terminators.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp:73,Deployability,Update,Update,73,"// All CmpBB instructions are moved into Head, and CmpBB is deleted.; // Update the CFG first.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp:4,Deployability,Update,Update,4,/// Update the dominator tree after if-conversion erased some blocks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp:4,Deployability,Update,Update,4,/// Update LoopInfo after if-conversion.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp:10,Testability,test,testing,10,// Stress testing mode disables all cost considerations.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp:177,Integrability,depend,dependent,177,"// Heuristic: The compare conversion delays the execution of the branch; // instruction because we must wait for the inputs to the second compare as; // well. The branch has no dependent instructions, but delaying it increases; // the cost of a misprediction.; //; // Set a limit on the delay we will accept.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp:136,Deployability,update,updateDomTree,136,// Visit blocks in dominator tree pre-order. The pre-order enables multiple; // cmp-conversions from the same head block.; // Note that updateDomTree() modifies the children of the DomTree node; // currently being visited. The df_iterator supports that; it doesn't look at; // child_begin() / child_end() until after a node has been visited.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionalCompares.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionOptimizer.cpp:121,Availability,failure,failures,121,// Finds compare instruction that corresponds to supported types of branching.; // Returns the instruction or nullptr on failures or detecting unsupported; // instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionOptimizer.cpp:133,Safety,detect,detecting,133,// Finds compare instruction that corresponds to supported types of branching.; // Returns the instruction or nullptr on failures or detecting unsupported; // instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionOptimizer.cpp:20,Usability,simpl,simply,20,// A normal br.cond simply has the condition code.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionOptimizer.cpp:136,Deployability,update,updateDomTree,136,// Visit blocks in dominator tree pre-order. The pre-order enables multiple; // cmp-conversions from the same head block.; // Note that updateDomTree() modifies the children of the DomTree node; // currently being visited. The df_iterator supports that; it doesn't look at; // child_begin() / child_end() until after a node has been visited.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ConditionOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp:4,Usability,Clear,Clear,4,/// Clear or set all bits in the chunk at the given index.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp:3,Usability,Clear,Clear,3,// Clear chunk in the immediate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp:686,Integrability,wrap,wraps,686,"/// Check whether the constant contains a sequence of contiguous ones,; /// which might be interrupted by one or two chunks. If so, materialize the; /// sequence of contiguous ones with an ORR instruction.; /// Materialize the chunks which are either interrupting the sequence or outside; /// of the sequence with a MOVK instruction.; ///; /// Assuming S is a chunk which starts the sequence (1...0...), E is a chunk; /// which ends the sequence (0...1...). Then we are looking for constants which; /// contain at least one S and E chunk.; /// E.g. |E|A|B|S|, |A|E|B|S| or |A|B|E|S|.; ///; /// We are also looking for constants like |S|A|B|E| where the contiguous; /// sequence of ones wraps around the MSB into the LSB.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp:8,Modifiability,extend,extend,8,// Sign extend the 16-bit chunk to 64-bit.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp:38,Integrability,wrap,wraps,38,"// If our contiguous sequence of ones wraps around from the MSB into the LSB,; // just swap indices and pretend we are materializing a contiguous sequence; // of zeros surrounded by a contiguous sequence of ones.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp:36,Deployability,patch,patch,36,// Find out which chunks we need to patch up to obtain a contiguous sequence; // of ones.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp:33,Deployability,patch,patch,33,// Remember the index we need to patch.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp:33,Deployability,patch,patch,33,// Remember the index we need to patch.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp:12,Testability,log,logical,12,"// Find the logical immediate that covers the most bits in RemainingBits,; // allowing for additional bits to be set that were set in OriginalBits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp:20,Testability,log,logical,20,// Find the largest logical immediate that fits within the full immediate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp:40,Availability,mask,mask,40,// Remove all bits that are set by this mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp:20,Testability,log,logical,20,"// Find the largest logical immediate covering the remaining bits, allowing; // for additional bits to be set that were also set in the original immediate.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp:58,Testability,log,logical,58,// Attempt to expand an immediate as the ORR of a pair of logical immediates.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp:58,Testability,log,logical,58,"// Attempt to expand an immediate as the AND of a pair of logical immediates.; // This is done by applying DeMorgan's law, under which logical immediates; // are closed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp:135,Testability,log,logical,135,"// Attempt to expand an immediate as the AND of a pair of logical immediates.; // This is done by applying DeMorgan's law, under which logical immediates; // are closed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp:83,Testability,log,logical,83,"// Check whether the constant can be represented by exclusive-or of two 64-bit; // logical immediates. If so, materialize it with an ORR instruction followed; // by an EOR instruction.; //; // This encoding allows all remaining repeated byte patterns, and many repeated; // 16-bit values, to be encoded without needing four instructions. It can also; // represent some irregular bitmasks (although those would mostly only need; // three instructions otherwise).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp:60,Testability,log,logical,60,"// Determine the larger repetition size of the two possible logical; // immediates, by finding the repetition size of Imm.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp:69,Integrability,wrap,wrap,69,"// Find the last bit of each run of ones, circularly. For runs which wrap; // around from bit 0 to bit 63, this is the bit before the most-significant; // zero, otherwise it is the least-significant bit in the run of ones.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp:233,Energy Efficiency,power,power-of-two,233,"// Find the smaller repetition size of the two possible logical immediates by; // counting the number of runs of one-bits within the BigSize-bit value. Both; // sizes may be the same. The EOR may add one or subtract one from the; // power-of-two count that can be represented by a logical immediate, or it; // may be left unchanged.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp:56,Testability,log,logical,56,"// Find the smaller repetition size of the two possible logical immediates by; // counting the number of runs of one-bits within the BigSize-bit value. Both; // sizes may be the same. The EOR may add one or subtract one from the; // power-of-two count that can be represented by a logical immediate, or it; // may be left unchanged.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp:281,Testability,log,logical,281,"// Find the smaller repetition size of the two possible logical immediates by; // counting the number of runs of one-bits within the BigSize-bit value. Both; // sizes may be the same. The EOR may add one or subtract one from the; // power-of-two count that can be represented by a logical immediate, or it; // may be left unchanged.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp:45,Energy Efficiency,power,power-of-two,45,// Early-exit if the big chunk couldn't be a power-of-two number of runs; // EORed with another single run.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandImm.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp:487,Energy Efficiency,schedul,scheduling,487,"//===- AArch64ExpandPseudoInsts.cpp - Expand pseudo instructions ----------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass that expands pseudo instructions into target; // instructions to allow proper scheduling and other late optimizations. This; // pass should be run after register allocation but before the post-regalloc; // scheduling pass.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp:615,Energy Efficiency,schedul,scheduling,615,"//===- AArch64ExpandPseudoInsts.cpp - Expand pseudo instructions ----------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass that expands pseudo instructions into target; // instructions to allow proper scheduling and other late optimizations. This; // pass should be run after register allocation but before the post-regalloc; // scheduling pass.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp:513,Performance,optimiz,optimizations,513,"//===- AArch64ExpandPseudoInsts.cpp - Expand pseudo instructions ----------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass that expands pseudo instructions into target; // instructions to allow proper scheduling and other late optimizations. This; // pass should be run after register allocation but before the post-regalloc; // scheduling pass.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp:37,Safety,risk,risk,37,"// Useless def, and we don't want to risk creating an invalid ORR (which; // would really write to sp).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp:56,Integrability,depend,dependencies,56,// Do an extra pass in the loop to get the loop carried dependencies right.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp:56,Integrability,depend,dependencies,56,// Do an extra pass in the loop to get the loop carried dependencies right.; // FIXME: is this necessary?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp:209,Safety,avoid,avoid,209,"// Expand CALL_RVMARKER pseudo to:; // - a branch to the call target, followed by; // - the special `mov x29, x29` marker, and; // - another branch, to the runtime function; // Mark the sequence as bundle, to avoid passes moving other code in between.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp:126,Safety,avoid,avoid,126,"// Expand CALL_BTI pseudo to:; // - a branch to the call target; // - a BTI instruction; // Mark the sequence as a bundle, to avoid passes moving other code in; // between.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp:2,Deployability,Update,UpdateLiveIns,2,/*UpdateLiveIns*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp:2,Deployability,Update,UpdateLiveIns,2,/*UpdateLiveIns*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp:198,Deployability,toggle,toggle,198,"// Create the conditional branch based on the third operand of the; // instruction, which tells us if we are wrapping a normal or streaming; // function.; // We test the live value of pstate.sm and toggle pstate.sm if this is not the; // expected value for the callee (0 for a normal callee and 1 for a streaming; // callee).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp:109,Integrability,wrap,wrapping,109,"// Create the conditional branch based on the third operand of the; // instruction, which tells us if we are wrapping a normal or streaming; // function.; // We test the live value of pstate.sm and toggle pstate.sm if this is not the; // expected value for the callee (0 for a normal callee and 1 for a streaming; // callee).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp:161,Testability,test,test,161,"// Create the conditional branch based on the third operand of the; // instruction, which tells us if we are wrapping a normal or streaming; // function.; // We test the live value of pstate.sm and toggle pstate.sm if this is not the; // expected value for the callee (0 for a normal callee and 1 for a streaming; // callee).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp:2,Deployability,Update,UpdateLiveIns,2,/*UpdateLiveIns*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp:2,Deployability,Update,UpdateLiveIns,2,/*UpdateLiveIns*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp:369,Performance,load,loaded,369,"// MO_TAGGED on the page indicates a tagged address. Set the tag now.; // We do so by creating a MOVK that sets bits 48-63 of the register to; // (global address + 0x100000000 - PC) >> 48. This assumes that we're in; // the small code model so we can assume a binary size of <= 4GB, which; // makes the untagged PC relative offset positive. The binary must also be; // loaded into address range [0, 2^48). Both of these properties need to; // be ensured at runtime when using tagged addresses.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp:559,Performance,load,loads,559,"//===- AArch64FalkorHWPFFix.cpp - Avoid HW prefetcher pitfalls on Falkor --===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; /// \file For Falkor, we want to avoid HW prefetcher instruction tag collisions; /// that may inhibit the HW prefetching. This is done in two steps. Before; /// ISel, we mark strided loads (i.e. those that will likely benefit from; /// prefetching) with metadata. Then, after opcodes have been finalized, we; /// insert MOVs and re-write loads to prevent unintentional tag collisions.; // ===---------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp:714,Performance,load,loads,714,"//===- AArch64FalkorHWPFFix.cpp - Avoid HW prefetcher pitfalls on Falkor --===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; /// \file For Falkor, we want to avoid HW prefetcher instruction tag collisions; /// that may inhibit the HW prefetching. This is done in two steps. Before; /// ISel, we mark strided loads (i.e. those that will likely benefit from; /// prefetching) with metadata. Then, after opcodes have been finalized, we; /// insert MOVs and re-write loads to prevent unintentional tag collisions.; // ===---------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp:34,Safety,Avoid,Avoid,34,"//===- AArch64FalkorHWPFFix.cpp - Avoid HW prefetcher pitfalls on Falkor --===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; /// \file For Falkor, we want to avoid HW prefetcher instruction tag collisions; /// that may inhibit the HW prefetching. This is done in two steps. Before; /// ISel, we mark strided loads (i.e. those that will likely benefit from; /// prefetching) with metadata. Then, after opcodes have been finalized, we; /// insert MOVs and re-write loads to prevent unintentional tag collisions.; // ===---------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp:409,Safety,avoid,avoid,409,"//===- AArch64FalkorHWPFFix.cpp - Avoid HW prefetcher pitfalls on Falkor --===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; /// \file For Falkor, we want to avoid HW prefetcher instruction tag collisions; /// that may inhibit the HW prefetching. This is done in two steps. Before; /// ISel, we mark strided loads (i.e. those that will likely benefit from; /// prefetching) with metadata. Then, after opcodes have been finalized, we; /// insert MOVs and re-write loads to prevent unintentional tag collisions.; // ===---------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp:21,Performance,load,loads,21,// Only mark strided loads in the inner-most loop,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp:14,Performance,load,load,14,/// Bits from load opcodes used to compute HW prefetcher instruction tags.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp:3,Performance,Load,Loads,3,// Loads from the stack pointer don't get prefetched.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp:81,Performance,load,loads,81,// Go through all the basic blocks in the current loop and fix any streaming; // loads to avoid collisions with any other loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp:122,Performance,load,loads,122,// Go through all the basic blocks in the current loop and fix any streaming; // loads to avoid collisions with any other loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp:90,Safety,avoid,avoid,90,// Go through all the basic blocks in the current loop and fix any streaming; // loads to avoid collisions with any other loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp:3,Modifiability,Rewrite,Rewrite,3,"// Rewrite:; // Xd = LOAD Xb, off; // to:; // Xc = MOV Xb; // Xd = LOAD Xc, off",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp:21,Performance,LOAD,LOAD,21,"// Rewrite:; // Xd = LOAD Xb, off; // to:; // Xc = MOV Xb; // Xd = LOAD Xc, off",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp:67,Performance,LOAD,LOAD,67,"// Rewrite:; // Xd = LOAD Xb, off; // to:; // Xc = MOV Xb; // Xd = LOAD Xc, off",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp:81,Deployability,update,update,81,"// If the load does a pre/post increment, then insert a MOV after as; // well to update the real base register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp:10,Performance,load,load,10,"// If the load does a pre/post increment, then insert a MOV after as; // well to update the real base register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp:32,Deployability,update,update,32,// Change tied operand pre/post update dest.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp:3,Deployability,Update,Update,3,// Update TagMap to reflect instruction changes to reduce the number; // of later MOVs to be inserted. This needs to be done after; // OldCollisions is updated since it may be relocated by this; // insertion.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp:152,Deployability,update,updated,152,// Update TagMap to reflect instruction changes to reduce the number; // of later MOVs to be inserted. This needs to be done after; // OldCollisions is updated since it may be relocated by this; // insertion.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp:51,Energy Efficiency,reduce,reduce,51,// Update TagMap to reflect instruction changes to reduce the number; // of later MOVs to be inserted. This needs to be done after; // OldCollisions is updated since it may be relocated by this; // insertion.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:13,Integrability,rout,routines,13,// Selection routines.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:18,Integrability,rout,routines,18,// Utility helper routines.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:15,Integrability,rout,routines,15,// Emit helper routines.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:17,Integrability,rout,routines,17,// Call handling routines.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:56,Modifiability,extend,extend,56,// end anonymous namespace; /// Check if the sign-/zero-extend will be a noop.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:32,Modifiability,variab,variables,32,// We can't handle thread-local variables quickly yet.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:45,Security,access,accesses,45,"// MachO still uses GOT for large code-model accesses, but ELF requires; // movz/movk sequences, which FastISel doesn't handle yet.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:90,Modifiability,extend,extend,90,"// LDRWui produces a 32-bit register, but pointers in-register are 64-bits; // so we must extend the result on ILP32.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:369,Performance,load,loaded,369,"// MO_TAGGED on the page indicates a tagged address. Set the tag now.; // We do so by creating a MOVK that sets bits 48-63 of the register to; // (global address + 0x100000000 - PC) >> 48. This assumes that we're in; // the small code model so we can assume a binary size of <= 4GB, which; // makes the untagged PC relative offset positive. The binary must also be; // loaded into address range [0, 2^48). Both of these properties need to; // be ensured at runtime when using tagged addresses.; //; // TODO: There is duplicate logic in AArch64ExpandPseudoInsts.cpp that; // also uses BuildMI for making an ADRP (+ MOVK) + ADD, but the operands; // are not exactly 1:1 with FastISel so we cannot easily abstract this; // out. At some point, it would be nice to find a way to not have this; // duplciate code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:527,Testability,log,logic,527,"// MO_TAGGED on the page indicates a tagged address. Set the tag now.; // We do so by creating a MOVK that sets bits 48-63 of the register to; // (global address + 0x100000000 - PC) >> 48. This assumes that we're in; // the small code model so we can assume a binary size of <= 4GB, which; // makes the untagged PC relative offset positive. The binary must also be; // loaded into address range [0, 2^48). Both of these properties need to; // be ensured at runtime when using tagged addresses.; //; // TODO: There is duplicate logic in AArch64ExpandPseudoInsts.cpp that; // also uses BuildMI for making an ADRP (+ MOVK) + ADD, but the operands; // are not exactly 1:1 with FastISel so we cannot easily abstract this; // out. At some point, it would be nice to find a way to not have this; // duplciate code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:15,Usability,simpl,simple,15,// Only handle simple types.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:34,Energy Efficiency,power,power-of-,34,/// Check if the multiply is by a power-of-2 constant.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:16,Energy Efficiency,power,power-of-,16,// Canonicalize power-of-2 value to the RHS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:15,Usability,simpl,simple,15,// Only handle simple types.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:148,Usability,simpl,simple,148,"/// Determine if the value type is supported by FastISel.; ///; /// FastISel for AArch64 can handle more value types than are legal. This adds; /// simple value type such as i1, i8, and i16.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:46,Modifiability,extend,extended,46,// If this is a type than can be sign or zero-extended to a basic operation; // go ahead and accept it now.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:124,Performance,load,load,124,// Cannot encode an offset register and an immediate offset in the same; // instruction. Fold the immediate offset into the load/store instruction and; // emit an additional add to take care of the offset register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:57,Usability,simpl,simplified,57,"// If this is a stack pointer and the offset needs to be simplified then put; // the alloca address into a register, set the base type back to register and; // continue. This should almost never happen.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:41,Performance,load,load,41,// Since the offset is too large for the load/store instruction get the; // reg+offset into a register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:23,Energy Efficiency,power,power,23,// Canonicalize mul by power of 2 to the RHS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:8,Modifiability,extend,extend,8,// Only extend the RHS within the instruction if there is a valid extend type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:66,Modifiability,extend,extend,66,// Only extend the RHS within the instruction if there is a valid extend type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:21,Integrability,wrap,wrapper,21,"/// This method is a wrapper to simplify add emission.; ///; /// First try to emit an add with an immediate operand using emitAddSub_ri. If; /// that fails, then try to materialize the immediate into a register and use; /// emitAddSub_rr instead.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:32,Usability,simpl,simplify,32,"/// This method is a wrapper to simplify add emission.; ///; /// First try to emit an add with an immediate operand using emitAddSub_ri. If; /// that fails, then try to materialize the immediate into a register and use; /// emitAddSub_rr instead.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:23,Energy Efficiency,power,power-of-,23,// Canonicalize mul by power-of-2 to the RHS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:17,Availability,down,down,17,// Simplify this down to something we can handle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:3,Usability,Simpl,Simplify,3,// Simplify this down to something we can handle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:8,Modifiability,extend,extend,8,// Sign-extend.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:8,Modifiability,extend,extend,8,// Zero-extend.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:3,Performance,Load,Loading,3,// Loading an i1 requires special handling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:12,Modifiability,extend,extending,12,// For zero-extending loads to 64bit we emit a 32bit load and then convert; // the 32bit reg to a 64bit reg.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:22,Performance,load,loads,22,// For zero-extending loads to 64bit we emit a 32bit load and then convert; // the 32bit reg to a 64bit reg.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:53,Performance,load,load,53,// For zero-extending loads to 64bit we emit a 32bit load and then convert; // the 32bit reg to a 64bit reg.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:188,Modifiability,extend,extended,188,"// Verify we have a legal type before going any further. Currently, we handle; // simple types that will directly fit in a register (i32/f32/i64/f64) or; // those that can be sign or zero-extended to a basic operation (i1/i8/i16).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:82,Usability,simpl,simple,82,"// Verify we have a legal type before going any further. Currently, we handle; // simple types that will directly fit in a register (i32/f32/i64/f64) or; // those that can be sign or zero-extended to a basic operation (i1/i8/i16).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:33,Modifiability,extend,extend,33,// Fold the following sign-/zero-extend into the load instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:49,Performance,load,load,49,// Fold the following sign-/zero-extend into the load instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:93,Modifiability,extend,extend,93,"// There are a few different cases we have to handle, because the load or the; // sign-/zero-extend might not be selected by FastISel if we fall-back to; // SelectionDAG. There is also an ordering issue when both instructions are in; // different basic blocks.; // 1.) The load instruction is selected by FastISel, but the integer extend; // not. This usually happens when the integer extend is in a different; // basic block and SelectionDAG took over for that basic block.; // 2.) The load instruction is selected before the integer extend. This only; // happens when the integer extend is in a different basic block.; // 3.) The load instruction is selected by SelectionDAG and the integer extend; // by FastISel. This happens if there are instructions between the load; // and the integer extend that couldn't be selected by FastISel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:331,Modifiability,extend,extend,331,"// There are a few different cases we have to handle, because the load or the; // sign-/zero-extend might not be selected by FastISel if we fall-back to; // SelectionDAG. There is also an ordering issue when both instructions are in; // different basic blocks.; // 1.) The load instruction is selected by FastISel, but the integer extend; // not. This usually happens when the integer extend is in a different; // basic block and SelectionDAG took over for that basic block.; // 2.) The load instruction is selected before the integer extend. This only; // happens when the integer extend is in a different basic block.; // 3.) The load instruction is selected by SelectionDAG and the integer extend; // by FastISel. This happens if there are instructions between the load; // and the integer extend that couldn't be selected by FastISel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:385,Modifiability,extend,extend,385,"// There are a few different cases we have to handle, because the load or the; // sign-/zero-extend might not be selected by FastISel if we fall-back to; // SelectionDAG. There is also an ordering issue when both instructions are in; // different basic blocks.; // 1.) The load instruction is selected by FastISel, but the integer extend; // not. This usually happens when the integer extend is in a different; // basic block and SelectionDAG took over for that basic block.; // 2.) The load instruction is selected before the integer extend. This only; // happens when the integer extend is in a different basic block.; // 3.) The load instruction is selected by SelectionDAG and the integer extend; // by FastISel. This happens if there are instructions between the load; // and the integer extend that couldn't be selected by FastISel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:535,Modifiability,extend,extend,535,"// There are a few different cases we have to handle, because the load or the; // sign-/zero-extend might not be selected by FastISel if we fall-back to; // SelectionDAG. There is also an ordering issue when both instructions are in; // different basic blocks.; // 1.) The load instruction is selected by FastISel, but the integer extend; // not. This usually happens when the integer extend is in a different; // basic block and SelectionDAG took over for that basic block.; // 2.) The load instruction is selected before the integer extend. This only; // happens when the integer extend is in a different basic block.; // 3.) The load instruction is selected by SelectionDAG and the integer extend; // by FastISel. This happens if there are instructions between the load; // and the integer extend that couldn't be selected by FastISel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:582,Modifiability,extend,extend,582,"// There are a few different cases we have to handle, because the load or the; // sign-/zero-extend might not be selected by FastISel if we fall-back to; // SelectionDAG. There is also an ordering issue when both instructions are in; // different basic blocks.; // 1.) The load instruction is selected by FastISel, but the integer extend; // not. This usually happens when the integer extend is in a different; // basic block and SelectionDAG took over for that basic block.; // 2.) The load instruction is selected before the integer extend. This only; // happens when the integer extend is in a different basic block.; // 3.) The load instruction is selected by SelectionDAG and the integer extend; // by FastISel. This happens if there are instructions between the load; // and the integer extend that couldn't be selected by FastISel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:693,Modifiability,extend,extend,693,"// There are a few different cases we have to handle, because the load or the; // sign-/zero-extend might not be selected by FastISel if we fall-back to; // SelectionDAG. There is also an ordering issue when both instructions are in; // different basic blocks.; // 1.) The load instruction is selected by FastISel, but the integer extend; // not. This usually happens when the integer extend is in a different; // basic block and SelectionDAG took over for that basic block.; // 2.) The load instruction is selected before the integer extend. This only; // happens when the integer extend is in a different basic block.; // 3.) The load instruction is selected by SelectionDAG and the integer extend; // by FastISel. This happens if there are instructions between the load; // and the integer extend that couldn't be selected by FastISel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:793,Modifiability,extend,extend,793,"// There are a few different cases we have to handle, because the load or the; // sign-/zero-extend might not be selected by FastISel if we fall-back to; // SelectionDAG. There is also an ordering issue when both instructions are in; // different basic blocks.; // 1.) The load instruction is selected by FastISel, but the integer extend; // not. This usually happens when the integer extend is in a different; // basic block and SelectionDAG took over for that basic block.; // 2.) The load instruction is selected before the integer extend. This only; // happens when the integer extend is in a different basic block.; // 3.) The load instruction is selected by SelectionDAG and the integer extend; // by FastISel. This happens if there are instructions between the load; // and the integer extend that couldn't be selected by FastISel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:66,Performance,load,load,66,"// There are a few different cases we have to handle, because the load or the; // sign-/zero-extend might not be selected by FastISel if we fall-back to; // SelectionDAG. There is also an ordering issue when both instructions are in; // different basic blocks.; // 1.) The load instruction is selected by FastISel, but the integer extend; // not. This usually happens when the integer extend is in a different; // basic block and SelectionDAG took over for that basic block.; // 2.) The load instruction is selected before the integer extend. This only; // happens when the integer extend is in a different basic block.; // 3.) The load instruction is selected by SelectionDAG and the integer extend; // by FastISel. This happens if there are instructions between the load; // and the integer extend that couldn't be selected by FastISel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:273,Performance,load,load,273,"// There are a few different cases we have to handle, because the load or the; // sign-/zero-extend might not be selected by FastISel if we fall-back to; // SelectionDAG. There is also an ordering issue when both instructions are in; // different basic blocks.; // 1.) The load instruction is selected by FastISel, but the integer extend; // not. This usually happens when the integer extend is in a different; // basic block and SelectionDAG took over for that basic block.; // 2.) The load instruction is selected before the integer extend. This only; // happens when the integer extend is in a different basic block.; // 3.) The load instruction is selected by SelectionDAG and the integer extend; // by FastISel. This happens if there are instructions between the load; // and the integer extend that couldn't be selected by FastISel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:487,Performance,load,load,487,"// There are a few different cases we have to handle, because the load or the; // sign-/zero-extend might not be selected by FastISel if we fall-back to; // SelectionDAG. There is also an ordering issue when both instructions are in; // different basic blocks.; // 1.) The load instruction is selected by FastISel, but the integer extend; // not. This usually happens when the integer extend is in a different; // basic block and SelectionDAG took over for that basic block.; // 2.) The load instruction is selected before the integer extend. This only; // happens when the integer extend is in a different basic block.; // 3.) The load instruction is selected by SelectionDAG and the integer extend; // by FastISel. This happens if there are instructions between the load; // and the integer extend that couldn't be selected by FastISel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:632,Performance,load,load,632,"// There are a few different cases we have to handle, because the load or the; // sign-/zero-extend might not be selected by FastISel if we fall-back to; // SelectionDAG. There is also an ordering issue when both instructions are in; // different basic blocks.; // 1.) The load instruction is selected by FastISel, but the integer extend; // not. This usually happens when the integer extend is in a different; // basic block and SelectionDAG took over for that basic block.; // 2.) The load instruction is selected before the integer extend. This only; // happens when the integer extend is in a different basic block.; // 3.) The load instruction is selected by SelectionDAG and the integer extend; // by FastISel. This happens if there are instructions between the load; // and the integer extend that couldn't be selected by FastISel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:768,Performance,load,load,768,"// There are a few different cases we have to handle, because the load or the; // sign-/zero-extend might not be selected by FastISel if we fall-back to; // SelectionDAG. There is also an ordering issue when both instructions are in; // different basic blocks.; // 1.) The load instruction is selected by FastISel, but the integer extend; // not. This usually happens when the integer extend is in a different; // basic block and SelectionDAG took over for that basic block.; // 2.) The load instruction is selected before the integer extend. This only; // happens when the integer extend is in a different basic block.; // 3.) The load instruction is selected by SelectionDAG and the integer extend; // by FastISel. This happens if there are instructions between the load; // and the integer extend that couldn't be selected by FastISel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:15,Modifiability,extend,extend,15,// The integer extend hasn't been emitted yet. FastISel or SelectionDAG; // could select it. Emit a copy to subreg if necessary. FastISel will remove; // it when it selects the integer extend.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:185,Modifiability,extend,extend,185,// The integer extend hasn't been emitted yet. FastISel or SelectionDAG; // could select it. Emit a copy to subreg if necessary. FastISel will remove; // it when it selects the integer extend.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:15,Modifiability,extend,extend,15,// The integer extend has already been emitted - delete all the instructions; // that have been emitted by the integer extend lowering code and use the; // result from the load instruction directly.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:119,Modifiability,extend,extend,119,// The integer extend has already been emitted - delete all the instructions; // that have been emitted by the integer extend lowering code and use the; // result from the load instruction directly.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:172,Performance,load,load,172,// The integer extend has already been emitted - delete all the instructions; // that have been emitted by the integer extend lowering code and use the; // result from the load instruction directly.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:17,Availability,down,down,17,// Simplify this down to something we can handle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:3,Usability,Simpl,Simplify,3,// Simplify this down to something we can handle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:188,Modifiability,extend,extended,188,"// Verify we have a legal type before going any further. Currently, we handle; // simple types that will directly fit in a register (i32/f32/i64/f64) or; // those that can be sign or zero-extended to a basic operation (i1/i8/i16).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:82,Usability,simpl,simple,82,"// Verify we have a legal type before going any further. Currently, we handle; // simple types that will directly fit in a register (i32/f32/i64/f64) or; // those that can be sign or zero-extended to a basic operation (i1/i8/i16).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:99,Safety,avoid,avoid,99,// Get the value to be stored into a register. Use the zero register directly; // when possible to avoid an unnecessary copy and a wasted register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:34,Deployability,release,release,34,// Try to emit a STLR for seq_cst/release.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:41,Performance,optimiz,optimized,41,"// Speculation tracking/SLH assumes that optimized TB(N)Z/CB(N)Z instructions; // will not be produced, as they are conditional branch instructions that do; // not set flags.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:10,Performance,optimiz,optimize,10,// Try to optimize or fold the cmp.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:79,Performance,optimiz,optimized,79,"// Fake request the condition, otherwise the intrinsic might be completely; // optimized away.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:37,Testability,test,test,37,"// i1 conditions come as i32 values, test the lowest bit with tb(n)z.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:10,Performance,optimiz,optimize,10,// Try to optimize or fold the cmp.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:4,Performance,Optimiz,Optimize,4,/// Optimize selects of i1 if one of the operands has a 'true' or 'false'; /// value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:10,Performance,optimiz,optimize,10,// Try to optimize or fold the cmp.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:15,Usability,simpl,simple,15,// Only handle simple cases of up to 8 GPR and FPR each.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:96,Testability,stub,stub,96,// The weak function target may be zero; in that case we must use indirect; // addressing via a stub on windows as it may be out of range for a; // PC-relative jump.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:18,Availability,mask,mask,18,// Add a register mask with the call-preserved registers.; // Proper defs for return values will be added by setPhysRegsDeadExcept().,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:126,Deployability,update,updated,126,/// Check if it is possible to fold the condition from the XALU intrinsic; /// into the user. The condition code will only be updated on success.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:3,Usability,Simpl,Simplify,3,// Simplify multiplies.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:15,Performance,load,load,15,"// Recursively load frame address; // ldr x0, [fp]; // ldr x0, [x0]; // ldr x0, [x0]; // ...",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:3,Usability,Simpl,Simplify,3,// Simplify multiplies.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:3,Safety,Avoid,Avoid,3,// Avoid a cross-class copy. This is very unlikely.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:24,Modifiability,extend,extended,24,// Special handling for extended integers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:39,Modifiability,extend,extends,39,"// ""Callee"" (i.e. value producer) zero extends pointers at function; // boundary.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:36,Performance,perform,performs,36,// Create the AND instruction which performs the actual truncation.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:112,Modifiability,extend,extend,112,"// We're ZExt i1 to i64. The ANDWri Wd, Ws, #1 implicitly clears the; // upper 32 bits. Emit a SUBREG_TO_REG to extend from Wd to Xd.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:58,Usability,clear,clears,58,"// We're ZExt i1 to i64. The ANDWri Wd, Ws, #1 implicitly clears the; // upper 32 bits. Emit a SUBREG_TO_REG to extend from Wd to Xd.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:37,Modifiability,extend,extend,37,// It is not possible to fold a sign-extend into the LShr instruction. In this; // case emit a sign-extend.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:100,Modifiability,extend,extend,100,// It is not possible to fold a sign-extend into the LShr instruction. In this; // case emit a sign-extend.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:108,Testability,test,test,108,"// FastISel does not have plumbing to deal with extensions where the SrcVT or; // DestVT are odd things, so test to make sure that they are both types we can; // handle (i1/i8/i16/i32 for SrcVT and i8/i16/i32/i64 for DestVT), otherwise; // bail out to SelectionDAG.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:16,Performance,load,load,16,// Check if the load instruction has already been selected.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:103,Modifiability,extend,extending,103,"// Check if the correct load instruction has been emitted - SelectionDAG might; // have emitted a zero-extending load, but we need a sign-extending load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:138,Modifiability,extend,extending,138,"// Check if the correct load instruction has been emitted - SelectionDAG might; // have emitted a zero-extending load, but we need a sign-extending load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:24,Performance,load,load,24,"// Check if the correct load instruction has been emitted - SelectionDAG might; // have emitted a zero-extending load, but we need a sign-extending load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:113,Performance,load,load,113,"// Check if the correct load instruction has been emitted - SelectionDAG might; // have emitted a zero-extending load, but we need a sign-extending load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:148,Performance,load,load,148,"// Check if the correct load instruction has been emitted - SelectionDAG might; // have emitted a zero-extending load, but we need a sign-extending load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:38,Modifiability,extend,extended,38,// Try to optimize already sign-/zero-extended values from load instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:10,Performance,optimiz,optimize,10,// Try to optimize already sign-/zero-extended values from load instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:59,Performance,load,load,59,// Try to optimize already sign-/zero-extended values from load instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:38,Modifiability,extend,extended,38,// Try to optimize already sign-/zero-extended values from function arguments.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:10,Performance,optimiz,optimize,10,// Try to optimize already sign-/zero-extended values from function arguments.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:10,Usability,simpl,simplify,10,// Try to simplify to a shift instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:170,Modifiability,extend,extend,170,"/// This is mostly a copy of the existing FastISel getRegForGEPIndex code. We; /// have to duplicate it for AArch64, because otherwise we would fail during the; /// sign-extend emission.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:64,Modifiability,extend,extend,64,"// If the index is smaller or larger than intptr_t, truncate or extend it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:157,Usability,simpl,simple,157,"/// This is mostly a copy of the existing FastISel GEP code, but we have to; /// duplicate it for AArch64, because otherwise we would bail out even for; /// simple cases. This is because the standard fastEmit functions don't cover; /// MUL at all and ADD is lowered very inefficientily.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:543,Availability,down,downward,543,"//===- AArch64FrameLowering.cpp - AArch64 Frame Lowering -------*- C++ -*-====//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains the AArch64 implementation of TargetFrameLowering class.; //; // On AArch64, stack frames are structured as follows:; //; // The stack grows downward.; //; // All of the individual frame areas on the frame below are optional, i.e. it's; // possible to create a function so that the particular area isn't present; // in the frame.; //; // At function entry, the ""frame"" looks as follows:; //; // | | Higher address; // |-----------------------------------|; // | |; // | arguments passed on the stack |; // | |; // |-----------------------------------| <- sp; // | | Lower address; //; //; // After the prologue has run, the frame has the following general structure.; // Note that this doesn't depict the case where a red-zone is used. Also,; // technically the last frame area (VLAs) doesn't get created until in the; // main function body, after the prologue is run. However, it's depicted here; // for completeness.; //; // | | Higher address; // |-----------------------------------|; // | |; // | arguments passed on the stack |; // | |; // |-----------------------------------|; // | |; // | (Win64 only) varargs from reg |; // | |; // |-----------------------------------|; // | |; // | callee-saved gpr registers | <--.; // | | | On Darwin platforms these; // |- - - - - - - - - - - - - - - - - -| | callee saves are swapped,; // | prev_lr | | (frame record first); // | prev_fp | <--'; // | async context if needed |; // | (a.k.a. ""frame record"") |; // |-----------------------------------| <- fp(=x29); // | |; // | callee-saved fp/simd/SVE regs |; // | |; // |----------------------------",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:3963,Availability,avail,available,3963,"eas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are local variables with; // more-than-default alignment requirements.; //; // For Darwin platforms the frame-record (fp, lr) is stored at the top of the; // callee-saved area, since the unwind encoding does not allow for encoding; // this dynamically and existing tools depend on this layout. For other; // platforms, the frame-record is stored at the bottom of the (gpr) callee-saved; // area to allow SVE stack objects (allocated directly below the callee-saves,; // if available) to be accessed directly from the framepointer.; // The SVE spill/fill instructions have VL-scaled addressing modes such; // as:; // ldr z8, [fp, #-7 mul vl]; // For SVE the size of the vector length (VL) is not known at compile-time, so; // '#-7 mul vl' is an offset that can only be evaluated at runtime. With this; // layout, we don't need to add an unscaled offset to the framepointer before; // accessing the SVE object in the frame.; //; // In some cases when a base pointer is not strictly needed, it is generated; // anyway when offsets from the frame pointer to access local variables become; // so large that the offset can't be encoded in the immediate fields of loads; // or stores.; //; // Outgoing function arguments must be at the bottom of the stack frame when; // calling another function. If we do not have variable-sized stack objects, we; // can allocate a ""reserved call frame"" area at the bottom of the loca",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:3913,Energy Efficiency,allocate,allocated,3913,"eas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are local variables with; // more-than-default alignment requirements.; //; // For Darwin platforms the frame-record (fp, lr) is stored at the top of the; // callee-saved area, since the unwind encoding does not allow for encoding; // this dynamically and existing tools depend on this layout. For other; // platforms, the frame-record is stored at the bottom of the (gpr) callee-saved; // area to allow SVE stack objects (allocated directly below the callee-saves,; // if available) to be accessed directly from the framepointer.; // The SVE spill/fill instructions have VL-scaled addressing modes such; // as:; // ldr z8, [fp, #-7 mul vl]; // For SVE the size of the vector length (VL) is not known at compile-time, so; // '#-7 mul vl' is an offset that can only be evaluated at runtime. With this; // layout, we don't need to add an unscaled offset to the framepointer before; // accessing the SVE object in the frame.; //; // In some cases when a base pointer is not strictly needed, it is generated; // anyway when offsets from the frame pointer to access local variables become; // so large that the offset can't be encoded in the immediate fields of loads; // or stores.; //; // Outgoing function arguments must be at the bottom of the stack frame when; // calling another function. If we do not have variable-sized stack objects, we; // can allocate a ""reserved call frame"" area at the bottom of the loca",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:4839,Energy Efficiency,allocate,allocate,4839,"-saved; // area to allow SVE stack objects (allocated directly below the callee-saves,; // if available) to be accessed directly from the framepointer.; // The SVE spill/fill instructions have VL-scaled addressing modes such; // as:; // ldr z8, [fp, #-7 mul vl]; // For SVE the size of the vector length (VL) is not known at compile-time, so; // '#-7 mul vl' is an offset that can only be evaluated at runtime. With this; // layout, we don't need to add an unscaled offset to the framepointer before; // accessing the SVE object in the frame.; //; // In some cases when a base pointer is not strictly needed, it is generated; // anyway when offsets from the frame pointer to access local variables become; // so large that the offset can't be encoded in the immediate fields of loads; // or stores.; //; // Outgoing function arguments must be at the bottom of the stack frame when; // calling another function. If we do not have variable-sized stack objects, we; // can allocate a ""reserved call frame"" area at the bottom of the local; // variable area, large enough for all outgoing calls. If we do have VLAs, then; // the stack pointer must be decremented and incremented around each call to; // make space for the arguments below the VLAs.; //; // FIXME: also explain the redzone concept.; //; // An example of the prologue:; //; // .globl __foo; // .align 2; // __foo:; // Ltmp0:; // .cfi_startproc; // .cfi_personality 155, ___gxx_personality_v0; // Leh_func_begin:; // .cfi_lsda 16, Lexception33; //; // stp xa,bx, [sp, -#offset]!; // ...; // stp x28, x27, [sp, #offset-32]; // stp fp, lr, [sp, #offset-16]; // add fp, sp, #offset - 16; // sub sp, sp, #1360; //; // The Stack:; // +-------------------------------------------+; // 10000 | ........ | ........ | ........ | ........ |; // 10004 | ........ | ........ | ........ | ........ |; // +-------------------------------------------+; // 10008 | ........ | ........ | ........ | ........ |; // 1000c | ........ | ........ | ........ | ....",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:3761,Integrability,depend,depend,3761,"-------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are local variables with; // more-than-default alignment requirements.; //; // For Darwin platforms the frame-record (fp, lr) is stored at the top of the; // callee-saved area, since the unwind encoding does not allow for encoding; // this dynamically and existing tools depend on this layout. For other; // platforms, the frame-record is stored at the bottom of the (gpr) callee-saved; // area to allow SVE stack objects (allocated directly below the callee-saves,; // if available) to be accessed directly from the framepointer.; // The SVE spill/fill instructions have VL-scaled addressing modes such; // as:; // ldr z8, [fp, #-7 mul vl]; // For SVE the size of the vector length (VL) is not known at compile-time, so; // '#-7 mul vl' is an offset that can only be evaluated at runtime. With this; // layout, we don't need to add an unscaled offset to the framepointer before; // accessing the SVE object in the frame.; //; // In some cases when a base pointer is not strictly needed, it is generated; // anyway when offsets from the frame pointer to access local variables become; // so large that the offset can't be encoded in the immediate fields of loads; // or stores.; //;",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:2341,Modifiability,variab,variables,2341,"---------------------------|; // | |; // | arguments passed on the stack |; // | |; // |-----------------------------------|; // | |; // | (Win64 only) varargs from reg |; // | |; // |-----------------------------------|; // | |; // | callee-saved gpr registers | <--.; // | | | On Darwin platforms these; // |- - - - - - - - - - - - - - - - - -| | callee saves are swapped,; // | prev_lr | | (frame record first); // | prev_fp | <--'; // | async context if needed |; // | (a.k.a. ""frame record"") |; // |-----------------------------------| <- fp(=x29); // | |; // | callee-saved fp/simd/SVE regs |; // | |; // |-----------------------------------|; // | |; // | SVE stack objects |; // | |; // |-----------------------------------|; // |.empty.space.to.make.part.below....|; // |.aligned.in.case.it.needs.more.than| (size of this area is unknown at; // |.the.standard.16-byte.alignment....| compile time; if present); // |-----------------------------------|; // | |; // | local variables of fixed size |; // | including spill slots |; // |-----------------------------------| <- bp(not defined by ABI,; // |.variable-sized.local.variables....| LLVM chooses X19); // |.(VLAs)............................| (size of this area is unknown at; // |...................................| compile time); // |-----------------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both V",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:2471,Modifiability,variab,variable-sized,2471,"------|; // | |; // | (Win64 only) varargs from reg |; // | |; // |-----------------------------------|; // | |; // | callee-saved gpr registers | <--.; // | | | On Darwin platforms these; // |- - - - - - - - - - - - - - - - - -| | callee saves are swapped,; // | prev_lr | | (frame record first); // | prev_fp | <--'; // | async context if needed |; // | (a.k.a. ""frame record"") |; // |-----------------------------------| <- fp(=x29); // | |; // | callee-saved fp/simd/SVE regs |; // | |; // |-----------------------------------|; // | |; // | SVE stack objects |; // | |; // |-----------------------------------|; // |.empty.space.to.make.part.below....|; // |.aligned.in.case.it.needs.more.than| (size of this area is unknown at; // |.the.standard.16-byte.alignment....| compile time; if present); // |-----------------------------------|; // | |; // | local variables of fixed size |; // | including spill slots |; // |-----------------------------------| <- bp(not defined by ABI,; // |.variable-sized.local.variables....| LLVM chooses X19); // |.(VLAs)............................| (size of this area is unknown at; // |...................................| compile time); // |-----------------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:2492,Modifiability,variab,variables,2492," | (Win64 only) varargs from reg |; // | |; // |-----------------------------------|; // | |; // | callee-saved gpr registers | <--.; // | | | On Darwin platforms these; // |- - - - - - - - - - - - - - - - - -| | callee saves are swapped,; // | prev_lr | | (frame record first); // | prev_fp | <--'; // | async context if needed |; // | (a.k.a. ""frame record"") |; // |-----------------------------------| <- fp(=x29); // | |; // | callee-saved fp/simd/SVE regs |; // | |; // |-----------------------------------|; // | |; // | SVE stack objects |; // | |; // |-----------------------------------|; // |.empty.space.to.make.part.below....|; // |.aligned.in.case.it.needs.more.than| (size of this area is unknown at; // |.the.standard.16-byte.alignment....| compile time; if present); // |-----------------------------------|; // | |; // | local variables of fixed size |; // | including spill slots |; // |-----------------------------------| <- bp(not defined by ABI,; // |.variable-sized.local.variables....| LLVM chooses X19); // |.(VLAs)............................| (size of this area is unknown at; // |...................................| compile time); // |-----------------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are loc",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:3379,Modifiability,variab,variables,3379,"/ | local variables of fixed size |; // | including spill slots |; // |-----------------------------------| <- bp(not defined by ABI,; // |.variable-sized.local.variables....| LLVM chooses X19); // |.(VLAs)............................| (size of this area is unknown at; // |...................................| compile time); // |-----------------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are local variables with; // more-than-default alignment requirements.; //; // For Darwin platforms the frame-record (fp, lr) is stored at the top of the; // callee-saved area, since the unwind encoding does not allow for encoding; // this dynamically and existing tools depend on this layout. For other; // platforms, the frame-record is stored at the bottom of the (gpr) callee-saved; // area to allow SVE stack objects (allocated directly below the callee-saves,; // if available) to be accessed directly from the framepointer.; // The SVE spill/fill instructions have VL-scaled addressing modes such; // as:; // ldr z8, [fp, #-7 mul vl]; // For SVE the size of the vector length (VL) is not known at compile-time, so; // '#-7 mul vl' is an offset that can only be evaluated at runtime. With this; // layout, we don't need to add an unscal",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:3500,Modifiability,variab,variables,3500,"bles....| LLVM chooses X19); // |.(VLAs)............................| (size of this area is unknown at; // |...................................| compile time); // |-----------------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are local variables with; // more-than-default alignment requirements.; //; // For Darwin platforms the frame-record (fp, lr) is stored at the top of the; // callee-saved area, since the unwind encoding does not allow for encoding; // this dynamically and existing tools depend on this layout. For other; // platforms, the frame-record is stored at the bottom of the (gpr) callee-saved; // area to allow SVE stack objects (allocated directly below the callee-saves,; // if available) to be accessed directly from the framepointer.; // The SVE spill/fill instructions have VL-scaled addressing modes such; // as:; // ldr z8, [fp, #-7 mul vl]; // For SVE the size of the vector length (VL) is not known at compile-time, so; // '#-7 mul vl' is an offset that can only be evaluated at runtime. With this; // layout, we don't need to add an unscaled offset to the framepointer before; // accessing the SVE object in the frame.; //; // In some cases when a base pointer is not strictly needed, it is generated; // ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:4557,Modifiability,variab,variables,4557,"ignment requirements.; //; // For Darwin platforms the frame-record (fp, lr) is stored at the top of the; // callee-saved area, since the unwind encoding does not allow for encoding; // this dynamically and existing tools depend on this layout. For other; // platforms, the frame-record is stored at the bottom of the (gpr) callee-saved; // area to allow SVE stack objects (allocated directly below the callee-saves,; // if available) to be accessed directly from the framepointer.; // The SVE spill/fill instructions have VL-scaled addressing modes such; // as:; // ldr z8, [fp, #-7 mul vl]; // For SVE the size of the vector length (VL) is not known at compile-time, so; // '#-7 mul vl' is an offset that can only be evaluated at runtime. With this; // layout, we don't need to add an unscaled offset to the framepointer before; // accessing the SVE object in the frame.; //; // In some cases when a base pointer is not strictly needed, it is generated; // anyway when offsets from the frame pointer to access local variables become; // so large that the offset can't be encoded in the immediate fields of loads; // or stores.; //; // Outgoing function arguments must be at the bottom of the stack frame when; // calling another function. If we do not have variable-sized stack objects, we; // can allocate a ""reserved call frame"" area at the bottom of the local; // variable area, large enough for all outgoing calls. If we do have VLAs, then; // the stack pointer must be decremented and incremented around each call to; // make space for the arguments below the VLAs.; //; // FIXME: also explain the redzone concept.; //; // An example of the prologue:; //; // .globl __foo; // .align 2; // __foo:; // Ltmp0:; // .cfi_startproc; // .cfi_personality 155, ___gxx_personality_v0; // Leh_func_begin:; // .cfi_lsda 16, Lexception33; //; // stp xa,bx, [sp, -#offset]!; // ...; // stp x28, x27, [sp, #offset-32]; // stp fp, lr, [sp, #offset-16]; // add fp, sp, #offset - 16; // sub sp, sp, #1360; //; /",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:4798,Modifiability,variab,variable-sized,4798,"-saved; // area to allow SVE stack objects (allocated directly below the callee-saves,; // if available) to be accessed directly from the framepointer.; // The SVE spill/fill instructions have VL-scaled addressing modes such; // as:; // ldr z8, [fp, #-7 mul vl]; // For SVE the size of the vector length (VL) is not known at compile-time, so; // '#-7 mul vl' is an offset that can only be evaluated at runtime. With this; // layout, we don't need to add an unscaled offset to the framepointer before; // accessing the SVE object in the frame.; //; // In some cases when a base pointer is not strictly needed, it is generated; // anyway when offsets from the frame pointer to access local variables become; // so large that the offset can't be encoded in the immediate fields of loads; // or stores.; //; // Outgoing function arguments must be at the bottom of the stack frame when; // calling another function. If we do not have variable-sized stack objects, we; // can allocate a ""reserved call frame"" area at the bottom of the local; // variable area, large enough for all outgoing calls. If we do have VLAs, then; // the stack pointer must be decremented and incremented around each call to; // make space for the arguments below the VLAs.; //; // FIXME: also explain the redzone concept.; //; // An example of the prologue:; //; // .globl __foo; // .align 2; // __foo:; // Ltmp0:; // .cfi_startproc; // .cfi_personality 155, ___gxx_personality_v0; // Leh_func_begin:; // .cfi_lsda 16, Lexception33; //; // stp xa,bx, [sp, -#offset]!; // ...; // stp x28, x27, [sp, #offset-32]; // stp fp, lr, [sp, #offset-16]; // add fp, sp, #offset - 16; // sub sp, sp, #1360; //; // The Stack:; // +-------------------------------------------+; // 10000 | ........ | ........ | ........ | ........ |; // 10004 | ........ | ........ | ........ | ........ |; // +-------------------------------------------+; // 10008 | ........ | ........ | ........ | ........ |; // 1000c | ........ | ........ | ........ | ....",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:4908,Modifiability,variab,variable,4908,"-saved; // area to allow SVE stack objects (allocated directly below the callee-saves,; // if available) to be accessed directly from the framepointer.; // The SVE spill/fill instructions have VL-scaled addressing modes such; // as:; // ldr z8, [fp, #-7 mul vl]; // For SVE the size of the vector length (VL) is not known at compile-time, so; // '#-7 mul vl' is an offset that can only be evaluated at runtime. With this; // layout, we don't need to add an unscaled offset to the framepointer before; // accessing the SVE object in the frame.; //; // In some cases when a base pointer is not strictly needed, it is generated; // anyway when offsets from the frame pointer to access local variables become; // so large that the offset can't be encoded in the immediate fields of loads; // or stores.; //; // Outgoing function arguments must be at the bottom of the stack frame when; // calling another function. If we do not have variable-sized stack objects, we; // can allocate a ""reserved call frame"" area at the bottom of the local; // variable area, large enough for all outgoing calls. If we do have VLAs, then; // the stack pointer must be decremented and incremented around each call to; // make space for the arguments below the VLAs.; //; // FIXME: also explain the redzone concept.; //; // An example of the prologue:; //; // .globl __foo; // .align 2; // __foo:; // Ltmp0:; // .cfi_startproc; // .cfi_personality 155, ___gxx_personality_v0; // Leh_func_begin:; // .cfi_lsda 16, Lexception33; //; // stp xa,bx, [sp, -#offset]!; // ...; // stp x28, x27, [sp, #offset-32]; // stp fp, lr, [sp, #offset-16]; // add fp, sp, #offset - 16; // sub sp, sp, #1360; //; // The Stack:; // +-------------------------------------------+; // 10000 | ........ | ........ | ........ | ........ |; // 10004 | ........ | ........ | ........ | ........ |; // +-------------------------------------------+; // 10008 | ........ | ........ | ........ | ........ |; // 1000c | ........ | ........ | ........ | ....",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:4647,Performance,load,loads,4647,"ignment requirements.; //; // For Darwin platforms the frame-record (fp, lr) is stored at the top of the; // callee-saved area, since the unwind encoding does not allow for encoding; // this dynamically and existing tools depend on this layout. For other; // platforms, the frame-record is stored at the bottom of the (gpr) callee-saved; // area to allow SVE stack objects (allocated directly below the callee-saves,; // if available) to be accessed directly from the framepointer.; // The SVE spill/fill instructions have VL-scaled addressing modes such; // as:; // ldr z8, [fp, #-7 mul vl]; // For SVE the size of the vector length (VL) is not known at compile-time, so; // '#-7 mul vl' is an offset that can only be evaluated at runtime. With this; // layout, we don't need to add an unscaled offset to the framepointer before; // accessing the SVE object in the frame.; //; // In some cases when a base pointer is not strictly needed, it is generated; // anyway when offsets from the frame pointer to access local variables become; // so large that the offset can't be encoded in the immediate fields of loads; // or stores.; //; // Outgoing function arguments must be at the bottom of the stack frame when; // calling another function. If we do not have variable-sized stack objects, we; // can allocate a ""reserved call frame"" area at the bottom of the local; // variable area, large enough for all outgoing calls. If we do have VLAs, then; // the stack pointer must be decremented and incremented around each call to; // make space for the arguments below the VLAs.; //; // FIXME: also explain the redzone concept.; //; // An example of the prologue:; //; // .globl __foo; // .align 2; // __foo:; // Ltmp0:; // .cfi_startproc; // .cfi_personality 155, ___gxx_personality_v0; // Leh_func_begin:; // .cfi_lsda 16, Lexception33; //; // stp xa,bx, [sp, -#offset]!; // ...; // stp x28, x27, [sp, #offset-32]; // stp fp, lr, [sp, #offset-16]; // add fp, sp, #offset - 16; // sub sp, sp, #1360; //; /",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:2741,Security,access,access,2741,"e record first); // | prev_fp | <--'; // | async context if needed |; // | (a.k.a. ""frame record"") |; // |-----------------------------------| <- fp(=x29); // | |; // | callee-saved fp/simd/SVE regs |; // | |; // |-----------------------------------|; // | |; // | SVE stack objects |; // | |; // |-----------------------------------|; // |.empty.space.to.make.part.below....|; // |.aligned.in.case.it.needs.more.than| (size of this area is unknown at; // |.the.standard.16-byte.alignment....| compile time; if present); // |-----------------------------------|; // | |; // | local variables of fixed size |; // | including spill slots |; // |-----------------------------------| <- bp(not defined by ABI,; // |.variable-sized.local.variables....| LLVM chooses X19); // |.(VLAs)............................| (size of this area is unknown at; // |...................................| compile time); // |-----------------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are local variables with; // more-than-default alignment requirements.; //; // For Darwin platforms the frame-record (fp, lr) is stored at the top of the; // callee-saved area, since the unwind encoding does not allow for encoding; // this dynamically and existing tool",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:2868,Security,access,access,2868,"e record first); // | prev_fp | <--'; // | async context if needed |; // | (a.k.a. ""frame record"") |; // |-----------------------------------| <- fp(=x29); // | |; // | callee-saved fp/simd/SVE regs |; // | |; // |-----------------------------------|; // | |; // | SVE stack objects |; // | |; // |-----------------------------------|; // |.empty.space.to.make.part.below....|; // |.aligned.in.case.it.needs.more.than| (size of this area is unknown at; // |.the.standard.16-byte.alignment....| compile time; if present); // |-----------------------------------|; // | |; // | local variables of fixed size |; // | including spill slots |; // |-----------------------------------| <- bp(not defined by ABI,; // |.variable-sized.local.variables....| LLVM chooses X19); // |.(VLAs)............................| (size of this area is unknown at; // |...................................| compile time); // |-----------------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are local variables with; // more-than-default alignment requirements.; //; // For Darwin platforms the frame-record (fp, lr) is stored at the top of the; // callee-saved area, since the unwind encoding does not allow for encoding; // this dynamically and existing tool",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:3075,Security,access,access,3075," | SVE stack objects |; // | |; // |-----------------------------------|; // |.empty.space.to.make.part.below....|; // |.aligned.in.case.it.needs.more.than| (size of this area is unknown at; // |.the.standard.16-byte.alignment....| compile time; if present); // |-----------------------------------|; // | |; // | local variables of fixed size |; // | including spill slots |; // |-----------------------------------| <- bp(not defined by ABI,; // |.variable-sized.local.variables....| LLVM chooses X19); // |.(VLAs)............................| (size of this area is unknown at; // |...................................| compile time); // |-----------------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are local variables with; // more-than-default alignment requirements.; //; // For Darwin platforms the frame-record (fp, lr) is stored at the top of the; // callee-saved area, since the unwind encoding does not allow for encoding; // this dynamically and existing tools depend on this layout. For other; // platforms, the frame-record is stored at the bottom of the (gpr) callee-saved; // area to allow SVE stack objects (allocated directly below the callee-saves,; // if available) to be accessed directly from the framepointer.; ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:3980,Security,access,accessed,3980,"eas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are local variables with; // more-than-default alignment requirements.; //; // For Darwin platforms the frame-record (fp, lr) is stored at the top of the; // callee-saved area, since the unwind encoding does not allow for encoding; // this dynamically and existing tools depend on this layout. For other; // platforms, the frame-record is stored at the bottom of the (gpr) callee-saved; // area to allow SVE stack objects (allocated directly below the callee-saves,; // if available) to be accessed directly from the framepointer.; // The SVE spill/fill instructions have VL-scaled addressing modes such; // as:; // ldr z8, [fp, #-7 mul vl]; // For SVE the size of the vector length (VL) is not known at compile-time, so; // '#-7 mul vl' is an offset that can only be evaluated at runtime. With this; // layout, we don't need to add an unscaled offset to the framepointer before; // accessing the SVE object in the frame.; //; // In some cases when a base pointer is not strictly needed, it is generated; // anyway when offsets from the frame pointer to access local variables become; // so large that the offset can't be encoded in the immediate fields of loads; // or stores.; //; // Outgoing function arguments must be at the bottom of the stack frame when; // calling another function. If we do not have variable-sized stack objects, we; // can allocate a ""reserved call frame"" area at the bottom of the loca",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:4373,Security,access,accessing,4373,"there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are local variables with; // more-than-default alignment requirements.; //; // For Darwin platforms the frame-record (fp, lr) is stored at the top of the; // callee-saved area, since the unwind encoding does not allow for encoding; // this dynamically and existing tools depend on this layout. For other; // platforms, the frame-record is stored at the bottom of the (gpr) callee-saved; // area to allow SVE stack objects (allocated directly below the callee-saves,; // if available) to be accessed directly from the framepointer.; // The SVE spill/fill instructions have VL-scaled addressing modes such; // as:; // ldr z8, [fp, #-7 mul vl]; // For SVE the size of the vector length (VL) is not known at compile-time, so; // '#-7 mul vl' is an offset that can only be evaluated at runtime. With this; // layout, we don't need to add an unscaled offset to the framepointer before; // accessing the SVE object in the frame.; //; // In some cases when a base pointer is not strictly needed, it is generated; // anyway when offsets from the frame pointer to access local variables become; // so large that the offset can't be encoded in the immediate fields of loads; // or stores.; //; // Outgoing function arguments must be at the bottom of the stack frame when; // calling another function. If we do not have variable-sized stack objects, we; // can allocate a ""reserved call frame"" area at the bottom of the local; // variable area, large enough for all outgoing calls. If we do have VLAs, then; // the stack pointer must be decremented and incremented around each call to; // make space for the arguments below the VLAs.; //; // FIXME: also explain the redzone concept.; //; // An example of the prologue:; //; // .globl __foo; // .align 2; // __foo:; // Ltmp0:; // .cfi_startproc; // .cfi_personality 155, ___gxx_personality_v0; // Leh_func_begin:; // .",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:4544,Security,access,access,4544,"ignment requirements.; //; // For Darwin platforms the frame-record (fp, lr) is stored at the top of the; // callee-saved area, since the unwind encoding does not allow for encoding; // this dynamically and existing tools depend on this layout. For other; // platforms, the frame-record is stored at the bottom of the (gpr) callee-saved; // area to allow SVE stack objects (allocated directly below the callee-saves,; // if available) to be accessed directly from the framepointer.; // The SVE spill/fill instructions have VL-scaled addressing modes such; // as:; // ldr z8, [fp, #-7 mul vl]; // For SVE the size of the vector length (VL) is not known at compile-time, so; // '#-7 mul vl' is an offset that can only be evaluated at runtime. With this; // layout, we don't need to add an unscaled offset to the framepointer before; // accessing the SVE object in the frame.; //; // In some cases when a base pointer is not strictly needed, it is generated; // anyway when offsets from the frame pointer to access local variables become; // so large that the offset can't be encoded in the immediate fields of loads; // or stores.; //; // Outgoing function arguments must be at the bottom of the stack frame when; // calling another function. If we do not have variable-sized stack objects, we; // can allocate a ""reserved call frame"" area at the bottom of the local; // variable area, large enough for all outgoing calls. If we do have VLAs, then; // the stack pointer must be decremented and incremented around each call to; // make space for the arguments below the VLAs.; //; // FIXME: also explain the redzone concept.; //; // An example of the prologue:; //; // .globl __foo; // .align 2; // __foo:; // Ltmp0:; // .cfi_startproc; // .cfi_personality 155, ___gxx_personality_v0; // Leh_func_begin:; // .cfi_lsda 16, Lexception33; //; // stp xa,bx, [sp, -#offset]!; // ...; // stp x28, x27, [sp, #offset-32]; // stp fp, lr, [sp, #offset-16]; // add fp, sp, #offset - 16; // sub sp, sp, #1360; //; /",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:139,Integrability,inject,injected,139,"/// Returns true if a homogeneous prolog or epilog code can be emitted; /// for the size optimization. If possible, a frame helper call is injected.; /// When Exit block is given, this check is for epilog.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:89,Performance,optimiz,optimization,89,"/// Returns true if a homogeneous prolog or epilog code can be emitted; /// for the size optimization. If possible, a frame helper call is injected.; /// When Exit block is given, this check is for epilog.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:139,Security,inject,injected,139,"/// Returns true if a homogeneous prolog or epilog code can be emitted; /// for the size optimization. If possible, a frame helper call is injected.; /// When Exit block is given, this check is for epilog.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:49,Usability,simpl,simplicity,49,// Bail on stack adjustment needed on return for simplicity.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:207,Performance,load,loads,207,"/// This is the biggest offset to the stack pointer we can encode in aarch64; /// instructions (without using a separate calculation and a temp register).; /// Note that the exception here are vector stores/loads which cannot encode any; /// displacements (see estimateRSStackSizeLimit(), isAArch64FrameOffsetLegal()).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:47,Energy Efficiency,allocate,allocated,47,/// Returns the size of the fixed object area (allocated next to sp on entry); /// On Win64 this may include a var args area and an UnwindHelp object for EH.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:29,Energy Efficiency,allocate,allocate,29,// To support EH funclets we allocate an UnwindHelp object,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:84,Security,access,accessed,84,"// Win64 EH requires a frame pointer if funclets are present, as the locals; // are accessed off the frame pointer in both the parent function and the; // funclets.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:57,Security,access,access,57,"// With large callframes around we may need to use FP to access the scavenging; // emergency spillslot.; //; // Unfortunately some calls to hasFP() like machine verifier ->; // getReservedReg() -> hasFP in the middle of global isel are too early; // to know the max call frame size. Hopefully conservatively returning ""true""; // in those cases is fine.; // DefaultSafeSPDisplacement is fine as we only emergency spill GP regs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:46,Energy Efficiency,allocate,allocated,46,"// The stack probing code for the dynamically allocated outgoing arguments; // area assumes that the stack is probed at the top - either by the prologue; // code, which issues a probe if `hasVarSizedObjects` return true, or by the; // most recent variable-sized object allocation. Changing the condition here; // may need to be followed up by changes to the probe issuing logic.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:247,Modifiability,variab,variable-sized,247,"// The stack probing code for the dynamically allocated outgoing arguments; // area assumes that the stack is probed at the top - either by the prologue; // code, which issues a probe if `hasVarSizedObjects` return true, or by the; // most recent variable-sized object allocation. Changing the condition here; // may need to be followed up by changes to the probe issuing logic.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:372,Testability,log,logic,372,"// The stack probing code for the dynamically allocated outgoing arguments; // area assumes that the stack is probed at the top - either by the prologue; // code, which issues a probe if `hasVarSizedObjects` return true, or by the; // most recent variable-sized object allocation. Changing the condition here; // may need to be followed up by changes to the probe issuing logic.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:124,Availability,avail,available,124,"// FIXME: in-function stack adjustment for calls is limited to 24-bits; // because there's no guaranteed temporary register available.; //; // ADD/SUB (immediate) has only LSL #0 and LSL #12 available.; // 1) For offset <= 12-bit, we use LSL #0; // 2) For 12-bit <= offset <= 24-bit, we use two instructions. One uses; // LSL #0, and the other uses LSL #12.; //; // Most call frames will be allocated at the start of a function so; // this is OK, but it is a limitation that needs dealing with.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:191,Availability,avail,available,191,"// FIXME: in-function stack adjustment for calls is limited to 24-bits; // because there's no guaranteed temporary register available.; //; // ADD/SUB (immediate) has only LSL #0 and LSL #12 available.; // 1) For offset <= 12-bit, we use LSL #0; // 2) For 12-bit <= offset <= 24-bit, we use two instructions. One uses; // LSL #0, and the other uses LSL #12.; //; // Most call frames will be allocated at the start of a function so; // this is OK, but it is a limitation that needs dealing with.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:391,Energy Efficiency,allocate,allocated,391,"// FIXME: in-function stack adjustment for calls is limited to 24-bits; // because there's no guaranteed temporary register available.; //; // ADD/SUB (immediate) has only LSL #0 and LSL #12 available.; // 1) For offset <= 12-bit, we use LSL #0; // 2) For 12-bit <= offset <= 24-bit, we use two instructions. One uses; // LSL #0, and the other uses LSL #12.; //; // Most call frames will be allocated at the start of a function so; // this is OK, but it is a limitation that needs dealing with.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:155,Energy Efficiency,efficient,efficient,155,"//; // Stack probing allocation.; //; // Fixed length allocation. If we don't need to re-align the stack and don't; // have SVE objects, we can use a more efficient sequence for stack probing.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:127,Modifiability,variab,variable-sized,127,"// The fixed allocation may leave unprobed bytes at the top of the; // stack. If we have subsequent alocation (e.g. if we have variable-sized; // objects), we need to issue an extra probe, so these allocations start in; // a known state.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:3,Modifiability,Variab,Variable,3,"// Variable length allocation.; // If the (unknown) allocation size cannot exceed the probe size, decrement; // the stack pointer right away.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:10,Modifiability,variab,variable-length,10,"// Emit a variable-length allocation probing loop.; // TODO: As an optimisation, the loop can be ""unrolled"" into a few parts,; // each of them guaranteed to adjust the stack by less than the probe size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:14,Integrability,rout,routine,14,// The called routine is expected to preserve r19-r28; // r29 and r30 are used as frame pointer and link register resp.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:29,Usability,clear,clear,29,"// For GPRs, we only care to clear out the 64-bit register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:242,Integrability,wrap,wrapping,242,"// Find a scratch register that we can use at the start of the prologue to; // re-align the stack pointer. We avoid using callee-save registers since they; // may appear to be free when this is called from canUseAsPrologue (during; // shrink wrapping), but then no longer be free when this is called from; // emitPrologue.; //; // FIXME: This is a bit conservative, since in the above case we could use one; // of the callee-save registers as a scratch temp to re-align the stack pointer,; // but we would then have to make sure that we were in fact saving at least one; // callee-save register in the prologue, which is additional complexity that; // doesn't seem worth the benefit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:110,Safety,avoid,avoid,110,"// Find a scratch register that we can use at the start of the prologue to; // re-align the stack pointer. We avoid using callee-save registers since they; // may appear to be free when this is called from canUseAsPrologue (during; // shrink wrapping), but then no longer be free when this is called from; // emitPrologue.; //; // FIXME: This is a bit conservative, since in the above case we could use one; // of the callee-save registers as a scratch temp to re-align the stack pointer,; // but we would then have to make sure that we were in fact saving at least one; // callee-save register in the prologue, which is additional complexity that; // doesn't seem worth the benefit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:75,Availability,avail,available,75,// The StoreSwiftAsyncContext clobbers X16 and X17. Make sure they are; // available.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:76,Availability,avail,available,76,"// Otherwise, we can use any block as long as it has a scratch register; // available.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:449,Energy Efficiency,reduce,reduces,449,"// For WinCFI, if optimizing for size, prefer to not combine the stack bump; // (to force a stp with predecrement) to match the packed unwind format,; // provided that there actually are any callee saved registers to merge the; // decrement with.; // This is potentially marginally slower, but allows using the packed; // unwind format for functions that both have a local area and callee saved; // registers. Using the packed unwind format notably reduces the size of; // the unwind info.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:18,Performance,optimiz,optimizing,18,"// For WinCFI, if optimizing for size, prefer to not combine the stack bump; // (to force a stp with predecrement) to match the packed unwind format,; // provided that there actually are any callee saved registers to merge the; // decrement with.; // This is potentially marginally slower, but allows using the packed; // unwind format for functions that both have a local area and callee saved; // registers. Using the packed unwind format notably reduces the size of; // the unwind info.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:41,Usability,simpl,simplifies,41,"// This isn't strictly necessary, but it simplifies things a bit since the; // current RedZone handling code assumes the SP is adjusted by the; // callee-save save/restore code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:50,Energy Efficiency,allocate,allocate,50,"// When there is an SVE area on the stack, always allocate the; // callee-saves and spills/locals separately.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:11,Performance,load,load,11,"// Given a load or a store instruction, generate an appropriate unwinding SEH; // code on Windows.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:104,Energy Efficiency,allocate,allocate,104,// Convert callee-save register save/restore instruction to do stack pointer; // decrement/increment to allocate/deallocate the callee-save stack area by; // converting store/load to use pre/post increment version.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:175,Performance,load,load,175,// Convert callee-save register save/restore instruction to do stack pointer; // decrement/increment to allocate/deallocate the callee-save stack area by; // converting store/load to use pre/post increment version.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:78,Deployability,update,update,78,// If the first store isn't right where we want SP then we can't fold the; // update in so create a normal arithmetic instruction instead.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:37,Modifiability,extend,extended,37,"// We signal the presence of a Swift extended frame to external tools by; // storing FP with 0b0001 in bits 63:60. In normal userland operation a simple; // ORR is sufficient, it is assumed a Swift kernel would initialize the TBI; // bits so that is still true.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:146,Usability,simpl,simple,146,"// We signal the presence of a Swift extended frame to external tools by; // storing FP with 0b0001 in bits 63:60. In normal userland operation a simple; // ORR is sufficient, it is assumed a Swift kernel would initialize the TBI; // bits so that is still true.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:119,Modifiability,extend,extended,119,// The special symbol below is absolute and has a *value* that can be; // combined with the frame pointer to signal an extended frame.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:161,Energy Efficiency,allocate,allocated,161,"// getStackSize() includes all the locals in its size calculation. We don't; // include these locals when computing the stack size of a funclet, as they; // are allocated in the parent's stack frame and accessed via the frame; // pointer from the funclet. We only save the callee saved registers in the; // funclet, which are really the callee saved registers of the parent; // function, including the funclet.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:203,Security,access,accessed,203,"// getStackSize() includes all the locals in its size calculation. We don't; // include these locals when computing the stack size of a funclet, as they; // are allocated in the parent's stack frame and accessed via the frame; // pointer from the funclet. We only save the callee saved registers in the; // funclet, which are really the callee saved registers of the parent; // function, including the funclet.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:84,Energy Efficiency,allocate,allocate,84,"// REDZONE: If the stack size is less than 128 bytes, we don't need; // to actually allocate.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:13,Deployability,update,update,13,"// Before we update the live FP we have to ensure there's a valid (or; // null) asynchronous context in its slot just before FP in the frame; // record, so store it now.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:72,Energy Efficiency,allocate,allocated,72,// Process the SVE callee-saves to determine what space needs to be; // allocated.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:3,Energy Efficiency,Allocate,Allocate,3,// Allocate space for the callee saves (if any).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:3,Energy Efficiency,Allocate,Allocate,3,// Allocate space for the rest of the frame including SVE locals. Align the; // stack as necessary.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:147,Energy Efficiency,allocate,allocated,147,"// If we need a base pointer, set it up here. It's whatever the value of the; // stack pointer is at this point. Any variable size objects will be allocated; // after this, so we can still use the base pointer to reference locals.; //; // FIXME: Clarify FrameSetup flags here.; // Note: Use emitFrameOffset() like above for FP if the FrameSetup flag is; // needed.; // For funclets the BP belongs to the containing function.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:117,Modifiability,variab,variable,117,"// If we need a base pointer, set it up here. It's whatever the value of the; // stack pointer is at this point. Any variable size objects will be allocated; // after this, so we can still use the base pointer to reference locals.; //; // FIXME: Clarify FrameSetup flags here.; // Note: Use emitFrameOffset() like above for FP if the FrameSetup flag is; // needed.; // For funclets the BP belongs to the containing function.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:73,Energy Efficiency,allocate,allocate,73,"// If the offset is 0 and the AfterCSR pop is not actually trying to; // allocate more stack for arguments (in space that an untimely interrupt; // may clobber), convert it to a post-index ldp.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:293,Safety,avoid,avoid,293,"// Note that there are cases where we insert SEH opcodes in the; // epilogue when we had no SEH opcodes in the prologue. For; // example, when there is no stack frame but there are stack; // arguments. Insert the SEH_EpilogStart and remove it later if it; // we didn't emit any SEH opcodes to avoid generating WinCFI for; // functions that don't need it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:3,Safety,Avoid,Avoid,3,"// Avoid the reload as it is GOT relative, and instead fall back to the; // hardcoded value below. This allows a mismatch between the OS and; // application without immediately terminating on the difference.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:112,Modifiability,extend,extended,112,"// We need to reset FP to its untagged state on return. Bit 60 is; // currently used to show the presence of an extended frame.; // BIC x29, x29, #0x1000_0000_0000_0000",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:27,Deployability,update,update,27,"// If there is a single SP update, insert it before the ret and we're done.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:35,Modifiability,variab,variable,35,"// If we have stack realignment or variable sized objects on the stack,; // restore the stack pointer from the frame pointer prior to SVE CSR; // restoration.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:17,Modifiability,variab,variables,17,"// Pop the local variables off the stack. If there are no callee-saved; // registers, it means we are actually positioned at the terminator and can; // combine stack increment for the locals and the stack increment for; // callee-popped arguments into (possibly) a single instruction and be done.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:131,Performance,load,loads,131,"// Restore the original stack pointer.; // FIXME: Rather than doing the math here, we should instead just use; // non-post-indexed loads for the restores if we aren't actually going to; // be able to save any instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:243,Usability,simpl,simple,243,/// getFrameIndexReference - Provide a base+offset reference to an FI slot for; /// debug info. It's the same as what we use for resolving the code-gen; /// references for now. FIXME: This can go wrong when references are; /// SP-relative and simple call frames aren't used.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:51,Performance,scalab,scalable,51,// TODO: This function currently does not work for scalable vectors.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:146,Availability,reliab,reliable,146,// Use frame pointer to reference fixed objects. Use it for locals if; // there are VLAs or a dynamically realigned SP (and thus the SP isn't; // reliable as a base). Make sure useFPForScavengingIndex() does the; // right thing for the emergency spill slot.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:91,Performance,scalab,scalable,91,// We shouldn't prefer using the FP to access fixed-sized stack objects when; // there are scalable (SVE) objects in between the FP and the fixed-sized; // objects.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:39,Security,access,access,39,// We shouldn't prefer using the FP to access fixed-sized stack objects when; // there are scalable (SVE) objects in between the FP and the fixed-sized; // objects.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:140,Security,access,access,140,// Note: Keeping the following as multiple 'if' statements rather than; // merging to a single expression for readability.; //; // Argument access should always use the FP.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:108,Availability,avail,available,108,"// If the FPOffset is negative and we're producing a signed immediate, we; // have to keep in mind that the available offset range for negative; // offsets is smaller than for positive ones. If an offset is available; // via the FP and the SP, use whichever is closest.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:207,Availability,avail,available,207,"// If the FPOffset is negative and we're producing a signed immediate, we; // have to keep in mind that the available offset range for negative; // offsets is smaller than for positive ones. If an offset is available; // via the FP and the SP, use whichever is closest.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:14,Modifiability,variab,variable,14,"// If we have variable sized objects, we can use either FP or BP, as the; // SP offset is unknown. We can use the base pointer if we have one and; // FP is not preferred. If not, we're stuck with using FP.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:127,Safety,avoid,avoid,127,"// else we can use BP and FP, but the offset from FP won't fit.; // That will make us scavenge registers which we can probably avoid by; // using BP. If it won't fit for BP either, we'll scavenge anyway.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:96,Security,access,access,96,"// Use SP or FP, whichever gives us the best chance of the offset; // being in range for direct access. If the FPOffset is positive,; // that'll always be best, as the SP will be even further away.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:12,Security,access,access,12,"// Funclets access the locals contained in the parent's stack frame; // via the frame pointer, so we have to use the FP in the parent; // function.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:39,Availability,avail,available,39,// Always use the FP for SVE spills if available and beneficial.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:178,Energy Efficiency,allocate,allocated,178,/// Returns true if Reg1 and Reg2 cannot be paired using a ldp/stp instruction.; /// WindowsCFI requires that only consecutive registers can be paired.; /// LR and FP need to be allocated together when the frame needs to save; /// the frame-record. This means any other register pairing with LR is invalid.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:67,Integrability,wrap,wraparound,67,"// When iterating backwards, the loop condition relies on unsigned wraparound.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:175,Testability,Assert,Assert,175,"// GPRs and FPRs are saved in pairs of 64-bit regs. We expect the CSI; // list to come in sorted by frame index so that we can issue the store; // pair instructions directly. Assert if we see anything otherwise.; //; // The order of the registers in the list is controlled by; // getCalleeSavedRegs(), so they will always be in-order, as well.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:51,Energy Efficiency,allocate,allocate,51,"// Swift's async context is directly before FP, so allocate an extra; // 8 bytes for it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:18,Availability,down,down,18,"// If filling top down (default), we want the offset after incrementing it.; // If filling bottom up (WinCFI) we need the original offset.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:254,Availability,down,down,254,"// If we need an alignment gap in the stack, align the topmost stack; // object. A stack frame with a gap looks like this, bottom up:; // x19, d8. d9, gap.; // Set extra alignment on the topmost stack object (the first element in; // CSI, which goes top down), to create the gap above it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:75,Availability,down,down,75,// We iterated bottom up over the registers; flip RegPairs back to top; // down order.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:3,Deployability,Update,Update,3,// Update register live in.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:405,Deployability,update,updates,405,"// Issue sequence of spills for cs regs. The first spill may be converted; // to a pre-decrement store later by emitPrologue if the callee-save stack; // area allocation can't be combined with the local stack area allocation.; // For example:; // stp x22, x21, [sp, #0] // addImm(+0); // stp x20, x19, [sp, #16] // addImm(+2); // stp fp, lr, [sp, #32] // addImm(+4); // Rationale: This sequence saves uop updates compared to a sequence of; // pre-increment spills like stp xi,xj,[sp,#-16]!; // Note: Similar rationale and sequence for restores in epilog.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:3,Deployability,Update,Update,3,// Update the StackIDs of the SVE stack slots.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:101,Performance,load,load,101,"// Issue sequence of restores for cs regs. The last restore may be converted; // to a post-increment load later by emitEpilogue if the callee-save stack; // area allocation can't be combined with the local stack area allocation.; // For example:; // ldp fp, lr, [sp, #32] // addImm(+4); // ldp x20, x19, [sp, #16] // addImm(+2); // ldp x22, x21, [sp, #0] // addImm(+0); // Note: see comment in spillCalleeSavedRegisters()",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:47,Deployability,update,update,47,"// Save number of saved regs, so we can easily update CSStackSize later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:37,Energy Efficiency,allocate,allocated,37,"// The CSR spill slots have not been allocated yet, so estimateStackSize; // won't include them.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:32,Modifiability,extend,extends,32,// A Swift asynchronous context extends the frame record with a pointer; // directly before FP.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:42,Safety,avoid,avoid,42,// Round up to register pair alignment to avoid additional SP adjustment; // instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:218,Availability,down,down,218,"// To match the canonical windows frame layout, reverse the list of; // callee saved registers to get them laid out by PrologEpilogInserter; // in the right order. (PrologEpilogInserter allocates stack objects top; // down. Windows canonical prologs store higher numbered registers at; // the top, thus have the CSI array start from the highest registers.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:186,Energy Efficiency,allocate,allocates,186,"// To match the canonical windows frame layout, reverse the list of; // callee saved registers to get them laid out by PrologEpilogInserter; // in the right order. (PrologEpilogInserter allocates stack objects top; // down. Windows canonical prologs store higher numbered registers at; // the top, thus have the CSI array start from the highest registers.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:125,Energy Efficiency,allocate,allocate,125,"// Early exit if no callee saved registers are modified!; // Now that we know which registers need to be saved and restored, allocate; // stack slots for them.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:33,Modifiability,extend,extended,33,// Grab 8 bytes below FP for the extended asynchronous frame info.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:37,Energy Efficiency,allocate,allocate,37,// Create a buffer of SVE objects to allocate and sort it.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:3,Energy Efficiency,Allocate,Allocate,3,// Allocate all SVE locals and spills,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:68,Energy Efficiency,power,power,68,"// FIXME: Given that the length of SVE vectors is not necessarily a power of; // two, we'd need to align every object dynamically at runtime if the; // alignment is larger than 16. This is not yet supported.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:61,Energy Efficiency,allocate,allocated,61,// Create an UnwindHelp object.; // The UnwindHelp object is allocated at the start of the fixed object area,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:104,Deployability,update,update,104,"// If the loop size is not a multiple of 32, split off one 16-byte store at; // the end to fold BaseReg update into.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:36,Deployability,update,update,36,// Tag 16 more bytes at BaseReg and update BaseReg.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:3,Deployability,Update,Update,3,// Update BaseReg.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:30,Deployability,update,update,30,// Check if *II is a register update that can be merged into STGloop that ends; // at (Reg + Size). RemainingOffset is the required adjustment to Reg after the; // end of the loop.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:46,Security,access,access,46,// An instruction without memory operands may access anything. Be; // conservative and return an empty list.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:37,Deployability,update,update,37,"// See if we can merge base register update into the STGloop.; // This is done in AArch64LoadStoreOptimizer for ""normal"" stores,; // but STGloop is way too unusual for that, and also it only; // realistically happens in function epilogue. Also, STGloop is expanded; // before that pass.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:3,Safety,Detect,Detect,3,"// Detect a run of memory tagging instructions for adjacent stack frame slots,; // and replace them with a shorter instruction sequence:; // * replace STG + STG with ST2G; // * replace STGloop + STGloop with STGloop; // This code needs to run when stack slot offsets are already known, but before; // FrameIndex operands in STG instructions are eliminated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:29,Deployability,update,update,29,"// Collect instructions that update memory tags with a FrameIndex operand; // and (when applicable) constant size, and whose output registers are dead; // (the latter is almost always the case in practice). Since these; // instructions effectively have no inputs or outputs, we are free to skip; // any non-aliasing instructions in between without tracking used registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:18,Deployability,update,updates,18,// Multiple FP/SP updates in a loop cannot be described by CFI instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:89,Deployability,update,update,89,"/// For Win64 AArch64 EH, the offset to the Unwind object is from the SP; /// before the update. This is easily retrieved as it is exactly the offset; /// that is set in processFunctionBeforeFrameFinalized.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:50,Energy Efficiency,allocate,allocate,50,// This is the amount of stack a funclet needs to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:52,Safety,safe,safe,52,"// All invalid items are sorted at the end, so it's safe to stop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:3,Deployability,Update,Update,3,// Update liveins.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:110,Safety,avoid,avoid,110,// Get the instructions that need to be replaced. We emit at most two of; // these. Remember them in order to avoid complications coming from the need; // to traverse the block while potentially creating more blocks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.h:60,Integrability,wrap,wrapping,60,/// Returns true if the target will correctly handle shrink wrapping.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.h:53,Energy Efficiency,allocate,allocated,53,// We don't support putting SVE objects into the pre-allocated local; // frame block at the moment.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.h:89,Performance,optimiz,optimization,89,"/// Returns true if a homogeneous prolog or epilog code can be emitted; /// for the size optimization. If so, HOM_Prolog/HOM_Epilog pseudo; /// instructions are emitted in place. When Exit block is given, this check is; /// for epilog.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.h:25,Testability,stub,stub,25,/// Replace a StackProbe stub (if any) with the actual probe code inline,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64GlobalsTagging.cpp:645,Modifiability,variab,variables,645,"// Globals can be placed implicitly or explicitly in sections. There's two; // different types of globals that meet this criteria that cause problems:; // 1. Function pointers that are going into various init arrays (either; // explicitly through `__attribute__((section(<foo>)))` or implicitly; // through `__attribute__((constructor)))`, such as "".(pre)init(_array)"",; // "".fini(_array)"", "".ctors"", and "".dtors"". These function pointers end up; // overaligned and overpadded, making iterating over them problematic, and; // each function pointer is individually tagged (so the iteration over; // them causes SIGSEGV/MTE[AS]ERR).; // 2. Global variables put into an explicit section, where the section's name; // is a valid C-style identifier. The linker emits a `__start_<name>` and; // `__stop_<na,e>` symbol for the section, so that you can iterate over; // globals within this section. Unfortunately, again, these globals would; // be tagged and so iteration causes SIGSEGV/MTE[AS]ERR.; //; // To mitigate both these cases, and because specifying a section is rare; // outside of these two cases, disable MTE protection for globals in any; // section.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64GlobalsTagging.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64GlobalsTagging.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64GlobalsTagging.cpp:725,Integrability,contract,contract,725,"// Technically, due to ELF symbol interposition semantics, we can't change the; // alignment or size of symbols. If we increase the alignment or size of a; // symbol, the compiler may make optimisations based on this new alignment or; // size. If the symbol is interposed, this optimisation could lead to; // alignment-related or OOB read/write crashes.; //; // This is handled in the linker. When the linker sees multiple declarations of; // a global variable, and some are tagged, and some are untagged, it resolves it; // to be an untagged definition - but preserves the tag-granule-rounded size and; // tag-granule-alignment. This should prevent these kind of crashes intra-DSO.; // For cross-DSO, it's been a reasonable contract that if you're interposing a; // sanitizer-instrumented global, then the interposer also needs to be; // sanitizer-instrumented.; //; // FIXME: In theory, this can be fixed by splitting the size/alignment of; // globals into two uses: an ""output alignment"" that's emitted to the ELF file,; // and an ""optimisation alignment"" that's used for optimisation. Thus, we could; // adjust the output alignment only, and still optimise based on the pessimistic; // pre-tagging size/alignment.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64GlobalsTagging.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64GlobalsTagging.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64GlobalsTagging.cpp:452,Modifiability,variab,variable,452,"// Technically, due to ELF symbol interposition semantics, we can't change the; // alignment or size of symbols. If we increase the alignment or size of a; // symbol, the compiler may make optimisations based on this new alignment or; // size. If the symbol is interposed, this optimisation could lead to; // alignment-related or OOB read/write crashes.; //; // This is handled in the linker. When the linker sees multiple declarations of; // a global variable, and some are tagged, and some are untagged, it resolves it; // to be an untagged definition - but preserves the tag-granule-rounded size and; // tag-granule-alignment. This should prevent these kind of crashes intra-DSO.; // For cross-DSO, it's been a reasonable contract that if you're interposing a; // sanitizer-instrumented global, then the interposer also needs to be; // sanitizer-instrumented.; //; // FIXME: In theory, this can be fixed by splitting the size/alignment of; // globals into two uses: an ""output alignment"" that's emitted to the ELF file,; // and an ""optimisation alignment"" that's used for optimisation. Thus, we could; // adjust the output alignment only, and still optimise based on the pessimistic; // pre-tagging size/alignment.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64GlobalsTagging.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64GlobalsTagging.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64GlobalsTagging.cpp:767,Security,sanitiz,sanitizer-instrumented,767,"// Technically, due to ELF symbol interposition semantics, we can't change the; // alignment or size of symbols. If we increase the alignment or size of a; // symbol, the compiler may make optimisations based on this new alignment or; // size. If the symbol is interposed, this optimisation could lead to; // alignment-related or OOB read/write crashes.; //; // This is handled in the linker. When the linker sees multiple declarations of; // a global variable, and some are tagged, and some are untagged, it resolves it; // to be an untagged definition - but preserves the tag-granule-rounded size and; // tag-granule-alignment. This should prevent these kind of crashes intra-DSO.; // For cross-DSO, it's been a reasonable contract that if you're interposing a; // sanitizer-instrumented global, then the interposer also needs to be; // sanitizer-instrumented.; //; // FIXME: In theory, this can be fixed by splitting the size/alignment of; // globals into two uses: an ""output alignment"" that's emitted to the ELF file,; // and an ""optimisation alignment"" that's used for optimisation. Thus, we could; // adjust the output alignment only, and still optimise based on the pessimistic; // pre-tagging size/alignment.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64GlobalsTagging.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64GlobalsTagging.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64GlobalsTagging.cpp:839,Security,sanitiz,sanitizer-instrumented,839,"// Technically, due to ELF symbol interposition semantics, we can't change the; // alignment or size of symbols. If we increase the alignment or size of a; // symbol, the compiler may make optimisations based on this new alignment or; // size. If the symbol is interposed, this optimisation could lead to; // alignment-related or OOB read/write crashes.; //; // This is handled in the linker. When the linker sees multiple declarations of; // a global variable, and some are tagged, and some are untagged, it resolves it; // to be an untagged definition - but preserves the tag-granule-rounded size and; // tag-granule-alignment. This should prevent these kind of crashes intra-DSO.; // For cross-DSO, it's been a reasonable contract that if you're interposing a; // sanitizer-instrumented global, then the interposer also needs to be; // sanitizer-instrumented.; //; // FIXME: In theory, this can be fixed by splitting the size/alignment of; // globals into two uses: an ""output alignment"" that's emitted to the ELF file,; // and an ""optimisation alignment"" that's used for optimisation. Thus, we could; // adjust the output alignment only, and still optimise based on the pessimistic; // pre-tagging size/alignment.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64GlobalsTagging.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64GlobalsTagging.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:142,Modifiability,variab,variable,142,// Size should be preferably set in; // llvm/lib/Target/AArch64/AArch64InstrInfo.td (default case).; // Specific cases handle instructions of variable sizes,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:19,Deployability,patch,patchpoint,19,// The size of the patchpoint intrinsic is the number of bytes requested,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:6,Deployability,patch,patch,6,// No patch bytes means a normal call inst is emitted,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:7,Deployability,patch,patchable-function-entry,7,"// If `patchable-function-entry` is set, PATCHABLE_FUNCTION_ENTER; // instructions are expanded to the specified number of NOPs. Otherwise,; // they are expanded to 36-byte XRay sleds.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:91,Usability,simpl,simply,91,"// If we're allowed to modify and the block ends in a unconditional branch; // which could simply fallthrough, remove the branch. (Note: This case only; // matters when we can't understand the whole sequence, otherwise it's also; // handled by BranchFolding.cpp.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:65,Performance,optimiz,optimize,65,"// Also need to check the dest regclass, in case we're trying to optimize; // something like:; // %1(gpr) = PHI %2(fpr), bb1, %(fpr), bb2",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:48,Performance,latency,latency,48,// Expanding cbz/tbz requires an extra cycle of latency on the condition.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:15,Usability,simpl,simple,15,// Try folding simple instructions into the csel.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:7,Modifiability,extend,extends,7,// The extends the live range of NewVReg.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:29,Performance,load,loaded,29,"// Return true if Imm can be loaded into a register by a ""cheap"" sequence of; // instructions. For now, ""cheap"" means at most two instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:59,Integrability,depend,dependent,59,"// FIXME: this implementation should be micro-architecture dependent, so a; // micro-architecture target hook should be introduced here in future.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:7,Usability,ux,uxtw,7,"// aka uxtw; // Check for the 32 -> 64 bit extension case, these instructions can do; // much more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:100,Performance,load,loaded,100,"// Retrieve the base, offset from the base and width. Width; // is the size of memory that is being loaded/stored (e.g. 1, 2, 4, 8). If; // base are identical, and the offset of a lower memory access +; // the width doesn't overlap the offset of a higher memory access,; // then the memory accesses are different.; // If OffsetAIsScalable and OffsetBIsScalable are both true, they; // are assumed to have the same scale (vscale).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:193,Security,access,access,193,"// Retrieve the base, offset from the base and width. Width; // is the size of memory that is being loaded/stored (e.g. 1, 2, 4, 8). If; // base are identical, and the offset of a lower memory access +; // the width doesn't overlap the offset of a higher memory access,; // then the memory accesses are different.; // If OffsetAIsScalable and OffsetBIsScalable are both true, they; // are assumed to have the same scale (vscale).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:262,Security,access,access,262,"// Retrieve the base, offset from the base and width. Width; // is the size of memory that is being loaded/stored (e.g. 1, 2, 4, 8). If; // base are identical, and the offset of a lower memory access +; // the width doesn't overlap the offset of a higher memory access,; // then the memory accesses are different.; // If OffsetAIsScalable and OffsetBIsScalable are both true, they; // are assumed to have the same scale (vscale).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:290,Security,access,accesses,290,"// Retrieve the base, offset from the base and width. Width; // is the size of memory that is being loaded/stored (e.g. 1, 2, 4, 8). If; // base are identical, and the offset of a lower memory access +; // the width doesn't overlap the offset of a higher memory access,; // then the memory accesses are different.; // If OffsetAIsScalable and OffsetBIsScalable are both true, they; // are assumed to have the same scale (vscale).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:18,Energy Efficiency,schedul,scheduling,18,// CSDB hints are scheduling barriers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:24,Energy Efficiency,schedul,scheduling,24,// DSB and ISB also are scheduling barriers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:31,Energy Efficiency,schedul,scheduling,31,// SMSTART and SMSTOP are also scheduling barriers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:22,Availability,mask,mask,22,// Not sure about the mask and value for now...,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:34,Security,access,accessed,34,/// True when condition flags are accessed (either by writing or reading); /// on the instruction trace starting at From and ending at To.; ///; /// Note: If From and To are from different blocks it's assumed CC are accessed; /// on the path.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:216,Security,access,accessed,216,/// True when condition flags are accessed (either by writing or reading); /// on the instruction trace starting at From and ending at To.; ///; /// Note: If From and To are from different blocks it's assumed CC are accessed; /// on the path.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:4,Performance,optimiz,optimizePTestInstr,4,/// optimizePTestInstr - Attempt to remove a ptest of a predicate-generating; /// operation which could set the flags in an identical manner,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:78,Availability,redundant,redundant,78,"// For PTEST(PTRUE_ALL, WHILE), if the element size matches, the PTEST is; // redundant since WHILE performs an implicit PTEST with an all active; // mask. Must be an all active predicate of matching element size.; // For PTEST(PTRUE_ALL, PTEST_LIKE), the PTEST is redundant if the; // PTEST_LIKE instruction uses the same all active mask and the element; // size matches. If the PTEST has a condition of any then it is always; // redundant.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:150,Availability,mask,mask,150,"// For PTEST(PTRUE_ALL, WHILE), if the element size matches, the PTEST is; // redundant since WHILE performs an implicit PTEST with an all active; // mask. Must be an all active predicate of matching element size.; // For PTEST(PTRUE_ALL, PTEST_LIKE), the PTEST is redundant if the; // PTEST_LIKE instruction uses the same all active mask and the element; // size matches. If the PTEST has a condition of any then it is always; // redundant.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:265,Availability,redundant,redundant,265,"// For PTEST(PTRUE_ALL, WHILE), if the element size matches, the PTEST is; // redundant since WHILE performs an implicit PTEST with an all active; // mask. Must be an all active predicate of matching element size.; // For PTEST(PTRUE_ALL, PTEST_LIKE), the PTEST is redundant if the; // PTEST_LIKE instruction uses the same all active mask and the element; // size matches. If the PTEST has a condition of any then it is always; // redundant.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:334,Availability,mask,mask,334,"// For PTEST(PTRUE_ALL, WHILE), if the element size matches, the PTEST is; // redundant since WHILE performs an implicit PTEST with an all active; // mask. Must be an all active predicate of matching element size.; // For PTEST(PTRUE_ALL, PTEST_LIKE), the PTEST is redundant if the; // PTEST_LIKE instruction uses the same all active mask and the element; // size matches. If the PTEST has a condition of any then it is always; // redundant.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:431,Availability,redundant,redundant,431,"// For PTEST(PTRUE_ALL, WHILE), if the element size matches, the PTEST is; // redundant since WHILE performs an implicit PTEST with an all active; // mask. Must be an all active predicate of matching element size.; // For PTEST(PTRUE_ALL, PTEST_LIKE), the PTEST is redundant if the; // PTEST_LIKE instruction uses the same all active mask and the element; // size matches. If the PTEST has a condition of any then it is always; // redundant.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:100,Performance,perform,performs,100,"// For PTEST(PTRUE_ALL, WHILE), if the element size matches, the PTEST is; // redundant since WHILE performs an implicit PTEST with an all active; // mask. Must be an all active predicate of matching element size.; // For PTEST(PTRUE_ALL, PTEST_LIKE), the PTEST is redundant if the; // PTEST_LIKE instruction uses the same all active mask and the element; // size matches. If the PTEST has a condition of any then it is always; // redundant.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:78,Safety,redund,redundant,78,"// For PTEST(PTRUE_ALL, WHILE), if the element size matches, the PTEST is; // redundant since WHILE performs an implicit PTEST with an all active; // mask. Must be an all active predicate of matching element size.; // For PTEST(PTRUE_ALL, PTEST_LIKE), the PTEST is redundant if the; // PTEST_LIKE instruction uses the same all active mask and the element; // size matches. If the PTEST has a condition of any then it is always; // redundant.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:265,Safety,redund,redundant,265,"// For PTEST(PTRUE_ALL, WHILE), if the element size matches, the PTEST is; // redundant since WHILE performs an implicit PTEST with an all active; // mask. Must be an all active predicate of matching element size.; // For PTEST(PTRUE_ALL, PTEST_LIKE), the PTEST is redundant if the; // PTEST_LIKE instruction uses the same all active mask and the element; // size matches. If the PTEST has a condition of any then it is always; // redundant.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:431,Safety,redund,redundant,431,"// For PTEST(PTRUE_ALL, WHILE), if the element size matches, the PTEST is; // redundant since WHILE performs an implicit PTEST with an all active; // mask. Must be an all active predicate of matching element size.; // For PTEST(PTRUE_ALL, PTEST_LIKE), the PTEST is redundant if the; // PTEST_LIKE instruction uses the same all active mask and the element; // size matches. If the PTEST has a condition of any then it is always; // redundant.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:17,Usability,simpl,simply,17,// Fallthough to simply remove the PTEST.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:31,Availability,redundant,redundant,31,"// For PTEST(PG, PG), PTEST is redundant when PG is the result of an; // instruction that sets the flags as PTEST would. This is only valid when; // the condition is any.; // Fallthough to simply remove the PTEST.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:31,Safety,redund,redundant,31,"// For PTEST(PG, PG), PTEST is redundant when PG is the result of an; // instruction that sets the flags as PTEST would. This is only valid when; // the condition is any.; // Fallthough to simply remove the PTEST.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:189,Usability,simpl,simply,189,"// For PTEST(PG, PG), PTEST is redundant when PG is the result of an; // instruction that sets the flags as PTEST would. This is only valid when; // the condition is any.; // Fallthough to simply remove the PTEST.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:52,Availability,redundant,redundant,52,"// For PTEST(PG, PTEST_LIKE(PG, ...)), the PTEST is redundant since the; // flags are set based on the same mask 'PG', but PTEST_LIKE must operate; // on 8-bit predicates like the PTEST. Otherwise, for instructions like; // compare that also support 16/32/64-bit predicates, the implicit PTEST; // performed by the compare could consider fewer lanes for these element; // sizes.; //; // For example, consider; //; // ptrue p0.b ; P0=1111-1111-1111-1111; // index z0.s, #0, #1 ; Z0=<0,1,2,3>; // index z1.s, #1, #1 ; Z1=<1,2,3,4>; // cmphi p1.s, p0/z, z1.s, z0.s ; P1=0001-0001-0001-0001; // ; ^ last active; // ptest p0, p1.b ; P1=0001-0001-0001-0001; // ; ^ last active; //; // where the compare generates a canonical all active 32-bit predicate; // (equivalent to 'ptrue p1.s, all'). The implicit PTEST sets the last; // active flag, whereas the PTEST instruction with the same mask doesn't.; // For PTEST_ANY this doesn't apply as the flags in this case would be; // identical regardless of element size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:108,Availability,mask,mask,108,"// For PTEST(PG, PTEST_LIKE(PG, ...)), the PTEST is redundant since the; // flags are set based on the same mask 'PG', but PTEST_LIKE must operate; // on 8-bit predicates like the PTEST. Otherwise, for instructions like; // compare that also support 16/32/64-bit predicates, the implicit PTEST; // performed by the compare could consider fewer lanes for these element; // sizes.; //; // For example, consider; //; // ptrue p0.b ; P0=1111-1111-1111-1111; // index z0.s, #0, #1 ; Z0=<0,1,2,3>; // index z1.s, #1, #1 ; Z1=<1,2,3,4>; // cmphi p1.s, p0/z, z1.s, z0.s ; P1=0001-0001-0001-0001; // ; ^ last active; // ptest p0, p1.b ; P1=0001-0001-0001-0001; // ; ^ last active; //; // where the compare generates a canonical all active 32-bit predicate; // (equivalent to 'ptrue p1.s, all'). The implicit PTEST sets the last; // active flag, whereas the PTEST instruction with the same mask doesn't.; // For PTEST_ANY this doesn't apply as the flags in this case would be; // identical regardless of element size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:880,Availability,mask,mask,880,"// For PTEST(PG, PTEST_LIKE(PG, ...)), the PTEST is redundant since the; // flags are set based on the same mask 'PG', but PTEST_LIKE must operate; // on 8-bit predicates like the PTEST. Otherwise, for instructions like; // compare that also support 16/32/64-bit predicates, the implicit PTEST; // performed by the compare could consider fewer lanes for these element; // sizes.; //; // For example, consider; //; // ptrue p0.b ; P0=1111-1111-1111-1111; // index z0.s, #0, #1 ; Z0=<0,1,2,3>; // index z1.s, #1, #1 ; Z1=<1,2,3,4>; // cmphi p1.s, p0/z, z1.s, z0.s ; P1=0001-0001-0001-0001; // ; ^ last active; // ptest p0, p1.b ; P1=0001-0001-0001-0001; // ; ^ last active; //; // where the compare generates a canonical all active 32-bit predicate; // (equivalent to 'ptrue p1.s, all'). The implicit PTEST sets the last; // active flag, whereas the PTEST instruction with the same mask doesn't.; // For PTEST_ANY this doesn't apply as the flags in this case would be; // identical regardless of element size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:298,Performance,perform,performed,298,"// For PTEST(PG, PTEST_LIKE(PG, ...)), the PTEST is redundant since the; // flags are set based on the same mask 'PG', but PTEST_LIKE must operate; // on 8-bit predicates like the PTEST. Otherwise, for instructions like; // compare that also support 16/32/64-bit predicates, the implicit PTEST; // performed by the compare could consider fewer lanes for these element; // sizes.; //; // For example, consider; //; // ptrue p0.b ; P0=1111-1111-1111-1111; // index z0.s, #0, #1 ; Z0=<0,1,2,3>; // index z1.s, #1, #1 ; Z1=<1,2,3,4>; // cmphi p1.s, p0/z, z1.s, z0.s ; P1=0001-0001-0001-0001; // ; ^ last active; // ptest p0, p1.b ; P1=0001-0001-0001-0001; // ; ^ last active; //; // where the compare generates a canonical all active 32-bit predicate; // (equivalent to 'ptrue p1.s, all'). The implicit PTEST sets the last; // active flag, whereas the PTEST instruction with the same mask doesn't.; // For PTEST_ANY this doesn't apply as the flags in this case would be; // identical regardless of element size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:52,Safety,redund,redundant,52,"// For PTEST(PG, PTEST_LIKE(PG, ...)), the PTEST is redundant since the; // flags are set based on the same mask 'PG', but PTEST_LIKE must operate; // on 8-bit predicates like the PTEST. Otherwise, for instructions like; // compare that also support 16/32/64-bit predicates, the implicit PTEST; // performed by the compare could consider fewer lanes for these element; // sizes.; //; // For example, consider; //; // ptrue p0.b ; P0=1111-1111-1111-1111; // index z0.s, #0, #1 ; Z0=<0,1,2,3>; // index z1.s, #1, #1 ; Z1=<1,2,3,4>; // cmphi p1.s, p0/z, z1.s, z0.s ; P1=0001-0001-0001-0001; // ; ^ last active; // ptest p0, p1.b ; P1=0001-0001-0001-0001; // ; ^ last active; //; // where the compare generates a canonical all active 32-bit predicate; // (equivalent to 'ptrue p1.s, all'). The implicit PTEST sets the last; // active flag, whereas the PTEST instruction with the same mask doesn't.; // For PTEST_ANY this doesn't apply as the flags in this case would be; // identical regardless of element size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:17,Usability,simpl,simply,17,// Fallthough to simply remove the PTEST.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:105,Availability,redundant,redundant,105,"// If OP in PTEST(PG, OP(PG, ...)) has a flag-setting variant change the; // opcode so the PTEST becomes redundant.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:105,Safety,redund,redundant,105,"// If OP in PTEST(PG, OP(PG, ...)) has a flag-setting variant change the; // opcode so the PTEST becomes redundant.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:23,Availability,mask,mask,23,// Check to see if our mask is the same. If not the resulting flag bits; // may be different and we can't remove the ptest.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:36,Availability,mask,mask,36,"// BRKN uses an all active implicit mask to set flags unlike the other; // flag-setting instructions.; // PTEST(PTRUE_B(31), BRKN(PG, A, B)) -> BRKNS(PG, A, B).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:95,Deployability,update,update,95,"// If another instruction between Pred and PTest accesses flags, don't remove; // the ptest or update the earlier instruction to modify them.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:49,Security,access,accesses,49,"// If another instruction between Pred and PTest accesses flags, don't remove; // the ptest or update the earlier instruction to modify them.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:35,Safety,safe,safe,35,"// If we pass all the checks, it's safe to remove the PTEST and use the flags; // as they are prior to PTEST. Sometimes this requires the tested PTEST; // operand to be replaced with an equivalent instruction that also sets the; // flags.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:138,Testability,test,tested,138,"// If we pass all the checks, it's safe to remove the PTEST and use the flags; // as they are prior to PTEST. Sometimes this requires the tested PTEST; // operand to be replaced with an equivalent instruction that also sets the; // flags.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:11,Performance,optimiz,optimize,11,/// Try to optimize a compare instruction. A compare instruction is an; /// instruction which produces AArch64::NZCV. It can be truly compare; /// instruction; /// when there are no uses of its destination register.; ///; /// The following steps are tried in order:; /// 1. Convert CmpInstr into an unconditional version.; /// 2. Remove CmpInstr if above there is an instruction producing a needed; /// condition code or an instruction which can be converted into such an; /// instruction.; /// Only comparison with zero is supported.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:37,Availability,alive,alive,37,/// Check if AArch64::NZCV should be alive in successors of MBB.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:155,Performance,optimiz,optimize,155,/// Find a condition code used by the instruction.; /// Returns AArch64CC::Invalid if either the instruction does not use condition; /// codes or we don't optimize CmpInstr in the presence of such instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:5,Usability,clear,clear,5,// Z clear,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:5,Usability,clear,clear,5,// Z clear and C set,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:14,Usability,clear,clear,14,// Z set or C clear,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:5,Usability,clear,clear,5,// C clear,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:5,Usability,clear,clear,5,// N clear,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:5,Usability,clear,clear,5,// V clear,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:5,Usability,clear,clear,5,"// Z clear, N and V the same",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:97,Availability,alive,alive,97,/// \returns Conditions flags used after \p CmpInstr in its MachineBB if NZCV; /// flags are not alive in successors of the same \p CmpInstr and \p MI parent.; /// \returns std::nullopt otherwise.; ///; /// Collect instructions using that flags in \p CCUseInstrs if provided.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:240,Availability,alive,alive,240,"/// Check if CmpInstr can be substituted by MI.; ///; /// CmpInstr can be substituted:; /// - CmpInstr is either 'ADDS %vreg, 0' or 'SUBS %vreg, 0'; /// - and, MI and CmpInstr are from the same MachineBB; /// - and, condition flags are not alive in successors of the CmpInstr parent; /// - and, if MI opcode is the S form there must be no defs of flags between; /// MI and CmpInstr; /// or if MI opcode is not the S form there must be neither defs of flags; /// nor uses of flags between MI and CmpInstr.; /// - and, if C/V flags are not used after CmpInstr; /// or if N flag is used but MI produces poison value if signed overflow; /// occurs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:13,Testability,assert,assertion,13,// NOTE this assertion guarantees that MI.getOpcode() is add or subtraction; // that may or may not set flags.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:270,Integrability,wrap,wrap,270,"// CmpInstr is either 'ADDS %vreg, 0' or 'SUBS %vreg, 0', and MI is either; // '%vreg = add ...' or '%vreg = sub ...'.; // Condition flag V is used to indicate signed overflow.; // 1) MI and CmpInstr set N and V to the same value.; // 2) If MI is add/sub with no-signed-wrap, it produces a poison value when; // signed overflow occurs, so CmpInstr could still be simplified away.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:363,Usability,simpl,simplified,363,"// CmpInstr is either 'ADDS %vreg, 0' or 'SUBS %vreg, 0', and MI is either; // '%vreg = add ...' or '%vreg = sub ...'.; // Condition flag V is used to indicate signed overflow.; // 1) MI and CmpInstr set N and V to the same value.; // 2) If MI is add/sub with no-signed-wrap, it produces a poison value when; // signed overflow occurs, so CmpInstr could still be simplified away.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:3,Deployability,Update,Update,3,// Update the instruction to set NZCV.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:35,Usability,simpl,simply,35,// Return true if this instruction simply sets its single destination register; // to zero. This is equivalent to a register rename of the zero-register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:35,Usability,simpl,simply,35,// Return true if this instruction simply renames a general register without; // modifying bits.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:35,Usability,simpl,simply,35,// Return true if this instruction simply renames a general register without; // modifying bits.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:53,Performance,load,load,53,/// Check all MachineMemOperands for a hint that the load/store is strided.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:98,Performance,load,load,98,"// Is this a candidate for ld/st merging or pairing? For example, we don't; // touch volatiles or load/stores that have a hint to avoid pair formation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:130,Safety,avoid,avoid,130,"// Is this a candidate for ld/st merging or pairing? For example, we don't; // touch volatiles or load/stores that have a hint to avoid pair formation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:25,Performance,load,load,25,"// If this is a volatile load/store, don't mess with it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:17,Performance,load,load,17,// Check if this load/store has a hint to avoid pair formation.; // MachineMemOperands hints are set by the AArch64StorePairSuppress pass.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:42,Safety,avoid,avoid,42,// Check if this load/store has a hint to avoid pair formation.; // MachineMemOperands hints are set by the AArch64StorePairSuppress pass.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:21,Performance,load,load,21,// On some CPUs quad load/store pairs are slower than two single load/stores.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:65,Performance,load,load,65,// On some CPUs quad load/store pairs are slower than two single load/stores.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:37,Performance,load,loaded,37,// Check the fold operand is not the loaded/stored value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:71,Usability,ux,uxtw,71,"// mov Wa, Wm; // ldr Xd, [Xn, Xa, lsl #N]; // ->; // ldr Xd, [Xn, Wm, uxtw #N]; // Zero-extension looks like an ORRWrs followed by a SUBREG_TO_REG.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:28,Modifiability,extend,extend,28,// Can fold only sign-/zero-extend of a word.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:114,Performance,perform,performing,114,"// Given an opcode for an instruction with a [Reg, #Imm] addressing mode,; // return the opcode of an instruction performing the same operation, but using; // the [Reg, Reg] addressing mode.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:114,Performance,perform,performing,114,"// Given an opcode for an instruction with a [Reg, #Imm] addressing mode, return; // the opcode of an instruction performing the same operation, but using the; // [Reg, #Imm] addressing mode with scaled offset.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:114,Performance,perform,performing,114,"// Given an opcode for an instruction with a [Reg, #Imm] addressing mode, return; // the opcode of an instruction performing the same operation, but using the; // [Reg, #Imm] addressing mode with unscaled offset.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:201,Modifiability,extend,extend,201,"// Given the opcode of a memory load/store instruction, return the opcode of an; // instruction performing the same operation, but using; // the [Reg, Reg, {s,u}xtw #N] addressing mode with sign-/zero-extend of the; // offset register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:32,Performance,load,load,32,"// Given the opcode of a memory load/store instruction, return the opcode of an; // instruction performing the same operation, but using; // the [Reg, Reg, {s,u}xtw #N] addressing mode with sign-/zero-extend of the; // offset register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:96,Performance,perform,performing,96,"// Given the opcode of a memory load/store instruction, return the opcode of an; // instruction performing the same operation, but using; // the [Reg, Reg, {s,u}xtw #N] addressing mode with sign-/zero-extend of the; // offset register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:15,Performance,load,loads,15,// Handle only loads/stores with base register followed by immediate offset.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:31,Performance,load,load,31,// Scaling factor for unscaled load or store.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:97,Performance,load,load,97,"// Convert the byte-offset used by unscaled into an ""element"" offset used; // by the scaled pair load/store instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:3,Security,Access,Accesses,3,// Accesses through fixed stack object frame indices may access a different; // fixed stack slot. Check that the object offsets + offsets match.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:57,Security,access,access,57,// Accesses through fixed stack object frame indices may access a different; // fixed stack slot. Check that the object offsets + offsets match.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:4,Safety,Detect,Detect,4,/// Detect opportunities for ldp/stp formation.; ///; /// Only called for LdSt for which getMemOperandWithOffset returns true.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:28,Performance,load,load,28,"// Can't merge volatiles or load/stores that have a hint to avoid pair; // formation, for example.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:60,Safety,avoid,avoid,60,"// Can't merge volatiles or load/stores that have a hint to avoid pair; // formation, for example.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:102,Availability,mask,mask,102,"// We really want the positive remainder mod 32 here, that happens to be; // easily obtainable with a mask.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:101,Availability,mask,mask,101,// Copy a predicate-as-counter register by ORRing with itself as if it; // were a regular predicate (mask) register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:16,Performance,scalab,scalable,16,"// The smallest scalable element supported by scaled SVE addressing; // modes are predicates, which are 2 scalable bytes in size. So the scalable; // byte offset must always be a multiple of 2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:106,Performance,scalab,scalable,106,"// The smallest scalable element supported by scaled SVE addressing; // modes are predicates, which are 2 scalable bytes in size. So the scalable; // byte offset must always be a multiple of 2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:137,Performance,scalab,scalable,137,"// The smallest scalable element supported by scaled SVE addressing; // modes are predicates, which are 2 scalable bytes in size. So the scalable; // byte offset must always be a multiple of 2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:141,Performance,scalab,scalable,141,/// Returns the offset in parts to which this frame offset can be; /// decomposed for the purpose of describing a frame offset.; /// For non-scalable offsets this is simply its byte size.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:166,Usability,simpl,simply,166,/// Returns the offset in parts to which this frame offset can be; /// decomposed for the purpose of describing a frame offset.; /// For non-scalable offsets this is simply its byte size.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:16,Performance,scalab,scalable,16,"// The smallest scalable element supported by scaled SVE addressing; // modes are predicates, which are 2 scalable bytes in size. So the scalable; // byte offset must always be a multiple of 2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:106,Performance,scalab,scalable,106,"// The smallest scalable element supported by scaled SVE addressing; // modes are predicates, which are 2 scalable bytes in size. So the scalable; // byte offset must always be a multiple of 2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:137,Performance,scalab,scalable,137,"// The smallest scalable element supported by scaled SVE addressing; // modes are predicates, which are 2 scalable bytes in size. So the scalable; // byte offset must always be a multiple of 2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:3,Integrability,Wrap,Wrap,3,// Wrap this into DW_CFA_def_cfa.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:7,Performance,scalab,scalable,7,// Non-scalable offsets can use DW_CFA_offset directly.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:3,Integrability,Wrap,Wrap,3,// Wrap this into DW_CFA_expression,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:35,Performance,scalab,scalable,35,"// `Offset` can be in bytes or in ""scalable bytes"".",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:435,Modifiability,extend,extending,435,"// FIXME: If the offset won't fit in 24-bits, compute the offset into a; // scratch register. If DestReg is a virtual register, use it as the; // scratch register; otherwise, create a new virtual register (to be; // replaced by the scavenger at the end of PEI). That case can be optimized; // slightly if DestReg is SP which is always 16-byte aligned, so the scratch; // register can be loaded with offset%8 and the add/sub can use an extending; // instruction with LSL#3.; // Currently the function handles any offsets but generates a poor sequence; // of code.; // assert(Offset < (1 << 24) && ""unimplemented reg plus immediate"");",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:279,Performance,optimiz,optimized,279,"// FIXME: If the offset won't fit in 24-bits, compute the offset into a; // scratch register. If DestReg is a virtual register, use it as the; // scratch register; otherwise, create a new virtual register (to be; // replaced by the scavenger at the end of PEI). That case can be optimized; // slightly if DestReg is SP which is always 16-byte aligned, so the scratch; // register can be loaded with offset%8 and the add/sub can use an extending; // instruction with LSL#3.; // Currently the function handles any offsets but generates a poor sequence; // of code.; // assert(Offset < (1 << 24) && ""unimplemented reg plus immediate"");",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:387,Performance,load,loaded,387,"// FIXME: If the offset won't fit in 24-bits, compute the offset into a; // scratch register. If DestReg is a virtual register, use it as the; // scratch register; otherwise, create a new virtual register (to be; // replaced by the scavenger at the end of PEI). That case can be optimized; // slightly if DestReg is SP which is always 16-byte aligned, so the scratch; // register can be loaded with offset%8 and the add/sub can use an extending; // instruction with LSL#3.; // Currently the function handles any offsets but generates a poor sequence; // of code.; // assert(Offset < (1 << 24) && ""unimplemented reg plus immediate"");",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:567,Testability,assert,assert,567,"// FIXME: If the offset won't fit in 24-bits, compute the offset into a; // scratch register. If DestReg is a virtual register, use it as the; // scratch register; otherwise, create a new virtual register (to be; // replaced by the scavenger at the end of PEI). That case can be optimized; // slightly if DestReg is SP which is always 16-byte aligned, so the scratch; // register can be loaded with offset%8 and the add/sub can use an extending; // instruction with LSL#3.; // Currently the function handles any offsets but generates a poor sequence; // of code.; // assert(Offset < (1 << 24) && ""unimplemented reg plus immediate"");",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:257,Energy Efficiency,allocate,allocate,257,"// If a function is marked as arm_locally_streaming, then the runtime value of; // vscale in the prologue/epilogue is different the runtime value of vscale; // in the function's body. To avoid having to consider multiple vscales,; // we can use `addsvl` to allocate any scalable stack-slots, which under; // most circumstances will be only locals, not callee-save slots.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:270,Performance,scalab,scalable,270,"// If a function is marked as arm_locally_streaming, then the runtime value of; // vscale in the prologue/epilogue is different the runtime value of vscale; // in the function's body. To avoid having to consider multiple vscales,; // we can use `addsvl` to allocate any scalable stack-slots, which under; // most circumstances will be only locals, not callee-save slots.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:187,Safety,avoid,avoid,187,"// If a function is marked as arm_locally_streaming, then the runtime value of; // vscale in the prologue/epilogue is different the runtime value of vscale; // in the function's body. To avoid having to consider multiple vscales,; // we can use `addsvl` to allocate any scalable stack-slots, which under; // most circumstances will be only locals, not callee-save slots.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:18,Performance,scalab,scalable,18,"// First emit non-scalable frame offsets, or a simple 'mov'.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:47,Usability,simpl,simple,47,"// First emit non-scalable frame offsets, or a simple 'mov'.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:208,Safety,safe,safely,208,"// Handle the case where a copy is being spilled or filled but the source; // and destination register class don't match. For example:; //; // %0 = COPY %xzr; GPR64common:%0; //; // In this case we can still safely fold away the COPY and generate the; // following spill code:; //; // STRXui %xzr, %stack.0; //; // This also eliminates spilled cross register class COPYs (e.g. between x and; // d regs) of the same size. For example:; //; // %0 = COPY %1; GPR64:%0, FPR64:%1; //; // will be filled as; //; // LDRDui %0, fi<#0>; //; // instead of; //; // LDRXui %Temp, fi<#0>; // %0 = FMOV %Temp; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:122,Performance,load,load,122,"// Handle cases like filling use of:; //; // %0:sub_32<def,read-undef> = COPY %1; GPR64:%0, GPR32:%1; //; // where we can load the full virtual reg source stack slot, into the subreg; // destination, in this case producing:; //; // LDRWui %0:sub_32<def,read-undef>, %stack.0; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:45,Modifiability,rewrite,rewrite,45,"// If the offset doesn't match the scale, we rewrite the instruction to; // use the unscaled instruction instead. Likewise, if we have a negative; // offset and there is an unscaled op to use.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:123,Integrability,contract,contract,123,"// We can fuse FADD/FSUB with FMUL, if fusion is either allowed globally by; // the target options or if FADD/FSUB has the contract fast-math flag.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:15,Integrability,rout,routine,15,//; // Utility routine that checks if \param MO is defined by an; // \param CombineOpc instruction in the basic block \param MBB,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:130,Testability,log,logic,130,"// TODO: There are many more machine instruction opcodes to match:; // 1. Other data types (integer, vectors); // 2. Other math / logic operations (xor, or); // 3. Other forms of the same operation (intrinsics and other variants)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:49,Performance,throughput,throughput,49,/// Return true when a code sequence can improve throughput. It; /// should be called only for instructions in loops.; /// \param Pattern - combiner pattern,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:511,Energy Efficiency,power,power,511,"/// Replace csincr-branch sequence by simple conditional branch; ///; /// Examples:; /// 1. \code; /// csinc w9, wzr, wzr, <condition code>; /// tbnz w9, #0, 0x44; /// \endcode; /// to; /// \code; /// b.<inverted condition code>; /// \endcode; ///; /// 2. \code; /// csinc w9, wzr, wzr, <condition code>; /// tbz w9, #0, 0x44; /// \endcode; /// to; /// \code; /// b.<condition code>; /// \endcode; ///; /// Replace compare and branch sequence by TBZ/TBNZ instruction when the; /// compare's constant operand is power of 2.; ///; /// Examples:; /// \code; /// and w8, w8, #0x400; /// cbnz w8, L1; /// \endcode; /// to; /// \code; /// tbnz w8, #10, L1; /// \endcode; ///; /// \param MI Conditional Branch; /// \return True when the simple conditional branch is generated; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:38,Usability,simpl,simple,38,"/// Replace csincr-branch sequence by simple conditional branch; ///; /// Examples:; /// 1. \code; /// csinc w9, wzr, wzr, <condition code>; /// tbnz w9, #0, 0x44; /// \endcode; /// to; /// \code; /// b.<inverted condition code>; /// \endcode; ///; /// 2. \code; /// csinc w9, wzr, wzr, <condition code>; /// tbz w9, #0, 0x44; /// \endcode; /// to; /// \code; /// b.<condition code>; /// \endcode; ///; /// Replace compare and branch sequence by TBZ/TBNZ instruction when the; /// compare's constant operand is power of 2.; ///; /// Examples:; /// \code; /// and w8, w8, #0x400; /// cbnz w8, L1; /// \endcode; /// to; /// \code; /// tbnz w8, #10, L1; /// \endcode; ///; /// \param MI Conditional Branch; /// \return True when the simple conditional branch is generated; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:730,Usability,simpl,simple,730,"/// Replace csincr-branch sequence by simple conditional branch; ///; /// Examples:; /// 1. \code; /// csinc w9, wzr, wzr, <condition code>; /// tbnz w9, #0, 0x44; /// \endcode; /// to; /// \code; /// b.<inverted condition code>; /// \endcode; ///; /// 2. \code; /// csinc w9, wzr, wzr, <condition code>; /// tbz w9, #0, 0x44; /// \endcode; /// to; /// \code; /// b.<condition code>; /// \endcode; ///; /// Replace compare and branch sequence by TBZ/TBNZ instruction when the; /// compare's constant operand is power of 2.; ///; /// Examples:; /// \code; /// and w8, w8, #0x400; /// cbnz w8, L1; /// \endcode; /// to; /// \code; /// tbnz w8, #10, L1; /// \endcode; ///; /// \param MI Conditional Branch; /// \return True when the simple conditional branch is generated; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:39,Testability,test,test,39,// So we increment a zero register and test for bits other; // than bit 0? Conservatively bail out in case the verifier; // missed this case.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:51,Energy Efficiency,power,power,51,// Fold AND into a TBZ/TBNZ if constant operand is power of 2.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:2050,Availability,avail,available,2050,"ET; ///; /// * Call construction overhead: 3 (save + BL + restore); /// * Frame construction overhead: 1 (ret); /// * Requires stack fixups? Yes; ///; /// \p MachineOutlinerTailCall implies that the function is being created from; /// a sequence of instructions ending in a return.; ///; /// That is,; ///; /// I1 OUTLINED_FUNCTION:; /// I2 --> B OUTLINED_FUNCTION I1; /// RET I2; /// RET; ///; /// * Call construction overhead: 1 (B); /// * Frame construction overhead: 0 (Return included in sequence); /// * Requires stack fixups? No; ///; /// \p MachineOutlinerNoLRSave implies that the function should be called using; /// a BL instruction, but doesn't require LR to be saved and restored. This; /// happens when LR is known to be dead.; ///; /// That is,; ///; /// I1 OUTLINED_FUNCTION:; /// I2 --> BL OUTLINED_FUNCTION I1; /// I3 I2; /// I3; /// RET; ///; /// * Call construction overhead: 1 (BL); /// * Frame construction overhead: 1 (RET); /// * Requires stack fixups? No; ///; /// \p MachineOutlinerThunk implies that the function is being created from; /// a sequence of instructions ending in a call. The outlined function is; /// called with a BL instruction, and the outlined function tail-calls the; /// original call destination.; ///; /// That is,; ///; /// I1 OUTLINED_FUNCTION:; /// I2 --> BL OUTLINED_FUNCTION I1; /// BL f I2; /// B f; /// * Call construction overhead: 1 (BL); /// * Frame construction overhead: 0; /// * Requires stack fixups? No; ///; /// \p MachineOutlinerRegSave implies that the function should be called with a; /// save and restore of LR to an available register. This allows us to avoid; /// stack fixups. Note that this outlining variant is compatible with the; /// NoLRSave case.; ///; /// That is,; ///; /// I1 Save LR OUTLINED_FUNCTION:; /// I2 --> BL OUTLINED_FUNCTION I1; /// I3 Restore LR I2; /// I3; /// RET; ///; /// * Call construction overhead: 3 (save + BL + restore); /// * Frame construction overhead: 1 (ret); /// * Requires stack fixups? No",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:2088,Safety,avoid,avoid,2088,"ET; ///; /// * Call construction overhead: 3 (save + BL + restore); /// * Frame construction overhead: 1 (ret); /// * Requires stack fixups? Yes; ///; /// \p MachineOutlinerTailCall implies that the function is being created from; /// a sequence of instructions ending in a return.; ///; /// That is,; ///; /// I1 OUTLINED_FUNCTION:; /// I2 --> B OUTLINED_FUNCTION I1; /// RET I2; /// RET; ///; /// * Call construction overhead: 1 (B); /// * Frame construction overhead: 0 (Return included in sequence); /// * Requires stack fixups? No; ///; /// \p MachineOutlinerNoLRSave implies that the function should be called using; /// a BL instruction, but doesn't require LR to be saved and restored. This; /// happens when LR is known to be dead.; ///; /// That is,; ///; /// I1 OUTLINED_FUNCTION:; /// I2 --> BL OUTLINED_FUNCTION I1; /// I3 I2; /// I3; /// RET; ///; /// * Call construction overhead: 1 (BL); /// * Frame construction overhead: 1 (RET); /// * Requires stack fixups? No; ///; /// \p MachineOutlinerThunk implies that the function is being created from; /// a sequence of instructions ending in a call. The outlined function is; /// called with a BL instruction, and the outlined function tail-calls the; /// original call destination.; ///; /// That is,; ///; /// I1 OUTLINED_FUNCTION:; /// I2 --> BL OUTLINED_FUNCTION I1; /// BL f I2; /// B f; /// * Call construction overhead: 1 (BL); /// * Frame construction overhead: 0; /// * Requires stack fixups? No; ///; /// \p MachineOutlinerRegSave implies that the function should be called with a; /// save and restore of LR to an available register. This allows us to avoid; /// stack fixups. Note that this outlining variant is compatible with the; /// NoLRSave case.; ///; /// That is,; ///; /// I1 Save LR OUTLINED_FUNCTION:; /// I2 --> BL OUTLINED_FUNCTION I1; /// I3 Restore LR I2; /// I3; /// RET; ///; /// * Call construction overhead: 3 (save + BL + restore); /// * Frame construction overhead: 1 (ret); /// * Requires stack fixups? No",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:24,Availability,avail,available,24,// Check if there is an available register across the sequence that we can; // use.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:3,Performance,Perform,Performing,3,"// Performing a tail call may require extra checks when PAuth is enabled.; // If PAuth is disabled, set it to zero for uniformity.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:16,Security,authenticat,authenticated,16,"// Checking the authenticated LR value may significantly impact; // SequenceSize, so account for it for more precise results.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:38,Safety,safe,safe,38,"// Returns true if an instructions is safe to fix up, false otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:106,Performance,load,load,106,"// At this point, we have a stack instruction that we might need to; // fix up. We'll handle it if it's a load or store.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:3,Deployability,Update,Update,3,// Update the offset to what it would be if we outlined.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:9,Availability,avail,available,9,"// Is LR available? If so, we don't need a save.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:25,Availability,avail,available,25,"// Is an unused register available? If so, we won't modify the stack, so; // we can outline with the same frame type as those that don't save LR.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:88,Deployability,update,update,88,"// If there are no places where we have to save LR, then note that we; // don't have to update the stack. Otherwise, give every candidate the; // default call type, as long as it's safe to do so.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:181,Safety,safe,safe,181,"// If there are no places where we have to save LR, then note that we; // don't have to update the stack. Otherwise, give every candidate the; // default call type, as long as it's safe to do so.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:278,Availability,avail,available,278,"// Bugzilla ID: 46767; // TODO: Check if fixing up the stack more than once is safe so we can; // outline these.; //; // An outline resulting in a caller that requires stack fixups at the; // callsite to a callee that also requires stack fixups can happen when; // there are no available registers at the candidate callsite for a; // candidate that itself also has calls.; //; // In other words if function_containing_sequence in the following pseudo; // assembly requires that we save LR at the point of the call, but there; // are no available registers: in this case we save using SP and as a; // result the SP offsets requires stack fixups by multiples of 16.; //; // function_containing_sequence:; // ...; // save LR to SP <- Requires stack instr fixups in OUTLINED_FUNCTION_N; // call OUTLINED_FUNCTION_N; // restore LR from SP; // ...; //; // OUTLINED_FUNCTION_N:; // save LR to SP <- Requires stack instr fixups in OUTLINED_FUNCTION_N; // ...; // bl foo; // restore LR from SP; // ret; //; // Because the code to handle more than one stack fixup does not; // currently have the proper checks for legality, these cases will assert; // in the AArch64 MachineOutliner. This is because the code to do this; // needs more hardening, testing, better checks that generated code is; // legal, etc and because it is only verified to handle a single pass of; // stack fixup.; //; // The assert happens in AArch64InstrInfo::buildOutlinedFrame to catch; // these cases until they are known to be handled. Bugzilla 46767 is; // referenced in comments at the assert site.; //; // To avoid asserting (or generating non-legal code on noassert builds); // we remove all candidates which would need more than one stack fixup by; // pruning the cases where the candidate has calls while also having no; // available LR and having no available general purpose registers to copy; // LR to (ie one extra stack save/restore).; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:536,Availability,avail,available,536,"// Bugzilla ID: 46767; // TODO: Check if fixing up the stack more than once is safe so we can; // outline these.; //; // An outline resulting in a caller that requires stack fixups at the; // callsite to a callee that also requires stack fixups can happen when; // there are no available registers at the candidate callsite for a; // candidate that itself also has calls.; //; // In other words if function_containing_sequence in the following pseudo; // assembly requires that we save LR at the point of the call, but there; // are no available registers: in this case we save using SP and as a; // result the SP offsets requires stack fixups by multiples of 16.; //; // function_containing_sequence:; // ...; // save LR to SP <- Requires stack instr fixups in OUTLINED_FUNCTION_N; // call OUTLINED_FUNCTION_N; // restore LR from SP; // ...; //; // OUTLINED_FUNCTION_N:; // save LR to SP <- Requires stack instr fixups in OUTLINED_FUNCTION_N; // ...; // bl foo; // restore LR from SP; // ret; //; // Because the code to handle more than one stack fixup does not; // currently have the proper checks for legality, these cases will assert; // in the AArch64 MachineOutliner. This is because the code to do this; // needs more hardening, testing, better checks that generated code is; // legal, etc and because it is only verified to handle a single pass of; // stack fixup.; //; // The assert happens in AArch64InstrInfo::buildOutlinedFrame to catch; // these cases until they are known to be handled. Bugzilla 46767 is; // referenced in comments at the assert site.; //; // To avoid asserting (or generating non-legal code on noassert builds); // we remove all candidates which would need more than one stack fixup by; // pruning the cases where the candidate has calls while also having no; // available LR and having no available general purpose registers to copy; // LR to (ie one extra stack save/restore).; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:1795,Availability,avail,available,1795,"// Bugzilla ID: 46767; // TODO: Check if fixing up the stack more than once is safe so we can; // outline these.; //; // An outline resulting in a caller that requires stack fixups at the; // callsite to a callee that also requires stack fixups can happen when; // there are no available registers at the candidate callsite for a; // candidate that itself also has calls.; //; // In other words if function_containing_sequence in the following pseudo; // assembly requires that we save LR at the point of the call, but there; // are no available registers: in this case we save using SP and as a; // result the SP offsets requires stack fixups by multiples of 16.; //; // function_containing_sequence:; // ...; // save LR to SP <- Requires stack instr fixups in OUTLINED_FUNCTION_N; // call OUTLINED_FUNCTION_N; // restore LR from SP; // ...; //; // OUTLINED_FUNCTION_N:; // save LR to SP <- Requires stack instr fixups in OUTLINED_FUNCTION_N; // ...; // bl foo; // restore LR from SP; // ret; //; // Because the code to handle more than one stack fixup does not; // currently have the proper checks for legality, these cases will assert; // in the AArch64 MachineOutliner. This is because the code to do this; // needs more hardening, testing, better checks that generated code is; // legal, etc and because it is only verified to handle a single pass of; // stack fixup.; //; // The assert happens in AArch64InstrInfo::buildOutlinedFrame to catch; // these cases until they are known to be handled. Bugzilla 46767 is; // referenced in comments at the assert site.; //; // To avoid asserting (or generating non-legal code on noassert builds); // we remove all candidates which would need more than one stack fixup by; // pruning the cases where the candidate has calls while also having no; // available LR and having no available general purpose registers to copy; // LR to (ie one extra stack save/restore).; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:1822,Availability,avail,available,1822,"// Bugzilla ID: 46767; // TODO: Check if fixing up the stack more than once is safe so we can; // outline these.; //; // An outline resulting in a caller that requires stack fixups at the; // callsite to a callee that also requires stack fixups can happen when; // there are no available registers at the candidate callsite for a; // candidate that itself also has calls.; //; // In other words if function_containing_sequence in the following pseudo; // assembly requires that we save LR at the point of the call, but there; // are no available registers: in this case we save using SP and as a; // result the SP offsets requires stack fixups by multiples of 16.; //; // function_containing_sequence:; // ...; // save LR to SP <- Requires stack instr fixups in OUTLINED_FUNCTION_N; // call OUTLINED_FUNCTION_N; // restore LR from SP; // ...; //; // OUTLINED_FUNCTION_N:; // save LR to SP <- Requires stack instr fixups in OUTLINED_FUNCTION_N; // ...; // bl foo; // restore LR from SP; // ret; //; // Because the code to handle more than one stack fixup does not; // currently have the proper checks for legality, these cases will assert; // in the AArch64 MachineOutliner. This is because the code to do this; // needs more hardening, testing, better checks that generated code is; // legal, etc and because it is only verified to handle a single pass of; // stack fixup.; //; // The assert happens in AArch64InstrInfo::buildOutlinedFrame to catch; // these cases until they are known to be handled. Bugzilla 46767 is; // referenced in comments at the assert site.; //; // To avoid asserting (or generating non-legal code on noassert builds); // we remove all candidates which would need more than one stack fixup by; // pruning the cases where the candidate has calls while also having no; // available LR and having no available general purpose registers to copy; // LR to (ie one extra stack save/restore).; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:79,Safety,safe,safe,79,"// Bugzilla ID: 46767; // TODO: Check if fixing up the stack more than once is safe so we can; // outline these.; //; // An outline resulting in a caller that requires stack fixups at the; // callsite to a callee that also requires stack fixups can happen when; // there are no available registers at the candidate callsite for a; // candidate that itself also has calls.; //; // In other words if function_containing_sequence in the following pseudo; // assembly requires that we save LR at the point of the call, but there; // are no available registers: in this case we save using SP and as a; // result the SP offsets requires stack fixups by multiples of 16.; //; // function_containing_sequence:; // ...; // save LR to SP <- Requires stack instr fixups in OUTLINED_FUNCTION_N; // call OUTLINED_FUNCTION_N; // restore LR from SP; // ...; //; // OUTLINED_FUNCTION_N:; // save LR to SP <- Requires stack instr fixups in OUTLINED_FUNCTION_N; // ...; // bl foo; // restore LR from SP; // ret; //; // Because the code to handle more than one stack fixup does not; // currently have the proper checks for legality, these cases will assert; // in the AArch64 MachineOutliner. This is because the code to do this; // needs more hardening, testing, better checks that generated code is; // legal, etc and because it is only verified to handle a single pass of; // stack fixup.; //; // The assert happens in AArch64InstrInfo::buildOutlinedFrame to catch; // these cases until they are known to be handled. Bugzilla 46767 is; // referenced in comments at the assert site.; //; // To avoid asserting (or generating non-legal code on noassert builds); // we remove all candidates which would need more than one stack fixup by; // pruning the cases where the candidate has calls while also having no; // available LR and having no available general purpose registers to copy; // LR to (ie one extra stack save/restore).; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:1577,Safety,avoid,avoid,1577,"// Bugzilla ID: 46767; // TODO: Check if fixing up the stack more than once is safe so we can; // outline these.; //; // An outline resulting in a caller that requires stack fixups at the; // callsite to a callee that also requires stack fixups can happen when; // there are no available registers at the candidate callsite for a; // candidate that itself also has calls.; //; // In other words if function_containing_sequence in the following pseudo; // assembly requires that we save LR at the point of the call, but there; // are no available registers: in this case we save using SP and as a; // result the SP offsets requires stack fixups by multiples of 16.; //; // function_containing_sequence:; // ...; // save LR to SP <- Requires stack instr fixups in OUTLINED_FUNCTION_N; // call OUTLINED_FUNCTION_N; // restore LR from SP; // ...; //; // OUTLINED_FUNCTION_N:; // save LR to SP <- Requires stack instr fixups in OUTLINED_FUNCTION_N; // ...; // bl foo; // restore LR from SP; // ret; //; // Because the code to handle more than one stack fixup does not; // currently have the proper checks for legality, these cases will assert; // in the AArch64 MachineOutliner. This is because the code to do this; // needs more hardening, testing, better checks that generated code is; // legal, etc and because it is only verified to handle a single pass of; // stack fixup.; //; // The assert happens in AArch64InstrInfo::buildOutlinedFrame to catch; // these cases until they are known to be handled. Bugzilla 46767 is; // referenced in comments at the assert site.; //; // To avoid asserting (or generating non-legal code on noassert builds); // we remove all candidates which would need more than one stack fixup by; // pruning the cases where the candidate has calls while also having no; // available LR and having no available general purpose registers to copy; // LR to (ie one extra stack save/restore).; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:1131,Testability,assert,assert,1131,"// Bugzilla ID: 46767; // TODO: Check if fixing up the stack more than once is safe so we can; // outline these.; //; // An outline resulting in a caller that requires stack fixups at the; // callsite to a callee that also requires stack fixups can happen when; // there are no available registers at the candidate callsite for a; // candidate that itself also has calls.; //; // In other words if function_containing_sequence in the following pseudo; // assembly requires that we save LR at the point of the call, but there; // are no available registers: in this case we save using SP and as a; // result the SP offsets requires stack fixups by multiples of 16.; //; // function_containing_sequence:; // ...; // save LR to SP <- Requires stack instr fixups in OUTLINED_FUNCTION_N; // call OUTLINED_FUNCTION_N; // restore LR from SP; // ...; //; // OUTLINED_FUNCTION_N:; // save LR to SP <- Requires stack instr fixups in OUTLINED_FUNCTION_N; // ...; // bl foo; // restore LR from SP; // ret; //; // Because the code to handle more than one stack fixup does not; // currently have the proper checks for legality, these cases will assert; // in the AArch64 MachineOutliner. This is because the code to do this; // needs more hardening, testing, better checks that generated code is; // legal, etc and because it is only verified to handle a single pass of; // stack fixup.; //; // The assert happens in AArch64InstrInfo::buildOutlinedFrame to catch; // these cases until they are known to be handled. Bugzilla 46767 is; // referenced in comments at the assert site.; //; // To avoid asserting (or generating non-legal code on noassert builds); // we remove all candidates which would need more than one stack fixup by; // pruning the cases where the candidate has calls while also having no; // available LR and having no available general purpose registers to copy; // LR to (ie one extra stack save/restore).; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:1236,Testability,test,testing,1236,"// Bugzilla ID: 46767; // TODO: Check if fixing up the stack more than once is safe so we can; // outline these.; //; // An outline resulting in a caller that requires stack fixups at the; // callsite to a callee that also requires stack fixups can happen when; // there are no available registers at the candidate callsite for a; // candidate that itself also has calls.; //; // In other words if function_containing_sequence in the following pseudo; // assembly requires that we save LR at the point of the call, but there; // are no available registers: in this case we save using SP and as a; // result the SP offsets requires stack fixups by multiples of 16.; //; // function_containing_sequence:; // ...; // save LR to SP <- Requires stack instr fixups in OUTLINED_FUNCTION_N; // call OUTLINED_FUNCTION_N; // restore LR from SP; // ...; //; // OUTLINED_FUNCTION_N:; // save LR to SP <- Requires stack instr fixups in OUTLINED_FUNCTION_N; // ...; // bl foo; // restore LR from SP; // ret; //; // Because the code to handle more than one stack fixup does not; // currently have the proper checks for legality, these cases will assert; // in the AArch64 MachineOutliner. This is because the code to do this; // needs more hardening, testing, better checks that generated code is; // legal, etc and because it is only verified to handle a single pass of; // stack fixup.; //; // The assert happens in AArch64InstrInfo::buildOutlinedFrame to catch; // these cases until they are known to be handled. Bugzilla 46767 is; // referenced in comments at the assert site.; //; // To avoid asserting (or generating non-legal code on noassert builds); // we remove all candidates which would need more than one stack fixup by; // pruning the cases where the candidate has calls while also having no; // available LR and having no available general purpose registers to copy; // LR to (ie one extra stack save/restore).; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:1385,Testability,assert,assert,1385,"// Bugzilla ID: 46767; // TODO: Check if fixing up the stack more than once is safe so we can; // outline these.; //; // An outline resulting in a caller that requires stack fixups at the; // callsite to a callee that also requires stack fixups can happen when; // there are no available registers at the candidate callsite for a; // candidate that itself also has calls.; //; // In other words if function_containing_sequence in the following pseudo; // assembly requires that we save LR at the point of the call, but there; // are no available registers: in this case we save using SP and as a; // result the SP offsets requires stack fixups by multiples of 16.; //; // function_containing_sequence:; // ...; // save LR to SP <- Requires stack instr fixups in OUTLINED_FUNCTION_N; // call OUTLINED_FUNCTION_N; // restore LR from SP; // ...; //; // OUTLINED_FUNCTION_N:; // save LR to SP <- Requires stack instr fixups in OUTLINED_FUNCTION_N; // ...; // bl foo; // restore LR from SP; // ret; //; // Because the code to handle more than one stack fixup does not; // currently have the proper checks for legality, these cases will assert; // in the AArch64 MachineOutliner. This is because the code to do this; // needs more hardening, testing, better checks that generated code is; // legal, etc and because it is only verified to handle a single pass of; // stack fixup.; //; // The assert happens in AArch64InstrInfo::buildOutlinedFrame to catch; // these cases until they are known to be handled. Bugzilla 46767 is; // referenced in comments at the assert site.; //; // To avoid asserting (or generating non-legal code on noassert builds); // we remove all candidates which would need more than one stack fixup by; // pruning the cases where the candidate has calls while also having no; // available LR and having no available general purpose registers to copy; // LR to (ie one extra stack save/restore).; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:1553,Testability,assert,assert,1553,"// Bugzilla ID: 46767; // TODO: Check if fixing up the stack more than once is safe so we can; // outline these.; //; // An outline resulting in a caller that requires stack fixups at the; // callsite to a callee that also requires stack fixups can happen when; // there are no available registers at the candidate callsite for a; // candidate that itself also has calls.; //; // In other words if function_containing_sequence in the following pseudo; // assembly requires that we save LR at the point of the call, but there; // are no available registers: in this case we save using SP and as a; // result the SP offsets requires stack fixups by multiples of 16.; //; // function_containing_sequence:; // ...; // save LR to SP <- Requires stack instr fixups in OUTLINED_FUNCTION_N; // call OUTLINED_FUNCTION_N; // restore LR from SP; // ...; //; // OUTLINED_FUNCTION_N:; // save LR to SP <- Requires stack instr fixups in OUTLINED_FUNCTION_N; // ...; // bl foo; // restore LR from SP; // ret; //; // Because the code to handle more than one stack fixup does not; // currently have the proper checks for legality, these cases will assert; // in the AArch64 MachineOutliner. This is because the code to do this; // needs more hardening, testing, better checks that generated code is; // legal, etc and because it is only verified to handle a single pass of; // stack fixup.; //; // The assert happens in AArch64InstrInfo::buildOutlinedFrame to catch; // these cases until they are known to be handled. Bugzilla 46767 is; // referenced in comments at the assert site.; //; // To avoid asserting (or generating non-legal code on noassert builds); // we remove all candidates which would need more than one stack fixup by; // pruning the cases where the candidate has calls while also having no; // available LR and having no available general purpose registers to copy; // LR to (ie one extra stack save/restore).; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:1583,Testability,assert,asserting,1583,"// Bugzilla ID: 46767; // TODO: Check if fixing up the stack more than once is safe so we can; // outline these.; //; // An outline resulting in a caller that requires stack fixups at the; // callsite to a callee that also requires stack fixups can happen when; // there are no available registers at the candidate callsite for a; // candidate that itself also has calls.; //; // In other words if function_containing_sequence in the following pseudo; // assembly requires that we save LR at the point of the call, but there; // are no available registers: in this case we save using SP and as a; // result the SP offsets requires stack fixups by multiples of 16.; //; // function_containing_sequence:; // ...; // save LR to SP <- Requires stack instr fixups in OUTLINED_FUNCTION_N; // call OUTLINED_FUNCTION_N; // restore LR from SP; // ...; //; // OUTLINED_FUNCTION_N:; // save LR to SP <- Requires stack instr fixups in OUTLINED_FUNCTION_N; // ...; // bl foo; // restore LR from SP; // ret; //; // Because the code to handle more than one stack fixup does not; // currently have the proper checks for legality, these cases will assert; // in the AArch64 MachineOutliner. This is because the code to do this; // needs more hardening, testing, better checks that generated code is; // legal, etc and because it is only verified to handle a single pass of; // stack fixup.; //; // The assert happens in AArch64InstrInfo::buildOutlinedFrame to catch; // these cases until they are known to be handled. Bugzilla 46767 is; // referenced in comments at the assert site.; //; // To avoid asserting (or generating non-legal code on noassert builds); // we remove all candidates which would need more than one stack fixup by; // pruning the cases where the candidate has calls while also having no; // available LR and having no available general purpose registers to copy; // LR to (ie one extra stack save/restore).; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:45,Safety,unsafe,unsafe,45,"// Outlining from functions with redzones is unsafe since the outliner may; // modify the stack. Check if hasRedZone is true or unknown; if yes, don't; // outline from it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:8,Safety,safe,safe,8,// It's safe to outline from MF.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:422,Safety,avoid,avoid,422,"// According to the AArch64 Procedure Call Standard, the following are; // undefined on entry/exit from a function call:; //; // * Registers x16, x17, (and thus w16, w17); // * Condition codes (and thus the NZCV register); //; // If any of these registers are used inside or live across an outlined; // function, then they may be modified later, either by the compiler or; // some other tool (like the linker).; //; // To avoid outlining in these situations, partition each block into ranges; // where these registers are dead. We will only outline from those ranges.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:566,Availability,avail,available,566,"// We need to know if LR is live across an outlining boundary later on in; // order to decide how we'll create the outlined call, frame, etc.; //; // It's pretty expensive to check this for *every candidate* within a block.; // That's some potentially n^2 behaviour, since in the worst case, we'd need; // to compute liveness from the end of the block for O(n) candidates within; // the block.; //; // So, to improve the average case, let's keep track of liveness from the end; // of the block to the beginning of *every outlinable range*. If we know that; // LR is available in every range we could outline from, then we know that; // we don't need to check liveness for any candidate within that range.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:3,Deployability,Update,Update,3,// Update flags that require info about the entire MBB.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:16,Safety,unsafe,unsafe,16,"// At least one unsafe register is not dead. We do not want to outline at; // this point. If it is long enough to outline from, save the range; // [RangeBegin, RangeEnd).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:34,Safety,unsafe,unsafe,34,// Find the first point where all unsafe registers are dead.; // FIND: <safe instr> <-- end of first potential range; // SKIP: <unsafe def>; // SKIP: ... everything between ...; // SKIP: <unsafe use>,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:72,Safety,safe,safe,72,// Find the first point where all unsafe registers are dead.; // FIND: <safe instr> <-- end of first potential range; // SKIP: <unsafe def>; // SKIP: ... everything between ...; // SKIP: <unsafe use>,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:128,Safety,unsafe,unsafe,128,// Find the first point where all unsafe registers are dead.; // FIND: <safe instr> <-- end of first potential range; // SKIP: <unsafe def>; // SKIP: ... everything between ...; // SKIP: <unsafe use>,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:188,Safety,unsafe,unsafe,188,// Find the first point where all unsafe registers are dead.; // FIND: <safe instr> <-- end of first potential range; // SKIP: <unsafe def>; // SKIP: ... everything between ...; // SKIP: <unsafe use>,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:3,Deployability,Update,Update,3,"// Update flags that impact how we outline across the entire block,; // regardless of safety.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:86,Safety,safe,safety,86,"// Update flags that impact how we outline across the entire block,; // regardless of safety.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:48,Safety,safe,safe,48,"// If we exhausted the entire block, we have no safe ranges to outline.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:47,Safety,unsafe,unsafe,47,// StartPt points to the first place where all unsafe registers; // are dead (if there is any such point). Begin partitioning the MBB into; // ranges.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:63,Safety,safe,safe,63,"// Above loop misses the last (or only) range. If we are still safe, then; // let's save the range.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:58,Availability,down,down,58,// We found the ranges bottom-up. Mapping expects the top-down. Reverse; // the order.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:91,Testability,test,tests,91,"// Special cases for instructions that can always be outlined, but will fail; // the later tests. e.g, ADRPs, which are PC-relative use LR, but can always; // be outlined because they don't require a *specific* value to be in LR.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:579,Security,access,access,579,"// If MI is a call we might be able to outline it. We don't want to outline; // any calls that rely on the position of items on the stack. When we outline; // something containing a call, we have to emit a save and restore of LR in; // the outlined function. Currently, this always happens by saving LR to the; // stack. Thus, if we outline, say, half the parameters for a function call; // plus the call, then we'll break the callee's expectations for the layout; // of the stack.; //; // FIXME: Allow calls to functions which construct a stack frame, as long; // as they don't access arguments on the stack.; // FIXME: Figure out some way to analyze functions defined in other modules.; // We should be able to compute the memory usage based on the IR calling; // convention, even if we can't see the definition.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:124,Integrability,depend,depends,124,"// Never outline calls to mcount. There isn't any rule that would require; // this, but the Linux kernel's ""ftrace"" feature depends on it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:57,Integrability,depend,depends,57,"// If we don't know anything about the callee, assume it depends on the; // stack layout of the caller. In that case, it's only legal to outline; // as a tail-call. Explicitly list the call instructions we know about so we; // don't get unexpected results with call pseudo-instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:84,Safety,safe,safely,84,// We have a function we have information about. Check it if it's something; // can safely outline.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:13,Performance,load,load,13,// Is this a load or store with an immediate offset with SP as the base?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:86,Safety,safe,safe,86,"// We've pushed the return address to the stack, so add 16 to the offset.; // This is safe, since we already checked if it would overflow when we; // checked if this instruction was legal to outline.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:24,Modifiability,rewrite,rewrite,24,"// For thunk outlining, rewrite the last instruction from a call to a; // tail-call.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:145,Safety,safe,safe,145,"// Fix up the instructions in the range, since we're going to modify the; // stack.; // Bugzilla ID: 46767; // TODO: Check if fixing up twice is safe so we can outline these.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:45,Availability,down,down,45,// Add a CFI saying the stack was moved 16 B down.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:81,Security,access,accesses,81,// We modified the stack.; // Walk over the basic block and fix up all the stack accesses.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:59,Integrability,interface,interface,59,// FIXME: This logic should be sunk into a target-specific interface so that; // we don't have to recompute the register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:15,Testability,log,logic,15,// FIXME: This logic should be sunk into a target-specific interface so that; // we don't have to recompute the register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:42,Modifiability,extend,extending,42,// Check that the w->w move is not a zero-extending w->x mov.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:15,Modifiability,extend,extends,15,"// ORRWrs zero-extends to 64-bits, so we need to consider such cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:37,Safety,safe,safe,37,"// MBB isn't a special case, so it's safe to be split to the cold section.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:41,Modifiability,extend,extended,41,"// MOVZWi may be used for producing zero-extended 32-bit immediates in; // 64-bit parameters, so we need to consider super-registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:49,Energy Efficiency,power,power,49,// Must be a multiple of NumBytes (NumBytes is a power of 2),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:75,Performance,scalab,scalable,75,"// Avoid rematerializing rematerializable instructions that use/define; // scalable values, such as 'pfalse' or 'ptrue', which result in different; // results when the runtime vector length is different.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:3,Safety,Avoid,Avoid,3,"// Avoid rematerializing rematerializable instructions that use/define; // scalable values, such as 'pfalse' or 'ptrue', which result in different; // results when the runtime vector length is different.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:80,Integrability,depend,depending,80,"// Avoid rematerializing instructions that return a value that is; // different depending on vector length, even when it is not returned; // in a scalable vector/predicate register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:146,Performance,scalab,scalable,146,"// Avoid rematerializing instructions that return a value that is; // different depending on vector length, even when it is not returned; // in a scalable vector/predicate register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:3,Safety,Avoid,Avoid,3,"// Avoid rematerializing instructions that return a value that is; // different depending on vector length, even when it is not returned; // in a scalable vector/predicate register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:3,Deployability,Update,Update,3,// Update liveins.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:37,Performance,load,load,37,/// Return true if pairing the given load or store is hinted to be; /// unprofitable.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:29,Performance,load,load,29,/// Return true if the given load or store is a strided memory access.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:63,Security,access,access,63,/// Return true if the given load or store is a strided memory access.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:38,Performance,load,load,38,/// Return true if it has an unscaled load/store offset.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:120,Availability,avail,available,120,"/// Returns the unscaled load/store for the scaled load/store opcode,; /// if there is a corresponding unscaled variant available.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:25,Performance,load,load,25,"/// Returns the unscaled load/store for the scaled load/store opcode,; /// if there is a corresponding unscaled variant available.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:51,Performance,load,load,51,"/// Returns the unscaled load/store for the scaled load/store opcode,; /// if there is a corresponding unscaled variant available.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:44,Performance,load,load,44,/// Scaling factor for (scaled or unscaled) load or store.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:53,Performance,load,load,53,/// Returns whether the instruction is a pre-indexed load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:53,Performance,load,load,53,/// Returns whether the instruction is a pre-indexed load/store.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:48,Performance,load,load,48,/// Returns whether the instruction is a paired load/store.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:44,Performance,load,load,44,/// Returns the base register operator of a load/store.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:47,Performance,load,load,47,/// Returns the immediate offset operator of a load/store.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:37,Performance,load,load,37,/// Return true if pairing the given load or store may be paired with another.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:29,Performance,load,load,29,/// Return true if this is a load/store that can be potentially paired/merged.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:32,Performance,load,load,32,/// Hint that pairing the given load or store is unprofitable.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:225,Performance,scalab,scalable,225,"/// If \p OffsetIsScalable is set to 'true', the offset is scaled by `vscale`.; /// This is true for some SVE instructions like ldr/str that have a; /// 'reg + imm' addressing mode where the immediate is an index to the; /// scalable vector located at 'reg + imm * vscale x #bytes'.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:58,Performance,load,load,58,/// Return the immediate offset of the base register in a load/store \p LdSt.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:4,Performance,optimiz,optimizeCompareInstr,4,/// optimizeCompareInstr - Convert the instruction supplying the argument to; /// the comparison into one that sets the zero bit in the flags register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:49,Performance,throughput,throughput,49,/// Return true when a code sequence can improve throughput. It; /// should be called only for instructions in loops.; /// \param Pattern - combiner pattern,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:97,Performance,load,load,97,// Return true if address of the form BaseReg + Scale * ScaledReg + Offset can; // be used for a load/store of NumBytes. BaseReg is always present and; // implicit.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:97,Availability,alive,alive,97,/// \returns Conditions flags used after \p CmpInstr in its MachineBB if NZCV; /// flags are not alive in successors of the same \p CmpInstr and \p MI parent.; /// \returns std::nullopt otherwise.; ///; /// Collect instructions using that flags in \p CCUseInstrs if provided.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:218,Energy Efficiency,allocate,allocated,218,"/// emitFrameOffset - Emit instructions as needed to set DestReg to SrcReg; /// plus Offset. This is intended to be used from within the prolog/epilog; /// insertion (PEI) pass, where a virtual scratch register may be allocated; /// if necessary, to be replaced by the scavenger at the end of PEI.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:31,Modifiability,Rewrite,Rewrite,31,"/// rewriteAArch64FrameIndex - Rewrite MI to access 'Offset' bytes from the; /// FP. Return false if the offset could not be handled directly in MI, and; /// return the left-over portion by reference.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:45,Security,access,access,45,"/// rewriteAArch64FrameIndex - Rewrite MI to access 'Offset' bytes from the; /// FP. Return false if the offset could not be handled directly in MI, and; /// return the left-over portion by reference.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h:274,Deployability,update,updated,274,"/// Check if the @p Offset is a valid frame offset for @p MI.; /// The returned value reports the validity of the frame offset for @p MI.; /// It uses the values defined by AArch64FrameOffsetStatus for that.; /// If result == AArch64FrameOffsetCannotUpdate, @p MI cannot be updated to; /// use an offset.eq; /// If result & AArch64FrameOffsetIsLegal, @p Offset can completely be; /// rewritten in @p MI.; /// If result & AArch64FrameOffsetCanUpdate, @p Offset contains the; /// amount that is off the limit of the legal offset.; /// If set, @p OutUseUnscaledOp will contain the whether @p MI should be; /// turned into an unscaled operator, which opcode is in @p OutUnscaledOp.; /// If set, @p EmittableOffset contains the amount that can be set in @p MI; /// (possibly with @p OutUnscaledOp if OutUseUnscaledOp is true) and that; /// is a legal offset.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:6,Security,access,access,6,// To access function attributes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:3,Testability,Test,Test,3,// Test if there is an appropriate addressing mode and check if the; // immediate fits.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:96,Performance,load,loads,96,"// Form a sequence of SVE registers for instructions using list of vectors,; // e.g. structured loads and stores (ldN, stN).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:33,Testability,test,tests,33,/// isIntImmediate - This method tests to see if the node is a constant; /// operand. If so Imm will receive the 32-bit value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:32,Testability,test,tests,32,// isIntImmediate - This method tests to see if a constant operand.; // If so Imm will receive the value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:39,Testability,test,tests,39,// isOpcWithIntImmediate - This method tests to see if the node is a specific; // opcode and that it has a immediate integer right operand.; // If so Imm will receive the 32 bit value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:34,Testability,test,tests,34,// isIntImmediateEq - This method tests to see if N is a constant operand that; // is equivalent to 'ImmExpected'.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:47,Modifiability,extend,extended,47,// The immediate operand must be a 24-bit zero-extended immediate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:23,Testability,log,logical,23,// It is worth folding logical shift of up to three places.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:52,Modifiability,extend,extended,52,/// Determine whether it is worth to fold V into an extended register addressing; /// mode.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:21,Performance,optimiz,optimizing,21,// Trivial if we are optimizing for code size or if there is only; // one use of the value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:51,Testability,log,logical,51,// If a subtarget has a fastpath LSL we can fold a logical shift into; // the addressing mode and save a cycle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:29,Availability,mask,mask,29,"/// and (shl/srl/sra, x, c), mask --> shl (srl/sra, x, c1), c2; /// to select more shifted register",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:91,Availability,Mask,MaskLen,91,// LowZBits <= ShiftAmtC will fall into isBitfieldPositioningOp; // BitWidth != LowZBits + MaskLen doesn't match the pattern,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:40,Modifiability,extend,extend,40,/// getExtendTypeForNode - Translate an extend node to the corresponding; /// ExtendType value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:78,Modifiability,Extend,ExtendType,78,/// getExtendTypeForNode - Translate an extend node to the corresponding; /// ExtendType value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:52,Modifiability,extend,extended,52,"/// Determine whether it is worth to fold V into an extended register of an; /// Add/Sub. LSL means we are folding into an `add w0, w1, w2, lsl #N`; /// instruction, and the shift should be treated as worth folding even if has; /// multiple uses.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:21,Performance,optimiz,optimizing,21,// Trivial if we are optimizing for code size or if there is only; // one use of the value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:51,Testability,log,logical,51,// If a subtarget has a fastpath LSL we can fold a logical shift into; // the add/sub and save a cycle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:148,Testability,log,logical,148,"/// SelectShiftedRegister - Select a ""shifted register"" operand. If the value; /// is not shifted, set the Shift operand to default of ""LSL 0"". The logical; /// instructions allow the shifted register to be rotated, but the arithmetic; /// instructions do not. The AllowROR parameter specifies whether ROR is; /// supported.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:29,Modifiability,extend,extend,29,"/// Instructions that accept extend modifiers like UXTW expect the register; /// being extended to be a GPR32, but the incoming DAG might be acting on a; /// GPR64 (either via SEXT_INREG or AND). Extract the appropriate low bits if; /// this is the case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:87,Modifiability,extend,extended,87,"/// Instructions that accept extend modifiers like UXTW expect the register; /// being extended to be a GPR32, but the incoming DAG might be acting on a; /// GPR64 (either via SEXT_INREG or AND). Extract the appropriate low bits if; /// this is the case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:51,Usability,UX,UXTW,51,"/// Instructions that accept extend modifiers like UXTW expect the register; /// being extended to be a GPR32, but the incoming DAG might be acting on a; /// GPR64 (either via SEXT_INREG or AND). Extract the appropriate low bits if; /// this is the case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:44,Modifiability,extend,extended,44,"/// SelectArithExtendedRegister - Select a ""extended register"" operand. This; /// operand folds in an extend followed by an optional left shift.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:102,Modifiability,extend,extend,102,"/// SelectArithExtendedRegister - Select a ""extended register"" operand. This; /// operand folds in an extend followed by an optional left shift.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:327,Integrability,inject,injected,327,"// AArch64 mandates that the RHS of the operation must use the smallest; // register class that could contain the size being extended from. Thus,; // if we're folding a (sext i8), we need the RHS to be a GPR32, even though; // there might not be an actual 32-bit value in the program. We can; // (harmlessly) synthesize one by injected an EXTRACT_SUBREG here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:125,Modifiability,extend,extended,125,"// AArch64 mandates that the RHS of the operation must use the smallest; // register class that could contain the size being extended from. Thus,; // if we're folding a (sext i8), we need the RHS to be a GPR32, even though; // there might not be an actual 32-bit value in the program. We can; // (harmlessly) synthesize one by injected an EXTRACT_SUBREG here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:327,Security,inject,injected,327,"// AArch64 mandates that the RHS of the operation must use the smallest; // register class that could contain the size being extended from. Thus,; // if we're folding a (sext i8), we need the RHS to be a GPR32, even though; // there might not be an actual 32-bit value in the program. We can; // (harmlessly) synthesize one by injected an EXTRACT_SUBREG here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:40,Usability,UX,UXTX,40,"/// SelectArithUXTXRegister - Select a ""UXTX register"" operand. This; /// operand is refered by the instructions have SP operand",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:56,Performance,load,load,56,"/// If there's a use of this ADDlow that's not itself a load/store then we'll; /// need to create a real ADD instruction from it anyway and there's no point in; /// folding it into the mem op. Theoretically, it shouldn't matter, but there's; /// a single pseudo-instruction for an ADRP/ADD pair so over-aggressive folding; /// leads to duplicated ADRP instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:88,Security,access,accessed,88,"// Base only. The address will be materialized into a register before; // the memory is accessed.; // add x0, Xbase, #offset; // stp x1, x2, [x0]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:88,Security,access,accessed,88,"// Base only. The address will be materialized into a register before; // the memory is accessed.; // add x0, Xbase, #offset; // ldr x0, [x0]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:68,Modifiability,extend,extended,68,"/// Check if the given SHL node (\p N), can be used to form an; /// extended register for an addressing mode.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:54,Modifiability,extend,extended,54,// Remember if it is worth folding N when it produces extended register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:26,Modifiability,extend,extend,26,// Try to match a shifted extend on the RHS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:26,Modifiability,extend,extend,26,// Try to match a shifted extend on the LHS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:29,Modifiability,extend,extend,29,// Try to match an unshifted extend on the LHS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:29,Modifiability,extend,extend,29,// Try to match an unshifted extend on the RHS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:41,Performance,load,load,41,// Skip the immediate can be selected by load/store addressing mode.; // Also skip the immediate can be encoded by a single ADD (SUB is also; // checked by using -ImmOff).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:54,Modifiability,extend,extended,54,// Remember if it is worth folding N when it produces extended register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:26,Modifiability,extend,extend,26,// Try to match a shifted extend on the RHS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:26,Modifiability,extend,extend,26,// Try to match a shifted extend on the LHS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:30,Modifiability,extend,extend,30,"// Match any non-shifted, non-extend, non-immediate add expression.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:19,Integrability,interface,interface,19,// The createTuple interface requires 3 RegClassIDs for each possible; // tuple type even though we only have them for ZPR2 and ZPR4.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:97,Performance,load,load,97,// We're not doing validity checking here. That was done when checking; // if we should mark the load as indexed or not. We're just selecting; // the right instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:21,Performance,load,load,21,// The result of the load is only i32. It's the subreg_to_reg that makes; // it into an i64.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:21,Performance,load,load,21,// The result of the load is only i32. It's the subreg_to_reg that makes; // it into an i64.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:21,Performance,load,load,21,// The result of the load is only i32. It's the subreg_to_reg that makes; // it into an i64.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:97,Usability,simpl,simple,97,"// Transfer memoperands. In the case of AArch64::LD64B, there won't be one,; // because it's too simple to have needed special treatment during lowering.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:3,Deployability,Update,Update,3,// Update uses of write back register,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:3,Deployability,Update,Update,3,// Update uses of vector list,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:3,Deployability,Update,Update,3,// Update the chain,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:4,Performance,Optimiz,Optimize,4,"/// Optimize \param OldBase and \param OldOffset selecting the best addressing; /// mode. Returns a tuple consisting of an Opcode, an SDValue representing the; /// new Base and an SDValue representing the new offset.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:3,Safety,Detect,Detect,3,// Detect a possible Reg+Imm addressing mode.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:3,Safety,Detect,Detect,3,"// Detect a possible reg+reg addressing mode, but only if we haven't already; // detected a Reg+Imm one.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:81,Safety,detect,detected,81,"// Detect a possible reg+reg addressing mode, but only if we haven't already; // detected a Reg+Imm one.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:14,Performance,scalab,scalable,14,// Only match scalable vector VTs,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:3,Performance,Optimiz,Optimize,3,// Optimize addressing mode.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:3,Performance,Optimiz,Optimize,3,// Optimize addressing mode.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:3,Deployability,Update,Update,3,// Update uses of the write back register,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:3,Deployability,Update,Update,3,// Update uses of the vector list,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:3,Deployability,Update,Update,3,// Update the Chain,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:192,Availability,redundant,redundant,192,"// Here we can test the type of VT and return false when the type does not; // match, but since it is done prior to that call in the current context; // we turned that into an assert to avoid redundant code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:186,Safety,avoid,avoid,186,"// Here we can test the type of VT and return false when the type does not; // match, but since it is done prior to that call in the current context; // we turned that into an assert to avoid redundant code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:192,Safety,redund,redundant,192,"// Here we can test the type of VT and return false when the type does not; // match, but since it is done prior to that call in the current context; // we turned that into an assert to avoid redundant code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:15,Testability,test,test,15,"// Here we can test the type of VT and return false when the type does not; // match, but since it is done prior to that call in the current context; // we turned that into an assert to avoid redundant code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:176,Testability,assert,assert,176,"// Here we can test the type of VT and return false when the type does not; // match, but since it is done prior to that call in the current context; // we turned that into an assert to avoid redundant code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:103,Availability,mask,mask,103,"// FIXME: simplify-demanded-bits in DAGCombine will probably have; // changed the AND node to a 32-bit mask operation. We'll have to; // undo that as part of the transform here if we want to catch all; // the opportunities.; // Currently the NumberOfIgnoredLowBits argument helps to recover; // from these situations when matching bigger pattern (bitfield insert).; // For unsigned extracts, check for a shift right and mask",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:283,Availability,recover,recover,283,"// FIXME: simplify-demanded-bits in DAGCombine will probably have; // changed the AND node to a 32-bit mask operation. We'll have to; // undo that as part of the transform here if we want to catch all; // the opportunities.; // Currently the NumberOfIgnoredLowBits argument helps to recover; // from these situations when matching bigger pattern (bitfield insert).; // For unsigned extracts, check for a shift right and mask",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:420,Availability,mask,mask,420,"// FIXME: simplify-demanded-bits in DAGCombine will probably have; // changed the AND node to a 32-bit mask operation. We'll have to; // undo that as part of the transform here if we want to catch all; // the opportunities.; // Currently the NumberOfIgnoredLowBits argument helps to recover; // from these situations when matching bigger pattern (bitfield insert).; // For unsigned extracts, check for a shift right and mask",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:283,Safety,recover,recover,283,"// FIXME: simplify-demanded-bits in DAGCombine will probably have; // changed the AND node to a 32-bit mask operation. We'll have to; // undo that as part of the transform here if we want to catch all; // the opportunities.; // Currently the NumberOfIgnoredLowBits argument helps to recover; // from these situations when matching bigger pattern (bitfield insert).; // For unsigned extracts, check for a shift right and mask",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:10,Usability,simpl,simplify-demanded-bits,10,"// FIXME: simplify-demanded-bits in DAGCombine will probably have; // changed the AND node to a 32-bit mask operation. We'll have to; // undo that as part of the transform here if we want to catch all; // the opportunities.; // Currently the NumberOfIgnoredLowBits argument helps to recover; // from these situations when matching bigger pattern (bitfield insert).; // For unsigned extracts, check for a shift right and mask",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:137,Usability,undo,undo,137,"// FIXME: simplify-demanded-bits in DAGCombine will probably have; // changed the AND node to a 32-bit mask operation. We'll have to; // undo that as part of the transform here if we want to catch all; // the opportunities.; // Currently the NumberOfIgnoredLowBits argument helps to recover; // from these situations when matching bigger pattern (bitfield insert).; // For unsigned extracts, check for a shift right and mask",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:56,Availability,mask,mask,56,"// Because of simplify-demanded-bits in DAGCombine, the mask may have been; // simplified. Try to undo that",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:14,Usability,simpl,simplify-demanded-bits,14,"// Because of simplify-demanded-bits in DAGCombine, the mask may have been; // simplified. Try to undo that",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:79,Usability,simpl,simplified,79,"// Because of simplify-demanded-bits in DAGCombine, the mask may have been; // simplified. Try to undo that",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:98,Usability,undo,undo,98,"// Because of simplify-demanded-bits in DAGCombine, the mask may have been; // simplified. Try to undo that",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:22,Availability,mask,mask,22,// The immediate is a mask of the low bits iff imm & (imm+1) == 0,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:3,Modifiability,Extend,Extend,3,// Extend the incoming operand of the SRL to 64-bit.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:42,Performance,perform,performed,42,"// Let's pretend a 0 shift right has been performed.; // The resulting code will be at least as good as the original one; // plus it may expose more opportunities for bitfield insert pattern.; // FIXME: Currently we limit this to the bigger pattern, because; // some optimizations expect AND and not UBFM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:267,Performance,optimiz,optimizations,267,"// Let's pretend a 0 shift right has been performed.; // The resulting code will be at least as good as the original one; // plus it may expose more opportunities for bitfield insert pattern.; // FIXME: Currently we limit this to the bigger pattern, because; // some optimizations expect AND and not UBFM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:137,Security,expose,expose,137,"// Let's pretend a 0 shift right has been performed.; // The resulting code will be at least as good as the original one; // plus it may expose more opportunities for bitfield insert pattern.; // FIXME: Currently we limit this to the bigger pattern, because; // some optimizations expect AND and not UBFM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:96,Performance,perform,performed,96,// Bail out on large immediates. This happens when no proper; // combining/constant folding was performed.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:26,Modifiability,extend,extend,26,"// Since we're moving the extend before the right shift operation, we need; // to clamp the MSB to make sure we don't shift in undefined bits instead of; // the zeros which would get shifted in with the original right shift; // operation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:260,Availability,Mask,MaskImm,260,"// We are looking for the following pattern which basically extracts several; // continuous bits from the source value and places it from the LSB of the; // destination value, all other bits of the destination value or set to zero:; //; // Value2 = AND Value, MaskImm; // SRL Value2, ShiftImm; //; // with MaskImm >> ShiftImm to search for the bit width.; //; // This gets selected into a single UBFM:; //; // UBFM Value, ShiftImm, Log2_64(MaskImm); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:306,Availability,Mask,MaskImm,306,"// We are looking for the following pattern which basically extracts several; // continuous bits from the source value and places it from the LSB of the; // destination value, all other bits of the destination value or set to zero:; //; // Value2 = AND Value, MaskImm; // SRL Value2, ShiftImm; //; // with MaskImm >> ShiftImm to search for the bit width.; //; // This gets selected into a single UBFM:; //; // UBFM Value, ShiftImm, Log2_64(MaskImm); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:440,Availability,Mask,MaskImm,440,"// We are looking for the following pattern which basically extracts several; // continuous bits from the source value and places it from the LSB of the; // destination value, all other bits of the destination value or set to zero:; //; // Value2 = AND Value, MaskImm; // SRL Value2, ShiftImm; //; // with MaskImm >> ShiftImm to search for the bit width.; //; // This gets selected into a single UBFM:; //; // UBFM Value, ShiftImm, Log2_64(MaskImm); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:81,Deployability,continuous,continuous,81,"// We are looking for the following pattern which basically extracts several; // continuous bits from the source value and places it from the LSB of the; // destination value, all other bits of the destination value or set to zero:; //; // Value2 = AND Value, MaskImm; // SRL Value2, ShiftImm; //; // with MaskImm >> ShiftImm to search for the bit width.; //; // This gets selected into a single UBFM:; //; // UBFM Value, ShiftImm, Log2_64(MaskImm); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:192,Availability,redundant,redundant,192,"// Here we can test the type of VT and return false when the type does not; // match, but since it is done prior to that call in the current context; // we turned that into an assert to avoid redundant code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:186,Safety,avoid,avoid,186,"// Here we can test the type of VT and return false when the type does not; // match, but since it is done prior to that call in the current context; // we turned that into an assert to avoid redundant code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:192,Safety,redund,redundant,192,"// Here we can test the type of VT and return false when the type does not; // match, but since it is done prior to that call in the current context; // we turned that into an assert to avoid redundant code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:15,Testability,test,test,15,"// Here we can test the type of VT and return false when the type does not; // match, but since it is done prior to that call in the current context; // we turned that into an assert to avoid redundant code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:176,Testability,assert,assert,176,"// Here we can test the type of VT and return false when the type does not; // match, but since it is done prior to that call in the current context; // we turned that into an assert to avoid redundant code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:241,Safety,redund,redundancy,241,// We are looking for a shift of truncate. Truncate from i64 to i32 could; // be considered as setting high 32 bits as zero. Our strategy here is to; // always generate 64bit UBFM. This consistency will help the CSE pass; // later find more redundancy.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:41,Performance,perform,performed,41,"// Let's pretend a 0 shift left has been performed.; // FIXME: Currently we limit this to the bigger pattern case,; // because some optimizations expect AND and not UBFM",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:132,Performance,optimiz,optimizations,132,"// Let's pretend a 0 shift left has been performed.; // FIXME: Currently we limit this to the bigger pattern case,; // because some optimizations expect AND and not UBFM",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:3,Modifiability,Extend,Extend,3,// Extend the incoming operand of the shift to 64-bits.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:52,Availability,mask,mask,52,"/// Does DstMask form a complementary pair with the mask provided by; /// BitsToBeInserted, suitable for use in a BFI instruction. Roughly speaking,; /// this asks whether DstMask zeroes precisely those bits that will be set by; /// the other half.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:3,Modifiability,inherit,inherit,3,// inherit the bitwidth value,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:104,Testability,assert,assert,104,// Users of this node should have already been instruction selected; // FIXME: Can we turn that into an assert?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:26,Performance,perform,performing,26,"/// Create a machine node performing a notional SHL of Op by ShlAmount. If; /// ShlAmount is negative, do a (logical) right-shift instead. If ShlAmount is; /// 0, return Op unchanged.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:109,Testability,log,logical,109,"/// Create a machine node performing a notional SHL of Op by ShlAmount. If; /// ShlAmount is negative, do a (logical) right-shift instead. If ShlAmount is; /// 0, return Op unchanged.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:112,Availability,Mask,Mask,112,"/// Does this tree qualify as an attempt to move a bitfield into position,; /// essentially ""(and (shl VAL, N), Mask)"" or (shl VAL, N).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:41,Availability,mask,mask,41,"// For pattern ""and(shl(val, N), shifted-mask)"", 'ShlOp0' is set to 'val'.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:53,Availability,mask,mask,53,"// For pattern ""and(any_extend(shl(val, N)), shifted-mask)""; // ShlVal == shl(val, N), which is a left shift on a smaller type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:55,Modifiability,extend,extended,55,"// Since this is after type legalization and ShlVal is extended to MVT::i64,; // expect VT to be MVT::i32.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:93,Performance,perform,performed,93,// Bail out on large Width. This happens when no proper combining / constant; // folding was performed.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:256,Availability,mask,mask,256,"// If VT is i64, Width > 64 is insensible since NonZeroBits is uint64_t, and; // Width == 64 indicates a missed dag-combine from ""(and val, AllOnes)"" to; // ""val"".; // If VT is i32, what Width >= 32 means:; // - For ""(and (any_extend(shl val, N)), shifted-mask)"", the`and` Op; // demands at least 'Width' bits (after dag-combiner). This together with; // `any_extend` Op (undefined higher bits) indicates missed combination; // when lowering the 'and' IR instruction to an machine IR instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:97,Availability,mask,mask,97,"// BFI encompasses sufficiently many nodes that it's worth inserting an extra; // LSL/LSR if the mask in NonZeroBits doesn't quite match up with the ISD::SHL; // amount. BiggerPattern is true when this pattern is being matched for BFI,; // BiggerPattern is false when this pattern is being matched for UBFIZ, in; // which case it is not profitable to insert an extra shift.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:27,Availability,mask,mask,27,"// For node (shl (and val, mask), N)), returns true if the node is equivalent to; // UBFIZ.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:86,Testability,assert,asserts,86,// Caller should have verified that N is a left shift with constant shift; // amount; asserts that.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:91,Availability,Mask,Mask,91,"// AndImm is a superset of (AllOnes >> ShlImm); in other words, AndImm; // should end with Mask, and could be prefixed with random bits if those; // bits are shifted out.; //; // For example, xyz11111 (with {x,y,z} being 0 or 1) is fine if ShlImm >= 3;; // the AND result corresponding to those bits are shifted out, so it's fine; // to not extract them.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:41,Availability,Mask,MaskImm,41,"// Generate a BFI/BFXIL from 'or (and X, MaskImm), OrImm' iff the value being; // inserted only sets known zero bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:158,Performance,perform,performance,158,"// Skip this transformation if the ORR immediate can be encoded in the ORR.; // Otherwise, we'll trade an AND+ORR for ORR+BFI/BFXIL, which is most likely; // performance neutral.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:17,Availability,mask,mask,17,"// The KnownZero mask must be a shifted mask (e.g., 1110..011, 11100..00).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:40,Availability,mask,mask,40,"// The KnownZero mask must be a shifted mask (e.g., 1110..011, 11100..00).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:39,Safety,avoid,avoid,39,"// If we're creating a BFI instruction avoid cases where we need more; // instructions to materialize the BFI constant as compared to the original; // ORR. A BFXIL will use the same constant as the original ORR, so the code; // should be no worse in this case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:3,Safety,Avoid,Avoid,3,// Avoid folding Dst into ORR-with-shift if Dst has other uses than ORR.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:3,Safety,Avoid,Avoid,3,// Avoid transforming 'DstOp0' if it has other uses than the AND node.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:376,Integrability,depend,dependency,376,"// An example to illustrate the transformation; // From:; // lsr x8, x1, #1; // and x8, x8, #0x3f80; // bfxil x8, x1, #0, #7; // To:; // and x8, x23, #0x7f; // ubfx x9, x23, #8, #7; // orr x23, x8, x9, lsl #7; //; // The number of instructions remains the same, but ORR is faster than BFXIL; // on many AArch64 processors (or as good as BFXIL if not faster). Besides,; // the dependency chain is improved after the transformation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:161,Energy Efficiency,efficient,efficient,161,"// Given an 'ISD::OR' node that is going to be selected as BFM, analyze; // the operands and select it to AArch64::ORR with shifted registers if; // that's more efficient. Returns true iff selection to AArch64::ORR happens.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:19,Usability,simpl,simplifies,19,// Bail out if BFM simplifies away one node in BFM Dst.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:55,Usability,simpl,simplifies,55,"// For ""BFM Rd, Rn, #immr, #imms"", it's known that BFM simplifies away fewer; // nodes from Rn (or inserts additional shift node) if BiggerPattern is true.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:22,Availability,Mask,Mask,22,"// OrOpd0 = AND Src, #Mask; // So BFM simplifies away one AND node from Src and doesn't simplify away; // nodes from Dst. If ORR with left-shifted operand also simplifies away; // one node (from Rd), ORR is better since it has higher throughput and; // smaller latency than BFM on many AArch64 processors (and for the rest; // ORR is at least as good as BFM).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:234,Performance,throughput,throughput,234,"// OrOpd0 = AND Src, #Mask; // So BFM simplifies away one AND node from Src and doesn't simplify away; // nodes from Dst. If ORR with left-shifted operand also simplifies away; // one node (from Rd), ORR is better since it has higher throughput and; // smaller latency than BFM on many AArch64 processors (and for the rest; // ORR is at least as good as BFM).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:261,Performance,latency,latency,261,"// OrOpd0 = AND Src, #Mask; // So BFM simplifies away one AND node from Src and doesn't simplify away; // nodes from Dst. If ORR with left-shifted operand also simplifies away; // one node (from Rd), ORR is better since it has higher throughput and; // smaller latency than BFM on many AArch64 processors (and for the rest; // ORR is at least as good as BFM).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:38,Usability,simpl,simplifies,38,"// OrOpd0 = AND Src, #Mask; // So BFM simplifies away one AND node from Src and doesn't simplify away; // nodes from Dst. If ORR with left-shifted operand also simplifies away; // one node (from Rd), ORR is better since it has higher throughput and; // smaller latency than BFM on many AArch64 processors (and for the rest; // ORR is at least as good as BFM).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:88,Usability,simpl,simplify,88,"// OrOpd0 = AND Src, #Mask; // So BFM simplifies away one AND node from Src and doesn't simplify away; // nodes from Dst. If ORR with left-shifted operand also simplifies away; // one node (from Rd), ORR is better since it has higher throughput and; // smaller latency than BFM on many AArch64 processors (and for the rest; // ORR is at least as good as BFM).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:160,Usability,simpl,simplifies,160,"// OrOpd0 = AND Src, #Mask; // So BFM simplifies away one AND node from Src and doesn't simplify away; // nodes from Dst. If ORR with left-shifted operand also simplifies away; // one node (from Rd), ORR is better since it has higher throughput and; // smaller latency than BFM on many AArch64 processors (and for the rest; // ORR is at least as good as BFM).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:61,Availability,mask,masks,61,"// Because of simplify-demanded-bits in DAGCombine, involved masks may not; // have the expected shape. Try to undo that.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:14,Usability,simpl,simplify-demanded-bits,14,"// Because of simplify-demanded-bits in DAGCombine, involved masks may not; // have the expected shape. Try to undo that.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:111,Usability,undo,undo,111,"// Because of simplify-demanded-bits in DAGCombine, involved masks may not; // have the expected shape. Try to undo that.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:183,Availability,mask,mask,183,"// Given a OR operation, check if we have the following pattern; // ubfm c, b, imm, imm2 (or something that does the same jobs, see; // isBitfieldExtractOp); // d = e & mask2 ; where mask is a binary sequence of 1..10..0 and; // countTrailingZeros(mask2) == imm2 - imm + 1; // f = d | c; // if yes, replace the OR instruction with:; // f = BFM Opd0, Opd1, LSB, MSB ; where LSB = imm, and MSB = imm2; // OR is commutative, check all combinations of operand order and values of; // BiggerPattern, i.e.; // Opd0, Opd1, BiggerPattern=false; // Opd1, Opd0, BiggerPattern=false; // Opd0, Opd1, BiggerPattern=true; // Opd1, Opd0, BiggerPattern=true; // Several of these combinations may match, so check with BiggerPattern=false; // first since that will produce better results by matching more instructions; // and/or inserting fewer extra instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:95,Modifiability,extend,extended,95,"// Check that the returned opcode is compatible with the pattern,; // i.e., same type and zero extended (U and not S)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:10,Availability,mask,mask,10,"// If the mask on the insertee is correct, we have a BFXIL operation. We; // can share the ImmR and ImmS values from the already-computed UBFM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:157,Usability,simpl,simplify-demanded-bits,157,"// Compute the Known Zero for the candidate of the first operand.; // This allows to catch more general case than just looking for; // AND with imm. Indeed, simplify-demanded-bits may have removed; // the AND instruction because it proves it was useless.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:37,Usability,simpl,simplify-demanded-bits,37,// Maybe the AND has been removed by simplify-demanded-bits; // or is useful because it discards more bits,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:106,Energy Efficiency,efficient,efficient,106,"// Before selecting ISD::OR node to AArch64::BFM, see if an AArch64::ORR; // with shifted operand is more efficient.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:128,Availability,Mask,MaskImms,128,"// Generate a BFXIL from 'or (and X, Mask0Imm), (and Y, Mask1Imm)' iff; // Mask0Imm and ~Mask1Imm are equivalent and one of the MaskImms is a shifted; // mask (e.g., 0x000ffff0).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:154,Availability,mask,mask,154,"// Generate a BFXIL from 'or (and X, Mask0Imm), (and Y, Mask1Imm)' iff; // Mask0Imm and ~Mask1Imm are equivalent and one of the MaskImms is a shifted; // mask (e.g., 0x000ffff0).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:127,Availability,mask,mask,127,"// ORR is commutative, so canonicalize to the form 'or (and X, Mask0Imm),; // (and Y, Mask1Imm)' where Mask1Imm is the shifted mask masking off the; // bits to be inserted.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:132,Availability,mask,masking,132,"// ORR is commutative, so canonicalize to the form 'or (and X, Mask0Imm),; // (and Y, Mask1Imm)' where Mask1Imm is the shifted mask masking off the; // bits to be inserted.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:145,Availability,mask,masking,145,/// SelectBitfieldInsertInZeroOp - Match a UBFIZ instruction that is the; /// equivalent of a left shift by a constant amount followed by an and masking; /// out a contiguous set of bits.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:79,Modifiability,variab,variable,79,/// tryShiftAmountMod - Take advantage of built-in mod of shift amount in; /// variable shift/rotate instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:16,Modifiability,extend,extend,16,// Skip over an extend of the shift amount.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:82,Safety,avoid,avoid,82,"// If we are shifting by X+/-N where N == 0 mod Size, then just shift by X; // to avoid the ADD/SUB.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:26,Availability,mask,masked,26,"// If the shift amount is masked with an AND, check that the mask covers the; // bits that are implicitly ANDed off by the above opcodes and if so, skip; // the AND.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:61,Availability,mask,mask,61,"// If the shift amount is masked with an AND, check that the mask covers the; // bits that are implicitly ANDed off by the above opcodes and if so, skip; // the AND.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:27,Performance,perform,performs,27,"// An FCVT[SU] instruction performs: convertToInt(Val * 2^fbits) where fbits; // is between 1 and 32 for a destination w-register, or 1 and 64 for an; // x-register.; //; // By this stage, we've detected (fp_to_[su]int (fmul Val, THIS_NODE)) so we; // want THIS_NODE to be 2^fbits. This is much easier to deal with using; // integers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:195,Safety,detect,detected,195,"// An FCVT[SU] instruction performs: convertToInt(Val * 2^fbits) where fbits; // is between 1 and 32 for a destination w-register, or 1 and 64 for an; // x-register.; //; // By this stage, we've detected (fp_to_[su]int (fmul Val, THIS_NODE)) so we; // want THIS_NODE to be 2^fbits. This is much easier to deal with using; // integers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:9,Availability,mask,mask,9,// Shift mask depending on type size.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:14,Integrability,depend,depending,14,// Shift mask depending on type size.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:31,Performance,load,load,31,// Try to select as an indexed load. Fall through to normal processing; // if we can't.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:118,Energy Efficiency,schedul,scheduling,118,"/// createAArch64ISelDag - This pass converts a legalized DAG into a; /// AArch64-specific DAG, ready for instruction scheduling.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:24,Performance,scalab,scalable,24,"/// When \p PredVT is a scalable vector predicate in the form; /// MVT::nx<M>xi1, it builds the correspondent scalable vector of; /// integers MVT::nx<M>xi<bits> s.t. M x bits = 128. When targeting; /// structured vectors (NumVec >1), the output data type is; /// MVT::nx<M*NumVec>xi<bits> s.t. M x bits = 128. If the input; /// PredVT is not in the form MVT::nx<M>xi1, it returns an invalid; /// EVT.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:110,Performance,scalab,scalable,110,"/// When \p PredVT is a scalable vector predicate in the form; /// MVT::nx<M>xi1, it builds the correspondent scalable vector of; /// integers MVT::nx<M>xi<bits> s.t. M x bits = 128. When targeting; /// structured vectors (NumVec >1), the output data type is; /// MVT::nx<M*NumVec>xi<bits> s.t. M x bits = 128. If the input; /// PredVT is not in the form MVT::nx<M>xi1, it returns an invalid; /// EVT.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:163,Security,access,access,163,/// SelectAddrModeIndexedSVE - Attempt selection of the addressing mode:; /// Base + OffImm * sizeof(MemVT) for Min >= OffImm <= Max; /// where Root is the memory access using N for its address.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:269,Performance,load,load,269,// Temporary option added for the purpose of testing functionality added; // to DAGCombiner.cpp in D92230. It is expected that this can be removed; // in future when both implementations will be based off MGATHER rather; // than the GLD1 nodes added for the SVE gather load intrinsics.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:45,Testability,test,testing,45,// Temporary option added for the purpose of testing functionality added; // to DAGCombiner.cpp in D92230. It is expected that this can be removed; // in future when both implementations will be based off MGATHER rather; // than the GLD1 nodes added for the SVE gather load intrinsics.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:54,Integrability,depend,dependency,54,"// All of the XOR, OR and CMP use ALU ports, and data dependency will become the; // bottleneck after this transform on high end CPU. So this max leaf node; // limitation is guard cmp+ccmp will be profitable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:85,Performance,bottleneck,bottleneck,85,"// All of the XOR, OR and CMP use ALU ports, and data dependency will become the; // bottleneck after this transform on high end CPU. So this max leaf node; // limitation is guard cmp+ccmp will be profitable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Modifiability,Variab,Variable,3,// Variable arguments.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Modifiability,Variab,Variable-sized,3,// Variable-sized objects.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:51,Safety,safe,safe,51,// promote v4f16 to v4f32 when that is known to be safe.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:11,Performance,load,loads,11,// 128-bit loads and stores can be done without expanding,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:19,Performance,load,loads,19,// Aligned 128-bit loads and stores are single-copy atomic according to the; // v8.4a spec. LRCPC3 introduces 128-bit STILP/LDIAPP but still requires LSE2.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:24,Performance,load,loads,24,"// 256 bit non-temporal loads can be lowered to LDNP. This is done using; // custom lowering, as there are no un-paired non-temporal loads legalization; // will break up 256 bit inputs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:133,Performance,load,loads,133,"// 256 bit non-temporal loads can be lowered to LDNP. This is done using; // custom lowering, as there are no un-paired non-temporal loads legalization; // will break up 256 bit inputs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:27,Availability,avail,available,27,// Issue __sincos_stret if available.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:90,Performance,load,loads,90,"// Make floating-point constants legal for the large code model, so they don't; // become loads from the constant pool.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:40,Modifiability,extend,extending,40,"// AArch64 does not have floating-point extending loads, i1 sign-extending; // load, floating-point truncating stores, or v2i32->v2i16 truncating store.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:65,Modifiability,extend,extending,65,"// AArch64 does not have floating-point extending loads, i1 sign-extending; // load, floating-point truncating stores, or v2i32->v2i16 truncating store.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:50,Performance,load,loads,50,"// AArch64 does not have floating-point extending loads, i1 sign-extending; // load, floating-point truncating stores, or v2i32->v2i16 truncating store.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:79,Performance,load,load,79,"// AArch64 does not have floating-point extending loads, i1 sign-extending; // load, floating-point truncating stores, or v2i32->v2i16 truncating store.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:11,Performance,load,loads,11,// Indexed loads and stores are supported.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:32,Safety,avoid,avoid,32,"// In case of strict alignment, avoid an excessive number of byte wide stores.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:45,Safety,avoid,avoid,45,"// FIXME: v1f64 shouldn't be legal if we can avoid it, because it leads to; // silliness like this:",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:49,Safety,detect,detect,49,// Custom handling for some quad-vector types to detect MULL.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:27,Modifiability,extend,extending,27,"// Likewise, narrowing and extending vector loads/stores aren't handled; // directly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:44,Performance,load,loads,44,"// Likewise, narrowing and extending vector loads/stores aren't handled; // directly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:24,Availability,mask,masked,24,"// NEON doesn't support masked loads/stores/gathers/scatters, but SVE does",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:31,Performance,load,loads,31,"// NEON doesn't support masked loads/stores/gathers/scatters, but SVE does",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:40,Modifiability,extend,extending,40,"// Firstly, exclude all scalable vector extending loads/truncating stores,; // include both integer and floating scalable vector.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:24,Performance,scalab,scalable,24,"// Firstly, exclude all scalable vector extending loads/truncating stores,; // include both integer and floating scalable vector.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:50,Performance,load,loads,50,"// Firstly, exclude all scalable vector extending loads/truncating stores,; // include both integer and floating scalable vector.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:113,Performance,scalab,scalable,113,"// Firstly, exclude all scalable vector extending loads/truncating stores,; // include both integer and floating scalable vector.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:21,Availability,avail,available,21,// [SU][MIN|MAX] are available for all NEON types apart from i64.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:56,Availability,avail,available,56,// F[MIN|MAX][NUM|NAN] and simple strict operations are available for all FP; // NEON types.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:27,Usability,simpl,simple,27,// F[MIN|MAX][NUM|NAN] and simple strict operations are available for all FP; // NEON types.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:13,Modifiability,extend,extend,13,// Strict fp extend and trunc are legal,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:34,Modifiability,extend,extending,34,// Mark integer truncating stores/extending loads as having custom lowering,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:44,Performance,load,loads,44,// Mark integer truncating stores/extending loads as having custom lowering,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:41,Modifiability,extend,extending,41,// Mark floating-point truncating stores/extending loads as having custom; // lowering,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:51,Performance,load,loads,51,// Mark floating-point truncating stores/extending loads as having custom; // lowering,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:43,Performance,scalab,scalable,43,// Lower fixed length vector operations to scalable equivalents.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:32,Testability,test,tests,32,// isIntImmediate - This method tests to see if the node is a constant; // operand. If so Imm will receive the value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:39,Testability,test,tests,39,// isOpcWithIntImmediate - This method tests to see if the node is a specific; // opcode and that it has a immediate integer right operand.; // If so Imm will receive the value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Usability,Clear,Clear,3,// Clear bits that are not demanded.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:45,Availability,mask,mask,45,"// If NewImm or its bitwise NOT is a shifted mask, it is a bitmask immediate; // or all-ones or all-zeros, in which case we can stop searching. Otherwise,; // we halve the element size and continue the search.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:102,Performance,optimiz,optimize,102,"// If the new constant immediate is all-zeros or all-ones, let the target; // independent DAG combine optimize this node.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:97,Performance,optimiz,optimization,97,"// Otherwise, create a machine node so that target independent DAG combine; // doesn't undo this optimization.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:87,Usability,undo,undo,87,"// Otherwise, create a machine node so that target independent DAG combine; // doesn't undo this optimization.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:14,Performance,optimiz,optimization,14,// Delay this optimization to as late as possible.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:82,Availability,Mask,Mask,82,/// computeKnownBitsForTargetNode - Determine which of the bits specified in; /// Mask are known to be either zero or one and return them Known.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:19,Usability,clear,cleared,19,// Compute the bit cleared value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:86,Modifiability,extend,extend,86,"// Figure out the datatype of the vector operand. The UMINV instruction; // will zero extend the result, so we can mark as known zero all the; // bits larger than the element datatype. 32-bit or larget doesn't need; // this as those are legal types and will be handled by isel directly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:19,Performance,perform,performSTORECombine,19,// See comments in performSTORECombine() for more details about; // these conditions.; // Code that uses clang vector extensions can mark that it; // wants unaligned accesses to be treated as fast by; // underspecifying alignment to be 1 or 2.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:166,Security,access,accesses,166,// See comments in performSTORECombine() for more details about; // these conditions.; // Code that uses clang vector extensions can mark that it; // wants unaligned accesses to be treated as fast by; // underspecifying alignment to be 1 or 2.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:84,Performance,perform,performance,84,// Disregard v2i64. Memcpy lowering produces those and splitting; // them regresses performance on micro-benchmarks and olden/bh.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:105,Testability,benchmark,benchmarks,105,// Disregard v2i64. Memcpy lowering produces those and splitting; // them regresses performance on micro-benchmarks and olden/bh.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:19,Performance,perform,performSTORECombine,19,// See comments in performSTORECombine() for more details about; // these conditions.; // Code that uses clang vector extensions can mark that it; // wants unaligned accesses to be treated as fast by; // underspecifying alignment to be 1 or 2.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:166,Security,access,accesses,166,// See comments in performSTORECombine() for more details about; // these conditions.; // Code that uses clang vector extensions can mark that it; // wants unaligned accesses to be treated as fast by; // underspecifying alignment to be 1 or 2.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:84,Performance,perform,performance,84,// Disregard v2i64. Memcpy lowering produces those and splitting; // them regresses performance on micro-benchmarks and olden/bh.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:105,Testability,benchmark,benchmarks,105,// Disregard v2i64. Memcpy lowering produces those and splitting; // them regresses performance on micro-benchmarks and olden/bh.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Availability,Mask,Mask,3,// Mask,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:146,Availability,avail,available,146,"/// changeVectorFPCCToAArch64CC - Convert a DAG fp condition code to an AArch64; /// CC usable with the vector instructions. Fewer operations are available; /// without a real NZCV register, so we have to use less efficient combinations; /// to get the same effect.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:214,Energy Efficiency,efficient,efficient,214,"/// changeVectorFPCCToAArch64CC - Convert a DAG fp condition code to an AArch64; /// CC usable with the vector instructions. Fewer operations are available; /// without a real NZCV register, so we have to use less efficient combinations; /// to get the same effect.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:88,Usability,usab,usable,88,"/// changeVectorFPCCToAArch64CC - Convert a DAG fp condition code to an AArch64; /// CC usable with the vector instructions. Fewer operations are available; /// without a real NZCV register, so we have to use less efficient combinations; /// to get the same effect.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:22,Availability,mask,mask,22,"// All of the compare-mask comparisons are ordered, but we can switch; // between the two by a double inversion. E.g. ULE == !OGT.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:204,Availability,down,down,204,"// Can a (CMP op1, (sub 0, op2) be turned into a CMN instruction on; // the grounds that ""op1 - (-op2) == op1 + op2"" ? Not always, the C and V flags; // can be set differently by this operation. It comes down to whether; // ""SInt(~op2)+1 == SInt(~op2+1)"" (and the same for UInt). If they are then; // everything is fine. If not then the optimization is wrong. Thus general; // comparisons are only valid if op2 != 0.; //; // So, finally, the only LLVM-native comparisons that don't mention C and V; // are SETEQ and SETNE. They're the only ones we can safely use CMN for in; // the absence of information about op2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:337,Performance,optimiz,optimization,337,"// Can a (CMP op1, (sub 0, op2) be turned into a CMN instruction on; // the grounds that ""op1 - (-op2) == op1 + op2"" ? Not always, the C and V flags; // can be set differently by this operation. It comes down to whether; // ""SInt(~op2)+1 == SInt(~op2+1)"" (and the same for UInt). If they are then; // everything is fine. If not then the optimization is wrong. Thus general; // comparisons are only valid if op2 != 0.; //; // So, finally, the only LLVM-native comparisons that don't mention C and V; // are SETEQ and SETNE. They're the only ones we can safely use CMN for in; // the absence of information about op2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:552,Safety,safe,safely,552,"// Can a (CMP op1, (sub 0, op2) be turned into a CMN instruction on; // the grounds that ""op1 - (-op2) == op1 + op2"" ? Not always, the C and V flags; // can be set differently by this operation. It comes down to whether; // ""SInt(~op2)+1 == SInt(~op2+1)"" (and the same for UInt). If they are then; // everything is fine. If not then the optimization is wrong. Thus general; // comparisons are only valid if op2 != 0.; //; // So, finally, the only LLVM-native comparisons that don't mention C and V; // are SETEQ and SETNE. They're the only ones we can safely use CMN for in; // the absence of information about op2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:166,Performance,perform,perform,166,"// The CMP instruction is just an alias for SUBS, and representing it as; // SUBS means that it's possible to get CSE with subtract operations.; // A later phase can perform the optimization of setting the destination; // register to WZR/XZR if it ends up being unused.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:178,Performance,optimiz,optimization,178,"// The CMP instruction is just an alias for SUBS, and representing it as; // SUBS means that it's possible to get CSE with subtract operations.; // A later phase can perform the optimization of setting the destination; // register to WZR/XZR if it ends up being unused.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:896,Testability,test,test,896,"/// \defgroup AArch64CCMP CMP;CCMP matching; ///; /// These functions deal with the formation of CMP;CCMP;... sequences.; /// The CCMP/CCMN/FCCMP/FCCMPE instructions allow the conditional execution of; /// a comparison. They set the NZCV flags to a predefined value if their; /// predicate is false. This allows to express arbitrary conjunctions, for; /// example ""cmp 0 (and (setCA (cmp A)) (setCB (cmp B)))""; /// expressed as:; /// cmp A; /// ccmp B, inv(CB), CA; /// check for CB flags; ///; /// This naturally lets us implement chains of AND operations with SETCC; /// operands. And we can even implement some other situations by transforming; /// them:; /// - We can implement (NEG SETCC) i.e. negating a single comparison by; /// negating the flags used in a CCMP/FCCMP operations.; /// - We can negate the result of a whole chain of CMP/CCMP/FCCMP operations; /// by negating the flags we test for afterwards. i.e.; /// NEG (CMP CCMP CCCMP ...) can be implemented.; /// - Note that we can only ever negate all previously processed results.; /// What we can not implement by flipping the flags to test is a negation; /// of two sub-trees (because the negation affects all sub-trees emitted so; /// far, so the 2nd sub-tree we emit would also affect the first).; /// With those tools we can implement some OR operations:; /// - (OR (SETCC A) (SETCC B)) can be implemented via:; /// NEG (AND (NEG (SETCC A)) (NEG (SETCC B))); /// - After transforming OR to NEG/AND combinations we may be able to use NEG; /// elimination rules from earlier to implement the whole thing as a; /// CCMP/FCCMP chain.; ///; /// As complete example:; /// or (or (setCA (cmp A)) (setCB (cmp B))); /// (and (setCC (cmp C)) (setCD (cmp D)))""; /// can be reassociated to:; /// or (and (setCC (cmp C)) setCD (cmp D)); // (or (setCA (cmp A)) (setCB (cmp B))); /// can be transformed to:; /// not (and (not (and (setCC (cmp C)) (setCD (cmp D)))); /// (and (not (setCA (cmp A)) (not (setCB (cmp B))))))""; /// which can be imple",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:1103,Testability,test,test,1103,"tructions allow the conditional execution of; /// a comparison. They set the NZCV flags to a predefined value if their; /// predicate is false. This allows to express arbitrary conjunctions, for; /// example ""cmp 0 (and (setCA (cmp A)) (setCB (cmp B)))""; /// expressed as:; /// cmp A; /// ccmp B, inv(CB), CA; /// check for CB flags; ///; /// This naturally lets us implement chains of AND operations with SETCC; /// operands. And we can even implement some other situations by transforming; /// them:; /// - We can implement (NEG SETCC) i.e. negating a single comparison by; /// negating the flags used in a CCMP/FCCMP operations.; /// - We can negate the result of a whole chain of CMP/CCMP/FCCMP operations; /// by negating the flags we test for afterwards. i.e.; /// NEG (CMP CCMP CCCMP ...) can be implemented.; /// - Note that we can only ever negate all previously processed results.; /// What we can not implement by flipping the flags to test is a negation; /// of two sub-trees (because the negation affects all sub-trees emitted so; /// far, so the 2nd sub-tree we emit would also affect the first).; /// With those tools we can implement some OR operations:; /// - (OR (SETCC A) (SETCC B)) can be implemented via:; /// NEG (AND (NEG (SETCC A)) (NEG (SETCC B))); /// - After transforming OR to NEG/AND combinations we may be able to use NEG; /// elimination rules from earlier to implement the whole thing as a; /// CCMP/FCCMP chain.; ///; /// As complete example:; /// or (or (setCA (cmp A)) (setCB (cmp B))); /// (and (setCC (cmp C)) (setCD (cmp D)))""; /// can be reassociated to:; /// or (and (setCC (cmp C)) setCD (cmp D)); // (or (setCA (cmp A)) (setCB (cmp B))); /// can be transformed to:; /// not (and (not (and (setCC (cmp C)) (setCD (cmp D)))); /// (and (not (setCA (cmp A)) (not (setCB (cmp B))))))""; /// which can be implemented as:; /// cmp C; /// ccmp D, inv(CD), CC; /// ccmp A, CA, inv(CD); /// ccmp B, CB, inv(CA); /// check for CB flags; ///; /// A counterexample is ""or ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:251,Testability,test,tests,251,/// Returns true if @p Val is a tree of AND/OR/SETCC operations that can be; /// expressed as a conjunction. See \ref AArch64CCMP.; /// \param CanNegate Set to true if we can negate the whole sub-tree just by; /// changing the conditions on the SETCC tests.; /// (this means we can call emitConjunctionRec() with; /// Negate==true on this sub-tree); /// \param MustBeFirst Set to true if this subtree needs to be negated and we; /// cannot do the negation naturally. We are required to; /// emit the subtree first in this case.; /// \param WillNegate Is true if are called when the result of this; /// subexpression must be negated. This happens when the; /// outer expression is an OR. We can use this fact to know; /// that we have a double negation (or (or ...) ...) that; /// can be implemented for free.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:334,Testability,test,tested,334,/// Emit conjunction or disjunction tree with the CMP/FCMP followed by a chain; /// of CCMP/CFCMP ops. See @ref AArch64CCMP.; /// Tries to transform the given i1 producing node @p Val to a series compare; /// and conditional compare operations. @returns an NZCV flags producing node; /// and sets @p OutCC to the flags that should be tested or returns SDValue() if; /// transformation was not possible.; /// \p Negate is true if we want this sub-tree being negated just by changing; /// SETCC conditions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:43,Testability,test,tested,43,// Some floating point conditions can't be tested with a single condition; // code. Construct an additional comparison in this case.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:180,Modifiability,extend,extend,180,"// Comparisons are canonicalized so that the RHS operand is simpler than the; // LHS one, the extreme case being when RHS is an immediate. However, AArch64; // can fold some shift+extend operations on the RHS operand, so swap the; // operands if that can be done.; //; // For example:; // lsl w13, w11, #1; // cmp w13, w12; // can be turned into:; // cmp w12, w11, lsl #1",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:60,Usability,simpl,simpler,60,"// Comparisons are canonicalized so that the RHS operand is simpler than the; // LHS one, the extreme case being when RHS is an immediate. However, AArch64; // can fold some shift+extend operations on the RHS operand, so swap the; // operands if that can be done.; //; // For example:; // lsl w13, w11, #1; // cmp w13, w12; // can be turned into:; // cmp w12, w11, lsl #1",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:309,Modifiability,extend,extending,309,"// The imm operand of ADDS is an unsigned immediate, in the range 0 to 4095.; // For the i8 operand, the largest immediate is 255, so this can be easily; // encoded in the compare instruction. For the i16 operand, however, the; // largest immediate cannot be encoded in the compare.; // Therefore, use a sign extending load and cmn to avoid materializing the; // -1 constant. For example,; // movz w1, #65535; // ldrh w0, [x0, #0]; // cmp w0, w1; // >; // ldrsh w0, [x0, #0]; // cmn w0, #1; // Fundamental, we're relying on the property that (zext LHS) == (zext RHS); // if and only if (sext LHS) == (sext RHS). The checks are in place to; // ensure both the LHS and RHS are truly zero extended and to make sure the; // transformation is profitable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:686,Modifiability,extend,extended,686,"// The imm operand of ADDS is an unsigned immediate, in the range 0 to 4095.; // For the i8 operand, the largest immediate is 255, so this can be easily; // encoded in the compare instruction. For the i16 operand, however, the; // largest immediate cannot be encoded in the compare.; // Therefore, use a sign extending load and cmn to avoid materializing the; // -1 constant. For example,; // movz w1, #65535; // ldrh w0, [x0, #0]; // cmp w0, w1; // >; // ldrsh w0, [x0, #0]; // cmn w0, #1; // Fundamental, we're relying on the property that (zext LHS) == (zext RHS); // if and only if (sext LHS) == (sext RHS). The checks are in place to; // ensure both the LHS and RHS are truly zero extended and to make sure the; // transformation is profitable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:319,Performance,load,load,319,"// The imm operand of ADDS is an unsigned immediate, in the range 0 to 4095.; // For the i8 operand, the largest immediate is 255, so this can be easily; // encoded in the compare instruction. For the i16 operand, however, the; // largest immediate cannot be encoded in the compare.; // Therefore, use a sign extending load and cmn to avoid materializing the; // -1 constant. For example,; // movz w1, #65535; // ldrh w0, [x0, #0]; // cmp w0, w1; // >; // ldrsh w0, [x0, #0]; // cmn w0, #1; // Fundamental, we're relying on the property that (zext LHS) == (zext RHS); // if and only if (sext LHS) == (sext RHS). The checks are in place to; // ensure both the LHS and RHS are truly zero extended and to make sure the; // transformation is profitable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:335,Safety,avoid,avoid,335,"// The imm operand of ADDS is an unsigned immediate, in the range 0 to 4095.; // For the i8 operand, the largest immediate is 255, so this can be easily; // encoded in the compare instruction. For the i16 operand, however, the; // largest immediate cannot be encoded in the compare.; // Therefore, use a sign extending load and cmn to avoid materializing the; // -1 constant. For example,; // movz w1, #65535; // ldrh w0, [x0, #0]; // cmp w0, w1; // >; // ldrsh w0, [x0, #0]; // cmn w0, #1; // Fundamental, we're relying on the property that (zext LHS) == (zext RHS); // if and only if (sext LHS) == (sext RHS). The checks are in place to; // ensure both the LHS and RHS are truly zero extended and to make sure the; // transformation is profitable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Modifiability,Extend,Extend,3,"// Extend to 64-bits, then perform a 64-bit multiply.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:27,Performance,perform,perform,27,"// Extend to 64-bits, then perform a 64-bit multiply.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:26,Performance,perform,perform,26,"// The folding we want to perform is:; // (xor x, (select_cc a, b, cc, 0, -1) ); // -->; // (csel x, (xor x, -1), cc ...); //; // The latter will get matched to a CSINV instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:29,Performance,perform,perform,29,"// If the constants line up, perform the transform!",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:46,Performance,cache,cache,46,// The locality degree is the opposite of the cache speed.; // Put the number the other way around.; // The encoding starts at 0 for level 1,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:13,Availability,mask,mask,13,// built the mask value encoding the expected behavior.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Performance,Load,Load,3,// Load/Store bit,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Performance,Cache,Cache,3,// Cache level bits,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:90,Performance,optimiz,optimization,90,// Warning: We maintain cost tables in AArch64TargetTransformInfo.cpp.; // Any additional optimization in this function should be recorded; // in the cost tables.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:128,Usability,simpl,simple,128,"// AArch64 FP-to-int conversions saturate to the destination element size, so; // we can lower common saturating conversions to simple instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:140,Performance,scalab,scalable,140,"// TODO: Consider lowering to SVE operations, as in LowerVectorFP_TO_INT.; // Currently, the `llvm.fpto[su]i.sat.*` intrinsics don't accept scalable; // types, so this is hard to reach.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:210,Usability,simpl,simpler,210,"// Otherwise we emit a cvt that saturates to a higher BW, and saturate the; // result. This is only valid if the legal cvt is larger than the saturate; // width. For double, as we don't have MIN/MAX, it can be simpler to scalarize; // (at least until sqxtn is selected).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:129,Usability,simpl,simple,129,"// AArch64 FP-to-int conversions saturate to the destination register size, so; // we can lower common saturating conversions to simple instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:90,Performance,optimiz,optimization,90,// Warning: We maintain cost tables in AArch64TargetTransformInfo.cpp.; // Any additional optimization in this function should be recorded; // in the cost tables.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:21,Modifiability,extend,extend,21,// We can't directly extend an SVE predicate; extend it first.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:46,Modifiability,extend,extend,46,// We can't directly extend an SVE predicate; extend it first.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:59,Modifiability,extend,extended,59,// The vector originally had a size of OrigTy. It was then extended to ExtTy.; // We expect the ExtTy to be 128-bits total. If the OrigTy is less than; // 64-bits we need to insert a new extension so that it will be 64-bits.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:8,Modifiability,extend,extend,8,// Must extend size to at least 64 bits to be used as an operand for VMULL.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:60,Modifiability,extend,extend,60,// Select UMULL if we can replace the other operand with an extend.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:99,Safety,detect,detected,99,// Multiplications are only custom-lowered for 128-bit and 64-bit vectors so; // that VMULL can be detected. Otherwise v2i64 multiplications are not legal.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:13,Availability,avail,available,13,// If SVE is available then i64 vector multiplications can also be made; // legal.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Performance,Optimiz,Optimizing,3,"// Optimizing (zext A + zext B) * C, to (S/UMULL A, C) + (S/UMULL B, C) during; // isel lowering to take advantage of no-stall back to back s/umul + s/umla.; // This is true for CPUs with accumulate forwarding such as Cortex-A53/A57",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:38,Performance,scalab,scalable,38,"// Returns a safe bitcast between two scalable vector predicates, where; // any newly created lanes from a widening bitcast are defined as zero.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:13,Safety,safe,safe,13,"// Returns a safe bitcast between two scalable vector predicates, where; // any newly created lanes from a widening bitcast are defined as zero.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:864,Deployability,update,updates,864,"// Lower an SME LDR/STR ZA intrinsic; // Case 1: If the vector number (vecnum) is an immediate in range, it gets; // folded into the instruction; // ldr(%tileslice, %ptr, 11) -> ldr [%tileslice, 11], [%ptr, 11]; // Case 2: If the vecnum is not an immediate, then it is used to modify the base; // and tile slice registers; // ldr(%tileslice, %ptr, %vecnum); // ->; // %svl = rdsvl; // %ptr2 = %ptr + %svl * %vecnum; // %tileslice2 = %tileslice + %vecnum; // ldr [%tileslice2, 0], [%ptr2, 0]; // Case 3: If the vecnum is an immediate out of range, then the same is done as; // case 2, but the base and slice registers are modified by the greatest; // multiple of 15 lower than the vecnum and the remainder is folded into the; // instruction. This means that successive loads and stores that are offset from; // each other can share the same base and slice register updates.; // ldr(%tileslice, %ptr, 22); // ldr(%tileslice, %ptr, 23); // ->; // %svl = rdsvl; // %ptr2 = %ptr + %svl * 15; // %tileslice2 = %tileslice + 15; // ldr [%tileslice2, 7], [%ptr2, 7]; // ldr [%tileslice2, 8], [%ptr2, 8]; // Case 4: If the vecnum is an add of an immediate, then the non-immediate; // operand and the immediate can be folded into the instruction, like case 2.; // ldr(%tileslice, %ptr, %vecnum + 7); // ldr(%tileslice, %ptr, %vecnum + 8); // ->; // %svl = rdsvl; // %ptr2 = %ptr + %svl * %vecnum; // %tileslice2 = %tileslice + %vecnum; // ldr [%tileslice2, 7], [%ptr2, 7]; // ldr [%tileslice2, 8], [%ptr2, 8]; // Case 5: The vecnum being an add of an immediate out of range is also handled,; // in which case the same remainder logic as case 3 is used.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:768,Performance,load,loads,768,"// Lower an SME LDR/STR ZA intrinsic; // Case 1: If the vector number (vecnum) is an immediate in range, it gets; // folded into the instruction; // ldr(%tileslice, %ptr, 11) -> ldr [%tileslice, 11], [%ptr, 11]; // Case 2: If the vecnum is not an immediate, then it is used to modify the base; // and tile slice registers; // ldr(%tileslice, %ptr, %vecnum); // ->; // %svl = rdsvl; // %ptr2 = %ptr + %svl * %vecnum; // %tileslice2 = %tileslice + %vecnum; // ldr [%tileslice2, 0], [%ptr2, 0]; // Case 3: If the vecnum is an immediate out of range, then the same is done as; // case 2, but the base and slice registers are modified by the greatest; // multiple of 15 lower than the vecnum and the remainder is folded into the; // instruction. This means that successive loads and stores that are offset from; // each other can share the same base and slice register updates.; // ldr(%tileslice, %ptr, 22); // ldr(%tileslice, %ptr, 23); // ->; // %svl = rdsvl; // %ptr2 = %ptr + %svl * 15; // %tileslice2 = %tileslice + 15; // ldr [%tileslice2, 7], [%ptr2, 7]; // ldr [%tileslice2, 8], [%ptr2, 8]; // Case 4: If the vecnum is an add of an immediate, then the non-immediate; // operand and the immediate can be folded into the instruction, like case 2.; // ldr(%tileslice, %ptr, %vecnum + 7); // ldr(%tileslice, %ptr, %vecnum + 8); // ->; // %svl = rdsvl; // %ptr2 = %ptr + %svl * %vecnum; // %tileslice2 = %tileslice + %vecnum; // ldr [%tileslice2, 7], [%ptr2, 7]; // ldr [%tileslice2, 8], [%ptr2, 8]; // Case 5: The vecnum being an add of an immediate out of range is also handled,; // in which case the same remainder logic as case 3 is used.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:1617,Testability,log,logic,1617,"// Lower an SME LDR/STR ZA intrinsic; // Case 1: If the vector number (vecnum) is an immediate in range, it gets; // folded into the instruction; // ldr(%tileslice, %ptr, 11) -> ldr [%tileslice, 11], [%ptr, 11]; // Case 2: If the vecnum is not an immediate, then it is used to modify the base; // and tile slice registers; // ldr(%tileslice, %ptr, %vecnum); // ->; // %svl = rdsvl; // %ptr2 = %ptr + %svl * %vecnum; // %tileslice2 = %tileslice + %vecnum; // ldr [%tileslice2, 0], [%ptr2, 0]; // Case 3: If the vecnum is an immediate out of range, then the same is done as; // case 2, but the base and slice registers are modified by the greatest; // multiple of 15 lower than the vecnum and the remainder is folded into the; // instruction. This means that successive loads and stores that are offset from; // each other can share the same base and slice register updates.; // ldr(%tileslice, %ptr, 22); // ldr(%tileslice, %ptr, 23); // ->; // %svl = rdsvl; // %ptr2 = %ptr + %svl * 15; // %tileslice2 = %tileslice + 15; // ldr [%tileslice2, 7], [%ptr2, 7]; // ldr [%tileslice2, 8], [%ptr2, 8]; // Case 4: If the vecnum is an add of an immediate, then the non-immediate; // operand and the immediate can be folded into the instruction, like case 2.; // ldr(%tileslice, %ptr, %vecnum + 7); // ldr(%tileslice, %ptr, %vecnum + 8); // ->; // %svl = rdsvl; // %ptr2 = %ptr + %svl * %vecnum; // %tileslice2 = %tileslice + %vecnum; // ldr [%tileslice2, 7], [%ptr2, 7]; // ldr [%tileslice2, 8], [%ptr2, 8]; // Case 5: The vecnum being an add of an immediate out of range is also handled,; // in which case the same remainder logic as case 3 is used.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Performance,Load,Load,3,// Load/Store bit,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Performance,Cache,Cache,3,// Cache level bits,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:245,Performance,load,load,245,"// 'aarch64_neon_pmull64' takes i64 parameters; while pmull/pmull2; // instructions execute on SIMD registers. So canonicalize i64 to v1i64,; // which ISel recognizes better. For example, generate a ldr into d*; // registers as opposed to a GPR load followed by a fmov.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:44,Modifiability,rewrite,rewrite,44,"// If the operand is an higher half itself, rewrite it to; // extract_high_v2i64; this way aarch64_neon_pmull64 could; // re-use the dag-combiner function with aarch64_neon_{pmull,smull,umull}.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:36,Modifiability,rewrite,rewrite,36,"// If this operand is a lower half, rewrite it to; // extract_high_v2i64(duplane(<2 x Ty>, 0)). This saves a roundtrip to; // align lanes of two operands. A roundtrip sequence (to move from lane; // 1 to lane 0) is like this:; // mov x8, v0.d[1]; // fmov d0, x8",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:104,Usability,simpl,simply,104,// FIXME: This needs to be implemented to correctly handle highly aligned; // stack objects. For now we simply return the incoming FP. Refer D53541; // for more details.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:15,Safety,avoid,avoid,15,"// In order to avoid insert_subvector, used v4i32 than v2i32.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Performance,Scalab,Scalable,3,"// Scalable vectors with ""vscale * 2"" or fewer elements sit within a 64-bit; // element container type, which would violate the previous clause.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:38,Availability,mask,masked,38,"// It may be worth creating extending masked loads if there are multiple; // masked loads using the same predicate. That way we'll end up creating; // extending masked loads that may then get split by the legaliser. This; // results in just one set of predicate unpacks at the start, instead of; // multiple sets of vector unpacks after each load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:77,Availability,mask,masked,77,"// It may be worth creating extending masked loads if there are multiple; // masked loads using the same predicate. That way we'll end up creating; // extending masked loads that may then get split by the legaliser. This; // results in just one set of predicate unpacks at the start, instead of; // multiple sets of vector unpacks after each load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:161,Availability,mask,masked,161,"// It may be worth creating extending masked loads if there are multiple; // masked loads using the same predicate. That way we'll end up creating; // extending masked loads that may then get split by the legaliser. This; // results in just one set of predicate unpacks at the start, instead of; // multiple sets of vector unpacks after each load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:28,Modifiability,extend,extending,28,"// It may be worth creating extending masked loads if there are multiple; // masked loads using the same predicate. That way we'll end up creating; // extending masked loads that may then get split by the legaliser. This; // results in just one set of predicate unpacks at the start, instead of; // multiple sets of vector unpacks after each load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:151,Modifiability,extend,extending,151,"// It may be worth creating extending masked loads if there are multiple; // masked loads using the same predicate. That way we'll end up creating; // extending masked loads that may then get split by the legaliser. This; // results in just one set of predicate unpacks at the start, instead of; // multiple sets of vector unpacks after each load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:45,Performance,load,loads,45,"// It may be worth creating extending masked loads if there are multiple; // masked loads using the same predicate. That way we'll end up creating; // extending masked loads that may then get split by the legaliser. This; // results in just one set of predicate unpacks at the start, instead of; // multiple sets of vector unpacks after each load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:84,Performance,load,loads,84,"// It may be worth creating extending masked loads if there are multiple; // masked loads using the same predicate. That way we'll end up creating; // extending masked loads that may then get split by the legaliser. This; // results in just one set of predicate unpacks at the start, instead of; // multiple sets of vector unpacks after each load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:168,Performance,load,loads,168,"// It may be worth creating extending masked loads if there are multiple; // masked loads using the same predicate. That way we'll end up creating; // extending masked loads that may then get split by the legaliser. This; // results in just one set of predicate unpacks at the start, instead of; // multiple sets of vector unpacks after each load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:342,Performance,load,load,342,"// It may be worth creating extending masked loads if there are multiple; // masked loads using the same predicate. That way we'll end up creating; // extending masked loads that may then get split by the legaliser. This; // results in just one set of predicate unpacks at the start, instead of; // multiple sets of vector unpacks after each load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:21,Availability,mask,masked,21,"// Disable extending masked loads for fixed-width for now, since the code; // quality doesn't look great.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:11,Modifiability,extend,extending,11,"// Disable extending masked loads for fixed-width for now, since the code; // quality doesn't look great.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:28,Performance,load,loads,28,"// Disable extending masked loads for fixed-width for now, since the code; // quality doesn't look great.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:2,Modifiability,Extend,Extend,2,/*Extend*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:2,Modifiability,Extend,Extend,2,/*Extend*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:2,Modifiability,Extend,Extend,2,/*Extend*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:2,Modifiability,Extend,Extend,2,/*Extend*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:2,Modifiability,Extend,Extend,2,/*Extend*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:2,Modifiability,Extend,Extend,2,/*Extend*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:2,Modifiability,Extend,Extend,2,/*Extend*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:2,Modifiability,Extend,Extend,2,/*Extend*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:135,Performance,load,load,135,"// SVE supports zero (and so undef) passthrough values only, everything else; // must be handled manually by an explicit select on the load's output.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:34,Performance,scalab,scalable,34,// Lower fixed length gather to a scalable equivalent.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:49,Modifiability,extend,extending,49,// A promoted result type forces the need for an extending load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:59,Performance,load,load,59,// A promoted result type forces the need for an extending load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:43,Performance,scalab,scalable,43,// Convert fixed length vector operands to scalable.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:19,Performance,scalab,scalable,19,// Emit equivalent scalable vector gather.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:35,Performance,scalab,scalable,35,// Lower fixed length scatter to a scalable equivalent.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:43,Performance,scalab,scalable,43,// Convert fixed length vector operands to scalable.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:19,Performance,scalab,scalable,19,// Emit equivalent scalable vector scatter.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:12,Modifiability,extend,extend,12,"// It first extend the promoted v4i16 to v8i16, truncate to v8i8, and extract; // the word lane which represent the v4i8 subvector. It optimizes the store; // to:; //; // xtn v0.8b, v0.8h; // str s0, [x0]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:135,Performance,optimiz,optimizes,135,"// It first extend the promoted v4i16 to v8i16, truncate to v8i8, and extract; // the word lane which represent the v4i8 subvector. It optimizes the store; // to:; //; // xtn v0.8b, v0.8h; // str s0, [x0]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:23,Modifiability,extend,extending,23,// Custom lowering for extending v4i8 vector loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:45,Performance,load,loads,45,// Custom lowering for extending v4i8 vector loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Safety,Avoid,Avoid,3,// Avoid reassociating expressions that can be lowered to smlal/umlal.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Energy Efficiency,Allocate,Allocate,3,// Allocate a lazy-save buffer object of size SVL.B * SVL.B (worst-case),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Energy Efficiency,Allocate,Allocate,3,// Allocate an additional TPIDR2 object on the stack (16 bytes),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:130,Energy Efficiency,schedul,scheduled,130,"// LocallyStreamingFunctions must insert the SMSTART in the correct; // position, so we use Glue to ensure no instructions can be scheduled; // between the chain of:; // t0: ch,glue = EntryNode; // t1: res,ch,glue = CopyFromReg; // ...; // tn: res,ch,glue = CopyFromReg t(n-1), ..; // t(n+1): ch, glue = SMSTART t0:0, ...., tn:2; // ^^^^^^; // This will be the new Chain/Root node.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:95,Testability,assert,assert,95,"// If this is an 8, 16 or 32-bit value, it is really passed promoted; // to 64 bits. Insert an assert[sz]ext to capture this, then; // truncate to the right size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:99,Security,access,accessed,99,"// In both the ARM64EC varargs convention and the thunk convention,; // arguments on the stack are accessed relative to x4, not sp. In; // the thunk convention, there's an additional offset of 32 bytes; // to account for the shadow store.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:10,Performance,load,load,10,// Create load nodes to retrieve arguments from the stack.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:44,Testability,assert,assert,44,"// For NON_EXTLOAD, generic code in getLoad assert(ValVT == MemVT)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:26,Performance,load,loads,26,"// Ensure we generate all loads for each tuple part, whilst updating the; // pointer after each load correctly using vscale.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:96,Performance,load,load,96,"// Ensure we generate all loads for each tuple part, whilst updating the; // pointer after each load correctly using vscale.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:25,Modifiability,extend,extended,25,// i1 arguments are zero-extended to i8 by the caller. Emit a; // hint to reflect this.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:40,Availability,avail,available,40,// This realignment carries over to the available bytes below. Our own; // callers will guarantee the space is free by giving an aligned value to; // CALLSEQ_START.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:68,Safety,avoid,avoid,68,"// Pass 'this' value directly from the argument to return value, to avoid; // reg unit interference",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Safety,Avoid,Avoid,3,// Avoid copying a physreg twice since RegAllocFast is incompetent and only; // allows one use of a physreg per block.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:23,Energy Efficiency,allocate,allocate,23,"// For Arm64EC thunks, allocate 32 extra bytes at the bottom of the stack; // for the shadow store.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:175,Energy Efficiency,efficient,efficient,175,// Byval parameters hand the function a pointer directly into the stack area; // we want to reuse during a tail call. Working around this *is* possible (see; // X86) but less efficient and uglier in LowerCall.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:260,Testability,assert,assert,260,// Now we search for cases where we can use a tail call without changing the; // ABI. Sibcall is used in some places (particularly gcc) to refer to this; // concept.; // I want anyone implementing a new calling convention to think long and hard; // about this assert.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:69,Safety,safe,safely,69,"// When we are musttail, additional checks have been done and we can safely ignore this check; // At least two cases here: if caller is fastcc then we can't have any; // memory arguments (we'd be expected to clean up the stack afterwards). If; // caller is C then we could potentially use its argument area.; // FIXME: for now we take the most conservative of these in both cases:; // disallow all variadic memory operands.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:155,Energy Efficiency,allocate,allocate,155,"// If any of the arguments is passed indirectly, it must be SVE, so the; // 'getBytesInStackArgArea' is not sufficient to determine whether we need to; // allocate space on the stack. That is why we determine this explicitly here; // the call cannot be a tailcall.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:30,Modifiability,extend,extended,30,// Check if the value is zero-extended from i1 to i8,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:232,Safety,avoid,avoid,232,"// Live-in physreg copies that are glued to SMSTART are applied as; // implicit-def's in the InstrEmitter. Here we remove them, allowing the; // register allocator to pass call args in callee saved regs, without extra; // copies to avoid these fake clobbers of actually-preserved GPRs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:98,Availability,avail,available,98,"// Since we're not changing the ABI to make this a tail call, the memory; // operands are already available in the caller's incoming argument space.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Deployability,Update,Update,3,// Update the required reserved area if this is the tail call requiring the; // most argument stack space.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:58,Performance,load,loads,58,"// Walk the register/memloc assignments, inserting copies/loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:32,Modifiability,extend,extended,32,"// AAPCS requires i1 to be zero-extended to 8-bits by the caller.; //; // Check if we actually have to do this, because the value may; // already be zero-extended.; //; // We cannot just emit a (zext i8 (trunc (assert-zext i8))); // and rely on DAGCombiner to fold this, because the following; // (anyext i32) is combined with (zext i8) in DAG.getNode:; //; // (ext (zext x)) -> (zext x); //; // This will give us (zext i32), which we cannot remove, so; // try to check this beforehand.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:154,Modifiability,extend,extended,154,"// AAPCS requires i1 to be zero-extended to 8-bits by the caller.; //; // Check if we actually have to do this, because the value may; // already be zero-extended.; //; // We cannot just emit a (zext i8 (trunc (assert-zext i8))); // and rely on DAGCombiner to fold this, because the following; // (anyext i32) is combined with (zext i8) in DAG.getNode:; //; // (ext (zext x)) -> (zext x); //; // This will give us (zext i32), which we cannot remove, so; // try to check this beforehand.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:211,Testability,assert,assert-zext,211,"// AAPCS requires i1 to be zero-extended to 8-bits by the caller.; //; // Check if we actually have to do this, because the value may; // already be zero-extended.; //; // We cannot just emit a (zext i8 (trunc (assert-zext i8))); // and rely on DAGCombiner to fold this, because the following; // (anyext i32) is combined with (zext i8) in DAG.getNode:; //; // (ext (zext x)) -> (zext x); //; // This will give us (zext i32), which we cannot remove, so; // try to check this beforehand.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:99,Usability,simpl,simple,99,// Call site info is used for function's parameter entry value; // tracking. For now we track only simple cases when parameter; // is transferred through whole register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:175,Usability,simpl,simple,175,// Add an extra level of indirection for streaming mode changes by; // using a pseudo copy node that cannot be rematerialised between a; // smstart/smstop and the call by the simple register coalescer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:78,Performance,load,loaded,78,// Make sure any stack arguments overlapping with where we're storing; // are loaded before this eventual operation. Otherwise they'll be; // clobbered.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:148,Energy Efficiency,consumption,consumption,148,"// Each tail call may have to adjust the stack by a different amount, so; // this information must travel along with the operation for eventual; // consumption by emitEpilogue.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:18,Availability,mask,mask,18,// Add a register mask operand representing the call-preserved registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:45,Availability,mask,mask,45,"// For 'this' returns, use the X0-preserving mask if applicable",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:19,Usability,resume,resume,19,// Unconditionally resume ZA.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:98,Availability,redundant,redundant,98,"// AAPCS requires i1 to be zero-extended to i8 by the producer of the; // value. This is strictly redundant on Darwin (which uses ""zeroext; // i1""), but will be optimised out before ISel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:32,Modifiability,extend,extended,32,"// AAPCS requires i1 to be zero-extended to i8 by the producer of the; // value. This is strictly redundant on Darwin (which uses ""zeroext; // i1""), but will be optimised out before ISel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:98,Safety,redund,redundant,98,"// AAPCS requires i1 to be zero-extended to i8 by the producer of the; // value. This is strictly redundant on Darwin (which uses ""zeroext; // i1""), but will be optimised out before ISel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Deployability,Update,Update,3,// Update chain.; // Add the glue if we have it.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:4,Performance,load,loadGOT,4,// (loadGOT sym),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:135,Integrability,wrap,wrapper,135,"// FIXME: Once remat is capable of dealing with instructions with register; // operands, expand this into two nodes instead of using a wrapper node.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:4,Integrability,wrap,wrapper,4,"// (wrapper %highest(sym), %higher(sym), %hi(sym), %lo(sym))",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:101,Modifiability,variab,variable,101,"/// Convert a TLS address reference into the correct sequence of loads; /// and calls to compute the variable's address (for Darwin, currently) and; /// return an SDValue containing the final node.; /// Darwin only has one TLS scheme which must be capable of dealing with the; /// fully general situation, in the worst case. This means:; /// + ""extern __thread"" declaration.; /// + Defined in a possibly unknown dynamic library.; ///; /// The general system is that each __thread variable has a [3 x i64] descriptor; /// which contains information used by the runtime to calculate the address. The; /// only part of this the compiler needs to know about is the first xword, which; /// contains a function pointer that must be called with the address of the; /// entire descriptor in ""x0"".; ///; /// Since this descriptor may be in a different unit, in general even the; /// descriptor must be accessed via an indirect load. The ""ideal"" code sequence; /// is:; /// adrp x0, _var@TLVPPAGE; /// ldr x0, [x0, _var@TLVPPAGEOFF] ; x0 now contains address of descriptor; /// ldr x1, [x0] ; x1 contains 1st entry of descriptor,; /// ; the function pointer; /// blr x1 ; Uses descriptor address in x0; /// ; Address of _var is now in x0.; ///; /// If the address of _var's descriptor *is* known to the linker, then it can; /// change the first ""ldr"" instruction to an appropriate ""add x0, x0, #imm"" for; /// a slight efficiency gain.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:480,Modifiability,variab,variable,480,"/// Convert a TLS address reference into the correct sequence of loads; /// and calls to compute the variable's address (for Darwin, currently) and; /// return an SDValue containing the final node.; /// Darwin only has one TLS scheme which must be capable of dealing with the; /// fully general situation, in the worst case. This means:; /// + ""extern __thread"" declaration.; /// + Defined in a possibly unknown dynamic library.; ///; /// The general system is that each __thread variable has a [3 x i64] descriptor; /// which contains information used by the runtime to calculate the address. The; /// only part of this the compiler needs to know about is the first xword, which; /// contains a function pointer that must be called with the address of the; /// entire descriptor in ""x0"".; ///; /// Since this descriptor may be in a different unit, in general even the; /// descriptor must be accessed via an indirect load. The ""ideal"" code sequence; /// is:; /// adrp x0, _var@TLVPPAGE; /// ldr x0, [x0, _var@TLVPPAGEOFF] ; x0 now contains address of descriptor; /// ldr x1, [x0] ; x1 contains 1st entry of descriptor,; /// ; the function pointer; /// blr x1 ; Uses descriptor address in x0; /// ; Address of _var is now in x0.; ///; /// If the address of _var's descriptor *is* known to the linker, then it can; /// change the first ""ldr"" instruction to an appropriate ""add x0, x0, #imm"" for; /// a slight efficiency gain.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:65,Performance,load,loads,65,"/// Convert a TLS address reference into the correct sequence of loads; /// and calls to compute the variable's address (for Darwin, currently) and; /// return an SDValue containing the final node.; /// Darwin only has one TLS scheme which must be capable of dealing with the; /// fully general situation, in the worst case. This means:; /// + ""extern __thread"" declaration.; /// + Defined in a possibly unknown dynamic library.; ///; /// The general system is that each __thread variable has a [3 x i64] descriptor; /// which contains information used by the runtime to calculate the address. The; /// only part of this the compiler needs to know about is the first xword, which; /// contains a function pointer that must be called with the address of the; /// entire descriptor in ""x0"".; ///; /// Since this descriptor may be in a different unit, in general even the; /// descriptor must be accessed via an indirect load. The ""ideal"" code sequence; /// is:; /// adrp x0, _var@TLVPPAGE; /// ldr x0, [x0, _var@TLVPPAGEOFF] ; x0 now contains address of descriptor; /// ldr x1, [x0] ; x1 contains 1st entry of descriptor,; /// ; the function pointer; /// blr x1 ; Uses descriptor address in x0; /// ; Address of _var is now in x0.; ///; /// If the address of _var's descriptor *is* known to the linker, then it can; /// change the first ""ldr"" instruction to an appropriate ""add x0, x0, #imm"" for; /// a slight efficiency gain.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:918,Performance,load,load,918,"/// Convert a TLS address reference into the correct sequence of loads; /// and calls to compute the variable's address (for Darwin, currently) and; /// return an SDValue containing the final node.; /// Darwin only has one TLS scheme which must be capable of dealing with the; /// fully general situation, in the worst case. This means:; /// + ""extern __thread"" declaration.; /// + Defined in a possibly unknown dynamic library.; ///; /// The general system is that each __thread variable has a [3 x i64] descriptor; /// which contains information used by the runtime to calculate the address. The; /// only part of this the compiler needs to know about is the first xword, which; /// contains a function pointer that must be called with the address of the; /// entire descriptor in ""x0"".; ///; /// Since this descriptor may be in a different unit, in general even the; /// descriptor must be accessed via an indirect load. The ""ideal"" code sequence; /// is:; /// adrp x0, _var@TLVPPAGE; /// ldr x0, [x0, _var@TLVPPAGEOFF] ; x0 now contains address of descriptor; /// ldr x1, [x0] ; x1 contains 1st entry of descriptor,; /// ; the function pointer; /// blr x1 ; Uses descriptor address in x0; /// ; Address of _var is now in x0.; ///; /// If the address of _var's descriptor *is* known to the linker, then it can; /// change the first ""ldr"" instruction to an appropriate ""add x0, x0, #imm"" for; /// a slight efficiency gain.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:893,Security,access,accessed,893,"/// Convert a TLS address reference into the correct sequence of loads; /// and calls to compute the variable's address (for Darwin, currently) and; /// return an SDValue containing the final node.; /// Darwin only has one TLS scheme which must be capable of dealing with the; /// fully general situation, in the worst case. This means:; /// + ""extern __thread"" declaration.; /// + Defined in a possibly unknown dynamic library.; ///; /// The general system is that each __thread variable has a [3 x i64] descriptor; /// which contains information used by the runtime to calculate the address. The; /// only part of this the compiler needs to know about is the first xword, which; /// contains a function pointer that must be called with the address of the; /// entire descriptor in ""x0"".; ///; /// Since this descriptor may be in a different unit, in general even the; /// descriptor must be accessed via an indirect load. The ""ideal"" code sequence; /// is:; /// adrp x0, _var@TLVPPAGE; /// ldr x0, [x0, _var@TLVPPAGEOFF] ; x0 now contains address of descriptor; /// ldr x1, [x0] ; x1 contains 1st entry of descriptor,; /// ; the function pointer; /// blr x1 ; Uses descriptor address in x0; /// ; Address of _var is now in x0.; ///; /// If the address of _var's descriptor *is* known to the linker, then it can; /// change the first ""ldr"" instruction to an appropriate ""add x0, x0, #imm"" for; /// a slight efficiency gain.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:110,Modifiability,variab,variable,110,// The first entry in the descriptor is a function pointer that we must call; // to obtain the address of the variable.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Modifiability,Extend,Extend,3,// Extend loaded pointer if necessary (i.e. if ILP32) to DAG pointer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:10,Performance,load,loaded,10,// Extend loaded pointer if necessary (i.e. if ILP32) to DAG pointer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:178,Modifiability,variab,variable,178,"// Finally, we can make the call. This is just a degenerate version of a; // normal AArch64 call node: x0 takes the address of the descriptor, and; // returns the address of the variable in this thread.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:180,Integrability,depend,depends,180,/// Convert a thread-local variable reference into a sequence of instructions to; /// compute the variable's address for the local exec TLS model of ELF targets.; /// The sequence depends on the maximum TLS area size.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:27,Modifiability,variab,variable,27,/// Convert a thread-local variable reference into a sequence of instructions to; /// compute the variable's address for the local exec TLS model of ELF targets.; /// The sequence depends on the maximum TLS area size.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:98,Modifiability,variab,variable,98,/// Convert a thread-local variable reference into a sequence of instructions to; /// compute the variable's address for the local exec TLS model of ELF targets.; /// The sequence depends on the maximum TLS area size.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:32,Modifiability,variab,variables,32,"/// When accessing thread-local variables under either the general-dynamic or; /// local-dynamic system, we make a ""TLS-descriptor"" call. The variable will; /// have a descriptor, accessible via a PC-relative ADRP, and whose first entry; /// is a function pointer to carry out the resolution.; ///; /// The sequence is:; /// adrp x0, :tlsdesc:var; /// ldr x1, [x0, #:tlsdesc_lo12:var]; /// add x0, x0, #:tlsdesc_lo12:var; /// .tlsdesccall var; /// blr x1; /// (TPIDR_EL0 offset now in x0); ///; /// The above sequence must be produced unscheduled, to enable the linker to; /// optimize/relax this sequence.; /// Therefore, a pseudo-instruction (TLSDESC_CALLSEQ) is used to represent the; /// above sequence, and expanded really late in the compilation flow, to ensure; /// the sequence is produced as per above.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:142,Modifiability,variab,variable,142,"/// When accessing thread-local variables under either the general-dynamic or; /// local-dynamic system, we make a ""TLS-descriptor"" call. The variable will; /// have a descriptor, accessible via a PC-relative ADRP, and whose first entry; /// is a function pointer to carry out the resolution.; ///; /// The sequence is:; /// adrp x0, :tlsdesc:var; /// ldr x1, [x0, #:tlsdesc_lo12:var]; /// add x0, x0, #:tlsdesc_lo12:var; /// .tlsdesccall var; /// blr x1; /// (TPIDR_EL0 offset now in x0); ///; /// The above sequence must be produced unscheduled, to enable the linker to; /// optimize/relax this sequence.; /// Therefore, a pseudo-instruction (TLSDESC_CALLSEQ) is used to represent the; /// above sequence, and expanded really late in the compilation flow, to ensure; /// the sequence is produced as per above.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:577,Performance,optimiz,optimize,577,"/// When accessing thread-local variables under either the general-dynamic or; /// local-dynamic system, we make a ""TLS-descriptor"" call. The variable will; /// have a descriptor, accessible via a PC-relative ADRP, and whose first entry; /// is a function pointer to carry out the resolution.; ///; /// The sequence is:; /// adrp x0, :tlsdesc:var; /// ldr x1, [x0, #:tlsdesc_lo12:var]; /// add x0, x0, #:tlsdesc_lo12:var; /// .tlsdesccall var; /// blr x1; /// (TPIDR_EL0 offset now in x0); ///; /// The above sequence must be produced unscheduled, to enable the linker to; /// optimize/relax this sequence.; /// Therefore, a pseudo-instruction (TLSDESC_CALLSEQ) is used to represent the; /// above sequence, and expanded really late in the compilation flow, to ensure; /// the sequence is produced as per above.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:9,Security,access,accessing,9,"/// When accessing thread-local variables under either the general-dynamic or; /// local-dynamic system, we make a ""TLS-descriptor"" call. The variable will; /// have a descriptor, accessible via a PC-relative ADRP, and whose first entry; /// is a function pointer to carry out the resolution.; ///; /// The sequence is:; /// adrp x0, :tlsdesc:var; /// ldr x1, [x0, #:tlsdesc_lo12:var]; /// add x0, x0, #:tlsdesc_lo12:var; /// .tlsdesccall var; /// blr x1; /// (TPIDR_EL0 offset now in x0); ///; /// The above sequence must be produced unscheduled, to enable the linker to; /// optimize/relax this sequence.; /// Therefore, a pseudo-instruction (TLSDESC_CALLSEQ) is used to represent the; /// above sequence, and expanded really late in the compilation flow, to ensure; /// the sequence is produced as per above.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:180,Security,access,accessible,180,"/// When accessing thread-local variables under either the general-dynamic or; /// local-dynamic system, we make a ""TLS-descriptor"" call. The variable will; /// have a descriptor, accessible via a PC-relative ADRP, and whose first entry; /// is a function pointer to carry out the resolution.; ///; /// The sequence is:; /// adrp x0, :tlsdesc:var; /// ldr x1, [x0, #:tlsdesc_lo12:var]; /// add x0, x0, #:tlsdesc_lo12:var; /// .tlsdesccall var; /// blr x1; /// (TPIDR_EL0 offset now in x0); ///; /// The above sequence must be produced unscheduled, to enable the linker to; /// optimize/relax this sequence.; /// Therefore, a pseudo-instruction (TLSDESC_CALLSEQ) is used to represent the; /// above sequence, and expanded really late in the compilation flow, to ensure; /// the sequence is produced as per above.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:242,Security,access,access,242,"// Different choices can be made for the maximum size of the TLS area for a; // module. For the small address model, the default TLS size is 16MiB and the; // maximum TLS size is 4GiB.; // FIXME: add tiny and large code model support for TLS access models other; // than local exec. We currently generate the same code as small for tiny,; // which may be larger than needed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:17,Security,access,accesses,17,"// Local-dynamic accesses proceed in two phases. A general-dynamic TLS; // descriptor call against the special symbol _TLS_MODULE_BASE_ to calculate; // the beginning of the module's TLS region, followed by a DTPREL offset; // calculation.; // These accesses will need deduplicating if there's more than one.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:250,Security,access,accesses,250,"// Local-dynamic accesses proceed in two phases. A general-dynamic TLS; // descriptor call against the special symbol _TLS_MODULE_BASE_ to calculate; // the beginning of the module's TLS region, followed by a DTPREL offset; // calculation.; // These accesses will need deduplicating if there's more than one.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:58,Modifiability,variab,variable,58,// Now use :dtprel_whatever: operations to calculate this variable's offset; // in its thread-storage area.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Performance,Load,Load,3,// Load the ThreadLocalStoragePointer from the TEB; // A pointer to the TLS array is located at offset 0x58 from the TEB.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Performance,Load,Load,3,"// Load the TLS index from the C runtime;; // This does the same as getAddr(), but without having a GlobalAddressSDNode.; // This also does the same as LOADgot, but using a generic i32 load,; // while LOADgot only loads i64.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:152,Performance,LOAD,LOADgot,152,"// Load the TLS index from the C runtime;; // This does the same as getAddr(), but without having a GlobalAddressSDNode.; // This also does the same as LOADgot, but using a generic i32 load,; // while LOADgot only loads i64.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:185,Performance,load,load,185,"// Load the TLS index from the C runtime;; // This does the same as getAddr(), but without having a GlobalAddressSDNode.; // This also does the same as LOADgot, but using a generic i32 load,; // while LOADgot only loads i64.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:201,Performance,LOAD,LOADgot,201,"// Load the TLS index from the C runtime;; // This does the same as getAddr(), but without having a GlobalAddressSDNode.; // This also does the same as LOADgot, but using a generic i32 load,; // while LOADgot only loads i64.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:214,Performance,load,loads,214,"// Load the TLS index from the C runtime;; // This does the same as getAddr(), but without having a GlobalAddressSDNode.; // This also does the same as LOADgot, but using a generic i32 load,; // while LOADgot only loads i64.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:41,Performance,optimiz,optimized,41,"// Speculation tracking/SLH assumes that optimized TB(N)Z/CB(N)Z instructions; // will not be produced, as they are conditional branch instructions that do; // not set flags.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Performance,Optimiz,Optimize,3,// Optimize {s|u}{add|sub|mul}.with.overflow feeding into a branch; // instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:163,Modifiability,rewrite,rewrites,163,"// See if we can use a TBZ to fold in an AND as well.; // TBZ has a smaller branch displacement than CBZ. If the offset is; // out of bounds, a late MI-layer pass rewrites branches.; // 403.gcc is an example that hits this case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:163,Modifiability,rewrite,rewrites,163,"// See if we can use a TBZ to fold in an AND as well.; // TBZ has a smaller branch displacement than CBZ. If the offset is; // out of bounds, a late MI-layer pass rewrites branches.; // 403.gcc is an example that hits this case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:151,Availability,redundant,redundant,151,// Don't combine AND since emitComparison converts the AND to an ANDS; // (a.k.a. TST) and the test in the test bit and branch instruction; // becomes redundant. This would also increase register pressure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:151,Safety,redund,redundant,151,// Don't combine AND since emitComparison converts the AND to an ANDS; // (a.k.a. TST) and the test in the test bit and branch instruction; // becomes redundant. This would also increase register pressure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:95,Testability,test,test,95,// Don't combine AND since emitComparison converts the AND to an ANDS; // (a.k.a. TST) and the test in the test bit and branch instruction; // becomes redundant. This would also increase register pressure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:107,Testability,test,test,107,// Don't combine AND since emitComparison converts the AND to an ANDS; // (a.k.a. TST) and the test in the test bit and branch instruction; // becomes redundant. This would also increase register pressure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:151,Availability,redundant,redundant,151,// Don't combine AND since emitComparison converts the AND to an ANDS; // (a.k.a. TST) and the test in the test bit and branch instruction; // becomes redundant. This would also increase register pressure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:151,Safety,redund,redundant,151,// Don't combine AND since emitComparison converts the AND to an ANDS; // (a.k.a. TST) and the test in the test bit and branch instruction; // becomes redundant. This would also increase register pressure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:95,Testability,test,test,95,// Don't combine AND since emitComparison converts the AND to an ANDS; // (a.k.a. TST) and the test in the test bit and branch instruction; // becomes redundant. This would also increase register pressure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:107,Testability,test,test,107,// Don't combine AND since emitComparison converts the AND to an ANDS; // (a.k.a. TST) and the test in the test bit and branch instruction; // becomes redundant. This would also increase register pressure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:28,Availability,mask,mask,28,"// We want to materialize a mask with every bit but the high bit set, but the; // AdvSIMD immediate moves cannot materialize that in a single instruction for; // 64-bit elements. Instead, materialize all bits set and then negate that.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:55,Energy Efficiency,efficient,efficient,55,"// for i32, general parity function using EORs is more efficient compared to; // using floating point",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:34,Availability,avail,available,34,"// If there is no CNT instruction available, GPR popcount can; // be more efficiently lowered to the following sequence that uses; // AdvSIMD registers/instructions as long as the copies to/from; // the AdvSIMD registers are cheap.; // FMOV D0, X0 // copy 64-bit int to vector, high bits zero'd; // CNT V0.8B, V0.8B // 8xbyte pop-counts; // ADDV B0, V0.8B // sum 8xbyte pop-counts; // UMOV X0, V0.B[0] // copy byte result back to integer reg",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:74,Energy Efficiency,efficient,efficiently,74,"// If there is no CNT instruction available, GPR popcount can; // be more efficiently lowered to the following sequence that uses; // AdvSIMD registers/instructions as long as the copies to/from; // the AdvSIMD registers are cheap.; // FMOV D0, X0 // copy 64-bit int to vector, high bits zero'd; // CNT V0.8B, V0.8B // 8xbyte pop-counts; // ADDV B0, V0.8B // sum 8xbyte pop-counts; // UMOV X0, V0.B[0] // copy byte result back to integer reg",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:21,Deployability,continuous,continuous,21,// Check whether the continuous comparison sequence.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:53,Energy Efficiency,reduce,reduce,53,"// Exit early by inverting the condition, which help reduce indentations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:32,Performance,perform,perform,32,"// If that fails, we'll need to perform an FCMP + CSEL sequence. Go ahead; // and do the comparison.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:173,Integrability,wrap,wrap,173,"// If our operands are only 32-bit wide, make sure we use 32-bit; // arithmetic for the check whether we can use CSINC. This ensures that; // the addition in the check will wrap around properly in case there is; // an overflow (which would not be the case if we do the check with; // 64-bit arithmetic).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:43,Usability,simpl,simply,43,// Drop FVal since we can get its value by simply inverting/negating; // TVal.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:107,Performance,perform,perform,107,"// Avoid materializing a constant when possible by reusing a known value in; // a register. However, don't perform this optimization if the known value; // is one, zero or negative one in the case of a CSEL. We can always; // materialize these values using CSINC, CSEL and CSINV with wzr/xzr as the; // FVal, respectively.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:120,Performance,optimiz,optimization,120,"// Avoid materializing a constant when possible by reusing a known value in; // a register. However, don't perform this optimization if the known value; // is one, zero or negative one in the case of a CSEL. We can always; // materialize these values using CSINC, CSEL and CSINV with wzr/xzr as the; // FVal, respectively.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Safety,Avoid,Avoid,3,"// Avoid materializing a constant when possible by reusing a known value in; // a register. However, don't perform this optimization if the known value; // is one, zero or negative one in the case of a CSEL. We can always; // materialize these values using CSINC, CSEL and CSINV with wzr/xzr as the; // FVal, respectively.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:98,Safety,avoid,avoid,98,"// Transform ""a == C ? C : x"" to ""a == C ? a : x"" and ""a != C ? x : C"" to; // ""a != C ? x : a"" to avoid materializing C.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:74,Safety,avoid,avoid,74,"// Use a CSINV to transform ""a == C ? 1 : -1"" to ""a == C ? a : -1"" to; // avoid materializing C.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:110,Safety,avoid,avoid,110,"// Transform ""a == 0.0 ? 0.0 : x"" to ""a == 0.0 ? a : x"" and; // ""a != 0.0 ? x : 0.0"" to ""a != 0.0 ? x : a"" to avoid materializing 0.0.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:87,Energy Efficiency,efficient,efficiently,87,"// We can use the splice instruction for certain index values where we are; // able to efficiently generate the correct predicate. The index will be; // inverted and used directly as the input to the ptrue instruction, i.e.; // -1 -> vl1, -2 -> vl2, etc. The predicate will then be reversed to get the; // splice predicate. However, we can only do this if we can guarantee that; // there are enough elements in the vector, hence we check the index <= min; // number of elements.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:159,Modifiability,extend,extend,159,"// FIXME: Ideally this would be the same as above using i1 types, however; // for the moment we can't deal with fixed i1 vector types properly, so; // instead extend the predicate to a result type sized integer vector.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Performance,Optimiz,Optimize,3,// Optimize {s|u}{add|sub|mul}.with.overflow feeding into a select; // instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:68,Modifiability,extend,extended,68,"// Scalar integer and FP values smaller than 64 bits are implicitly extended; // up to 64 bits. At the very least, we have to increase the striding of the; // vaargs list to match this, and for FP values we need to introduce; // FP_ROUND nodes as well.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Performance,Load,Load,3,// Load the actual argument out of the pointer VAList,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Performance,Load,Load,3,// Load the value as an f64.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:19,Availability,down,down,19,// Round the value down to an f32.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:56,Performance,load,load,56,// Merge the rounded value with the chain output of the load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:207,Availability,avail,available,207,// The XPACLRI instruction assembles to a hint-space instruction before; // Armv8.3-A therefore this instruction can be safely used for any pre; // Armv8.3-A architectures. On Armv8.3-A and onwards XPACI is available so use; // that instead.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:120,Safety,safe,safely,120,// The XPACLRI instruction assembles to a hint-space instruction before; // Armv8.3-A therefore this instruction can be safely used for any pre; // Armv8.3-A architectures. On Armv8.3-A and onwards XPACI is available so use; // that instead.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:126,Testability,log,logical,126,"// If we can not materialize in immediate field for fmov, check if the; // value can be encoded as the immediate operand of a logical instruction.; // The immediate value will be created with either MOVZ, MOVN, or ORR.; // TODO: fmov h0, w0 is also legal, however we don't have an isel pattern to; // generate that fmov.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:134,Energy Efficiency,reduce,reduced,134,"// The cost is actually exactly the same for mov+fmov vs. adrp+ldr;; // however the mov+fmov sequence is always better because of the reduced; // cache pressure. The timings are still the same if you consider; // movw+movk+fmov vs. adrp+ldr (it's one instruction longer, but the; // movw+movk is fused). So we limit up to 2 instrdduction at most.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:146,Performance,cache,cache,146,"// The cost is actually exactly the same for mov+fmov vs. adrp+ldr;; // however the mov+fmov sequence is always better because of the reduced; // cache pressure. The timings are still the same if you consider; // movw+movk+fmov vs. adrp+ldr (it's one instruction longer, but the; // movw+movk is fused). So we limit up to 2 instrdduction at most.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:93,Performance,Optimiz,Optimization,93,//===----------------------------------------------------------------------===//; // AArch64 Optimization Hooks; //===----------------------------------------------------------------------===//,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:1146,Modifiability,variab,variable,1146,"//===----------------------------------------------------------------------===//; // AArch64 Inline Assembly Support; //===----------------------------------------------------------------------===//; // Table of Constraints; // TODO: This is the current set of constraints supported by ARM for the; // compiler, not all of them may make sense.; //; // r - A general register; // w - An FP/SIMD register of some size in the range v0-v31; // x - An FP/SIMD register of some size in the range v0-v15; // I - Constant that can be used with an ADD instruction; // J - Constant that can be used with a SUB instruction; // K - Constant that can be used with a 32-bit logical instruction; // L - Constant that can be used with a 64-bit logical instruction; // M - Constant that can be used as a 32-bit MOV immediate; // N - Constant that can be used as a 64-bit MOV immediate; // Q - A memory reference with base register and no offset; // S - A symbolic address; // Y - Floating point constant zero; // Z - Integer constant zero; //; // Note that general register operands will be output using their 64-bit x; // register name, whatever the size of the variable, unless the asm operand; // is prefixed by the %w modifier. Floating-point and SIMD register operands; // will be output with the v prefix unless prefixed by the %b, %h, %s, %d or; // %q modifier.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:660,Testability,log,logical,660,"//===----------------------------------------------------------------------===//; // AArch64 Inline Assembly Support; //===----------------------------------------------------------------------===//; // Table of Constraints; // TODO: This is the current set of constraints supported by ARM for the; // compiler, not all of them may make sense.; //; // r - A general register; // w - An FP/SIMD register of some size in the range v0-v31; // x - An FP/SIMD register of some size in the range v0-v15; // I - Constant that can be used with an ADD instruction; // J - Constant that can be used with a SUB instruction; // K - Constant that can be used with a 32-bit logical instruction; // L - Constant that can be used with a 64-bit logical instruction; // M - Constant that can be used as a 32-bit MOV immediate; // N - Constant that can be used as a 64-bit MOV immediate; // Q - A memory reference with base register and no offset; // S - A symbolic address; // Y - Floating point constant zero; // Z - Integer constant zero; //; // Note that general register operands will be output using their 64-bit x; // register name, whatever the size of the variable, unless the asm operand; // is prefixed by the %w modifier. Floating-point and SIMD register operands; // will be output with the v prefix unless prefixed by the %b, %h, %s, %d or; // %q modifier.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:728,Testability,log,logical,728,"//===----------------------------------------------------------------------===//; // AArch64 Inline Assembly Support; //===----------------------------------------------------------------------===//; // Table of Constraints; // TODO: This is the current set of constraints supported by ARM for the; // compiler, not all of them may make sense.; //; // r - A general register; // w - An FP/SIMD register of some size in the range v0-v31; // x - An FP/SIMD register of some size in the range v0-v15; // I - Constant that can be used with an ADD instruction; // J - Constant that can be used with a SUB instruction; // K - Constant that can be used with a 32-bit logical instruction; // L - Constant that can be used with a 64-bit logical instruction; // M - Constant that can be used as a 32-bit MOV immediate; // N - Constant that can be used as a 64-bit MOV immediate; // Q - A memory reference with base register and no offset; // S - A symbolic address; // Y - Floating point constant zero; // Z - Integer constant zero; //; // Note that general register operands will be output using their 64-bit x; // register name, whatever the size of the variable, unless the asm operand; // is prefixed by the %w modifier. Floating-point and SIMD register operands; // will be output with the v prefix unless prefixed by the %b, %h, %s, %d or; // %q modifier.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:376,Energy Efficiency,efficient,efficient,376,"// At this point, we have to lower this constraint to something else, so we; // lower it to an ""r"" or ""w"". However, by doing this we will force the result; // to be in register, while the X constraint is much more permissive.; //; // Although we are correct (we are free to emit anything, without; // constraints), we might break use cases that would expect us to be more; // efficient and emit something else.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:79,Modifiability,Extend,Extended-Asm,79,// The set of cc code supported is from; // https://gcc.gnu.org/onlinedocs/gcc/Extended-Asm.html#Flag-Output-Operands,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:14,Modifiability,variab,variable,14,// The output variable should be a scalar integer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:27,Deployability,update,update,27,// Get NZCV register. Only update chain when copyfrom is glued.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:48,Integrability,depend,depending,48,// v0 - v31 are aliases of q0 - q31 or d0 - d31 depending on size.; // By default we'll emit v0-v31 for this unless there's a modifier where; // we'll emit the correct register as well.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:83,Security,Validat,Validate,83,// This set of constraints deal with valid constants for various instructions.; // Validate and return a target constant for them if we can.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:30,Security,validat,validation,30,// Grab the value and do some validation.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:36,Usability,simpl,simple,36,"// The I constraint applies only to simple ADD or SUB immediate operands:; // i.e. 0 to 4095 with optional shift by 12; // The J constraint applies only to ADD or SUB immediates that would be; // valid when negated, i.e. if [an add pattern] were to be output as a SUB; // instruction [or vice versa], in other words -1 to -4095 with optional; // left shift by 12.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:43,Testability,log,logical,43,"// The K and L constraints apply *only* to logical immediates, including; // what used to be the MOVI alias for ORR (though the MOVI alias has now; // been removed and MOV should be used). So these constraints have to; // distinguish between bit patterns that are valid 32-bit or 64-bit; // ""bitmask immediates"": for example 0xaaaaaaaa is a valid bimm32 (K), but; // not a valid bimm64 (L) where 0xaaaaaaaaaaaaaaaa would be valid, and vice; // versa.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:201,Performance,load,loaded,201,"// The M and N constraints are a superset of K and L respectively, for use; // with the MOV (immediate) alias. As well as the logical immediates they; // also match 32 or 64-bit immediates that can be loaded either using a; // *single* MOVZ or MOVN , such as 32-bit 0x12340000, 0x00001234, 0xffffedca; // (M) or 64-bit 0x1234000000000000 (N) etc.; // As a note some of this code is liberally stolen from the asm parser.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:126,Testability,log,logical,126,"// The M and N constraints are a superset of K and L respectively, for use; // with the MOV (immediate) alias. As well as the logical immediates they; // also match 32 or 64-bit immediates that can be loaded either using a; // *single* MOVZ or MOVN , such as 32-bit 0x12340000, 0x00001234, 0xffffedca; // (M) or 64-bit 0x1234000000000000 (N) etc.; // As a note some of this code is liberally stolen from the asm parser.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:105,Availability,mask,mask,105,"// Check if a vector is built from one vector via extracted elements of; // another together with an AND mask, ensuring that all elements fit; // within range. This can be reconstructed using AND and NEON's TBL1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:89,Availability,mask,mask,89,"// This only looks at shuffles with elements that are; // a) truncated by a constant AND mask extracted from a mask vector, or; // b) extracted directly from a mask vector.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:111,Availability,mask,mask,111,"// This only looks at shuffles with elements that are; // a) truncated by a constant AND mask extracted from a mask vector, or; // b) extracted directly from a mask vector.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:160,Availability,mask,mask,160,"// This only looks at shuffles with elements that are; // a) truncated by a constant AND mask extracted from a mask vector, or; // b) extracted directly from a mask vector.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:48,Availability,mask,mask,48,// Either all or no operands should have an AND mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:194,Availability,mask,mask,194,"// We need a v16i8 for TBL, so we extend the source with a placeholder vector; // for v8i8 to get a v16i8. As the pattern we are replacing is extract +; // insert, we know that the index in the mask must be smaller than the number; // of elements in the source, or we would have an out-of-bounds access.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:34,Modifiability,extend,extend,34,"// We need a v16i8 for TBL, so we extend the source with a placeholder vector; // for v8i8 to get a v16i8. As the pattern we are replacing is extract +; // insert, we know that the index in the mask must be smaller than the number; // of elements in the source, or we would have an out-of-bounds access.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:296,Security,access,access,296,"// We need a v16i8 for TBL, so we extend the source with a placeholder vector; // for v8i8 to get a v16i8. As the pattern we are replacing is extract +; // insert, we know that the index in the mask must be smaller than the number; // of elements in the source, or we would have an out-of-bounds access.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Deployability,Update,Update,3,// Update the minimum and maximum lane number seen.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:15,Availability,mask,mask,15,// Construct a mask for the tbl. We may need to adjust the index for types; // larger than i8.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:11,Availability,Mask,Mask,11,"// Set the Mask lanes adjusted for the size of the input and output; // lanes. The Mask is always i8, so it will set OutputFactor lanes per; // output element, adjusted in their positions per input and output types.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:83,Availability,Mask,Mask,83,"// Set the Mask lanes adjusted for the size of the input and output; // lanes. The Mask is always i8, so it will set OutputFactor lanes per; // output element, adjusted in their positions per input and output types.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:56,Availability,mask,mask,56,"// The stars all align, our next step is to produce the mask for the shuffle.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:22,Performance,perform,performs,22,"// EXTRACT_VECTOR_ELT performs an implicit any_ext; BUILD_VECTOR an implicit; // trunc. So only std::min(SrcBits, DestBits) actually get defined in this; // segment.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:54,Availability,mask,mask,54,// check if an EXT instruction can handle the shuffle mask when the; // vector sources of the shuffle are the same.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:39,Integrability,wrap,wraps,39,"// Increment the expected index. If it wraps around, just follow it; // back to index zero and keep going.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Safety,Detect,Detect,3,"// Detect patterns of a0,a1,a2,a3,b0,b1,b2,b3,c0,c1,c2,c3,d0,d1,d2,d3 from; // v4i32s. This is really a truncate, which we can construct out of (legal); // concats and truncate nodes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:22,Availability,mask,masks,22,"// We are looking for masks like; // [0, 1, 0, 1] or [2, 3, 2, 3] or [4, 5, 6, 7, 4, 5, 6, 7] where any element; // might be replaced by 'undefined'. BlockIndices will eventually contain; // lane indices of the duplicated block (i.e. [0, 1], [2, 3] and [4, 5, 6, 7]; // for the above examples)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:54,Availability,mask,mask,54,// check if an EXT instruction can handle the shuffle mask when the; // vector sources of the shuffle are different.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:334,Availability,mask,mask,334,"// The index of an EXT is the first element if it is not UNDEF.; // Watch out for the beginning UNDEFs. The EXT index should be the expected; // value of the first element. E.g.; // <-1, -1, 3, ...> is treated as <1, 2, 3, ...>.; // <-1, -1, 0, 1, ...> is treated as <2*NumElts-2, 2*NumElts-1, 0, 1, ...>.; // ExpectedElt is the last mask index plus 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:297,Availability,mask,mask,297,"// There are two difference cases requiring to reverse input vectors.; // For example, for vector <4 x i32> we have the following cases,; // Case 1: shufflevector(<4 x i32>,<4 x i32>,<-1, -1, -1, 0>); // Case 2: shufflevector(<4 x i32>,<4 x i32>,<-1, -1, 7, 0>); // For both cases, we finally use mask <5, 6, 7, 0>, which requires; // to reverse two input vectors.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:139,Availability,Mask,Mask,139,"/// isZIP_v_undef_Mask - Special case of isZIPMask for canonical form of; /// ""vector_shuffle v, v"", i.e., ""vector_shuffle v, undef"".; /// Mask is e.g., <0, 0, 1, 1> instead of <0, 4, 1, 5>.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:139,Availability,Mask,Mask,139,"/// isUZP_v_undef_Mask - Special case of isUZPMask for canonical form of; /// ""vector_shuffle v, v"", i.e., ""vector_shuffle v, undef"".; /// Mask is e.g., <0, 2, 0, 2> instead of <0, 2, 4, 6>,",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:139,Availability,Mask,Mask,139,"/// isTRN_v_undef_Mask - Special case of isTRNMask for canonical form of; /// ""vector_shuffle v, v"", i.e., ""vector_shuffle v, undef"".; /// Mask is e.g., <0, 0, 2, 2> instead of <0, 4, 2, 6>.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:44,Availability,Mask,Mask,44,// Decompose a PerfectShuffle ID to get the Mask for lane Elt,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Deployability,Update,Update,3,// Update the lane value by offsetting with the scaled extract index.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:43,Availability,mask,mask,43,// Return true if we can get a new shuffle mask by checking the parameter mask; // array to test whether every two adjacent mask values are continuous and; // starting from an even number.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:74,Availability,mask,mask,74,// Return true if we can get a new shuffle mask by checking the parameter mask; // array to test whether every two adjacent mask values are continuous and; // starting from an even number.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:124,Availability,mask,mask,124,// Return true if we can get a new shuffle mask by checking the parameter mask; // array to test whether every two adjacent mask values are continuous and; // starting from an even number.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:140,Deployability,continuous,continuous,140,// Return true if we can get a new shuffle mask by checking the parameter mask; // array to test whether every two adjacent mask values are continuous and; // starting from an even number.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:92,Testability,test,test,92,// Return true if we can get a new shuffle mask by checking the parameter mask; // array to test whether every two adjacent mask values are continuous and; // starting from an even number.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:35,Availability,mask,mask,35,"// If both elements are undef, new mask is undef too.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:42,Availability,mask,mask,42,"// Try to widen element type to get a new mask value for a better permutation; // sequence, so that we can use NEON shuffle instructions, such as zip1/2,; // UZP1/2, TRN1/2, REV, INS, etc.; // For example:; // shufflevector <4 x i32> %a, <4 x i32> %b,; // <4 x i32> <i32 6, i32 7, i32 2, i32 3>; // is equivalent to:; // shufflevector <2 x i64> %a, <2 x i64> %b, <2 x i32> <i32 3, i32 1>; // Finally, we can get:; // mov v0.d[0], v1.d[1]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:190,Energy Efficiency,efficient,efficient,190,"// Convert shuffles that are directly supported on NEON to target-specific; // DAG nodes, instead of keeping them as shuffles and matching them again; // during code selection. This is more efficient and avoids the possibility; // of inconsistencies between legalization and selection.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:204,Safety,avoid,avoids,204,"// Convert shuffles that are directly supported on NEON to target-specific; // DAG nodes, instead of keeping them as shuffles and matching them again; // during code selection. This is more efficient and avoids the possibility; // of inconsistencies between legalization and selection.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Testability,Test,Test,3,"// Test if V1 is a BUILD_VECTOR and the lane being referenced is a non-; // constant. If so, we can just reference the lane's definition directly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:16,Availability,mask,mask,16,// Check if the mask matches a DUP for a wider element,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:108,Performance,scalab,scalable,108,"// If we're compiling for a specific vector-length, we can check if the; // pattern's VL equals that of the scalable vector at runtime.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:342,Availability,mask,mask,342,"// Is one of the operands an AND or a BICi? The AND may have been optimised to; // a BICi in order to use an immediate instead of a register.; // Is the other operand an shl or lshr? This will have been turned into:; // AArch64ISD::VSHL vector, #shift or AArch64ISD::VLSHR vector, #shift; // or (AArch64ISD::SHL_PRED || AArch64ISD::SRL_PRED) mask, vector, #shiftVec.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:14,Availability,mask,mask,14,// Is the and mask vector all constant?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:18,Usability,simpl,simple,18,// Try to build a simple constant vector.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:82,Safety,abort,abort,82,"// Thought this might return a non-BUILD_VECTOR (e.g. CONCAT_VECTORS), if so,; // abort.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:57,Testability,log,logical,57,"// Certain vector constants, used to express things like logical NOT and; // arithmetic NEG, are passed through unmodified. This allows special; // patterns for these operations to match, which will lower these constants; // to whatever is proven necessary.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:159,Usability,Simpl,SimplifyDemandedBits,159,"// Convert BUILD_VECTOR where all elements but the lowest are undef into; // SCALAR_TO_VECTOR, except for when we have a single-element constant vector; // as SimplifyDemandedBits will just turn that back into BUILD_VECTOR.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:29,Safety,abort,abort,29,// Something does not match: abort.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:61,Energy Efficiency,reduce,reduce,61,"// Use DUP for non-constant splats. For f32 constant splats, reduce to; // i32 and try again.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:193,Performance,perform,perform,193,"// If there was only one constant value used and for more than one lane,; // start by splatting that value, then replace the non-constant lanes. This; // is better than the default, which will perform a separate initialization; // for each lane.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:24,Performance,load,load,24,// This will generate a load from the constant pool.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Safety,Detect,Detect,3,"// Detect patterns of a0,a1,a2,a3,b0,b1,b2,b3,c0,c1,c2,c3,d0,d1,d2,d3 from; // v4i32s. This is really a truncate, which we can construct out of (legal); // concats and truncate nodes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:13,Testability,test,tests,13,// Empirical tests suggest this is rarely worth it for vectors of length <= 2.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:9,Availability,mask,mask,9,// Build mask for VECTOR_SHUFLLE.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:392,Performance,load,load,392,"// If all else fails, just use a sequence of INSERT_VECTOR_ELT when we; // know the default expansion would otherwise fall back on something even; // worse. For a vector with one or two non-undef values, that's; // scalar_to_vector for the elements followed by a shuffle (provided the; // shuffle is valid for the target) and materialization element by element; // on the stack followed by a load for everything else.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:60,Integrability,depend,dependency,60,"// Use SCALAR_TO_VECTOR for lane zero to; // a) Avoid a RMW dependency on the full vector register, and; // b) Allow the register coalescer to fold away the copy if the; // value is already in an S or D register, and we're forced to emit an; // INSERT_SUBREG that we can't fold anywhere.; //; // We also allow types like i8 and i16 which are illegal scalar but legal; // vector element types. After type-legalization the inserted value is; // extended (i32) and it is safe to cast them to the vector type by ignoring; // the upper bits of the lowest lane (e.g. v8i8, v4i16).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:443,Modifiability,extend,extended,443,"// Use SCALAR_TO_VECTOR for lane zero to; // a) Avoid a RMW dependency on the full vector register, and; // b) Allow the register coalescer to fold away the copy if the; // value is already in an S or D register, and we're forced to emit an; // INSERT_SUBREG that we can't fold anywhere.; //; // We also allow types like i8 and i16 which are illegal scalar but legal; // vector element types. After type-legalization the inserted value is; // extended (i32) and it is safe to cast them to the vector type by ignoring; // the upper bits of the lowest lane (e.g. v8i8, v4i16).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:48,Safety,Avoid,Avoid,48,"// Use SCALAR_TO_VECTOR for lane zero to; // a) Avoid a RMW dependency on the full vector register, and; // b) Allow the register coalescer to fold away the copy if the; // value is already in an S or D register, and we're forced to emit an; // INSERT_SUBREG that we can't fold anywhere.; //; // We also allow types like i8 and i16 which are illegal scalar but legal; // vector element types. After type-legalization the inserted value is; // extended (i32) and it is safe to cast them to the vector type by ignoring; // the upper bits of the lowest lane (e.g. v8i8, v4i16).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:468,Safety,safe,safe,468,"// Use SCALAR_TO_VECTOR for lane zero to; // a) Avoid a RMW dependency on the full vector register, and; // b) Allow the register coalescer to fold away the copy if the; // value is already in an S or D register, and we're forced to emit an; // INSERT_SUBREG that we can't fold anywhere.; //; // We also allow types like i8 and i16 which are illegal scalar but legal; // vector element types. After type-legalization the inserted value is; // extended (i32) and it is safe to cast them to the vector type by ignoring; // the upper bits of the lowest lane (e.g. v8i8, v4i16).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:52,Modifiability,extend,extend,52,"// We can't directly extract from an SVE predicate; extend it first.; // (This isn't the only possible lowering, but it's straightforward.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:21,Performance,perform,perform,21,"// For V64 types, we perform extraction by expanding the value; // to a V128 type and perform the extraction on that.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:86,Performance,perform,perform,86,"// For V64 types, we perform extraction by expanding the value; // to a V128 type and perform the extraction on that.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:9,Availability,down,down,9,// Break down insert_subvector into simpler parts.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:36,Usability,simpl,simpler,36,// Break down insert_subvector into simpler parts.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:137,Integrability,depend,depending,137,"// To replace the top/bottom half of vector V with vector SubV we widen the; // preserved half of V, concatenate this to SubV (the order depending on the; // half being replaced) and then narrow the result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:48,Availability,mask,masks,48,"// Just delegate to the generic legality, clear masks aren't special.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:42,Usability,clear,clear,42,"// Just delegate to the generic legality, clear masks aren't special.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:136,Modifiability,extend,extending,136,"// Vectors that are less than 64 bits get widened to neatly fit a 64 bit; // register, so e.g. <4 x i1> gets lowered to <4 x i16>. Sign extending to; // this element size leads to the best codegen, since e.g. setcc results; // might need to be truncated otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:191,Performance,throughput,throughput,191,// Do the remaining work on a scalar since it allows the code generator to; // combine the shift and bitwise operation into one instruction and since; // integer instructions can have higher throughput than vector instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:21,Performance,load,load-clear,21,"// LSE has an atomic load-clear instruction, but not a load-and.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:55,Performance,load,load-and,55,"// LSE has an atomic load-clear instruction, but not a load-and.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:26,Usability,clear,clear,26,"// LSE has an atomic load-clear instruction, but not a load-and.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:20,Modifiability,extend,extended,20,"// When x and y are extended, lower:; // avgfloor(x, y) -> (x + y) >> 1; // avgceil(x, y) -> (x + y + 1) >> 1; // Otherwise, lower to:; // avgfloor(x, y) -> (x >> 1) + (y >> 1) + (x & y & 1); // avgceil(x, y) -> (x >> 1) + (y >> 1) + ((x || y) & 1)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:40,Performance,load,load,40,/// getTgtMemIntrinsic - Represent NEON load and store intrinsics as; /// MemIntrinsicNodes. The associated MachineMemOperands record the alignment; /// specified in the intrinsic calls.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:12,Performance,load,loads,12,// volatile loads with NEON intrinsics not supported,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:12,Performance,load,loads,12,// volatile loads with NEON intrinsics not supported,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:54,Testability,test,tests,54,// TODO: This may be worth removing. Check regression tests for diffs.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:25,Performance,load,load,25,// If we're reducing the load width in order to avoid having to use an extra; // instruction to do extension then it's probably a good idea.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:48,Safety,avoid,avoid,48,// If we're reducing the load width in order to avoid having to use an extra; // instruction to do extension then it's probably a good idea.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:9,Energy Efficiency,reduce,reduce,9,// Don't reduce load width if it would prevent us from combining a shift into; // the offset.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:16,Performance,load,load,16,// Don't reduce load width if it would prevent us from combining a shift into; // the offset.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:48,Energy Efficiency,power,power-of-,48,// It's unknown whether a scalable vector has a power-of-2 bitwidth.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:26,Performance,scalab,scalable,26,// It's unknown whether a scalable vector has a power-of-2 bitwidth.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:75,Performance,load,loaded,75,// The shift can be combined if it matches the size of the value being; // loaded (and so reducing the width would make it not match).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:46,Performance,load,load,46,"// We have no reason to disallow reducing the load width, so allow it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:57,Modifiability,extend,extend,57,"// 8-, 16-, and 32-bit integer loads all implicitly zero-extend.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:31,Performance,load,loads,31,"// 8-, 16-, and 32-bit integer loads all implicitly zero-extend.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:13,Availability,mask,mask,13,// Check the mask extracts either the lower or upper half of vector; // elements.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:31,Modifiability,extend,extends,31,"/// Check if Ext1 and Ext2 are extends of the same type, doubling the bitwidth; /// of the vector elements.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:8,Modifiability,extend,extends,8,// Sink extends that would allow us to use 32-bit offset vectors.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:19,Availability,Mask,MaskValue,19,"// Pattern: Or(And(MaskValue, A), And(Not(MaskValue), B)) ->; // bitselect(MaskValue, A, B) where Not(MaskValue) = Xor(MaskValue, -1)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:42,Availability,Mask,MaskValue,42,"// Pattern: Or(And(MaskValue, A), And(Not(MaskValue), B)) ->; // bitselect(MaskValue, A, B) where Not(MaskValue) = Xor(MaskValue, -1)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:75,Availability,Mask,MaskValue,75,"// Pattern: Or(And(MaskValue, A), And(Not(MaskValue), B)) ->; // bitselect(MaskValue, A, B) where Not(MaskValue) = Xor(MaskValue, -1)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:102,Availability,Mask,MaskValue,102,"// Pattern: Or(And(MaskValue, A), And(Not(MaskValue), B)) ->; // bitselect(MaskValue, A, B) where Not(MaskValue) = Xor(MaskValue, -1)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:119,Availability,Mask,MaskValue,119,"// Pattern: Or(And(MaskValue, A), And(Not(MaskValue), B)) ->; // bitselect(MaskValue, A, B) where Not(MaskValue) = Xor(MaskValue, -1)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:7,Availability,mask,mask,7,// Non-mask operands of both Ands should also be in same basic block,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:64,Modifiability,extend,extends,64,// Is it profitable to sink if we found two of the same type of extends.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:12,Availability,mask,mask,12,"// Create a mask that selects <0,...,Op[i]> for each lane of the destination; // vector to replace the original ZExt. This can later be lowered to a set of; // tbl instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:12,Availability,mask,mask,12,"// Create a mask to choose every nth byte from the source vector table of; // bytes to create the truncated destination vector, where 'n' is the truncate; // ratio. For example, for a truncate from Yxi64 to Yxi8, choose; // 0,8,16,..Y*8th bytes for the little-endian format",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:10,Performance,optimiz,optimize,10,"// Try to optimize conversions using tbl. This requires materializing constant; // index vectors, which can increase code size and add loads. Skip the; // transform unless the conversion is in a loop block guaranteed to execute; // and we are not optimizing for size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:135,Performance,load,loads,135,"// Try to optimize conversions using tbl. This requires materializing constant; // index vectors, which can increase code size and add loads. Skip the; // transform unless the conversion is in a loop block guaranteed to execute; // and we are not optimizing for size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:247,Performance,optimiz,optimizing,247,"// Try to optimize conversions using tbl. This requires materializing constant; // index vectors, which can increase code size and add loads. Skip the; // transform unless the conversion is in a loop block guaranteed to execute; // and we are not optimizing for size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:59,Energy Efficiency,power,power-of-,59,"// If the ZExt can be lowered to a single ZExt to the next power-of-2 and; // the remaining ZExt folded into the user, don't use tbl lowering.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:30,Security,access,accesses,30,// Cyclone supports unaligned accesses.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:64,Security,access,accesses,64,/// A helper function for determining the number of interleaved accesses we; /// will generate when lowering accesses of the given type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:109,Security,access,accesses,109,/// A helper function for determining the number of interleaved accesses we; /// will generate when lowering accesses of the given type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:60,Availability,avail,available,60,// Ensure that the predicate for this number of elements is available.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:126,Security,access,accesses,126,// Ensure the total vector size is 64 or a multiple of 128. Types larger than; // 128 will be split into multiple interleaved accesses.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:25,Performance,load,load,25,"/// Lower an interleaved load into a ldN intrinsic.; ///; /// E.g. Lower an interleaved load (Factor = 2):; /// %wide.vec = load <8 x i32>, <8 x i32>* %ptr; /// %v0 = shuffle %wide.vec, undef, <0, 2, 4, 6> ; Extract even elements; /// %v1 = shuffle %wide.vec, undef, <1, 3, 5, 7> ; Extract odd elements; ///; /// Into:; /// %ld2 = { <4 x i32>, <4 x i32> } call llvm.aarch64.neon.ld2(%ptr); /// %vec0 = extractelement { <4 x i32>, <4 x i32> } %ld2, i32 0; /// %vec1 = extractelement { <4 x i32>, <4 x i32> } %ld2, i32 1",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:88,Performance,load,load,88,"/// Lower an interleaved load into a ldN intrinsic.; ///; /// E.g. Lower an interleaved load (Factor = 2):; /// %wide.vec = load <8 x i32>, <8 x i32>* %ptr; /// %v0 = shuffle %wide.vec, undef, <0, 2, 4, 6> ; Extract even elements; /// %v1 = shuffle %wide.vec, undef, <1, 3, 5, 7> ; Extract odd elements; ///; /// Into:; /// %ld2 = { <4 x i32>, <4 x i32> } call llvm.aarch64.neon.ld2(%ptr); /// %vec0 = extractelement { <4 x i32>, <4 x i32> } %ld2, i32 0; /// %vec1 = extractelement { <4 x i32>, <4 x i32> } %ld2, i32 1",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:124,Performance,load,load,124,"/// Lower an interleaved load into a ldN intrinsic.; ///; /// E.g. Lower an interleaved load (Factor = 2):; /// %wide.vec = load <8 x i32>, <8 x i32>* %ptr; /// %v0 = shuffle %wide.vec, undef, <0, 2, 4, 6> ; Extract even elements; /// %v1 = shuffle %wide.vec, undef, <1, 3, 5, 7> ; Extract odd elements; ///; /// Into:; /// %ld2 = { <4 x i32>, <4 x i32> } call llvm.aarch64.neon.ld2(%ptr); /// %vec0 = extractelement { <4 x i32>, <4 x i32> } %ld2, i32 0; /// %vec1 = extractelement { <4 x i32>, <4 x i32> } %ld2, i32 1",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:128,Security,access,accesses,128,"// Skip if we do not have NEON and skip illegal vector types. We can; // ""legalize"" wide vector types into multiple interleaved accesses as long as; // the vector types are divisible by 128.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:82,Performance,load,load,82,// A pointer vector can not be the return type of the ldN intrinsics. Need to; // load integer vectors first and then convert to pointer vectors.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:44,Performance,load,load,44,"// If we're going to generate more than one load, reset the sub-vector type; // to something legal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:27,Performance,load,load,27,// The base address of the load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:40,Performance,load,load,40,// Holds sub-vectors extracted from the load intrinsic return values. The; // sub-vectors are associated with the shufflevector instructions they will; // replace.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:37,Performance,load,load,37,"// If we're generating more than one load, compute the base address of; // subsequent loads as an offset from the previous.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:86,Performance,load,loads,86,"// If we're generating more than one load, compute the base address of; // subsequent loads as an offset from the previous.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:53,Performance,load,load,53,// Extract and store the sub-vectors returned by the load intrinsic.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:91,Performance,load,load,91,"// Replace uses of the shufflevector instructions with the sub-vectors; // returned by the load intrinsic. If a shufflevector instruction is; // associated with more than one sub-vector, those sub-vectors will be; // concatenated into a single wide vector.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:698,Availability,mask,mask,698,"/// Lower an interleaved store into a stN intrinsic.; ///; /// E.g. Lower an interleaved store (Factor = 3):; /// %i.vec = shuffle <8 x i32> %v0, <8 x i32> %v1,; /// <0, 4, 8, 1, 5, 9, 2, 6, 10, 3, 7, 11>; /// store <12 x i32> %i.vec, <12 x i32>* %ptr; ///; /// Into:; /// %sub.v0 = shuffle <8 x i32> %v0, <8 x i32> v1, <0, 1, 2, 3>; /// %sub.v1 = shuffle <8 x i32> %v0, <8 x i32> v1, <4, 5, 6, 7>; /// %sub.v2 = shuffle <8 x i32> %v0, <8 x i32> v1, <8, 9, 10, 11>; /// call void llvm.aarch64.neon.st3(%sub.v0, %sub.v1, %sub.v2, %ptr); ///; /// Note that the new shufflevectors will be removed and we'll only generate one; /// st3 instruction in CodeGen.; ///; /// Example for a more general valid mask (Factor 3). Lower:; /// %i.vec = shuffle <32 x i32> %v0, <32 x i32> %v1,; /// <4, 32, 16, 5, 33, 17, 6, 34, 18, 7, 35, 19>; /// store <12 x i32> %i.vec, <12 x i32>* %ptr; ///; /// Into:; /// %sub.v0 = shuffle <32 x i32> %v0, <32 x i32> v1, <4, 5, 6, 7>; /// %sub.v1 = shuffle <32 x i32> %v0, <32 x i32> v1, <32, 33, 34, 35>; /// %sub.v2 = shuffle <32 x i32> %v0, <32 x i32> v1, <16, 17, 18, 19>; /// call void llvm.aarch64.neon.st3(%sub.v0, %sub.v1, %sub.v2, %ptr)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:128,Security,access,accesses,128,"// Skip if we do not have NEON and skip illegal vector types. We can; // ""legalize"" wide vector types into multiple interleaved accesses as long as; // the vector types are divisible by 128.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:60,Availability,mask,mask,60,"// Sanity check if all the indices are NOT in range.; // If mask is `poison`, `Mask` may be a vector of -1s.; // If all of them are `poison`, OOB read will happen later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:79,Availability,Mask,Mask,79,"// Sanity check if all the indices are NOT in range.; // If mask is `poison`, `Mask` may be a vector of -1s.; // If all of them are `poison`, OOB read will happen later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Safety,Sanity check,Sanity check,3,"// Sanity check if all the indices are NOT in range.; // If mask is `poison`, `Mask` may be a vector of -1s.; // If all of them are `poison`, OOB read will happen later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:265,Performance,throughput,throughput,265,"// A 64bit st2 which does not start at element 0 will involved adding extra; // ext elements making the st2 unprofitable, and if there is a nearby store; // that points to BaseAddr+16 or BaseAddr-16 then it can be better left as a; // zip;ldp pair which has higher throughput.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Energy Efficiency,Adapt,Adapt,3,// Adapt to the width of a register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Modifiability,Adapt,Adapt,3,// Adapt to the width of a register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:118,Performance,load,load,118,"/// isLegalAddressingMode - Return true if the addressing mode represented; /// by AM is legal for this target, for a load/store of the specified type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:198,Deployability,patch,patchpoints,198,"// LR is a callee-save register, but we must treat it as clobbered by any call; // site. Hence we include LR in the scratch registers, which are in turn added; // as implicit-defs for stackmaps and patchpoints.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:55,Availability,mask,mask,55,"// If ShiftLHS is unsigned bit extraction: ((x >> C) & mask), then do not; // combine it with shift 'N' to let it be lowered to UBFX except:; // ((x >> C) & mask) << C.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:157,Availability,mask,mask,157,"// If ShiftLHS is unsigned bit extraction: ((x >> C) & mask), then do not; // combine it with shift 'N' to let it be lowered to UBFX except:; // ((x >> C) & mask) << C.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:34,Availability,mask,mask,34,// Only commute if the entire NOT mask is a hidden shifted mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:59,Availability,mask,mask,59,// Only commute if the entire NOT mask is a hidden shifted mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:16,Testability,test,tests,16,"/// Turn vector tests of the signbit in the form of:; /// xor (sra X, elt_size(X)-1), -1; /// into:; /// cmge X, X, #0",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:31,Safety,detect,detect,31,"// Given a vecreduce_add node, detect the below pattern and convert it to the; // node sequence with UABDL, [S|U]ADB and UADDLP.; //; // i32 vecreduce_add(; // v16i32 abs(; // v16i32 sub(; // v16i32 [sign|zero]_extend(v16i8 a), v16i32 [sign|zero]_extend(v16i8 b)))); // =================>; // i32 vecreduce_add(; // v4i32 UADDLP(; // v8i16 add(; // v8i16 zext(; // v8i8 [S|U]ABD low8:v16i8 a, low8:v16i8 b; // v8i16 zext(; // v8i8 [S|U]ABD high8:v16i8 a, high8:v16i8 b",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:21,Modifiability,extend,extended,21,"// Turn a v8i8/v16i8 extended vecreduce into a udot/sdot and vecreduce; // vecreduce.add(ext(A)) to vecreduce.add(DOT(zero, A, one)); // vecreduce.add(mul(ext(A), ext(B))) to vecreduce.add(DOT(zero, A, B)); // If we have vectors larger than v16i8 we extract v16i8 vectors,; // Follow the same steps above to get DOT instructions concatenate them; // and generate vecreduce.add(concat_vector(DOT, DOT2, ..)).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:84,Modifiability,extend,extend,84,// For non-mla reductions B can be set to 1. For MLA we take the operand of; // the extend B.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:7,Performance,scalab,scalable,7,"// For scalable and fixed types, mark them as cheap so we can handle it much; // later. This allows us to handle larger than legal types.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:7,Performance,scalab,scalable,7,"// For scalable and fixed types, mark them as cheap so we can handle it much; // later. This allows us to handle larger than legal types.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:28,Modifiability,extend,extend,28,"/// Calculates what the pre-extend type is, based on the extension; /// operation node provided by \p Extend.; ///; /// In the case that \p Extend is a SIGN_EXTEND or a ZERO_EXTEND, the; /// pre-extend type is pulled directly from the operand, while other extend; /// operations need a bit more inspection to get this information.; ///; /// \param Extend The SDNode from the DAG that represents the extend operation; ///; /// \returns The type representing the \p Extend source type, or \p MVT::Other; /// if no valid type can be determined",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:102,Modifiability,Extend,Extend,102,"/// Calculates what the pre-extend type is, based on the extension; /// operation node provided by \p Extend.; ///; /// In the case that \p Extend is a SIGN_EXTEND or a ZERO_EXTEND, the; /// pre-extend type is pulled directly from the operand, while other extend; /// operations need a bit more inspection to get this information.; ///; /// \param Extend The SDNode from the DAG that represents the extend operation; ///; /// \returns The type representing the \p Extend source type, or \p MVT::Other; /// if no valid type can be determined",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:140,Modifiability,Extend,Extend,140,"/// Calculates what the pre-extend type is, based on the extension; /// operation node provided by \p Extend.; ///; /// In the case that \p Extend is a SIGN_EXTEND or a ZERO_EXTEND, the; /// pre-extend type is pulled directly from the operand, while other extend; /// operations need a bit more inspection to get this information.; ///; /// \param Extend The SDNode from the DAG that represents the extend operation; ///; /// \returns The type representing the \p Extend source type, or \p MVT::Other; /// if no valid type can be determined",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:195,Modifiability,extend,extend,195,"/// Calculates what the pre-extend type is, based on the extension; /// operation node provided by \p Extend.; ///; /// In the case that \p Extend is a SIGN_EXTEND or a ZERO_EXTEND, the; /// pre-extend type is pulled directly from the operand, while other extend; /// operations need a bit more inspection to get this information.; ///; /// \param Extend The SDNode from the DAG that represents the extend operation; ///; /// \returns The type representing the \p Extend source type, or \p MVT::Other; /// if no valid type can be determined",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:256,Modifiability,extend,extend,256,"/// Calculates what the pre-extend type is, based on the extension; /// operation node provided by \p Extend.; ///; /// In the case that \p Extend is a SIGN_EXTEND or a ZERO_EXTEND, the; /// pre-extend type is pulled directly from the operand, while other extend; /// operations need a bit more inspection to get this information.; ///; /// \param Extend The SDNode from the DAG that represents the extend operation; ///; /// \returns The type representing the \p Extend source type, or \p MVT::Other; /// if no valid type can be determined",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:348,Modifiability,Extend,Extend,348,"/// Calculates what the pre-extend type is, based on the extension; /// operation node provided by \p Extend.; ///; /// In the case that \p Extend is a SIGN_EXTEND or a ZERO_EXTEND, the; /// pre-extend type is pulled directly from the operand, while other extend; /// operations need a bit more inspection to get this information.; ///; /// \param Extend The SDNode from the DAG that represents the extend operation; ///; /// \returns The type representing the \p Extend source type, or \p MVT::Other; /// if no valid type can be determined",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:399,Modifiability,extend,extend,399,"/// Calculates what the pre-extend type is, based on the extension; /// operation node provided by \p Extend.; ///; /// In the case that \p Extend is a SIGN_EXTEND or a ZERO_EXTEND, the; /// pre-extend type is pulled directly from the operand, while other extend; /// operations need a bit more inspection to get this information.; ///; /// \param Extend The SDNode from the DAG that represents the extend operation; ///; /// \returns The type representing the \p Extend source type, or \p MVT::Other; /// if no valid type can be determined",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:464,Modifiability,Extend,Extend,464,"/// Calculates what the pre-extend type is, based on the extension; /// operation node provided by \p Extend.; ///; /// In the case that \p Extend is a SIGN_EXTEND or a ZERO_EXTEND, the; /// pre-extend type is pulled directly from the operand, while other extend; /// operations need a bit more inspection to get this information.; ///; /// \param Extend The SDNode from the DAG that represents the extend operation; ///; /// \returns The type representing the \p Extend source type, or \p MVT::Other; /// if no valid type can be determined",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:76,Modifiability,extend,extend,76,"// Use the first item in the buildvector/shuffle to get the size of the; // extend, and make sure it looks valid.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:22,Modifiability,extend,extend,22,// Restrict valid pre-extend data type,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:44,Modifiability,extend,extended,44,// Make sure all other operands are equally extended,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:13,Performance,optimiz,optimizations,13,// The below optimizations require a constant RHS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:23,Energy Efficiency,power,power,23,"// Multiplication of a power of two plus/minus one can be done more; // cheaply as shift+add/sub. For now, this is true unilaterally. If; // future CPUs have a cheaper MADD instruction, this may need to be; // gated on a subtarget feature. For Cyclone, 32-bit MADD is 4 cycles and; // 64-bit is 5 cycles, so this is always a win.; // More aggressively, some multiplications N0 * C can be lowered to; // shift+add+shift if the constant C = A * B where A = 2^N + 1 and B = 2^M,; // e.g. 6=3*2=(2+1)*2, 45=(1+4)*(1+8); // TODO: lower more cases.; // TrailingZeroes is used to test if the mul can be lowered to; // shift+add+shift.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:573,Testability,test,test,573,"// Multiplication of a power of two plus/minus one can be done more; // cheaply as shift+add/sub. For now, this is true unilaterally. If; // future CPUs have a cheaper MADD instruction, this may need to be; // gated on a subtarget feature. For Cyclone, 32-bit MADD is 4 cycles and; // 64-bit is 5 cycles, so this is always a win.; // More aggressively, some multiplications N0 * C can be lowered to; // shift+add+shift if the constant C = A * B where A = 2^N + 1 and B = 2^M,; // e.g. 6=3*2=(2+1)*2, 45=(1+4)*(1+8); // TODO: lower more cases.; // TrailingZeroes is used to test if the mul can be lowered to; // shift+add+shift.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:78,Performance,optimiz,optimize,78,"// Take advantage of vector comparisons producing 0 or -1 in each lane to; // optimize away operation when it's from a constant.; //; // The general transformation is:; // UNARYOP(AND(VECTOR_CMP(x,y), constant)) -->; // AND(VECTOR_CMP(x,y), constant2); // constant2 = UNARYOP(constant); // Early exit if this isn't a vector operation, the operand of the; // unary operation isn't a bitwise AND, or if the sizes of the operations; // aren't the same.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:228,Performance,perform,perform,228,"// Now check that the other operand of the AND is a constant. We could; // make the transformation for non-constant splats as well, but it's unclear; // that would be a benefit as it would not eliminate any operations, just; // perform one more step in scalar code before moving to the vector unit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:16,Performance,optimiz,optimize,16,// First try to optimize away the conversion when it's conditionally from; // a constant. Vectors only.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:8,Performance,optimiz,optimize,8,// Only optimize when the source and destination types have the same width.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:31,Performance,load,load,31,"// If the result of an integer load is only used by an integer-to-float; // conversion, use a fp load instead and a AdvSIMD scalar {S|U}CVTF instead.; // This eliminates an ""integer-to-vector-move"" UOP and improves throughput.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:97,Performance,load,load,97,"// If the result of an integer load is only used by an integer-to-float; // conversion, use a fp load instead and a AdvSIMD scalar {S|U}CVTF instead.; // This eliminates an ""integer-to-vector-move"" UOP and improves throughput.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:215,Performance,throughput,throughput,215,"// If the result of an integer load is only used by an integer-to-float; // conversion, use a fp load instead and a AdvSIMD scalar {S|U}CVTF instead.; // This eliminates an ""integer-to-vector-move"" UOP and improves throughput.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:41,Performance,load,load,41,// Do not change the width of a volatile load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:40,Performance,load,load,40,// Make sure successors of the original load stay after it by updating them; // to use the new Chain.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:38,Energy Efficiency,power,power,38,/// Fold a floating-point multiply by power of two into floating-point to; /// fixed-point conversion.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Safety,Avoid,Avoid,3,"// Avoid conversions where iN is larger than the float (e.g., float -> i64).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:36,Energy Efficiency,power,power,36,/// Fold a floating-point divide by power of two into fixed-point to; /// floating-point conversion.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Safety,Avoid,Avoid,3,"// Avoid conversions where iN is larger than the float (e.g., i64 -> float).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:119,Modifiability,variab,variable,119,"// (or (and a b) (and (not a) c)) => (bsl a b c); // We only have to look for constant vectors here since the general, variable; // case can be handled in TableGen.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:115,Integrability,depend,depending,115,"// Given a tree of and/or(csel(0, 1, cc0), csel(0, 1, cc1)), we may be able to; // convert to csel(ccmp(.., cc0)), depending on cc1:; // (AND (CSET cc0 cmp0) (CSET cc1 (CMP x1 y1))); // =>; // (CSET cc1 (CCMP x1 y1 !cc1 cc0 cmp0)); //; // (OR (CSET cc0 cmp0) (CSET cc1 (CMP x1 y1))); // =>; // (CSET cc1 (CCMP x1 y1 cc1 !cc0 cmp0))",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:132,Safety,avoid,avoid,132,"// CCMP accept the constant int the range [0, 31]; // if the Op1 is a constant in the range [-31, -1], we; // can select to CCMN to avoid the extra mov",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:12,Modifiability,extend,extend,12,// Zero/any extend of an unsigned unpack,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:10,Availability,mask,mask,10,"// If the mask is fully covered by the unpack, we don't need to push; // a new AND onto the operand",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:58,Availability,mask,mask,58,"// If this is 'and (uunpklo/hi (extload MemTy -> ExtTy)), mask', then check; // to see if the mask is all-ones of size MemTy.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:94,Availability,mask,mask,94,"// If this is 'and (uunpklo/hi (extload MemTy -> ExtTy)), mask', then check; // to see if the mask is all-ones of size MemTy.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:50,Modifiability,extend,extend,50,"// SVE load instructions perform an implicit zero-extend, which makes them; // perfect candidates for combining.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:7,Performance,load,load,7,"// SVE load instructions perform an implicit zero-extend, which makes them; // perfect candidates for combining.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:25,Performance,perform,perform,25,"// SVE load instructions perform an implicit zero-extend, which makes them; // perfect candidates for combining.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:17,Performance,perform,performs,17,// This function performs an optimization on a specific pattern involving; // an AND operation and SETCC (Set Condition Code) node.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:29,Performance,optimiz,optimization,29,// This function performs an optimization on a specific pattern involving; // an AND operation and SETCC (Set Condition Code) node.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:123,Performance,optimiz,optimization,123,// Checks if the current node (N) is used by any SELECT instruction and; // returns an empty SDValue to avoid applying the optimization to prevent; // incorrect results,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:104,Safety,avoid,avoid,104,// Checks if the current node (N) is used by any SELECT instruction and; // returns an empty SDValue to avoid applying the optimization to prevent; // incorrect results,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:80,Energy Efficiency,reduce,reduce,80,"// Any bits known to already be 0 need not be cleared again, which can help; // reduce the size of the immediate to one supported by the instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:46,Usability,clear,cleared,46,"// Any bits known to already be 0 need not be cleared again, which can help; // reduce the size of the immediate to one supported by the instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Modifiability,Rewrite,Rewrite,3,"// Rewrite for pairwise fadd pattern; // (f32 (extract_vector_elt; // (fadd (vXf32 Other); // (vector_shuffle (vXf32 Other) undef <1,X,...> )) 0)); // ->; // (f32 (fadd (extract_vector_elt (vXf32 Other) 0); // (extract_vector_elt (vXf32 Other) 1)); // For strict_fadd we need to make sure the old strict_fadd can be deleted, so; // we can only do this when it's used only by the extract_vector_elt.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Performance,Optimiz,Optimize,3,"// Optimize concat_vectors of truncated vectors, where the intermediate; // type is illegal, to avoid said illegality, e.g.,; // (v4i16 (concat_vectors (v2i16 (truncate (v2i64))),; // (v2i16 (truncate (v2i64))))); // ->; // (v4i16 (truncate (vector_shuffle (v4i32 (bitcast (v2i64))),; // (v4i32 (bitcast (v2i64))),; // <0, 2, 4, 6>))); // This isn't really target-specific, but ISD::TRUNCATE legality isn't keyed; // on both input and result type, so we might generate worse code.; // On AArch64 we know it's fine for v2i64->v4i16 and v4i32->v8i8.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:96,Safety,avoid,avoid,96,"// Optimize concat_vectors of truncated vectors, where the intermediate; // type is illegal, to avoid said illegality, e.g.,; // (v4i16 (concat_vectors (v2i16 (truncate (v2i64))),; // (v2i16 (truncate (v2i64))))); // ->; // (v4i16 (truncate (vector_shuffle (v4i32 (bitcast (v2i64))),; // (v4i32 (bitcast (v2i64))),; // <0, 2, 4, 6>))); // This isn't really target-specific, but ISD::TRUNCATE legality isn't keyed; // on both input and result type, so we might generate worse code.; // On AArch64 we know it's fine for v2i64->v4i16 and v4i32->v8i8.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:162,Modifiability,extend,extend,162,"// If we have a concat of v4i8 loads, convert them to a buildvector of f32; // loads to prevent having to go through the v4i8 load legalization that; // needs to extend each element into a larger type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:31,Performance,load,loads,31,"// If we have a concat of v4i8 loads, convert them to a buildvector of f32; // loads to prevent having to go through the v4i8 load legalization that; // needs to extend each element into a larger type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:79,Performance,load,loads,79,"// If we have a concat of v4i8 loads, convert them to a buildvector of f32; // loads to prevent having to go through the v4i8 load legalization that; // needs to extend each element into a larger type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:126,Performance,load,load,126,"// If we have a concat of v4i8 loads, convert them to a buildvector of f32; // loads to prevent having to go through the v4i8 load legalization that; // needs to extend each element into a larger type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:159,Availability,redundant,redundant,159,"// Canonicalise concat_vectors to replace concatenations of truncated nots; // with nots of concatenated truncates. This in some cases allows for multiple; // redundant negations to be eliminated.; // (concat_vectors (v4i16 (truncate (not (v4i32)))),; // (v4i16 (truncate (not (v4i32))))); // ->; // (not (concat_vectors (v4i16 (truncate (v4i32))),; // (v4i16 (truncate (v4i32)))))",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:159,Safety,redund,redundant,159,"// Canonicalise concat_vectors to replace concatenations of truncated nots; // with nots of concatenated truncates. This in some cases allows for multiple; // redundant negations to be eliminated.; // (concat_vectors (v4i16 (truncate (not (v4i32)))),; // (v4i16 (truncate (not (v4i32))))); // ->; // (not (concat_vectors (v4i16 (truncate (v4i32))),; // (v4i16 (truncate (v4i32)))))",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:233,Integrability,depend,depend,233,"// Canonicalise concat_vectors so that the right-hand vector has as few; // bit-casts as possible before its real operation. The primary matching; // destination for these operations will be the narrowing ""2"" instructions,; // which depend on the operation being performed on this right-hand vector.; // For example,; // (concat_vectors LHS, (v1i64 (bitconvert (v4i16 RHS)))); // becomes; // (bitconvert (concat_vectors (v4i16 (bitconvert LHS)), RHS))",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:263,Performance,perform,performed,263,"// Canonicalise concat_vectors so that the right-hand vector has as few; // bit-casts as possible before its real operation. The primary matching; // destination for these operations will be the narrowing ""2"" instructions,; // which depend on the operation being performed on this right-hand vector.; // For example,; // (concat_vectors LHS, (v1i64 (bitconvert (v4i16 RHS)))); // becomes; // (bitconvert (concat_vectors (v4i16 (bitconvert LHS)), RHS))",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:371,Safety,avoid,avoid,371,"// Transform a scalar conversion of a value from a lane extract into a; // lane extract of a vector conversion. E.g., from foo1 to foo2:; // double foo1(int64x2_t a) { return vcvtd_n_f64_s64(a[1], 9); }; // double foo2(int64x2_t a) { return vcvtq_n_f64_s64(a, 9)[1]; }; //; // The second form interacts better with instruction selection and the; // register allocator to avoid cross-class register copies that aren't; // coalescable due to a lane reference.; // Check the operand and see if it originates from a lane extract.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:42,Performance,Perform,Perform,42,"// Yep, no additional predication needed. Perform the transform.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:480,Integrability,rout,routine,480,"// AArch64 high-vector ""long"" operations are formed by performing the non-high; // version on an extract_subvector of each operand which gets the high half:; //; // (longop2 LHS, RHS) == (longop (extract_high LHS), (extract_high RHS)); //; // However, there are cases which don't have an extract_high explicitly, but; // have another operation that can be made compatible with one for free. For; // example:; //; // (dupv64 scalar) --> (extract_high (dup128 scalar)); //; // This routine does the actual conversion of such DUPs, once outer routines; // have determined that everything else is in order.; // It also supports immediate DUP-like nodes (MOVI/MVNi), which we can fold; // similarly here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:540,Integrability,rout,routines,540,"// AArch64 high-vector ""long"" operations are formed by performing the non-high; // version on an extract_subvector of each operand which gets the high half:; //; // (longop2 LHS, RHS) == (longop (extract_high LHS), (extract_high RHS)); //; // However, there are cases which don't have an extract_high explicitly, but; // have another operation that can be made compatible with one for free. For; // example:; //; // (dupv64 scalar) --> (extract_high (dup128 scalar)); //; // This routine does the actual conversion of such DUPs, once outer routines; // have determined that everything else is in order.; // It also supports immediate DUP-like nodes (MOVI/MVNi), which we can fold; // similarly here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:55,Performance,perform,performing,55,"// AArch64 high-vector ""long"" operations are formed by performing the non-high; // version on an extract_subvector of each operand which gets the high half:; //; // (longop2 LHS, RHS) == (longop (extract_high LHS), (extract_high RHS)); //; // However, there are cases which don't have an extract_high explicitly, but; // have another operation that can be made compatible with one for free. For; // example:; //; // (dupv64 scalar) --> (extract_high (dup128 scalar)); //; // This routine does the actual conversion of such DUPs, once outer routines; // have determined that everything else is in order.; // It also supports immediate DUP-like nodes (MOVI/MVNi), which we can fold; // similarly here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Deployability,Update,Update,3,// Update the comparison when we are interested in !cc.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:26,Performance,perform,perform,26,"// The folding we want to perform is:; // (add x, [zext] (setcc cc ...) ); // -->; // (csel x, (add x, 1), !cc ...); //; // The latter will get matched to a CSINC instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:56,Performance,perform,perform,56,"// If both operands are a SET_CC, then we don't want to perform this; // folding and create another csel as this results in more instructions; // (and higher register usage).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:4,Performance,Perform,Perform,4,"/// Perform the scalar expression combine in the form of:; /// CSEL(c, 1, cc) + b => CSINC(b+c, b, cc); /// CSNEG(c, -1, cc) + b => CSINC(b+c, b, cc)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:147,Availability,redundant,redundant,147,"// Try to fold; //; // (neg (csel X, Y)) -> (csel (neg X), (neg Y)); //; // The folding helps csel to be matched with csneg without generating; // redundant neg instruction, which includes negation of the csel expansion; // of abs node lowered by lowerABS.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:147,Safety,redund,redundant,147,"// Try to fold; //; // (neg (csel X, Y)) -> (csel (neg X), (neg Y)); //; // The folding helps csel to be matched with csneg without generating; // redundant neg instruction, which includes negation of the csel expansion; // of abs node lowered by lowerABS.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:31,Modifiability,extend,extended,31,// Make sure both branches are extended in the same way.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:199,Modifiability,extend,extends,199,"// Transform vector add(zext i8 to i32, zext i8 to i32); // into sext(add(zext(i8 to i16), zext(i8 to i16)) to i32); // This allows extra uses of saddl/uaddl at the lower vector widths, and less; // extends.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:116,Modifiability,extend,extended,116,"// A build vector of two extracted elements is equivalent to an; // extract subvector where the inner vector is any-extended to the; // extract_vector_elt VT.; // (build_vector (extract_elt_iXX_to_i32 vec Idx+0); // (extract_elt_iXX_to_i32 vec Idx+1)); // => (extract_subvector (anyext_iXX_to_i32 vec) Idx); // For now, only consider the v2i32 case, which arises as a result of; // legalization.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:54,Modifiability,extend,extending,54,"// Reminder, EXTRACT_VECTOR_ELT has the effect of any-extending to its VT.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:23,Modifiability,extend,extend,23,// Check an node is an extend or shift operand,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:48,Modifiability,extend,extend,48,// (N - Y) + Z --> (Z - Y) + N; // when N is an extend or shift operand,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:185,Performance,optimiz,optimization,185,"// DAGCombiner will revert the combination when Z is constant cause; // dead loop. So don't enable the combination when Z is constant.; // If Z is one use shift C, we also can't do the optimization.; // It will falling to self infinite loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:106,Availability,avail,available,106,"// Bail out when value type is not one of {i32, i64}, since AArch64 ADD with; // shifted register is only available for i32 and i64.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:289,Performance,latency,latency,289,"// If both operand are shifted by imm and shift amount is not greater than 4; // for one operand, swap LHS and RHS to put operand with smaller shift amount; // on RHS.; //; // On many AArch64 processors (Cortex A78, Neoverse N1/N2/V1, etc), ADD with; // LSL shift (shift <= 4) has smaller latency and larger throughput than ADD; // with LSL (shift > 4). For the rest of processors, this is no-op for; // performance or correctness.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:308,Performance,throughput,throughput,308,"// If both operand are shifted by imm and shift amount is not greater than 4; // for one operand, swap LHS and RHS to put operand with smaller shift amount; // on RHS.; //; // On many AArch64 processors (Cortex A78, Neoverse N1/N2/V1, etc), ADD with; // LSL shift (shift <= 4) has smaller latency and larger throughput than ADD; // with LSL (shift > 4). For the rest of processors, this is no-op for; // performance or correctness.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:404,Performance,perform,performance,404,"// If both operand are shifted by imm and shift amount is not greater than 4; // for one operand, swap LHS and RHS to put operand with smaller shift amount; // on RHS.; //; // On many AArch64 processors (Cortex A78, Neoverse N1/N2/V1, etc), ADD with; // LSL shift (shift <= 4) has smaller latency and larger throughput than ADD; // with LSL (shift > 4). For the rest of processors, this is no-op for; // performance or correctness.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:170,Availability,avail,available,170,"// Combine into mla/mls.; // This works on the patterns of:; // add v1, (mul v2, v3); // sub v1, (mul v2, v3); // for vectors of type <1 x i64> and <2 x i64> when SVE is available.; // It will transform the add/sub to a scalable version, so that we can; // make use of SVE's MLA/MLS that will be generated for that pattern",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:220,Performance,scalab,scalable,220,"// Combine into mla/mls.; // This works on the patterns of:; // add v1, (mul v2, v3); // sub v1, (mul v2, v3); // for vectors of type <1 x i64> and <2 x i64> when SVE is available.; // It will transform the add/sub to a scalable version, so that we can; // make use of SVE's MLA/MLS that will be generated for that pattern",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:52,Availability,avail,available,52,"// Before using SVE's features, check first if it's available.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:146,Performance,load,load,146,"// At least one of the operands should be an extract, and the other should be; // something that is easy to convert to v1i64 type (in this case a load).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:70,Performance,load,loads,70,"// Try to find a tree of shuffles and concats from how IR shuffles of loads; // are lowered. Note that this only comes up because we do not always visit; // operands before uses. After that is fixed this can be removed and in the; // meantime this is fairly specific to the lowering we expect from IR.; // t46: v16i8 = vector_shuffle<0,1,2,3,4,5,6,7,8,9,10,11,16,17,18,19> t44, t45; // t44: v16i8 = vector_shuffle<0,1,2,3,4,5,6,7,16,17,18,19,u,u,u,u> t42, t43; // t42: v16i8 = concat_vectors t40, t36, undef:v4i8, undef:v4i8; // t40: v4i8,ch = load<(load (s32) from %ir.17)> t0, t22, undef:i64; // t36: v4i8,ch = load<(load (s32) from %ir.13)> t0, t18, undef:i64; // t43: v16i8 = concat_vectors t32, undef:v4i8, undef:v4i8, undef:v4i8; // t32: v4i8,ch = load<(load (s32) from %ir.9)> t0, t14, undef:i64; // t45: v16i8 = concat_vectors t28, undef:v4i8, undef:v4i8, undef:v4i8; // t28: v4i8,ch = load<(load (s32) from %ir.0)> t0, t2, undef:i64",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:544,Performance,load,load,544,"// Try to find a tree of shuffles and concats from how IR shuffles of loads; // are lowered. Note that this only comes up because we do not always visit; // operands before uses. After that is fixed this can be removed and in the; // meantime this is fairly specific to the lowering we expect from IR.; // t46: v16i8 = vector_shuffle<0,1,2,3,4,5,6,7,8,9,10,11,16,17,18,19> t44, t45; // t44: v16i8 = vector_shuffle<0,1,2,3,4,5,6,7,16,17,18,19,u,u,u,u> t42, t43; // t42: v16i8 = concat_vectors t40, t36, undef:v4i8, undef:v4i8; // t40: v4i8,ch = load<(load (s32) from %ir.17)> t0, t22, undef:i64; // t36: v4i8,ch = load<(load (s32) from %ir.13)> t0, t18, undef:i64; // t43: v16i8 = concat_vectors t32, undef:v4i8, undef:v4i8, undef:v4i8; // t32: v4i8,ch = load<(load (s32) from %ir.9)> t0, t14, undef:i64; // t45: v16i8 = concat_vectors t28, undef:v4i8, undef:v4i8, undef:v4i8; // t28: v4i8,ch = load<(load (s32) from %ir.0)> t0, t2, undef:i64",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:550,Performance,load,load,550,"// Try to find a tree of shuffles and concats from how IR shuffles of loads; // are lowered. Note that this only comes up because we do not always visit; // operands before uses. After that is fixed this can be removed and in the; // meantime this is fairly specific to the lowering we expect from IR.; // t46: v16i8 = vector_shuffle<0,1,2,3,4,5,6,7,8,9,10,11,16,17,18,19> t44, t45; // t44: v16i8 = vector_shuffle<0,1,2,3,4,5,6,7,16,17,18,19,u,u,u,u> t42, t43; // t42: v16i8 = concat_vectors t40, t36, undef:v4i8, undef:v4i8; // t40: v4i8,ch = load<(load (s32) from %ir.17)> t0, t22, undef:i64; // t36: v4i8,ch = load<(load (s32) from %ir.13)> t0, t18, undef:i64; // t43: v16i8 = concat_vectors t32, undef:v4i8, undef:v4i8, undef:v4i8; // t32: v4i8,ch = load<(load (s32) from %ir.9)> t0, t14, undef:i64; // t45: v16i8 = concat_vectors t28, undef:v4i8, undef:v4i8, undef:v4i8; // t28: v4i8,ch = load<(load (s32) from %ir.0)> t0, t2, undef:i64",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:613,Performance,load,load,613,"// Try to find a tree of shuffles and concats from how IR shuffles of loads; // are lowered. Note that this only comes up because we do not always visit; // operands before uses. After that is fixed this can be removed and in the; // meantime this is fairly specific to the lowering we expect from IR.; // t46: v16i8 = vector_shuffle<0,1,2,3,4,5,6,7,8,9,10,11,16,17,18,19> t44, t45; // t44: v16i8 = vector_shuffle<0,1,2,3,4,5,6,7,16,17,18,19,u,u,u,u> t42, t43; // t42: v16i8 = concat_vectors t40, t36, undef:v4i8, undef:v4i8; // t40: v4i8,ch = load<(load (s32) from %ir.17)> t0, t22, undef:i64; // t36: v4i8,ch = load<(load (s32) from %ir.13)> t0, t18, undef:i64; // t43: v16i8 = concat_vectors t32, undef:v4i8, undef:v4i8, undef:v4i8; // t32: v4i8,ch = load<(load (s32) from %ir.9)> t0, t14, undef:i64; // t45: v16i8 = concat_vectors t28, undef:v4i8, undef:v4i8, undef:v4i8; // t28: v4i8,ch = load<(load (s32) from %ir.0)> t0, t2, undef:i64",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:619,Performance,load,load,619,"// Try to find a tree of shuffles and concats from how IR shuffles of loads; // are lowered. Note that this only comes up because we do not always visit; // operands before uses. After that is fixed this can be removed and in the; // meantime this is fairly specific to the lowering we expect from IR.; // t46: v16i8 = vector_shuffle<0,1,2,3,4,5,6,7,8,9,10,11,16,17,18,19> t44, t45; // t44: v16i8 = vector_shuffle<0,1,2,3,4,5,6,7,16,17,18,19,u,u,u,u> t42, t43; // t42: v16i8 = concat_vectors t40, t36, undef:v4i8, undef:v4i8; // t40: v4i8,ch = load<(load (s32) from %ir.17)> t0, t22, undef:i64; // t36: v4i8,ch = load<(load (s32) from %ir.13)> t0, t18, undef:i64; // t43: v16i8 = concat_vectors t32, undef:v4i8, undef:v4i8, undef:v4i8; // t32: v4i8,ch = load<(load (s32) from %ir.9)> t0, t14, undef:i64; // t45: v16i8 = concat_vectors t28, undef:v4i8, undef:v4i8, undef:v4i8; // t28: v4i8,ch = load<(load (s32) from %ir.0)> t0, t2, undef:i64",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:754,Performance,load,load,754,"// Try to find a tree of shuffles and concats from how IR shuffles of loads; // are lowered. Note that this only comes up because we do not always visit; // operands before uses. After that is fixed this can be removed and in the; // meantime this is fairly specific to the lowering we expect from IR.; // t46: v16i8 = vector_shuffle<0,1,2,3,4,5,6,7,8,9,10,11,16,17,18,19> t44, t45; // t44: v16i8 = vector_shuffle<0,1,2,3,4,5,6,7,16,17,18,19,u,u,u,u> t42, t43; // t42: v16i8 = concat_vectors t40, t36, undef:v4i8, undef:v4i8; // t40: v4i8,ch = load<(load (s32) from %ir.17)> t0, t22, undef:i64; // t36: v4i8,ch = load<(load (s32) from %ir.13)> t0, t18, undef:i64; // t43: v16i8 = concat_vectors t32, undef:v4i8, undef:v4i8, undef:v4i8; // t32: v4i8,ch = load<(load (s32) from %ir.9)> t0, t14, undef:i64; // t45: v16i8 = concat_vectors t28, undef:v4i8, undef:v4i8, undef:v4i8; // t28: v4i8,ch = load<(load (s32) from %ir.0)> t0, t2, undef:i64",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:760,Performance,load,load,760,"// Try to find a tree of shuffles and concats from how IR shuffles of loads; // are lowered. Note that this only comes up because we do not always visit; // operands before uses. After that is fixed this can be removed and in the; // meantime this is fairly specific to the lowering we expect from IR.; // t46: v16i8 = vector_shuffle<0,1,2,3,4,5,6,7,8,9,10,11,16,17,18,19> t44, t45; // t44: v16i8 = vector_shuffle<0,1,2,3,4,5,6,7,16,17,18,19,u,u,u,u> t42, t43; // t42: v16i8 = concat_vectors t40, t36, undef:v4i8, undef:v4i8; // t40: v4i8,ch = load<(load (s32) from %ir.17)> t0, t22, undef:i64; // t36: v4i8,ch = load<(load (s32) from %ir.13)> t0, t18, undef:i64; // t43: v16i8 = concat_vectors t32, undef:v4i8, undef:v4i8, undef:v4i8; // t32: v4i8,ch = load<(load (s32) from %ir.9)> t0, t14, undef:i64; // t45: v16i8 = concat_vectors t28, undef:v4i8, undef:v4i8, undef:v4i8; // t28: v4i8,ch = load<(load (s32) from %ir.0)> t0, t2, undef:i64",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:894,Performance,load,load,894,"// Try to find a tree of shuffles and concats from how IR shuffles of loads; // are lowered. Note that this only comes up because we do not always visit; // operands before uses. After that is fixed this can be removed and in the; // meantime this is fairly specific to the lowering we expect from IR.; // t46: v16i8 = vector_shuffle<0,1,2,3,4,5,6,7,8,9,10,11,16,17,18,19> t44, t45; // t44: v16i8 = vector_shuffle<0,1,2,3,4,5,6,7,16,17,18,19,u,u,u,u> t42, t43; // t42: v16i8 = concat_vectors t40, t36, undef:v4i8, undef:v4i8; // t40: v4i8,ch = load<(load (s32) from %ir.17)> t0, t22, undef:i64; // t36: v4i8,ch = load<(load (s32) from %ir.13)> t0, t18, undef:i64; // t43: v16i8 = concat_vectors t32, undef:v4i8, undef:v4i8, undef:v4i8; // t32: v4i8,ch = load<(load (s32) from %ir.9)> t0, t14, undef:i64; // t45: v16i8 = concat_vectors t28, undef:v4i8, undef:v4i8, undef:v4i8; // t28: v4i8,ch = load<(load (s32) from %ir.0)> t0, t2, undef:i64",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:900,Performance,load,load,900,"// Try to find a tree of shuffles and concats from how IR shuffles of loads; // are lowered. Note that this only comes up because we do not always visit; // operands before uses. After that is fixed this can be removed and in the; // meantime this is fairly specific to the lowering we expect from IR.; // t46: v16i8 = vector_shuffle<0,1,2,3,4,5,6,7,8,9,10,11,16,17,18,19> t44, t45; // t44: v16i8 = vector_shuffle<0,1,2,3,4,5,6,7,16,17,18,19,u,u,u,u> t42, t43; // t42: v16i8 = concat_vectors t40, t36, undef:v4i8, undef:v4i8; // t40: v4i8,ch = load<(load (s32) from %ir.17)> t0, t22, undef:i64; // t36: v4i8,ch = load<(load (s32) from %ir.13)> t0, t18, undef:i64; // t43: v16i8 = concat_vectors t32, undef:v4i8, undef:v4i8, undef:v4i8; // t32: v4i8,ch = load<(load (s32) from %ir.9)> t0, t14, undef:i64; // t45: v16i8 = concat_vectors t28, undef:v4i8, undef:v4i8, undef:v4i8; // t28: v4i8,ch = load<(load (s32) from %ir.0)> t0, t2, undef:i64",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:702,Energy Efficiency,efficient,efficient,702,"// This method attempts to fold trees of add(ext(load p), shl(ext(load p+4)); // into a single load of twice the size, that we extract the bottom part and top; // part so that the shl can use a shll2 instruction. The two loads in that; // example can also be larger trees of instructions, which are identical except; // for the leaves which are all loads offset from the LHS, including; // buildvectors of multiple loads. For example the RHS tree could be; // sub(zext(buildvec(load p+4, load q+4)), zext(buildvec(load r+4, load s+4))); // Whilst it can be common for the larger loads to replace LDP instructions; // (which doesn't gain anything on it's own), the larger loads can help create; // more efficient code, and in buildvectors prevent the need for ld1 lane; // inserts which can be slower than normal loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:49,Performance,load,load,49,"// This method attempts to fold trees of add(ext(load p), shl(ext(load p+4)); // into a single load of twice the size, that we extract the bottom part and top; // part so that the shl can use a shll2 instruction. The two loads in that; // example can also be larger trees of instructions, which are identical except; // for the leaves which are all loads offset from the LHS, including; // buildvectors of multiple loads. For example the RHS tree could be; // sub(zext(buildvec(load p+4, load q+4)), zext(buildvec(load r+4, load s+4))); // Whilst it can be common for the larger loads to replace LDP instructions; // (which doesn't gain anything on it's own), the larger loads can help create; // more efficient code, and in buildvectors prevent the need for ld1 lane; // inserts which can be slower than normal loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:66,Performance,load,load,66,"// This method attempts to fold trees of add(ext(load p), shl(ext(load p+4)); // into a single load of twice the size, that we extract the bottom part and top; // part so that the shl can use a shll2 instruction. The two loads in that; // example can also be larger trees of instructions, which are identical except; // for the leaves which are all loads offset from the LHS, including; // buildvectors of multiple loads. For example the RHS tree could be; // sub(zext(buildvec(load p+4, load q+4)), zext(buildvec(load r+4, load s+4))); // Whilst it can be common for the larger loads to replace LDP instructions; // (which doesn't gain anything on it's own), the larger loads can help create; // more efficient code, and in buildvectors prevent the need for ld1 lane; // inserts which can be slower than normal loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:95,Performance,load,load,95,"// This method attempts to fold trees of add(ext(load p), shl(ext(load p+4)); // into a single load of twice the size, that we extract the bottom part and top; // part so that the shl can use a shll2 instruction. The two loads in that; // example can also be larger trees of instructions, which are identical except; // for the leaves which are all loads offset from the LHS, including; // buildvectors of multiple loads. For example the RHS tree could be; // sub(zext(buildvec(load p+4, load q+4)), zext(buildvec(load r+4, load s+4))); // Whilst it can be common for the larger loads to replace LDP instructions; // (which doesn't gain anything on it's own), the larger loads can help create; // more efficient code, and in buildvectors prevent the need for ld1 lane; // inserts which can be slower than normal loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:221,Performance,load,loads,221,"// This method attempts to fold trees of add(ext(load p), shl(ext(load p+4)); // into a single load of twice the size, that we extract the bottom part and top; // part so that the shl can use a shll2 instruction. The two loads in that; // example can also be larger trees of instructions, which are identical except; // for the leaves which are all loads offset from the LHS, including; // buildvectors of multiple loads. For example the RHS tree could be; // sub(zext(buildvec(load p+4, load q+4)), zext(buildvec(load r+4, load s+4))); // Whilst it can be common for the larger loads to replace LDP instructions; // (which doesn't gain anything on it's own), the larger loads can help create; // more efficient code, and in buildvectors prevent the need for ld1 lane; // inserts which can be slower than normal loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:349,Performance,load,loads,349,"// This method attempts to fold trees of add(ext(load p), shl(ext(load p+4)); // into a single load of twice the size, that we extract the bottom part and top; // part so that the shl can use a shll2 instruction. The two loads in that; // example can also be larger trees of instructions, which are identical except; // for the leaves which are all loads offset from the LHS, including; // buildvectors of multiple loads. For example the RHS tree could be; // sub(zext(buildvec(load p+4, load q+4)), zext(buildvec(load r+4, load s+4))); // Whilst it can be common for the larger loads to replace LDP instructions; // (which doesn't gain anything on it's own), the larger loads can help create; // more efficient code, and in buildvectors prevent the need for ld1 lane; // inserts which can be slower than normal loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:415,Performance,load,loads,415,"// This method attempts to fold trees of add(ext(load p), shl(ext(load p+4)); // into a single load of twice the size, that we extract the bottom part and top; // part so that the shl can use a shll2 instruction. The two loads in that; // example can also be larger trees of instructions, which are identical except; // for the leaves which are all loads offset from the LHS, including; // buildvectors of multiple loads. For example the RHS tree could be; // sub(zext(buildvec(load p+4, load q+4)), zext(buildvec(load r+4, load s+4))); // Whilst it can be common for the larger loads to replace LDP instructions; // (which doesn't gain anything on it's own), the larger loads can help create; // more efficient code, and in buildvectors prevent the need for ld1 lane; // inserts which can be slower than normal loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:478,Performance,load,load,478,"// This method attempts to fold trees of add(ext(load p), shl(ext(load p+4)); // into a single load of twice the size, that we extract the bottom part and top; // part so that the shl can use a shll2 instruction. The two loads in that; // example can also be larger trees of instructions, which are identical except; // for the leaves which are all loads offset from the LHS, including; // buildvectors of multiple loads. For example the RHS tree could be; // sub(zext(buildvec(load p+4, load q+4)), zext(buildvec(load r+4, load s+4))); // Whilst it can be common for the larger loads to replace LDP instructions; // (which doesn't gain anything on it's own), the larger loads can help create; // more efficient code, and in buildvectors prevent the need for ld1 lane; // inserts which can be slower than normal loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:488,Performance,load,load,488,"// This method attempts to fold trees of add(ext(load p), shl(ext(load p+4)); // into a single load of twice the size, that we extract the bottom part and top; // part so that the shl can use a shll2 instruction. The two loads in that; // example can also be larger trees of instructions, which are identical except; // for the leaves which are all loads offset from the LHS, including; // buildvectors of multiple loads. For example the RHS tree could be; // sub(zext(buildvec(load p+4, load q+4)), zext(buildvec(load r+4, load s+4))); // Whilst it can be common for the larger loads to replace LDP instructions; // (which doesn't gain anything on it's own), the larger loads can help create; // more efficient code, and in buildvectors prevent the need for ld1 lane; // inserts which can be slower than normal loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:514,Performance,load,load,514,"// This method attempts to fold trees of add(ext(load p), shl(ext(load p+4)); // into a single load of twice the size, that we extract the bottom part and top; // part so that the shl can use a shll2 instruction. The two loads in that; // example can also be larger trees of instructions, which are identical except; // for the leaves which are all loads offset from the LHS, including; // buildvectors of multiple loads. For example the RHS tree could be; // sub(zext(buildvec(load p+4, load q+4)), zext(buildvec(load r+4, load s+4))); // Whilst it can be common for the larger loads to replace LDP instructions; // (which doesn't gain anything on it's own), the larger loads can help create; // more efficient code, and in buildvectors prevent the need for ld1 lane; // inserts which can be slower than normal loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:524,Performance,load,load,524,"// This method attempts to fold trees of add(ext(load p), shl(ext(load p+4)); // into a single load of twice the size, that we extract the bottom part and top; // part so that the shl can use a shll2 instruction. The two loads in that; // example can also be larger trees of instructions, which are identical except; // for the leaves which are all loads offset from the LHS, including; // buildvectors of multiple loads. For example the RHS tree could be; // sub(zext(buildvec(load p+4, load q+4)), zext(buildvec(load r+4, load s+4))); // Whilst it can be common for the larger loads to replace LDP instructions; // (which doesn't gain anything on it's own), the larger loads can help create; // more efficient code, and in buildvectors prevent the need for ld1 lane; // inserts which can be slower than normal loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:579,Performance,load,loads,579,"// This method attempts to fold trees of add(ext(load p), shl(ext(load p+4)); // into a single load of twice the size, that we extract the bottom part and top; // part so that the shl can use a shll2 instruction. The two loads in that; // example can also be larger trees of instructions, which are identical except; // for the leaves which are all loads offset from the LHS, including; // buildvectors of multiple loads. For example the RHS tree could be; // sub(zext(buildvec(load p+4, load q+4)), zext(buildvec(load r+4, load s+4))); // Whilst it can be common for the larger loads to replace LDP instructions; // (which doesn't gain anything on it's own), the larger loads can help create; // more efficient code, and in buildvectors prevent the need for ld1 lane; // inserts which can be slower than normal loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:671,Performance,load,loads,671,"// This method attempts to fold trees of add(ext(load p), shl(ext(load p+4)); // into a single load of twice the size, that we extract the bottom part and top; // part so that the shl can use a shll2 instruction. The two loads in that; // example can also be larger trees of instructions, which are identical except; // for the leaves which are all loads offset from the LHS, including; // buildvectors of multiple loads. For example the RHS tree could be; // sub(zext(buildvec(load p+4, load q+4)), zext(buildvec(load r+4, load s+4))); // Whilst it can be common for the larger loads to replace LDP instructions; // (which doesn't gain anything on it's own), the larger loads can help create; // more efficient code, and in buildvectors prevent the need for ld1 lane; // inserts which can be slower than normal loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:812,Performance,load,loads,812,"// This method attempts to fold trees of add(ext(load p), shl(ext(load p+4)); // into a single load of twice the size, that we extract the bottom part and top; // part so that the shl can use a shll2 instruction. The two loads in that; // example can also be larger trees of instructions, which are identical except; // for the leaves which are all loads offset from the LHS, including; // buildvectors of multiple loads. For example the RHS tree could be; // sub(zext(buildvec(load p+4, load q+4)), zext(buildvec(load r+4, load s+4))); // Whilst it can be common for the larger loads to replace LDP instructions; // (which doesn't gain anything on it's own), the larger loads can help create; // more efficient code, and in buildvectors prevent the need for ld1 lane; // inserts which can be slower than normal loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:133,Modifiability,extend,extend,133,"// Attempt to rule out some unprofitable cases using heuristics (some working; // around suboptimal code generation), notably if the extend not be able to; // use ushll2 instructions as the types are not large enough. Otherwise zip's; // will need to be created which can increase the instruction count.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:43,Performance,load,loads,43,// Recreate the tree with the new combined loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:42,Modifiability,extend,extend,42,"// Extract the top and bottom lanes, then extend the result. Possibly extend; // the result then extract the lanes if the two operands match as it produces; // slightly smaller code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:70,Modifiability,extend,extend,70,"// Extract the top and bottom lanes, then extend the result. Possibly extend; // the result then extract the lanes if the two operands match as it produces; // slightly smaller code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:59,Performance,perform,perform,59,"// For positive shift amounts we can use SHL, as ushl/sshl perform a regular; // left shift for positive shift amounts. For negative shifts we can use a; // VASHR/VLSHR as appropiate.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:110,Performance,scalab,scalable,110,// We can use the SVE whilelo instruction to lower this intrinsic by; // creating the appropriate sequence of scalable vector operations and; // then extracting a fixed-width subvector from the scalable vector.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:194,Performance,scalab,scalable,194,// We can use the SVE whilelo instruction to lower this intrinsic by; // creating the appropriate sequence of scalable vector operations and; // then extracting a fixed-width subvector from the scalable vector.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:16,Performance,scalab,scalable,16,"// Get promoted scalable vector VT, i.e. promote nxv4i1 -> nxv4i32.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:58,Modifiability,extend,extend,58,"// If we have (sext (setcc A B)) and A and B are cheap to extend,; // we can move the sext into the arguments and have the same result. For; // example, if A and B are both loads, we can make those extending loads and; // avoid an extra instruction. This pattern appears often in VLS code; // generation where the inputs to the setcc have a different size to the; // instruction that wants to use the result of the setcc.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:198,Modifiability,extend,extending,198,"// If we have (sext (setcc A B)) and A and B are cheap to extend,; // we can move the sext into the arguments and have the same result. For; // example, if A and B are both loads, we can make those extending loads and; // avoid an extra instruction. This pattern appears often in VLS code; // generation where the inputs to the setcc have a different size to the; // instruction that wants to use the result of the setcc.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:173,Performance,load,loads,173,"// If we have (sext (setcc A B)) and A and B are cheap to extend,; // we can move the sext into the arguments and have the same result. For; // example, if A and B are both loads, we can make those extending loads and; // avoid an extra instruction. This pattern appears often in VLS code; // generation where the inputs to the setcc have a different size to the; // instruction that wants to use the result of the setcc.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:208,Performance,load,loads,208,"// If we have (sext (setcc A B)) and A and B are cheap to extend,; // we can move the sext into the arguments and have the same result. For; // example, if A and B are both loads, we can make those extending loads and; // avoid an extra instruction. This pattern appears often in VLS code; // generation where the inputs to the setcc have a different size to the; // instruction that wants to use the result of the setcc.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:222,Safety,avoid,avoid,222,"// If we have (sext (setcc A B)) and A and B are cheap to extend,; // we can move the sext into the arguments and have the same result. For; // example, if A and B are both loads, we can make those extending loads and; // avoid an extra instruction. This pattern appears often in VLS code; // generation where the inputs to the setcc have a different size to the; // instruction that wants to use the result of the setcc.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:68,Modifiability,extend,extended,68,// Returns an SVE type that ContentTy can be trivially sign or zero extended; // into.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:85,Performance,load,load,85,"/// Replace a splat of zeros to a vector store by scalar stores of WZR/XZR. The; /// load store optimizer pass will merge them to store pair stores. This should; /// be better than a movi to create the vector zero followed by a vector store; /// if the zero constant is not re-used, since one instructions and one register; /// live range will be removed.; ///; /// For example, the final generated code should be:; ///; /// stp xzr, xzr, [x0]; ///; /// instead of:; ///; /// movi v0.2d, #0; /// str q0, [x0]; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:96,Performance,optimiz,optimizer,96,"/// Replace a splat of zeros to a vector store by scalar stores of WZR/XZR. The; /// load store optimizer pass will merge them to store pair stores. This should; /// be better than a movi to create the vector zero followed by a vector store; /// if the zero constant is not re-used, since one instructions and one register; /// live range will be removed.; ///; /// For example, the final generated code should be:; ///; /// stp xzr, xzr, [x0]; ///; /// instead of:; ///; /// movi v0.2d, #0; /// str q0, [x0]; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:43,Performance,scalab,scalable,43,// Avoid scalarizing zero splat stores for scalable vectors.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Safety,Avoid,Avoid,3,// Avoid scalarizing zero splat stores for scalable vectors.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:46,Availability,down,down,46,"// If the store is truncating then it's going down to i16 or smaller, which; // means it can be implemented in a single store anyway.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:90,Usability,undo,undoing,90,// Use a CopyFromReg WZR/XZR here to prevent; // DAGCombiner::MergeConsecutiveStores from undoing this transformation.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:97,Performance,load,load,97,"/// Replace a splat of a scalar to a vector store by scalar stores of the scalar; /// value. The load store optimizer pass will merge them to store pair stores.; /// This has better performance than a splat of the scalar followed by a split; /// vector store. Even if the stores are not merged it is four stores vs a dup,; /// followed by an ext.b and two stores.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:108,Performance,optimiz,optimizer,108,"/// Replace a splat of a scalar to a vector store by scalar stores of the scalar; /// value. The load store optimizer pass will merge them to store pair stores.; /// This has better performance than a splat of the scalar followed by a split; /// vector store. Even if the stores are not merged it is four stores vs a dup,; /// followed by an ext.b and two stores.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:182,Performance,perform,performance,182,"/// Replace a splat of a scalar to a vector store by scalar stores of the scalar; /// value. The load store optimizer pass will merge them to store pair stores.; /// This has better performance than a splat of the scalar followed by a split; /// vector store. Even if the stores are not merged it is four stores vs a dup,; /// followed by an ext.b and two stores.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:46,Availability,down,down,46,"// If the store is truncating then it's going down to i16 or smaller, which; // means it can be implemented in a single store anyway.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:14,Testability,log,logic,14,"// FIXME: The logic for deciding if an unaligned store should be split should; // be included in TLI.allowsMisalignedMemoryAccesses(), and there should be; // a call to that function here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:98,Performance,perform,performance,98,// Don't split v2i64 vectors. Memcpy lowering produces those and splitting; // those up regresses performance on micro-benchmarks and olden/bh.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:119,Testability,benchmark,benchmarks,119,// Don't split v2i64 vectors. Memcpy lowering produces those and splitting; // those up regresses performance on micro-benchmarks and olden/bh.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:53,Performance,perform,performance,53,"// Split unaligned 16B stores. They are terrible for performance.; // Don't split stores with alignment of 1 or 2. Code that uses clang vector; // extensions can use this to mark that it does not want splitting to happen; // (by underspecifying alignment to be 1 or 2). Furthermore, the chance of; // eliminating alignment hazards is only 1 in 8 for alignment of 2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:323,Safety,hazard,hazards,323,"// Split unaligned 16B stores. They are terrible for performance.; // Don't split stores with alignment of 1 or 2. Code that uses clang vector; // extensions can use this to mark that it does not want splitting to happen; // (by underspecifying alignment to be 1 or 2). Furthermore, the chance of; // eliminating alignment hazards is only 1 in 8 for alignment of 2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:16,Availability,mask,masked,16,"// If this is a masked load followed by an UUNPKLO, fold this into a masked; // extending load. We can do this even if this is already a masked; // {z,}extload.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:69,Availability,mask,masked,69,"// If this is a masked load followed by an UUNPKLO, fold this into a masked; // extending load. We can do this even if this is already a masked; // {z,}extload.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:137,Availability,mask,masked,137,"// If this is a masked load followed by an UUNPKLO, fold this into a masked; // extending load. We can do this even if this is already a masked; // {z,}extload.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:80,Modifiability,extend,extending,80,"// If this is a masked load followed by an UUNPKLO, fold this into a masked; // extending load. We can do this even if this is already a masked; // {z,}extload.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:23,Performance,load,load,23,"// If this is a masked load followed by an UUNPKLO, fold this into a masked; // extending load. We can do this even if this is already a masked; // {z,}extload.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:90,Performance,load,load,90,"// If this is a masked load followed by an UUNPKLO, fold this into a masked; // extending load. We can do this even if this is already a masked; // {z,}extload.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:250,Performance,perform,performed,250,"// Try to simplify:; // t1 = nxv8i16 add(X, 1 << (ShiftValue - 1)); // t2 = nxv8i16 srl(t1, ShiftValue); // to; // t1 = nxv8i16 rshrnb(X, shiftvalue).; // rshrnb will zero the top half bits of each element. Therefore, this combine; // should only be performed when a following instruction with the rshrnb; // as an operand does not care about the top half of each element. For example,; // a uzp1 or a truncating store.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:10,Usability,simpl,simplify,10,"// Try to simplify:; // t1 = nxv8i16 add(X, 1 << (ShiftValue - 1)); // t2 = nxv8i16 srl(t1, ShiftValue); // to; // t1 = nxv8i16 rshrnb(X, shiftvalue).; // rshrnb will zero the top half bits of each element. Therefore, this combine; // should only be performed when a following instruction with the rshrnb; // as an operand does not care about the top half of each element. For example,; // a uzp1 or a truncating store.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:8,Performance,optimiz,optimization,8,// This optimization only works on little endian.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:19,Performance,load,loads,19,// unsigned gather loads,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:17,Performance,load,loads,17,// signed gather loads,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:42,Modifiability,extend,extended,42,// If the predicate for the sign- or zero-extended offset is the; // same as the predicate used for this load and the sign-/zero-extension; // was from a 32-bits...,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:105,Performance,load,load,105,// If the predicate for the sign- or zero-extended offset is the; // same as the predicate used for this load and the sign-/zero-extension; // was from a 32-bits...,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:4,Performance,Optimiz,Optimize,4,/// Optimize a vector shift instruction and its operand if shifted out; /// bits are not used.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:100,Performance,perform,performSetCCPunpkCombine,100,// sunpklo(sext(pred)) -> sext(extract_low_half(pred)); // This transform works in partnership with performSetCCPunpkCombine to; // remove unnecessary transfer of predicates into standard registers and back,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:16,Performance,LOAD,LOAD,16,"// If it is not LOAD, can not do such combine.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:90,Performance,load,load,90,"// Check if there are other uses. If so, do not combine as it will introduce; // an extra load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:134,Availability,reliab,reliably,134,"// If there is one use and it can splat the value, prefer that operation.; // TODO: This could be expanded to more operations if they reliably use the; // index variants.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:58,Performance,load,load,58,// To avoid cycle construction make sure that neither the load nor the add; // are predecessors to each other or the Vector.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:6,Safety,avoid,avoid,6,// To avoid cycle construction make sure that neither the load nor the add; // are predecessors to each other or the Vector.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Deployability,Update,Update,3,// Update the uses.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:17,Performance,load,load,17,// The result of load,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:4,Usability,Simpl,Simplify,4,/// Simplify ``Addr`` given that the top byte of it is ignored by HW during; /// address translation.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Performance,Perform,Perform,3,// Perform TBI simplification if supported by the target and try to break up; // nontemporal loads larger than 256-bits loads for odd types so LDNPQ 256-bit; // load instructions can be selected.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:93,Performance,load,loads,93,// Perform TBI simplification if supported by the target and try to break up; // nontemporal loads larger than 256-bits loads for odd types so LDNPQ 256-bit; // load instructions can be selected.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:120,Performance,load,loads,120,// Perform TBI simplification if supported by the target and try to break up; // nontemporal loads larger than 256-bits loads for odd types so LDNPQ 256-bit; // load instructions can be selected.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:161,Performance,load,load,161,// Perform TBI simplification if supported by the target and try to break up; // nontemporal loads larger than 256-bits loads for odd types so LDNPQ 256-bit; // load instructions can be selected.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:15,Usability,simpl,simplification,15,// Perform TBI simplification if supported by the target and try to break up; // nontemporal loads larger than 256-bits loads for odd types so LDNPQ 256-bit; // load instructions can be selected.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:167,Energy Efficiency,reduce,reduce,167,// Replace any non temporal load over 256-bit with a series of 256 bit loads; // and a scalar/vector load less than 256. This way we can utilize 256-bit; // loads and reduce the amount of load instructions generated.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:28,Performance,load,load,28,// Replace any non temporal load over 256-bit with a series of 256 bit loads; // and a scalar/vector load less than 256. This way we can utilize 256-bit; // loads and reduce the amount of load instructions generated.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:71,Performance,load,loads,71,// Replace any non temporal load over 256-bit with a series of 256 bit loads; // and a scalar/vector load less than 256. This way we can utilize 256-bit; // loads and reduce the amount of load instructions generated.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:101,Performance,load,load,101,// Replace any non temporal load over 256-bit with a series of 256 bit loads; // and a scalar/vector load less than 256. This way we can utilize 256-bit; // loads and reduce the amount of load instructions generated.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:157,Performance,load,loads,157,// Replace any non temporal load over 256-bit with a series of 256 bit loads; // and a scalar/vector load less than 256. This way we can utilize 256-bit; // loads and reduce the amount of load instructions generated.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:188,Performance,load,load,188,// Replace any non temporal load over 256-bit with a series of 256 bit loads; // and a scalar/vector load less than 256. This way we can utilize 256-bit; // loads and reduce the amount of load instructions generated.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:22,Performance,load,loads,22,// Create all 256-bit loads starting from offset 0 and up to Num256Loads-1*32.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:33,Performance,load,load,33,// Process remaining bits of the load operation.; // This is done by creating an UNDEF vector to match the size of the; // 256-bit loads and inserting the remaining load to it. We extract the; // original load type at the end using EXTRACT_SUBVECTOR instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:131,Performance,load,loads,131,// Process remaining bits of the load operation.; // This is done by creating an UNDEF vector to match the size of the; // 256-bit loads and inserting the remaining load to it. We extract the; // original load type at the end using EXTRACT_SUBVECTOR instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:165,Performance,load,load,165,// Process remaining bits of the load operation.; // This is done by creating an UNDEF vector to match the size of the; // 256-bit loads and inserting the remaining load to it. We extract the; // original load type at the end using EXTRACT_SUBVECTOR instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:205,Performance,load,load,205,// Process remaining bits of the load operation.; // This is done by creating an UNDEF vector to match the size of the; // 256-bit loads and inserting the remaining load to it. We extract the; // original load type at the end using EXTRACT_SUBVECTOR instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:166,Performance,perform,performs,166,"// When converting a <N x iX> vector to <N x i1> to store or use as a scalar; // iN, we can use a trick that extracts the i^th bit from the i^th element and; // then performs a vector add to get a scalar bitmask. This requires that each; // element's bits are either all 1 or all 0.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:92,Modifiability,extend,extend,92,"// If we can find the original types to work on instead of a vector of i1,; // we can avoid extend/extract conversion instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:86,Safety,avoid,avoid,86,"// If we can find the original types to work on instead of a vector of i1,; // we can avoid extend/extract conversion instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:62,Safety,avoid,avoid,62,"// Large vectors don't map directly to this conversion, so to avoid too many; // edge cases, we don't apply it here. The conversion will likely still be; // applied later via multiple smaller vectors, whose results are concatenated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:131,Availability,mask,mask,131,"// v16i8 is a special case, as we have 16 entries but only 8 positional bits; // per entry. We split it into two halves, apply the mask, zip the halves to; // create 8x 16-bit values, and the perform the vector reduce.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:211,Energy Efficiency,reduce,reduce,211,"// v16i8 is a special case, as we have 16 entries but only 8 positional bits; // per entry. We split it into two halves, apply the mask, zip the halves to; // create 8x 16-bit values, and the perform the vector reduce.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:192,Performance,perform,perform,192,"// v16i8 is a special case, as we have 16 entries but only 8 positional bits; // per entry. We split it into two halves, apply the mask, zip the halves to; // create 8x 16-bit values, and the perform the vector reduce.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:112,Energy Efficiency,efficient,efficiently,112,"// If we are storing a vector that we are currently building, let; // `scalarizeVectorStore()` handle this more efficiently.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:236,Availability,down,down,236,"// If this is an FP_ROUND followed by a store, fold this into a truncating; // store. We can do this even if this is already a truncstore.; // We purposefully don't care about legality of the nodes here as we know; // they can be split down into something legal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:35,Availability,mask,masked,35,"// If this is a UZP1 followed by a masked store, fold this into a masked; // truncating store. We can do this even if this is already a masked; // truncstore.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:66,Availability,mask,masked,66,"// If this is a UZP1 followed by a masked store, fold this into a masked; // truncating store. We can do this even if this is already a masked; // truncstore.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:136,Availability,mask,masked,136,"// If this is a UZP1 followed by a masked store, fold this into a masked; // truncating store. We can do this even if this is already a masked; // truncstore.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Usability,Simpl,Simplify,3,// Simplify:; // BasePtr = Ptr; // Index = X + splat(Offset); // ->; // BasePtr = Ptr + Offset * scale.; // Index = X,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Usability,Simpl,Simplify,3,// Simplify:; // BasePtr = Ptr; // Index = (X + splat(Offset)) << splat(Shift); // ->; // BasePtr = Ptr + (Offset << Shift) * scale); // Index = X << splat(shift),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:89,Availability,avail,available,89,// Analyse the specified address returning true if a more optimal addressing; // mode is available. When returning true all parameters are updated to reflect; // their recommended values.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:139,Deployability,update,updated,139,// Analyse the specified address returning true if a more optimal addressing; // mode is available. When returning true all parameters are updated to reflect; // their recommended values.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:75,Usability,simpl,simplify,75,// Try to iteratively fold parts of the index into the base pointer to; // simplify the index as much as possible.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:100,Modifiability,extend,extended,100,// Don't attempt to shrink the index for fixed vectors of 64 bit data since it; // will later be re-extended to 64 bits in legalization,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:99,Deployability,update,updates,99,/// Target-specific DAG combine function for NEON load/store intrinsics; /// to merge base address updates.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:50,Performance,load,load,50,/// Target-specific DAG combine function for NEON load/store intrinsics; /// to merge base address updates.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:44,Performance,load,load,44,"// Check that the add is independent of the load/store. Otherwise, folding; // it would create a cycle.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:40,Performance,load,load,40,// Find the new opcode for the updating load/store.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:22,Performance,Load,Load,22,// Incoming chain; // Load lane and store have vector list as input.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Deployability,Update,Update,3,// Update the uses.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:108,Availability,mask,mask,108,"// This function does a whole lot of voodoo to determine if the tests are; // equivalent without and with a mask. Essentially what happens is that given a; // DAG resembling:; //; // +-------------+ +-------------+ +-------------+ +-------------+; // | Input | | AddConstant | | CompConstant| | CC |; // +-------------+ +-------------+ +-------------+ +-------------+; // | | | |; // V V | +----------+; // +-------------+ +----+ | |; // | ADD | |0xff| | |; // +-------------+ +----+ | |; // | | | |; // V V | |; // +-------------+ | |; // | AND | | |; // +-------------+ | |; // | | |; // +-----+ | |; // | | |; // V V V; // +-------------+; // | CMP |; // +-------------+; //; // The AND node may be safely removed for some combinations of inputs. In; // particular we need to take into account the extension type of the Input,; // the exact values of AddConstant, CompConstant, and CC, along with the nominal; // width of the input (this can work for any width inputs, the above graph is; // specific to 8 bits.; //; // The specific equations were worked out by generating output tables for each; // AArch64CC value in terms of and AddConstant (w1), CompConstant(w2). The; // problem was simplified by working with 4 bit inputs, which means we only; // needed to reason about 24 distinct bit patterns: 8 patterns unique to zero; // extension (8,15), 8 patterns unique to sign extensions (-8,-1), and 8; // patterns present in both extensions (0,7). For every distinct set of; // AddConstant and CompConstants bit patterns we can consider the masked and; // unmasked versions to be equivalent if the result of this function is true for; // all 16 distinct bit patterns of for the current extension type of Input (w0).; //; // sub w8, w0, w1; // and w10, w8, #0x0f; // cmp w8, w2; // cset w9, AArch64CC; // cmp w10, w2; // cset w11, AArch64CC; // cmp w9, w11; // cset w0, eq; // ret; //; // Since the above function shows when the outputs are equivalent it defines; // when it is safe to remove the A",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:1545,Availability,mask,masked,1545," // +-------------+ +----+ | |; // | | | |; // V V | |; // +-------------+ | |; // | AND | | |; // +-------------+ | |; // | | |; // +-----+ | |; // | | |; // V V V; // +-------------+; // | CMP |; // +-------------+; //; // The AND node may be safely removed for some combinations of inputs. In; // particular we need to take into account the extension type of the Input,; // the exact values of AddConstant, CompConstant, and CC, along with the nominal; // width of the input (this can work for any width inputs, the above graph is; // specific to 8 bits.; //; // The specific equations were worked out by generating output tables for each; // AArch64CC value in terms of and AddConstant (w1), CompConstant(w2). The; // problem was simplified by working with 4 bit inputs, which means we only; // needed to reason about 24 distinct bit patterns: 8 patterns unique to zero; // extension (8,15), 8 patterns unique to sign extensions (-8,-1), and 8; // patterns present in both extensions (0,7). For every distinct set of; // AddConstant and CompConstants bit patterns we can consider the masked and; // unmasked versions to be equivalent if the result of this function is true for; // all 16 distinct bit patterns of for the current extension type of Input (w0).; //; // sub w8, w0, w1; // and w10, w8, #0x0f; // cmp w8, w2; // cset w9, AArch64CC; // cmp w10, w2; // cset w11, AArch64CC; // cmp w9, w11; // cset w0, eq; // ret; //; // Since the above function shows when the outputs are equivalent it defines; // when it is safe to remove the AND. Unfortunately it only runs on AArch64 and; // would be expensive to run during compiles. The equations below were written; // in a test harness that confirmed they gave equivalent outputs to the above; // for all inputs function, so they can be used determine if the removal is; // legal instead.; //; // isEquivalentMaskless() is the code for testing if the AND can be removed; // factored out of the DAG recognition as the DAG can take several forms.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:702,Safety,safe,safely,702,"// This function does a whole lot of voodoo to determine if the tests are; // equivalent without and with a mask. Essentially what happens is that given a; // DAG resembling:; //; // +-------------+ +-------------+ +-------------+ +-------------+; // | Input | | AddConstant | | CompConstant| | CC |; // +-------------+ +-------------+ +-------------+ +-------------+; // | | | |; // V V | +----------+; // +-------------+ +----+ | |; // | ADD | |0xff| | |; // +-------------+ +----+ | |; // | | | |; // V V | |; // +-------------+ | |; // | AND | | |; // +-------------+ | |; // | | |; // +-----+ | |; // | | |; // V V V; // +-------------+; // | CMP |; // +-------------+; //; // The AND node may be safely removed for some combinations of inputs. In; // particular we need to take into account the extension type of the Input,; // the exact values of AddConstant, CompConstant, and CC, along with the nominal; // width of the input (this can work for any width inputs, the above graph is; // specific to 8 bits.; //; // The specific equations were worked out by generating output tables for each; // AArch64CC value in terms of and AddConstant (w1), CompConstant(w2). The; // problem was simplified by working with 4 bit inputs, which means we only; // needed to reason about 24 distinct bit patterns: 8 patterns unique to zero; // extension (8,15), 8 patterns unique to sign extensions (-8,-1), and 8; // patterns present in both extensions (0,7). For every distinct set of; // AddConstant and CompConstants bit patterns we can consider the masked and; // unmasked versions to be equivalent if the result of this function is true for; // all 16 distinct bit patterns of for the current extension type of Input (w0).; //; // sub w8, w0, w1; // and w10, w8, #0x0f; // cmp w8, w2; // cset w9, AArch64CC; // cmp w10, w2; // cset w11, AArch64CC; // cmp w9, w11; // cset w0, eq; // ret; //; // Since the above function shows when the outputs are equivalent it defines; // when it is safe to remove the A",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:1981,Safety,safe,safe,1981," // +-------------+ +----+ | |; // | | | |; // V V | |; // +-------------+ | |; // | AND | | |; // +-------------+ | |; // | | |; // +-----+ | |; // | | |; // V V V; // +-------------+; // | CMP |; // +-------------+; //; // The AND node may be safely removed for some combinations of inputs. In; // particular we need to take into account the extension type of the Input,; // the exact values of AddConstant, CompConstant, and CC, along with the nominal; // width of the input (this can work for any width inputs, the above graph is; // specific to 8 bits.; //; // The specific equations were worked out by generating output tables for each; // AArch64CC value in terms of and AddConstant (w1), CompConstant(w2). The; // problem was simplified by working with 4 bit inputs, which means we only; // needed to reason about 24 distinct bit patterns: 8 patterns unique to zero; // extension (8,15), 8 patterns unique to sign extensions (-8,-1), and 8; // patterns present in both extensions (0,7). For every distinct set of; // AddConstant and CompConstants bit patterns we can consider the masked and; // unmasked versions to be equivalent if the result of this function is true for; // all 16 distinct bit patterns of for the current extension type of Input (w0).; //; // sub w8, w0, w1; // and w10, w8, #0x0f; // cmp w8, w2; // cset w9, AArch64CC; // cmp w10, w2; // cset w11, AArch64CC; // cmp w9, w11; // cset w0, eq; // ret; //; // Since the above function shows when the outputs are equivalent it defines; // when it is safe to remove the AND. Unfortunately it only runs on AArch64 and; // would be expensive to run during compiles. The equations below were written; // in a test harness that confirmed they gave equivalent outputs to the above; // for all inputs function, so they can be used determine if the removal is; // legal instead.; //; // isEquivalentMaskless() is the code for testing if the AND can be removed; // factored out of the DAG recognition as the DAG can take several forms.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:64,Testability,test,tests,64,"// This function does a whole lot of voodoo to determine if the tests are; // equivalent without and with a mask. Essentially what happens is that given a; // DAG resembling:; //; // +-------------+ +-------------+ +-------------+ +-------------+; // | Input | | AddConstant | | CompConstant| | CC |; // +-------------+ +-------------+ +-------------+ +-------------+; // | | | |; // V V | +----------+; // +-------------+ +----+ | |; // | ADD | |0xff| | |; // +-------------+ +----+ | |; // | | | |; // V V | |; // +-------------+ | |; // | AND | | |; // +-------------+ | |; // | | |; // +-----+ | |; // | | |; // V V V; // +-------------+; // | CMP |; // +-------------+; //; // The AND node may be safely removed for some combinations of inputs. In; // particular we need to take into account the extension type of the Input,; // the exact values of AddConstant, CompConstant, and CC, along with the nominal; // width of the input (this can work for any width inputs, the above graph is; // specific to 8 bits.; //; // The specific equations were worked out by generating output tables for each; // AArch64CC value in terms of and AddConstant (w1), CompConstant(w2). The; // problem was simplified by working with 4 bit inputs, which means we only; // needed to reason about 24 distinct bit patterns: 8 patterns unique to zero; // extension (8,15), 8 patterns unique to sign extensions (-8,-1), and 8; // patterns present in both extensions (0,7). For every distinct set of; // AddConstant and CompConstants bit patterns we can consider the masked and; // unmasked versions to be equivalent if the result of this function is true for; // all 16 distinct bit patterns of for the current extension type of Input (w0).; //; // sub w8, w0, w1; // and w10, w8, #0x0f; // cmp w8, w2; // cset w9, AArch64CC; // cmp w10, w2; // cset w11, AArch64CC; // cmp w9, w11; // cset w0, eq; // ret; //; // Since the above function shows when the outputs are equivalent it defines; // when it is safe to remove the A",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:2136,Testability,test,test,2136," // +-------------+ +----+ | |; // | | | |; // V V | |; // +-------------+ | |; // | AND | | |; // +-------------+ | |; // | | |; // +-----+ | |; // | | |; // V V V; // +-------------+; // | CMP |; // +-------------+; //; // The AND node may be safely removed for some combinations of inputs. In; // particular we need to take into account the extension type of the Input,; // the exact values of AddConstant, CompConstant, and CC, along with the nominal; // width of the input (this can work for any width inputs, the above graph is; // specific to 8 bits.; //; // The specific equations were worked out by generating output tables for each; // AArch64CC value in terms of and AddConstant (w1), CompConstant(w2). The; // problem was simplified by working with 4 bit inputs, which means we only; // needed to reason about 24 distinct bit patterns: 8 patterns unique to zero; // extension (8,15), 8 patterns unique to sign extensions (-8,-1), and 8; // patterns present in both extensions (0,7). For every distinct set of; // AddConstant and CompConstants bit patterns we can consider the masked and; // unmasked versions to be equivalent if the result of this function is true for; // all 16 distinct bit patterns of for the current extension type of Input (w0).; //; // sub w8, w0, w1; // and w10, w8, #0x0f; // cmp w8, w2; // cset w9, AArch64CC; // cmp w10, w2; // cset w11, AArch64CC; // cmp w9, w11; // cset w0, eq; // ret; //; // Since the above function shows when the outputs are equivalent it defines; // when it is safe to remove the AND. Unfortunately it only runs on AArch64 and; // would be expensive to run during compiles. The equations below were written; // in a test harness that confirmed they gave equivalent outputs to the above; // for all inputs function, so they can be used determine if the removal is; // legal instead.; //; // isEquivalentMaskless() is the code for testing if the AND can be removed; // factored out of the DAG recognition as the DAG can take several forms.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:2349,Testability,test,testing,2349," // +-------------+ +----+ | |; // | | | |; // V V | |; // +-------------+ | |; // | AND | | |; // +-------------+ | |; // | | |; // +-----+ | |; // | | |; // V V V; // +-------------+; // | CMP |; // +-------------+; //; // The AND node may be safely removed for some combinations of inputs. In; // particular we need to take into account the extension type of the Input,; // the exact values of AddConstant, CompConstant, and CC, along with the nominal; // width of the input (this can work for any width inputs, the above graph is; // specific to 8 bits.; //; // The specific equations were worked out by generating output tables for each; // AArch64CC value in terms of and AddConstant (w1), CompConstant(w2). The; // problem was simplified by working with 4 bit inputs, which means we only; // needed to reason about 24 distinct bit patterns: 8 patterns unique to zero; // extension (8,15), 8 patterns unique to sign extensions (-8,-1), and 8; // patterns present in both extensions (0,7). For every distinct set of; // AddConstant and CompConstants bit patterns we can consider the masked and; // unmasked versions to be equivalent if the result of this function is true for; // all 16 distinct bit patterns of for the current extension type of Input (w0).; //; // sub w8, w0, w1; // and w10, w8, #0x0f; // cmp w8, w2; // cset w9, AArch64CC; // cmp w10, w2; // cset w11, AArch64CC; // cmp w9, w11; // cset w0, eq; // ret; //; // Since the above function shows when the outputs are equivalent it defines; // when it is safe to remove the AND. Unfortunately it only runs on AArch64 and; // would be expensive to run during compiles. The equations below were written; // in a test harness that confirmed they gave equivalent outputs to the above; // for all inputs function, so they can be used determine if the removal is; // legal instead.; //; // isEquivalentMaskless() is the code for testing if the AND can be removed; // factored out of the DAG recognition as the DAG can take several forms.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:1191,Usability,simpl,simplified,1191,"--------+ +-------------+ +-------------+ +-------------+; // | | | |; // V V | +----------+; // +-------------+ +----+ | |; // | ADD | |0xff| | |; // +-------------+ +----+ | |; // | | | |; // V V | |; // +-------------+ | |; // | AND | | |; // +-------------+ | |; // | | |; // +-----+ | |; // | | |; // V V V; // +-------------+; // | CMP |; // +-------------+; //; // The AND node may be safely removed for some combinations of inputs. In; // particular we need to take into account the extension type of the Input,; // the exact values of AddConstant, CompConstant, and CC, along with the nominal; // width of the input (this can work for any width inputs, the above graph is; // specific to 8 bits.; //; // The specific equations were worked out by generating output tables for each; // AArch64CC value in terms of and AddConstant (w1), CompConstant(w2). The; // problem was simplified by working with 4 bit inputs, which means we only; // needed to reason about 24 distinct bit patterns: 8 patterns unique to zero; // extension (8,15), 8 patterns unique to sign extensions (-8,-1), and 8; // patterns present in both extensions (0,7). For every distinct set of; // AddConstant and CompConstants bit patterns we can consider the masked and; // unmasked versions to be equivalent if the result of this function is true for; // all 16 distinct bit patterns of for the current extension type of Input (w0).; //; // sub w8, w0, w1; // and w10, w8, #0x0f; // cmp w8, w2; // cset w9, AArch64CC; // cmp w10, w2; // cset w11, AArch64CC; // cmp w9, w11; // cset w0, eq; // ret; //; // Since the above function shows when the outputs are equivalent it defines; // when it is safe to remove the AND. Unfortunately it only runs on AArch64 and; // would be expensive to run during compiles. The equations below were written; // in a test harness that confirmed they gave equivalent outputs to the above; // for all inputs function, so they can be used determine if the removal is; // legal instead.; //; // i",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:46,Modifiability,extend,extending,46,// For the purposes of these comparisons sign extending the type is; // equivalent to zero extending the add and displacing it by half the integer; // width. Provided we are careful and make sure our equations are valid over; // the whole range we can just adjust the input and avoid writing equations; // for sign extended inputs.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:91,Modifiability,extend,extending,91,// For the purposes of these comparisons sign extending the type is; // equivalent to zero extending the add and displacing it by half the integer; // width. Provided we are careful and make sure our equations are valid over; // the whole range we can just adjust the input and avoid writing equations; // for sign extended inputs.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:315,Modifiability,extend,extended,315,// For the purposes of these comparisons sign extending the type is; // equivalent to zero extending the add and displacing it by half the integer; // width. Provided we are careful and make sure our equations are valid over; // the whole range we can just adjust the input and avoid writing equations; // for sign extended inputs.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:278,Safety,avoid,avoid,278,// For the purposes of these comparisons sign extending the type is; // equivalent to zero extending the add and displacing it by half the integer; // width. Provided we are careful and make sure our equations are valid over; // the whole range we can just adjust the input and avoid writing equations; // for sign extended inputs.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:14,Availability,Mask,Mask,14,// (X & C) >u Mask --> (X & (C & (~Mask)) != 0; // (X & C) <u Pow2 --> (X & (C & ~(Pow2-1)) == 0,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:35,Availability,Mask,Mask,35,// (X & C) >u Mask --> (X & (C & (~Mask)) != 0; // (X & C) <u Pow2 --> (X & (C & ~(Pow2-1)) == 0,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:304,Modifiability,rewrite,rewrite,304,"// For now, only performCSELCombine and performBRCONDCombine call this; // function. And both of them pass 2 for CCIndex, 3 for CmpIndex with 4; // operands. So just init the ops direct to simplify the code. If we have some; // other case with different CCIndex, CmpIndex, we need to use for loop to; // rewrite the code here.; // TODO: Do we need to assert number of operand is 4 here?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:17,Performance,perform,performCSELCombine,17,"// For now, only performCSELCombine and performBRCONDCombine call this; // function. And both of them pass 2 for CCIndex, 3 for CmpIndex with 4; // operands. So just init the ops direct to simplify the code. If we have some; // other case with different CCIndex, CmpIndex, we need to use for loop to; // rewrite the code here.; // TODO: Do we need to assert number of operand is 4 here?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:40,Performance,perform,performBRCONDCombine,40,"// For now, only performCSELCombine and performBRCONDCombine call this; // function. And both of them pass 2 for CCIndex, 3 for CmpIndex with 4; // operands. So just init the ops direct to simplify the code. If we have some; // other case with different CCIndex, CmpIndex, we need to use for loop to; // rewrite the code here.; // TODO: Do we need to assert number of operand is 4 here?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:351,Testability,assert,assert,351,"// For now, only performCSELCombine and performBRCONDCombine call this; // function. And both of them pass 2 for CCIndex, 3 for CmpIndex with 4; // operands. So just init the ops direct to simplify the code. If we have some; // other case with different CCIndex, CmpIndex, we need to use for loop to; // rewrite the code here.; // TODO: Do we need to assert number of operand is 4 here?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:189,Usability,simpl,simplify,189,"// For now, only performCSELCombine and performBRCONDCombine call this; // function. And both of them pass 2 for CCIndex, 3 for CmpIndex with 4; // operands. So just init the ops direct to simplify the code. If we have some; // other case with different CCIndex, CmpIndex, we need to use for loop to; // rewrite the code here.; // TODO: Do we need to assert number of operand is 4 here?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:58,Availability,mask,mask,58,// There is a SUBS feeding this condition. Is it fed by a mask we can; // use?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:59,Security,validat,validate,59,"// The basic dag structure is correct, grab the inputs and validate them.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:7,Availability,mask,mask,7,"// The mask is present and the provenance of all the values is a smaller type,; // lets see if the mask is superfluous.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:99,Availability,mask,mask,99,"// The mask is present and the provenance of all the values is a smaller type,; // lets see if the mask is superfluous.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Performance,Optimiz,Optimize,3,// Optimize compare with zero and branch.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:41,Performance,optimiz,optimized,41,"// Speculation tracking/SLH assumes that optimized TB(N)Z/CB(N)Z instructions; // will not be produced, as they are conditional branch instructions that do; // not set flags.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Performance,Optimiz,Optimize,3,// Optimize CSEL instructions,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:28,Modifiability,extend,extended,28,// Try to re-use an already extended operand of a vector SetCC feeding a; // extended select. Doing so avoids requiring another full extension of the; // SET_CC result when lowering the select.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:77,Modifiability,extend,extended,77,// Try to re-use an already extended operand of a vector SetCC feeding a; // extended select. Doing so avoids requiring another full extension of the; // SET_CC result when lowering the select.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:103,Safety,avoid,avoids,103,// Try to re-use an already extended operand of a vector SetCC feeding a; // extended select. Doing so avoids requiring another full extension of the; // SET_CC result when lowering the select.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:55,Modifiability,extend,extended,55,"// Check if the first operand of the SET_CC is already extended. If it is,; // split the SET_CC and re-use the extended version of the operand.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:111,Modifiability,extend,extended,111,"// Check if the first operand of the SET_CC is already extended. If it is,; // split the SET_CC and re-use the extended version of the operand.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:10,Performance,perform,perform,10,// Try to perform the memcmp when the result is tested for [in]equality with 0,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:48,Testability,test,tested,48,// Try to perform the memcmp when the result is tested for [in]equality with 0,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:180,Availability,redundant,redundant,180,// By this point we've effectively got; // zero_inactive_lanes_and_trunc_i1(sext_i1(A)). If we can prove A's inactive; // lanes are already zero then the trunc(sext()) sequence is redundant and we; // can operate on A directly.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:180,Safety,redund,redundant,180,// By this point we've effectively got; // zero_inactive_lanes_and_trunc_i1(sext_i1(A)). If we can prove A's inactive; // lanes are already zero then the trunc(sext()) sequence is redundant and we; // can operate on A directly.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:31,Modifiability,extend,extend,31,"// setcc_merge_zero(; // pred, extend(setcc_merge_zero(pred, ...)), != splat(0)); // => setcc_merge_zero(pred, ...)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:37,Modifiability,extend,extend,37,"// setcc_merge_zero(; // all_active, extend(nxvNi1 ...), != splat(0)); // -> nxvNi1 ...",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:31,Modifiability,extend,extend,31,"// setcc_merge_zero(; // pred, extend(nxvNi1 ...), != splat(0)); // -> nxvNi1 and(pred, ...)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Performance,Optimiz,Optimize,3,// Optimize some simple tbz/tbnz cases. Returns the new operand and bit to test; // as well as whether the test should be inverted. This code is required to; // catch these cases (as opposed to standard dag combines) because; // AArch64ISD::TBZ is matched during legalization.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:75,Testability,test,test,75,// Optimize some simple tbz/tbnz cases. Returns the new operand and bit to test; // as well as whether the test should be inverted. This code is required to; // catch these cases (as opposed to standard dag combines) because; // AArch64ISD::TBZ is matched during legalization.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:107,Testability,test,test,107,// Optimize some simple tbz/tbnz cases. Returns the new operand and bit to test; // as well as whether the test should be inverted. This code is required to; // catch these cases (as opposed to standard dag combines) because; // AArch64ISD::TBZ is matched during legalization.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:17,Usability,simpl,simple,17,// Optimize some simple tbz/tbnz cases. Returns the new operand and bit to test; // as well as whether the test should be inverted. This code is required to; // catch these cases (as opposed to standard dag combines) because; // AArch64ISD::TBZ is matched during legalization.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:119,Testability,test,test,119,"// We don't handle undef/constant-fold cases below, as they should have; // already been taken care of (e.g. and of 0, test of undefined shifted bits,; // etc.); // (tbz (trunc x), b) -> (tbz x, b); // This case is just here to enable more of the below cases to be caught.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:58,Modifiability,extend,extended,58,"// (tbz (any_ext x), b) -> (tbz x, b) if we don't use the extended bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Performance,Optimiz,Optimize,3,// Optimize test single bit zero/non-zero and branch.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:12,Testability,test,test,12,// Optimize test single bit zero/non-zero and branch.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:97,Availability,mask,mask,97,"/// A vector select: ""(select vL, vR, (setcc LHS, RHS))"" is best performed with; /// the compare-mask instructions rather than going via NZCV, even if LHS and; /// RHS are really scalar. This replaces any scalar setcc in the above pattern; /// with a vector one followed by a DUP shuffle on the result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:65,Performance,perform,performed,65,"/// A vector select: ""(select vL, vR, (setcc LHS, RHS))"" is best performed with; /// the compare-mask instructions rather than going via NZCV, even if LHS and; /// RHS are really scalar. This replaces any scalar setcc in the above pattern; /// with a vector one followed by a DUP shuffle on the result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:24,Performance,optimiz,optimization,24,"// Don't try to do this optimization when the setcc itself has i1 operands.; // There are no legal vectors of i1, so this would be pointless. v1f16 is; // ruled out to prevent the creation of setcc that need to be scalarized.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:9,Performance,perform,perform,9,"// First perform a vector comparison, where lane 0 is the one we're interested; // in.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:32,Availability,mask,mask,32,// Now duplicate the comparison mask we want across all other lanes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:148,Modifiability,rewrite,rewrite,148,"// If all users of the globaladdr are of the form (globaladdr + constant), find; // the smallest constant, fold it into the globaladdr's offset and rewrite the; // globaladdr as (globaladdr + constant) - constant.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:108,Safety,avoid,avoid,108,"// Check whether folding this offset is legal. It must not go out of bounds of; // the referenced object to avoid violating the code model, and must be; // smaller than 2^20 because this is the largest offset expressible in all; // object formats. (The IMAGE_REL_ARM64_PAGEBASE_REL21 relocation in COFF; // stores an immediate signed 21 bit offset.); //; // This check also prevents us from folding negative offsets, which will end; // up being treated in the same way as large positive ones. They could also; // cause code model violations, and aren't really common enough to matter.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:91,Performance,load,load,91,"/// Check if the value of \p OffsetInBytes can be used as an immediate for; /// the gather load/prefetch and scatter store instructions with vector base and; /// immediate offset addressing mode:; ///; /// [<Zn>.[S|D]{, #<imm>}]; ///; /// where <imm> = sizeof(<T>) * k, for k = 0, 1, ..., 31.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:89,Performance,load,load,89,"/// Check if the value of \p Offset represents a valid immediate for the SVE; /// gather load/prefetch and scatter store instructiona with vector base and; /// immediate offset addressing mode:; ///; /// [<Zn>.[S|D]{, #<imm>}]; ///; /// where <imm> = sizeof(<T>) * k, for k = 0, 1, ..., 31.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Integrability,Depend,Depending,3,"// Depending on the addressing mode, this is either a pointer or a vector of; // pointers (that fits into one register)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Integrability,Depend,Depending,3,"// Depending on the addressing mode, this is either a single offset or a; // vector of offsets (that fits into one register)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:38,Performance,load,loads,38,"// In the case of non-temporal gather loads there's only one SVE instruction; // per data-size: ""scalar + vector"", i.e.; // * stnt1{b|h|w|d} { z0.s }, p0/z, [z0.s, x0]; // Since we do have intrinsics that allow the arguments to be in a different; // order, we may need to swap them to match the spec.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:136,Modifiability,extend,extend,136,"// Some scatter store variants allow unpacked offsets, but only as nxv2i32; // vectors. These are implicitly sign (sxtw) or zero (zxtw) extend to; // nxv2i64. Legalize accordingly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:22,Performance,load,loaded,22,// Make sure that the loaded data will fit into an SVE register,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Integrability,Depend,Depending,3,"// Depending on the addressing mode, this is either a pointer or a vector of; // pointers (that fits into one register)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Integrability,Depend,Depending,3,"// Depending on the addressing mode, this is either a single offset or a; // vector of offsets (that fits into one register)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:38,Performance,load,loads,38,"// In the case of non-temporal gather loads and quadword gather loads there's; // only one addressing mode : ""vector + scalar"", e.g.; // ldnt1{b|h|w|d} { z0.s }, p0/z, [z0.s, x0]; // Since we do have intrinsics that allow the arguments to be in a different; // order, we may need to swap them to match the spec.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:64,Performance,load,loads,64,"// In the case of non-temporal gather loads and quadword gather loads there's; // only one addressing mode : ""vector + scalar"", e.g.; // ldnt1{b|h|w|d} { z0.s }, p0/z, [z0.s, x0]; // Since we do have intrinsics that allow the arguments to be in a different; // order, we may need to swap them to match the spec.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:194,Performance,load,loaded,194,"// GLD{FF}1_IMM requires that the offset is an immediate that is:; // * a multiple of #SizeInBytes,; // * in the range [0, 31 x #SizeInBytes],; // where #SizeInBytes is the size in bytes of the loaded items. For; // immediates outside that range and non-immediate scalar offsets use; // GLD1_MERGE_ZERO or GLD1_UXTW_MERGE_ZERO instead.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:134,Modifiability,extend,extend,134,"// Some gather load variants allow unpacked offsets, but only as nxv2i32; // vectors. These are implicitly sign (sxtw) or zero (zxtw) extend to; // nxv2i64. Legalize accordingly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:15,Performance,load,load,15,"// Some gather load variants allow unpacked offsets, but only as nxv2i32; // vectors. These are implicitly sign (sxtw) or zero (zxtw) extend to; // nxv2i64. Legalize accordingly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:97,Safety,avoid,avoid,97,"// If the original return value was FP, bitcast accordingly. Doing it here; // means that we can avoid adding TableGen patterns for FPs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:8,Modifiability,extend,extend,8,// Sign extend of an unsigned unpack -> signed unpack,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:17,Modifiability,extend,extend,17,"// Push the sign extend to the operand of the unpack; // This is necessary where, for example, the operand of the unpack; // is another unpack:; // 4i32 sign_extend_inreg (4i32 uunpklo(8i16 uunpklo (16i8 opnd)), from 4i8); // ->; // 4i32 sunpklo (8i16 sign_extend_inreg(8i16 uunpklo (16i8 opnd), from 8i8); // ->; // 4i32 sunpklo(8i16 sunpklo(16i8 opnd))",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:7,Performance,load,load,7,// SVE load nodes (e.g. AArch64ISD::GLD1) are straightforward candidates; // for DAG Combine with SIGN_EXTEND_INREG. Bail out for all other nodes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:117,Performance,scalab,scalable,117,/// Legalize the gather prefetch (scalar + vector addressing mode) when the; /// offset vector is an unpacked 32-bit scalable vector. The other cases (Offset; /// != nxv2i32) do not need legalization.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Modifiability,Extend,Extend,3,// Extend the unpacked offset vector to 64-bit lanes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:27,Availability,redundant,redundant,27,// The explicit zeroing is redundant.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:27,Safety,redund,redundant,27,// The explicit zeroing is redundant.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:155,Availability,down,down,155,// fold (fpext (load x)) -> (fpext (fptrunc (extload x))); // We purposefully don't care about legality of the nodes here as we know; // they can be split down into something legal.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:16,Performance,load,load,16,// fold (fpext (load x)) -> (fpext (fptrunc (extload x))); // We purposefully don't care about legality of the nodes here as we know; // they can be split down into something legal.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:127,Testability,test,test,127,"// If the truncate's operand is BUILD_VECTOR with DUP, do not combine the op; // with uzp1.; // You can see the regressions on test/CodeGen/AArch64/aarch64-smull.ll",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:433,Safety,detect,detect,433,"// Check there is other extract_high with same source vector.; // For example,; //; // t18: v4i16 = extract_subvector t2, Constant:i64<0>; // t12: v4i16 = truncate t11; // t31: v4i32 = AArch64ISD::SMULL t18, t12; // t23: v4i16 = extract_subvector t2, Constant:i64<4>; // t16: v4i16 = truncate t15; // t30: v4i32 = AArch64ISD::SMULL t23, t1; //; // This dagcombine assumes the two extract_high uses same source vector in; // order to detect the pair of the mull. If they have different source vector,; // this code will not work.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:127,Testability,test,test,127,"// If the truncate's operand is BUILD_VECTOR with DUP, do not combine the op; // with uzp1.; // You can see the regressions on test/CodeGen/AArch64/aarch64-smull.ll",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:87,Performance,perform,perform,87,"// Check if the return value is used as only a return value, as otherwise; // we can't perform a tail-call. In particular, we need to check for; // target ISD nodes that are returns and any other ""odd"" constructs; // that the generic analysis code won't necessarily catch.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:81,Performance,perform,perform,81,"// If the copy has a glue operand, we conservatively assume it isn't safe to; // perform a tail call.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:69,Safety,safe,safe,69,"// If the copy has a glue operand, we conservatively assume it isn't safe to; // perform a tail call.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:56,Performance,optimiz,optimized,56,"// Return whether the an instruction can potentially be optimized to a tail; // call. This will cause the optimizers to attempt to move, or duplicate,; // return instructions to help enable tail call optimizations for this; // instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:106,Performance,optimiz,optimizers,106,"// Return whether the an instruction can potentially be optimized to a tail; // call. This will cause the optimizers to attempt to move, or duplicate,; // return instructions to help enable tail call optimizations for this; // instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:200,Performance,optimiz,optimizations,200,"// Return whether the an instruction can potentially be optimized to a tail; // call. This will cause the optimizers to attempt to move, or duplicate,; // return instructions to help enable tail call optimizations for this; // instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:48,Performance,load,loaded,48,// Non-null if there is exactly one user of the loaded value (ignoring chain).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:38,Performance,scalab,scalable,38,"// If the only user of the value is a scalable vector splat, it is; // preferable to do a replicating load (ld1r*).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:102,Performance,load,load,102,"// If the only user of the value is a scalable vector splat, it is; // preferable to do a replicating load (ld1r*).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:17,Deployability,update,updates,17,"// Post-indexing updates the base, so it's not a valid transform; // if that's not the same as the load's pointer.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:99,Performance,load,load,99,"// Post-indexing updates the base, so it's not a valid transform; // if that's not the same as the load's pointer.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:13,Availability,mask,mask,13,"// Check the mask is 1,0,3,2,5,4,...",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:98,Integrability,wrap,wrapped,98,"// LSE has a 128-bit compare and swap (CASP), but i128 is not a legal type,; // so lower it here, wrapped in REG_SEQUENCE and EXTRACT_SUBREG.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:40,Performance,load,loads,40,// Handle lowering 256 bit non temporal loads into LDNP for little-endian; // targets.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:26,Performance,load,loads,26,// Non-volatile or atomic loads are optimized later in AArch64's load/store; // optimizer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:36,Performance,optimiz,optimized,36,// Non-volatile or atomic loads are optimized later in AArch64's load/store; // optimizer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:65,Performance,load,load,65,// Non-volatile or atomic loads are optimized later in AArch64's load/store; // optimizer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:80,Performance,optimiz,optimizer,80,// Non-volatile or atomic loads are optimized later in AArch64's load/store; // optimizer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:9,Deployability,Release,Release,9,"// Store-Release instructions only provide seq_cst guarantees when paired with; // Load-Acquire instructions. MSVC CRT does not use these instructions to; // implement seq_cst loads and stores, so we need additional explicit fences; // after memory writes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:83,Performance,Load,Load-Acquire,83,"// Store-Release instructions only provide seq_cst guarantees when paired with; // Load-Acquire instructions. MSVC CRT does not use these instructions to; // implement seq_cst loads and stores, so we need additional explicit fences; // after memory writes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:176,Performance,load,loads,176,"// Store-Release instructions only provide seq_cst guarantees when paired with; // Load-Acquire instructions. MSVC CRT does not use these instructions to; // implement seq_cst loads and stores, so we need additional explicit fences; // after memory writes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Performance,Load,Loads,3,"// Loads and stores less than 128-bits are already atomic; ones above that; // are doomed anyway, so defer to the default libcall and blame the OS when; // things go wrong.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Performance,Load,Loads,3,"// Loads and stores less than 128-bits are already atomic; ones above that; // are doomed anyway, so defer to the default libcall and blame the OS when; // things go wrong.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:13,Performance,load,loads,13,// No LSE128 loads,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:238,Energy Efficiency,monitor,monitor,238,"// At -O0, fast-regalloc cannot cope with the live vregs necessary to; // implement atomicrmw without spilling. If the target address is also on the; // stack and close enough to the spill slot, this can lead to a situation; // where the monitor always gets cleared and the atomic operation can never; // succeed. So at -O0 lower this operation to a CAS loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:258,Usability,clear,cleared,258,"// At -O0, fast-regalloc cannot cope with the live vregs necessary to; // implement atomicrmw without spilling. If the target address is also on the; // stack and close enough to the spill slot, this can lead to a situation; // where the monitor always gets cleared and the atomic operation can never; // succeed. So at -O0 lower this operation to a CAS loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:117,Availability,avail,available,117,// Using CAS for an atomic load has a better chance of succeeding under high; // contention situations. So use it if available.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:27,Performance,load,load,27,// Using CAS for an atomic load has a better chance of succeeding under high; // contention situations. So use it if available.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:167,Integrability,rout,routines,167,"// The ""default"" for integer RMW operations is to expand to an LL/SC loop.; // However, with the LSE instructions (or outline-atomics mode, which provides; // library routines in place of the LSE-instructions), we can directly emit many; // operations instead.; //; // Floating-point operations are always emitted to a cmpxchg loop, because they; // may trigger a trap which aborts an LLSC sequence.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:375,Safety,abort,aborts,375,"// The ""default"" for integer RMW operations is to expand to an LL/SC loop.; // However, with the LSE instructions (or outline-atomics mode, which provides; // library routines in place of the LSE-instructions), we can directly emit many; // operations instead.; //; // Floating-point operations are always emitted to a cmpxchg loop, because they; // may trigger a trap which aborts an LLSC sequence.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:238,Energy Efficiency,monitor,monitor,238,"// At -O0, fast-regalloc cannot cope with the live vregs necessary to; // implement atomicrmw without spilling. If the target address is also on the; // stack and close enough to the spill slot, this can lead to a situation; // where the monitor always gets cleared and the atomic operation can never; // succeed. So at -O0 lower this operation to a CAS loop. Also worthwhile if; // we have a single CAS instruction that can replace the loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:258,Usability,clear,cleared,258,"// At -O0, fast-regalloc cannot cope with the live vregs necessary to; // implement atomicrmw without spilling. If the target address is also on the; // stack and close enough to the spill slot, this can lead to a situation; // where the monitor always gets cleared and the atomic operation can never; // succeed. So at -O0 lower this operation to a CAS loop. Also worthwhile if; // we have a single CAS instruction that can replace the loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:245,Energy Efficiency,monitor,monitor,245,"// At -O0, fast-regalloc cannot cope with the live vregs necessary to; // implement cmpxchg without spilling. If the address being exchanged is also; // on the stack and close enough to the spill slot, this can lead to a; // situation where the monitor always gets cleared and the atomic operation; // can never succeed. So at -O0 we need a late-expanded pseudo-inst instead.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:265,Usability,clear,cleared,265,"// At -O0, fast-regalloc cannot cope with the live vregs necessary to; // implement cmpxchg without spilling. If the address being exchanged is also; // on the stack and close enough to the spill slot, this can lead to a; // situation where the monitor always gets cleared and the atomic operation; // can never succeed. So at -O0 we need a late-expanded pseudo-inst instead.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:25,Modifiability,variab,variable,25,// MSVC CRT has a global variable holding security cookie.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:42,Security,secur,security,42,// MSVC CRT has a global variable holding security cookie.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:30,Security,validat,validate,30,// MSVC CRT has a function to validate security cookie.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:39,Security,secur,security,39,// MSVC CRT has a function to validate security cookie.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:25,Modifiability,variab,variable,25,// MSVC CRT has a global variable holding security cookie.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:42,Security,secur,security,42,// MSVC CRT has a global variable holding security cookie.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:30,Security,validat,validate,30,// MSVC CRT has a function to validate security cookie.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:39,Security,secur,security,39,// MSVC CRT has a function to validate security cookie.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:45,Safety,Safe,SafeStack,45,// Android provides a fixed TLS slot for the SafeStack pointer. See the; // definition of TLS_SLOT_SAFESTACK in; // https://android.googlesource.com/platform/bionic/+/master/libc/private/bionic_tls.h,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:19,Availability,mask,mask,19,"// Only sink 'and' mask to cmp use block if it is masking a single bit, since; // this is likely to be fold the and/cmp/br into a single tbz instruction. It; // may be beneficial to sink in other cases, but we would have to check that; // the cmp would not get folded into the br to form a cbz for these to be; // beneficial.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:50,Availability,mask,masking,50,"// Only sink 'and' mask to cmp use block if it is masking a single bit, since; // this is likely to be fold the and/cmp/br into a single tbz instruction. It; // may be beneficial to sink in other cases, but we would have to check that; // the cmp would not get folded into the br to form a cbz for these to be; // beneficial.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:34,Performance,perform,perform,34,// Does baseline recommend not to perform the fold by default?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Deployability,Update,Update,3,// Update IsSplitCSR in AArch64unctionInfo.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:171,Security,access,access,171,"// Create copy from CSR to a virtual register.; // FIXME: this currently does not emit CFI pseudo-instructions, it works; // fine for CXX_FAST_TLS since the C++-style TLS access functions should be; // nounwind. If we want to generalize this later, we may need to emit; // CFI pseudo-instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:76,Performance,optimiz,optimizing,76,"// Integer division on AArch64 is expensive. However, when aggressively; // optimizing for code size, we prefer to use a div instruction, as it is; // usually smaller than the alternative sequence.; // The exception to this is vector division. Since AArch64 doesn't have vector; // integer division, leaving the division as-is is a loss even in terms of; // size, because it will have to be scalarized, while the alternative code; // sequence can be performed in vector form.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:450,Performance,perform,performed,450,"// Integer division on AArch64 is expensive. However, when aggressively; // optimizing for code size, we prefer to use a div instruction, as it is; // usually smaller than the alternative sequence.; // The exception to this is vector division. Since AArch64 doesn't have vector; // integer division, leaving the division as-is is a loss even in terms of; // size, because it will have to be scalarized, while the alternative code; // sequence can be performed in vector form.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:33,Modifiability,extend,extended,33,"// v8f16 without fp16 need to be extended to v8f32, which is more difficult to; // legalize.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:192,Energy Efficiency,allocate,allocate,192,"// If we have any vulnerable SVE stack objects then the stack protector; // needs to be placed at the top of the SVE stack area, as the SVE locals; // are placed above the other locals, so we allocate it as if it were a; // scalable vector.; // FIXME: It may be worthwhile having a specific interface for this rather; // than doing it here in finalizeLowering.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:291,Integrability,interface,interface,291,"// If we have any vulnerable SVE stack objects then the stack protector; // needs to be placed at the top of the SVE stack area, as the SVE locals; // are placed above the other locals, so we allocate it as if it were a; // scalable vector.; // FIXME: It may be worthwhile having a specific interface for this rather; // than doing it here in finalizeLowering.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:224,Performance,scalab,scalable,224,"// If we have any vulnerable SVE stack objects then the stack protector; // needs to be placed at the top of the SVE stack area, as the SVE locals; // are placed above the other locals, so we allocate it as if it were a; // scalable vector.; // FIXME: It may be worthwhile having a specific interface for this rather; // than doing it here in finalizeLowering.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:37,Safety,avoid,avoid,37,// Always localize G_GLOBAL_VALUE to avoid high reg pressure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:28,Performance,scalab,scalable,28,// Return the largest legal scalable vector type that matches VT's element type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:181,Availability,avail,available,181,"// For vectors that are exactly getMaxSVEVectorSizeInBits big, we can use; // AArch64SVEPredPattern::all, which can enable the use of unpredicated; // variants of instructions when available.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:35,Performance,load,loads,35,// Convert all fixed length vector loads larger than NEON to masked_loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:35,Performance,load,loads,35,// Convert all fixed length vector loads larger than NEON to masked_loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:40,Availability,mask,mask,40,// If this is an extending load and the mask type is not the same as; // load's type then we have to extend the mask type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:112,Availability,mask,mask,112,// If this is an extending load and the mask type is not the same as; // load's type then we have to extend the mask type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:17,Modifiability,extend,extending,17,// If this is an extending load and the mask type is not the same as; // load's type then we have to extend the mask type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:101,Modifiability,extend,extend,101,// If this is an extending load and the mask type is not the same as; // load's type then we have to extend the mask type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:27,Performance,load,load,27,// If this is an extending load and the mask type is not the same as; // load's type then we have to extend the mask type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:73,Performance,load,load,73,// If this is an extending load and the mask type is not the same as; // load's type then we have to extend the mask type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Performance,Scalab,Scalable,3,// Scalable vector i32/i64 DIV is supported.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Performance,Scalab,Scalable,3,// Scalable vector i8/i16 DIV is not supported. Promote it to i32.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:31,Modifiability,extend,extend,31,"// If the wider type is legal: extend, op, and truncate.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:38,Modifiability,extend,extend,38,"// If wider type is not legal: split, extend, op, trunc and concat.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:58,Performance,scalab,scalable,58,// Create list of operands by converting existing ones to scalable types.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:116,Performance,scalab,scalable,116,"// If a fixed length vector operation has no side effects when applied to; // undefined elements, we can safely use scalable vectors to perform the same; // operation without needing to worry about predication.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:136,Performance,perform,perform,136,"// If a fixed length vector operation has no side effects when applied to; // undefined elements, we can safely use scalable vectors to perform the same; // operation without needing to worry about predication.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:105,Safety,safe,safely,105,"// If a fixed length vector operation has no side effects when applied to; // undefined elements, we can safely use scalable vectors to perform the same; // operation without needing to worry about predication.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:58,Performance,scalab,scalable,58,// Create list of operands by converting existing ones to scalable types.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:35,Performance,scalab,scalable,35,"// ""cast"" fixed length vector to a scalable vector.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:23,Performance,Scalab,Scalable,23,// Convert operands to Scalable.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Performance,Perform,Perform,3,// Perform reduction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:15,Availability,mask,mask,15,// Convert the mask to a predicated (NOTE: We don't need to worry about; // inactive lanes since VSELECT is safe when given undefined elements).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:108,Safety,safe,safe,108,// Convert the mask to a predicated (NOTE: We don't need to worry about; // inactive lanes since VSELECT is safe when given undefined elements).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Safety,Safe,Safe,3,// Safe to use a larger than specified operand because by promoting the; // value nothing has changed from an arithmetic point of view.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Safety,Safe,Safe,3,// Safe to use a larger than specified result since an fp_to_int where the; // result doesn't fit into the destination is undefined.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:98,Performance,perform,perform,98,// Choosing an out-of-range index leads to the lane being zeroed vs zero; // value where it would perform first lane duplication for out of; // index elements. For i8 elements an out-of-range index could be a valid; // for 2048-bit vector register size.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:69,Availability,mask,mask,69,"// Functions like isZIPMask return true when a ISD::VECTOR_SHUFFLE's mask; // represents the same logical operation as performed by a ZIP instruction. In; // isolation these functions do not mean the ISD::VECTOR_SHUFFLE is exactly; // equivalent to an AArch64 instruction. There's the extra component of; // ISD::VECTOR_SHUFFLE's value type to consider. Prior to SVE these functions; // only operated on 64/128bit vector types that have a direct mapping to a; // target register and so an exact mapping is implied.; // However, when using SVE for fixed length vectors, most legal vector types; // are actually sub-vectors of a larger SVE register. When mapping; // ISD::VECTOR_SHUFFLE to an SVE instruction care must be taken to consider; // how the mask's indices translate. Specifically, when the mapping requires; // an exact meaning for a specific vector index (e.g. Index X is the last; // vector element in the register) then such mappings are often only safe when; // the exact SVE register size is know. The main exception to this is when; // indices are logically relative to the first element of either; // ISD::VECTOR_SHUFFLE operand because these relative indices don't change; // when converting from fixed-length to scalable vector types (i.e. the start; // of a fixed length vector is always the start of a scalable vector).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:750,Availability,mask,mask,750,"// Functions like isZIPMask return true when a ISD::VECTOR_SHUFFLE's mask; // represents the same logical operation as performed by a ZIP instruction. In; // isolation these functions do not mean the ISD::VECTOR_SHUFFLE is exactly; // equivalent to an AArch64 instruction. There's the extra component of; // ISD::VECTOR_SHUFFLE's value type to consider. Prior to SVE these functions; // only operated on 64/128bit vector types that have a direct mapping to a; // target register and so an exact mapping is implied.; // However, when using SVE for fixed length vectors, most legal vector types; // are actually sub-vectors of a larger SVE register. When mapping; // ISD::VECTOR_SHUFFLE to an SVE instruction care must be taken to consider; // how the mask's indices translate. Specifically, when the mapping requires; // an exact meaning for a specific vector index (e.g. Index X is the last; // vector element in the register) then such mappings are often only safe when; // the exact SVE register size is know. The main exception to this is when; // indices are logically relative to the first element of either; // ISD::VECTOR_SHUFFLE operand because these relative indices don't change; // when converting from fixed-length to scalable vector types (i.e. the start; // of a fixed length vector is always the start of a scalable vector).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:119,Performance,perform,performed,119,"// Functions like isZIPMask return true when a ISD::VECTOR_SHUFFLE's mask; // represents the same logical operation as performed by a ZIP instruction. In; // isolation these functions do not mean the ISD::VECTOR_SHUFFLE is exactly; // equivalent to an AArch64 instruction. There's the extra component of; // ISD::VECTOR_SHUFFLE's value type to consider. Prior to SVE these functions; // only operated on 64/128bit vector types that have a direct mapping to a; // target register and so an exact mapping is implied.; // However, when using SVE for fixed length vectors, most legal vector types; // are actually sub-vectors of a larger SVE register. When mapping; // ISD::VECTOR_SHUFFLE to an SVE instruction care must be taken to consider; // how the mask's indices translate. Specifically, when the mapping requires; // an exact meaning for a specific vector index (e.g. Index X is the last; // vector element in the register) then such mappings are often only safe when; // the exact SVE register size is know. The main exception to this is when; // indices are logically relative to the first element of either; // ISD::VECTOR_SHUFFLE operand because these relative indices don't change; // when converting from fixed-length to scalable vector types (i.e. the start; // of a fixed length vector is always the start of a scalable vector).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:1230,Performance,scalab,scalable,1230,"// Functions like isZIPMask return true when a ISD::VECTOR_SHUFFLE's mask; // represents the same logical operation as performed by a ZIP instruction. In; // isolation these functions do not mean the ISD::VECTOR_SHUFFLE is exactly; // equivalent to an AArch64 instruction. There's the extra component of; // ISD::VECTOR_SHUFFLE's value type to consider. Prior to SVE these functions; // only operated on 64/128bit vector types that have a direct mapping to a; // target register and so an exact mapping is implied.; // However, when using SVE for fixed length vectors, most legal vector types; // are actually sub-vectors of a larger SVE register. When mapping; // ISD::VECTOR_SHUFFLE to an SVE instruction care must be taken to consider; // how the mask's indices translate. Specifically, when the mapping requires; // an exact meaning for a specific vector index (e.g. Index X is the last; // vector element in the register) then such mappings are often only safe when; // the exact SVE register size is know. The main exception to this is when; // indices are logically relative to the first element of either; // ISD::VECTOR_SHUFFLE operand because these relative indices don't change; // when converting from fixed-length to scalable vector types (i.e. the start; // of a fixed length vector is always the start of a scalable vector).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:1322,Performance,scalab,scalable,1322,"// Functions like isZIPMask return true when a ISD::VECTOR_SHUFFLE's mask; // represents the same logical operation as performed by a ZIP instruction. In; // isolation these functions do not mean the ISD::VECTOR_SHUFFLE is exactly; // equivalent to an AArch64 instruction. There's the extra component of; // ISD::VECTOR_SHUFFLE's value type to consider. Prior to SVE these functions; // only operated on 64/128bit vector types that have a direct mapping to a; // target register and so an exact mapping is implied.; // However, when using SVE for fixed length vectors, most legal vector types; // are actually sub-vectors of a larger SVE register. When mapping; // ISD::VECTOR_SHUFFLE to an SVE instruction care must be taken to consider; // how the mask's indices translate. Specifically, when the mapping requires; // an exact meaning for a specific vector index (e.g. Index X is the last; // vector element in the register) then such mappings are often only safe when; // the exact SVE register size is know. The main exception to this is when; // indices are logically relative to the first element of either; // ISD::VECTOR_SHUFFLE operand because these relative indices don't change; // when converting from fixed-length to scalable vector types (i.e. the start; // of a fixed length vector is always the start of a scalable vector).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:961,Safety,safe,safe,961,"// Functions like isZIPMask return true when a ISD::VECTOR_SHUFFLE's mask; // represents the same logical operation as performed by a ZIP instruction. In; // isolation these functions do not mean the ISD::VECTOR_SHUFFLE is exactly; // equivalent to an AArch64 instruction. There's the extra component of; // ISD::VECTOR_SHUFFLE's value type to consider. Prior to SVE these functions; // only operated on 64/128bit vector types that have a direct mapping to a; // target register and so an exact mapping is implied.; // However, when using SVE for fixed length vectors, most legal vector types; // are actually sub-vectors of a larger SVE register. When mapping; // ISD::VECTOR_SHUFFLE to an SVE instruction care must be taken to consider; // how the mask's indices translate. Specifically, when the mapping requires; // an exact meaning for a specific vector index (e.g. Index X is the last; // vector element in the register) then such mappings are often only safe when; // the exact SVE register size is know. The main exception to this is when; // indices are logically relative to the first element of either; // ISD::VECTOR_SHUFFLE operand because these relative indices don't change; // when converting from fixed-length to scalable vector types (i.e. the start; // of a fixed length vector is always the start of a scalable vector).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:98,Testability,log,logical,98,"// Functions like isZIPMask return true when a ISD::VECTOR_SHUFFLE's mask; // represents the same logical operation as performed by a ZIP instruction. In; // isolation these functions do not mean the ISD::VECTOR_SHUFFLE is exactly; // equivalent to an AArch64 instruction. There's the extra component of; // ISD::VECTOR_SHUFFLE's value type to consider. Prior to SVE these functions; // only operated on 64/128bit vector types that have a direct mapping to a; // target register and so an exact mapping is implied.; // However, when using SVE for fixed length vectors, most legal vector types; // are actually sub-vectors of a larger SVE register. When mapping; // ISD::VECTOR_SHUFFLE to an SVE instruction care must be taken to consider; // how the mask's indices translate. Specifically, when the mapping requires; // an exact meaning for a specific vector index (e.g. Index X is the last; // vector element in the register) then such mappings are often only safe when; // the exact SVE register size is know. The main exception to this is when; // indices are logically relative to the first element of either; // ISD::VECTOR_SHUFFLE operand because these relative indices don't change; // when converting from fixed-length to scalable vector types (i.e. the start; // of a fixed length vector is always the start of a scalable vector).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:1063,Testability,log,logically,1063,"// Functions like isZIPMask return true when a ISD::VECTOR_SHUFFLE's mask; // represents the same logical operation as performed by a ZIP instruction. In; // isolation these functions do not mean the ISD::VECTOR_SHUFFLE is exactly; // equivalent to an AArch64 instruction. There's the extra component of; // ISD::VECTOR_SHUFFLE's value type to consider. Prior to SVE these functions; // only operated on 64/128bit vector types that have a direct mapping to a; // target register and so an exact mapping is implied.; // However, when using SVE for fixed length vectors, most legal vector types; // are actually sub-vectors of a larger SVE register. When mapping; // ISD::VECTOR_SHUFFLE to an SVE instruction care must be taken to consider; // how the mask's indices translate. Specifically, when the mapping requires; // an exact meaning for a specific vector index (e.g. Index X is the last; // vector element in the register) then such mappings are often only safe when; // the exact SVE register size is know. The main exception to this is when; // indices are logically relative to the first element of either; // ISD::VECTOR_SHUFFLE operand because these relative indices don't change; // when converting from fixed-length to scalable vector types (i.e. the start; // of a fixed length vector is always the start of a scalable vector).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:102,Availability,avail,available,102,"// Avoid producing TBL instruction if we don't know SVE register minimal size,; // unless NEON is not available and we can assume minimal SVE register size is; // 128-bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Safety,Avoid,Avoid,3,"// Avoid producing TBL instruction if we don't know SVE register minimal size,; // unless NEON is not available and we can assume minimal SVE register size is; // 128-bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:3,Safety,Safe,Safe,3,// Safe bitcasting between unpacked vector types of different element counts; // is currently unsupported because the following is missing the necessary; // work to ensure the result's elements live where they're supposed to within; // an SVE register.; // 01234567; // e.g. nxv2i32 = XX??XX??; // nxv4f16 = X?X?X?X?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:73,Usability,simpl,simplify,73,// All bits that are zeroed by (VSHL (VLSHR Val X) X) are not; // used - simplify to just Val.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:253,Security,expose,exposed,253,"// The SVE count intrinsics don't support the multiplier immediate so we; // don't have to account for that here. The value returned may be slightly; // over the true required bits, as this is based on the ""ALL"" pattern. The; // other patterns are also exposed by these intrinsics, but they all; // return a value that's strictly less than ""ALL"".",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:143,Availability,avail,available,143,"// If the vector is scalable, SVE is enabled, implying support for complex; // numbers. Otherwise, we need to ensure complex number support is available",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:20,Performance,scalab,scalable,20,"// If the vector is scalable, SVE is enabled, implying support for complex; // numbers. Otherwise, we need to ensure complex number support is available",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:156,Energy Efficiency,power,power-of-,156,"// We can only process vectors that have a bit size of 128 or higher (with an; // additional 64 bits for Neon). Additionally, these vectors must have a; // power-of-2 size, as we later split them into the smallest supported size; // and merging them back together after applying complex operation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:140,Availability,avail,available,140,// A size mismatch here implies either type promotion or widening and would; // have resulted in scalarisation if larger vectors had not be available.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:52,Integrability,Interface,Interface,52,"//==-- AArch64ISelLowering.h - AArch64 DAG Lowering Interface ----*- C++ -*-==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file defines the interfaces that AArch64 uses to lower LLVM code into a; // selection DAG.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:405,Integrability,interface,interfaces,405,"//==-- AArch64ISelLowering.h - AArch64 DAG Lowering Interface ----*- C++ -*-==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file defines the interfaces that AArch64 uses to lower LLVM code into a; // selection DAG.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:93,Modifiability,variab,variable,93,"// Produces the full sequence of instructions for getting the thread pointer; // offset of a variable into X0, using the TLSDesc model.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:3,Performance,Load,Load,3,"// Load from automatically generated descriptor (e.g. Global; // Offset Table, TLS record).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:65,Performance,perform,performed,65,"// adc, sbc instructions; // To avoid stack clash, allocation is performed by block and each block is; // probed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:32,Safety,avoid,avoid,32,"// adc, sbc instructions; // To avoid stack clash, allocation is performed by block and each block is; // probed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:14,Performance,load,loads,14,// Structured loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:19,Performance,load,loads,19,// Unsigned gather loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:17,Performance,load,loads,17,// Signed gather loads,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:19,Performance,load,loads,19,// Unsigned gather loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:17,Performance,load,loads,17,// Signed gather loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:23,Performance,load,loads,23,// Non-temporal gather loads,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:14,Availability,mask,masked,14,// Contiguous masked store.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:50,Modifiability,extend,extended,50,// Asserts that a function argument (i32) is zero-extended to i8 by; // the caller,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:3,Testability,Assert,Asserts,3,// Asserts that a function argument (i32) is zero-extended to i8 by; // the caller,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:27,Security,access,accesses,27,"// 128-bit system register accesses; // lo64, hi64, chain = MRRS(chain, sysregname)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:10,Performance,load,loads,10,// SME ZA loads and stores,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:44,Deployability,update,updates,44,// NEON Load/Store with post-increment base updates,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:8,Performance,Load,Load,8,// NEON Load/Store with post-increment base updates,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:7,Availability,mask,mask,7,// Bit mask selecting rounding mode,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:45,Availability,Mask,Mask,45,/// Determine which of the bits specified in Mask are known to be either zero; /// or one and return them in the KnownZero/KnownOne bitsets.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:167,Modifiability,extend,extended,167,"// Returning i64 unconditionally here (i.e. even for ILP32) means that the; // *DAG* representation of pointers will always be 64-bits. They will be; // truncated and extended when transferred to memory, but the 64-bit DAG; // allows us to use AArch64's addressing modes much more easily.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:55,Security,access,accesses,55,/// Returns true if the target allows unaligned memory accesses of the; /// specified type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:37,Availability,mask,mask,37,"/// Return true if the given shuffle mask can be codegen'd directly, or if it; /// should be stack expanded.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:92,Availability,mask,mask,92,/// Similar to isShuffleMaskLegal. Return true is the given 'select with zero'; /// shuffle mask can be codegen'd directly.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:94,Performance,load,load,94,"/// Return true if the addressing mode represented by AM is legal for this; /// target, for a load/store of the specified type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:65,Availability,Mask,Mask,65,/// Returns false if N is a bit extraction pattern of (X >> C) & Mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:65,Availability,Mask,Mask,65,/// Returns false if N is a bit extraction pattern of (X >> C) & Mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:68,Availability,mask,mask,68,/// Return true if it is profitable to fold a pair of shifts into a mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:50,Performance,load,load,50,/// Returns true if it is beneficial to convert a load of a constant; /// to just the constant itself.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:50,Safety,unsafe,unsafe,50,"/// If the target has a standard location for the unsafe stack pointer,; /// returns the address of that location. Otherwise, returns nullptr.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:52,Security,access,access,52,/// Returns true if \p VecTy is a legal interleaved access type. This; /// function checks the vector element type and the overall width of the; /// vector.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:38,Security,access,accesses,38,/// Returns the number of interleaved accesses that will be generated when; /// lowering accesses of the given type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:89,Security,access,accesses,89,/// Returns the number of interleaved accesses that will be generated when; /// lowering accesses of the given type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:110,Performance,load,load,110,/// Finds the incoming stack arguments which overlap the given fixed stack; /// object and incorporates their load into the current chain. This prevents; /// an upcoming store from clobbering the stack argument before it's used.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:315,Modifiability,extend,extends,315,"// With the exception of data-predicate transitions, no instructions are; // required to cast between legal scalable vector types. However:; // 1. Packed and unpacked types have different bit lengths, meaning BITCAST; // is not universally useable.; // 2. Most unpacked integer types are not legal and thus integer extends; // cannot be used to convert between unpacked and packed types.; // These can make ""bitcasting"" a multiphase process. REINTERPRET_CAST is used; // to transition between unpacked and packed types of the same element type,; // with BITCAST used otherwise.; // This function does not handle predicate bitcasts.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h:108,Performance,scalab,scalable,108,"// With the exception of data-predicate transitions, no instructions are; // required to cast between legal scalable vector types. However:; // 1. Packed and unpacked types have different bit lengths, meaning BITCAST; // is not universally useable.; // 2. Most unpacked integer types are not legal and thus integer extends; // cannot be used to convert between unpacked and packed types.; // These can make ""bitcasting"" a multiphase process. REINTERPRET_CAST is used; // to transition between unpacked and packed types of the same element type,; // with BITCAST used otherwise.; // This function does not handle predicate bitcasts.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:47,Performance,load,load,47,"//===- AArch64LoadStoreOptimizer.cpp - AArch64 load/store opt. pass -------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass that performs load / store related peephole; // optimizations. This pass should be run after register allocation.; //; // The pass runs after the PrologEpilogInserter where we emit the CFI; // instructions. In order to preserve the correctness of the unwind informaiton,; // the pass should not change the order of any two instructions, one of which; // has the FrameSetup/FrameDestroy flag or, alternatively, apply an add-hoc fix; // to unwind information.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:414,Performance,perform,performs,414,"//===- AArch64LoadStoreOptimizer.cpp - AArch64 load/store opt. pass -------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass that performs load / store related peephole; // optimizations. This pass should be run after register allocation.; //; // The pass runs after the PrologEpilogInserter where we emit the CFI; // instructions. In order to preserve the correctness of the unwind informaiton,; // the pass should not change the order of any two instructions, one of which; // has the FrameSetup/FrameDestroy flag or, alternatively, apply an add-hoc fix; // to unwind information.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:423,Performance,load,load,423,"//===- AArch64LoadStoreOptimizer.cpp - AArch64 load/store opt. pass -------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass that performs load / store related peephole; // optimizations. This pass should be run after register allocation.; //; // The pass runs after the PrologEpilogInserter where we emit the CFI; // instructions. In order to preserve the correctness of the unwind informaiton,; // the pass should not change the order of any two instructions, one of which; // has the FrameSetup/FrameDestroy flag or, alternatively, apply an add-hoc fix; // to unwind information.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:457,Performance,optimiz,optimizations,457,"//===- AArch64LoadStoreOptimizer.cpp - AArch64 load/store opt. pass -------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass that performs load / store related peephole; // optimizations. This pass should be run after register allocation.; //; // The pass runs after the PrologEpilogInserter where we emit the CFI; // instructions. In order to preserve the correctness of the unwind informaiton,; // the pass should not change the order of any two instructions, one of which; // has the FrameSetup/FrameDestroy flag or, alternatively, apply an add-hoc fix; // to unwind information.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:46,Performance,load,load,46,// The LdStLimit limits how far we search for load/store pairs.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:7,Deployability,Update,UpdateLimit,7,// The UpdateLimit limits how far we search for update instructions when we form; // pre-/post-index instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:48,Deployability,update,update,48,// The UpdateLimit limits how far we search for update instructions when we form; // pre-/post-index instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:75,Modifiability,extend,extended,75,"// SExtIdx gives the index of the result of the load pair that must be; // extended. The value of SExtIdx assumes that the paired load produces the; // value in this order: (I, returned iterator), i.e., -1 means no value has; // to be extended, 0 means I, and 1 means the returned iterator.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:235,Modifiability,extend,extended,235,"// SExtIdx gives the index of the result of the load pair that must be; // extended. The value of SExtIdx assumes that the paired load produces the; // value in this order: (I, returned iterator), i.e., -1 means no value has; // to be extended, 0 means I, and 1 means the returned iterator.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:48,Performance,load,load,48,"// SExtIdx gives the index of the result of the load pair that must be; // extended. The value of SExtIdx assumes that the paired load produces the; // value in this order: (I, returned iterator), i.e., -1 means no value has; // to be extended, 0 means I, and 1 means the returned iterator.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:130,Performance,load,load,130,"// SExtIdx gives the index of the result of the load pair that must be; // extended. The value of SExtIdx assumes that the paired load produces the; // value in this order: (I, returned iterator), i.e., -1 means no value has; // to be extended, 0 means I, and 1 means the returned iterator.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:39,Performance,load,load,39,"// Scan the instructions looking for a load/store that can be combined; // with the current instruction into a load/store pair.; // Return the matching instruction if one is found, else MBB->end().",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:111,Performance,load,load,111,"// Scan the instructions looking for a load/store that can be combined; // with the current instruction into a load/store pair.; // Return the matching instruction if one is found, else MBB->end().",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:99,Performance,load,load,99,// Scan the instructions looking for a store that writes to the address from; // which the current load instruction reads. Return true if one is found.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:15,Performance,load,load,15,// Promote the load that reads directly from the address stored to.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:53,Deployability,update,update,53,// Scan the instruction list to find a base register update that can; // be combined with the current instruction (a load or store) using; // pre or post indexed addressing with writeback. Scan forwards.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:117,Performance,load,load,117,// Scan the instruction list to find a base register update that can; // be combined with the current instruction (a load or store) using; // pre or post indexed addressing with writeback. Scan forwards.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:53,Deployability,update,update,53,// Scan the instruction list to find a base register update that can; // be combined with the current instruction (a load or store) using; // pre or post indexed addressing with writeback. Scan backwards.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:117,Performance,load,load,117,// Scan the instruction list to find a base register update that can; // be combined with the current instruction (a load or store) using; // pre or post indexed addressing with writeback. Scan backwards.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:28,Deployability,update,updates,28,// Find an instruction that updates the base register of the ld/st; // instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:44,Deployability,update,update,44,// Merge a pre- or post-index base register update into a ld/st instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:20,Performance,load,load,20,// Find and promote load instructions which read directly from store.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:34,Deployability,update,updates,34,// Find and merge a base register updates before or after a ld/st instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:58,Performance,load,loads,58,// FIXME: We don't currently support creating pre-indexed loads/stores when; // the load or store is the unscaled version. If we decide to perform such an; // optimization in the future the cases for the unscaled loads/stores will; // need to be added here.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:84,Performance,load,load,84,// FIXME: We don't currently support creating pre-indexed loads/stores when; // the load or store is the unscaled version. If we decide to perform such an; // optimization in the future the cases for the unscaled loads/stores will; // need to be added here.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:139,Performance,perform,perform,139,// FIXME: We don't currently support creating pre-indexed loads/stores when; // the load or store is the unscaled version. If we decide to perform such an; // optimization in the future the cases for the unscaled loads/stores will; // need to be added here.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:159,Performance,optimiz,optimization,159,// FIXME: We don't currently support creating pre-indexed loads/stores when; // the load or store is the unscaled version. If we decide to perform such an; // optimization in the future the cases for the unscaled loads/stores will; // need to be added here.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:213,Performance,load,loads,213,// FIXME: We don't currently support creating pre-indexed loads/stores when; // the load or store is the unscaled version. If we decide to perform such an; // optimization in the future the cases for the unscaled loads/stores will; // need to be added here.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:41,Integrability,depend,depends,41,// Which register is Rt and which is Rt2 depends on the offset order.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:24,Performance,load,load,24,// For backward merging load:; // Make sure the register being renamed is not used between the; // paired instructions. That would trash the content after the new; // paired instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:41,Integrability,depend,depends,41,"// Which register is Rt and which is Rt2 depends on the offset order.; // However, for pre load/stores the Rt should be the one of the pre; // load/store.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:91,Performance,load,load,91,"// Which register is Rt and which is Rt2 depends on the offset order.; // However, for pre load/stores the Rt should be the one of the pre; // load/store.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:143,Performance,load,load,143,"// Which register is Rt and which is Rt2 depends on the offset order.; // However, for pre load/stores the Rt should be the one of the pre; // load/store.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:108,Deployability,Update,Update,108,"// Here we swapped the assumption made for SExtIdx.; // I.e., we turn ldp I, Paired into ldp Paired, I.; // Update the index accordingly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:3,Usability,Clear,Clear,3,"// Clear kill flags on store if moving upwards. Example:; // STRWui kill %w0, ...; // USE %w1; // STRWui kill %w1 ; need to clear kill flag when moving STRWui upwards; // We are about to move the store of w1, so its kill flag may become; // invalid; not the case for w0.; // Since w1 is used between the stores, the kill flag on w1 is cleared; // after merging.; // STPWi kill %w0, %w1, ...; // USE %w1",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:124,Usability,clear,clear,124,"// Clear kill flags on store if moving upwards. Example:; // STRWui kill %w0, ...; // USE %w1; // STRWui kill %w1 ; need to clear kill flag when moving STRWui upwards; // We are about to move the store of w1, so its kill flag may become; // invalid; not the case for w0.; // Since w1 is used between the stores, the kill flag on w1 is cleared; // after merging.; // STPWi kill %w0, %w1, ...; // USE %w1",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:335,Usability,clear,cleared,335,"// Clear kill flags on store if moving upwards. Example:; // STRWui kill %w0, ...; // USE %w1; // STRWui kill %w1 ; need to clear kill flag when moving STRWui upwards; // We are about to move the store of w1, so its kill flag may become; // invalid; not the case for w0.; // Since w1 is used between the stores, the kill flag on w1 is cleared; // after merging.; // STPWi kill %w0, %w1, ...; // USE %w1",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:141,Availability,down,downwards,141,"// Clear kill flags of the first stores register. Example:; // STRWui %w1, ...; // USE kill %w1 ; need to clear kill flag when moving STRWui downwards; // STRW %w0",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:3,Usability,Clear,Clear,3,"// Clear kill flags of the first stores register. Example:; // STRWui %w1, ...; // USE kill %w1 ; need to clear kill flag when moving STRWui downwards; // STRW %w0",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:106,Usability,clear,clear,106,"// Clear kill flags of the first stores register. Example:; // STRWui %w1, ...; // USE kill %w1 ; need to clear kill flag when moving STRWui downwards; // STRW %w0",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:28,Modifiability,extend,extended,28,"// Right now, DstMO has the extended register, since it comes from an; // extended opcode.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:74,Modifiability,extend,extended,74,"// Right now, DstMO has the extended register, since it comes from an; // extended opcode.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:3,Deployability,Update,Update,3,// Update the result of LDP to use the W instead of the X variant.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:14,Performance,load,load,14,"// Remove the load, if the destination register of the loads is the same; // register for stored value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:55,Performance,load,loads,55,"// Remove the load, if the destination register of the loads is the same; // register for stored value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:15,Performance,load,load,15,// Replace the load with a mov if the load and store are in the same size.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:38,Performance,load,load,38,// Replace the load with a mov if the load and store are in the same size.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:80,Performance,perform,performance,80,// FIXME: Currently we disable this transformation in big-endian targets as; // performance and correctness are verified only in little-endian.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:38,Performance,load,load,38,// Clear kill flags between store and load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:3,Usability,Clear,Clear,3,// Clear kill flags between store and load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:97,Performance,load,load,97,"// Convert the byte-offset used by unscaled into an ""element"" offset used; // by the scaled pair load/store instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:32,Energy Efficiency,power,power,32,"// Do alignment, specialized to power of 2 and for signed ints,; // avoiding having to do a C-style cast from uint_64t to int when; // using alignTo from include/llvm/Support/MathExtras.h.; // FIXME: Move this function to include/MathExtras.h?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:68,Safety,avoid,avoiding,68,"// Do alignment, specialized to power of 2 and for signed ints,; // avoiding having to do a C-style cast from uint_64t to int when; // using alignTo from include/llvm/Support/MathExtras.h.; // FIXME: Move this function to include/MathExtras.h?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:10,Performance,load,load,10,"// If the load is the first instruction in the block, there's obviously; // not any matching store.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:419,Modifiability,variab,variable,419,"// If the load instruction reads directly from the address to which the; // store instruction writes and the stored value is not modified, we can; // promote the load. Since we do not handle stores with pre-/post-index,; // it's unnecessary to check if BaseReg is modified by the store itself.; // Also we can't handle stores without an immediate offset operand,; // while the operand might be the address for a global variable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:10,Performance,load,load,10,"// If the load instruction reads directly from the address to which the; // store instruction writes and the stored value is not modified, we can; // promote the load. Since we do not handle stores with pre-/post-index,; // it's unnecessary to check if BaseReg is modified by the store itself.; // Also we can't handle stores without an immediate offset operand,; // while the operand might be the address for a global variable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:162,Performance,load,load,162,"// If the load instruction reads directly from the address to which the; // store instruction writes and the stored value is not modified, we can; // promote the load. Since we do not handle stores with pre-/post-index,; // it's unnecessary to check if BaseReg is modified by the store itself.; // Also we can't handle stores without an immediate offset operand,; // while the operand might be the address for a global variable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:3,Deployability,Update,Update,3,// Update modified / uses register units.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:44,Performance,load,load,44,"// If we encounter a store aliased with the load, return early.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:23,Modifiability,extend,extended,23,// Try to match a sign-extended load/store with a zero-extended load/store.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:55,Modifiability,extend,extended,55,// Try to match a sign-extended load/store with a zero-extended load/store.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:32,Performance,load,load,32,// Try to match a sign-extended load/store with a zero-extended load/store.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:64,Performance,load,load,64,// Try to match a sign-extended load/store with a zero-extended load/store.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:60,Performance,load,load,60,"// If the second instruction isn't even a mergable/pairable load/store, bail; // out.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:28,Performance,load,load,28,// Try to match an unscaled load/store with a scaled load/store.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:53,Performance,load,load,53,// Try to match an unscaled load/store with a scaled load/store.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:66,Modifiability,rewrite,rewrite,66,"// We cannot rename arbitrary implicit-defs, the specific rule to rewrite; // them must be known. For example, in ORRWrs the implicit-def; // corresponds to the result register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:105,Performance,load,load,105,// Check if we can find an unused register which we can use to rename; // the register used by the first load/store.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:31,Performance,load,load,31,// We want to merge the second load into the first by rewriting the usages of; // the same reg between first (incl.) and second (excl.). We don't need to care; // about any insns before FirstLoad or after SecondLoad.; // 1. The second load writes new value into the same reg.; // - The renaming is impossible to impact later use of the reg.; // - The second load always trash the value written by the first load which; // means the reg must be killed before the second load.; // 2. The first load must be a def for the same reg so we don't need to look; // into anything before it.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:235,Performance,load,load,235,// We want to merge the second load into the first by rewriting the usages of; // the same reg between first (incl.) and second (excl.). We don't need to care; // about any insns before FirstLoad or after SecondLoad.; // 1. The second load writes new value into the same reg.; // - The renaming is impossible to impact later use of the reg.; // - The second load always trash the value written by the first load which; // means the reg must be killed before the second load.; // 2. The first load must be a def for the same reg so we don't need to look; // into anything before it.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:358,Performance,load,load,358,// We want to merge the second load into the first by rewriting the usages of; // the same reg between first (incl.) and second (excl.). We don't need to care; // about any insns before FirstLoad or after SecondLoad.; // 1. The second load writes new value into the same reg.; // - The renaming is impossible to impact later use of the reg.; // - The second load always trash the value written by the first load which; // means the reg must be killed before the second load.; // 2. The first load must be a def for the same reg so we don't need to look; // into anything before it.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:407,Performance,load,load,407,// We want to merge the second load into the first by rewriting the usages of; // the same reg between first (incl.) and second (excl.). We don't need to care; // about any insns before FirstLoad or after SecondLoad.; // 1. The second load writes new value into the same reg.; // - The renaming is impossible to impact later use of the reg.; // - The second load always trash the value written by the first load which; // means the reg must be killed before the second load.; // 2. The first load must be a def for the same reg so we don't need to look; // into anything before it.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:469,Performance,load,load,469,// We want to merge the second load into the first by rewriting the usages of; // the same reg between first (incl.) and second (excl.). We don't need to care; // about any insns before FirstLoad or after SecondLoad.; // 1. The second load writes new value into the same reg.; // - The renaming is impossible to impact later use of the reg.; // - The second load always trash the value written by the first load which; // means the reg must be killed before the second load.; // 2. The first load must be a def for the same reg so we don't need to look; // into anything before it.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:492,Performance,load,load,492,// We want to merge the second load into the first by rewriting the usages of; // the same reg between first (incl.) and second (excl.). We don't need to care; // about any insns before FirstLoad or after SecondLoad.; // 1. The second load writes new value into the same reg.; // - The renaming is impossible to impact later use of the reg.; // - The second load always trash the value written by the first load which; // means the reg must be killed before the second load.; // 2. The first load must be a def for the same reg so we don't need to look; // into anything before it.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:391,Availability,avail,available,391,"// Check if we can find a physical register for renaming \p Reg. This register; // must:; // * not be defined already in \p DefinedInBB; DefinedInBB must contain all; // defined registers up to the point where the renamed register will be used,; // * not used in \p UsedInBetween; UsedInBetween must contain all accessed; // registers in the range the rename register will be used,; // * is available in all used register classes (checked using RequiredClasses).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:312,Security,access,accessed,312,"// Check if we can find a physical register for renaming \p Reg. This register; // must:; // * not be defined already in \p DefinedInBB; DefinedInBB must contain all; // defined registers up to the point where the renamed register will be used,; // * not used in \p UsedInBetween; UsedInBetween must contain all accessed; // registers in the range the rename register will be used,; // * is available in all used register classes (checked using RequiredClasses).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:115,Performance,load,load,115,// For store pairs: returns a register from FirstMI to the beginning of the; // block that can be renamed.; // For load pairs: returns a register from FirstMI to MI that can be renamed.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:40,Performance,load,load,40,/// Scan the instructions looking for a load/store that can be combined with the; /// current instruction into a wider equivalent or a load/store pair.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:135,Performance,load,load,135,/// Scan the instructions looking for a load/store that can be combined with the; /// current instruction into a wider equivalent or a load/store pair.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:107,Deployability,update,updated,107,"// If the stored value and the address of the second instruction is; // the same, it needs to be using the updated register and therefore; // it must not be folded.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:52,Performance,load,load,52,"// If the alignment requirements of the scaled wide load/store; // instruction can't express the offset of the scaled narrow input,; // bail and keep looking. For promotable zero stores, allow only when; // the stored value is the same (i.e., WZR).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:59,Performance,optimiz,optimization,59,"// If the BaseReg has been modified, then we cannot do the optimization.; // For example, in the following pattern; // ldr x1 [x2]; // ldr x2 [x3]; // ldr x4 [x2, #8],; // the first and third ldr cannot be converted to ldp x1, x4, [x2]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:72,Performance,load,load,72,"// If the Rt of the second instruction (destination register of the; // load) was not modified or used between the two instructions and none; // of the instructions between the second and first alias with the; // second, we can combine the second into the first.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:13,Performance,load,loading,13,"// For pairs loading into the same reg, try to find a renaming; // opportunity to allow the renaming of Reg between FirstMI and MI; // and combine MI into FirstMI; otherwise bail and keep looking.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:40,Performance,load,load,40,// If the instruction wasn't a matching load or store. Stop searching if we; // encounter a call instruction that might modify memory.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:3,Deployability,Update,Update,3,// Update modified / uses register units.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:3,Deployability,Update,Update,3,// Update list of instructions that read/write memory.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:112,Performance,load,load,112,"// Return the instruction following the merged instruction, which is; // the instruction following our unmerged load. Unless that's the add/sub; // instruction we're merging, in which case it's the one after that.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:7,Deployability,update,update,7,// The update instruction source and destination register must be the; // same as the load/store base register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:86,Performance,load,load,86,// The update instruction source and destination register must be the; // same as the load/store base register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:87,Deployability,update,update,87,"// If the base register overlaps a source/destination register, we can't; // merge the update. This does not apply to tag store instructions which; // ignore the address part of the source register.; // This does not apply to STGPi as well, which does not have unpredictable; // behavior in this case unlike normal stores, and always performs writeback; // after reading the source register value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:334,Performance,perform,performs,334,"// If the base register overlaps a source/destination register, we can't; // merge the update. This does not apply to tag store instructions which; // ignore the address part of the source register.; // This does not apply to STGPi as well, which does not have unpredictable; // behavior in this case unlike normal stores, and always performs writeback; // after reading the source register value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:87,Security,access,access,87,"// We can't post-increment the stack pointer if any instruction between; // the memory access (I) and the increment (MBBI) can access the memory; // region defined by [SP, MBBI].",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:127,Security,access,access,127,"// We can't post-increment the stack pointer if any instruction between; // the memory access (I) and the increment (MBBI) can access the memory; // region defined by [SP, MBBI].",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:39,Performance,optimiz,optimization,39,"// FIXME: For now, we always block the optimization over SP in windows; // targets as it requires to adjust the unwind/debug info, messing up; // the unwind info can actually cause a miscompile.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:3,Deployability,Update,Update,3,// Update the status of what the instruction clobbered and used.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:221,Deployability,update,update,221,"// Otherwise, if the base register is used or modified, we have no match, so; // return early.; // If we are optimizing SP, do not allow instructions that may load or store; // in between the load and the optimized value update.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:109,Performance,optimiz,optimizing,109,"// Otherwise, if the base register is used or modified, we have no match, so; // return early.; // If we are optimizing SP, do not allow instructions that may load or store; // in between the load and the optimized value update.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:159,Performance,load,load,159,"// Otherwise, if the base register is used or modified, we have no match, so; // return early.; // If we are optimizing SP, do not allow instructions that may load or store; // in between the load and the optimized value update.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:192,Performance,load,load,192,"// Otherwise, if the base register is used or modified, we have no match, so; // return early.; // If we are optimizing SP, do not allow instructions that may load or store; // in between the load and the optimized value update.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:205,Performance,optimiz,optimized,205,"// Otherwise, if the base register is used or modified, we have no match, so; // return early.; // If we are optimizing SP, do not allow instructions that may load or store; // in between the load and the optimized value update.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:99,Deployability,update,update,99,"// If the load/store is the first instruction in the block, there's obviously; // not any matching update. Ditto if the memory offset isn't zero.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:10,Performance,load,load,10,"// If the load/store is the first instruction in the block, there's obviously; // not any matching update. Ditto if the memory offset isn't zero.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:80,Deployability,update,update,80,"// If the base register overlaps a destination register, we can't; // merge the update.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:39,Performance,optimiz,optimization,39,"// FIXME: For now, we always block the optimization over SP in windows; // targets as it requires to adjust the unwind/debug info, messing up; // the unwind info can actually cause a miscompile.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:18,Deployability,update,update,18,// Check that the update value is within our red zone limit (which may be; // zero).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:3,Deployability,Update,Update,3,// Update the status of what the instruction clobbered and used.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:121,Deployability,update,update,121,"// Keep track if we have a memory access before an SP pre-increment, in this; // case we need to validate later that the update amount respects the red; // zone.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:34,Security,access,access,34,"// Keep track if we have a memory access before an SP pre-increment, in this; // case we need to validate later that the update amount respects the red; // zone.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:97,Security,validat,validate,97,"// Keep track if we have a memory access before an SP pre-increment, in this; // case we need to validate later that the update amount respects the red; // zone.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:25,Performance,load,load,25,"// If this is a volatile load, don't mess with it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:61,Modifiability,extend,extend,61,// Make sure this is a reg+imm.; // FIXME: It is possible to extend it to handle reg+reg cases.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:86,Integrability,rout,routine,86,"// Promote the load. Keeping the iterator straight is a; // pain, so we let the merge routine tell us what the next instruction; // is after it's done mucking about.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:15,Performance,load,load,15,"// Promote the load. Keeping the iterator straight is a; // pain, so we let the merge routine tell us what the next instruction; // is after it's done mucking about.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:64,Integrability,rout,routine,64,"// Keeping the iterator straight is a pain, so we let the merge routine tell; // us what the next instruction is after it's done mucking about.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:8,Performance,load,loads,8,// Find loads and stores that can be merged into a single load or store pair; // instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:58,Performance,load,load,58,// Find loads and stores that can be merged into a single load or store pair; // instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:64,Integrability,rout,routine,64,"// Keeping the iterator straight is a pain, so we let the merge routine tell; // us what the next instruction is after it's done mucking about.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:31,Performance,load,load,31,// Fetch the memoperand of the load/store that is a candidate for; // combination.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:8,Performance,load,load,8,"// If a load arrives and ldp-aligned-only feature is opted, check that the; // alignment of the source pointer is at least double the alignment of the; // type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:13,Deployability,update,update,13,// Merge the update into the ld/st.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:13,Deployability,update,update,13,// Merge the update into the ld/st.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:24,Performance,load,load,24,"// The immediate in the load/store is scaled by the size of the memory; // operation. The immediate in the add we're looking for,; // however, is not, so adjust here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:13,Deployability,update,update,13,// Merge the update into the ld/st.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:47,Performance,load,loads,47,"// Four tranformations to do here:; // 1) Find loads that directly read from stores and promote them by; // replacing with mov instructions. If the store is wider than the load,; // the load will be replaced with a bitfield extract.; // e.g.,; // str w1, [x0, #4]; // ldrh w2, [x0, #6]; // ; becomes; // str w1, [x0, #4]; // lsr w2, w1, #16",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:172,Performance,load,load,172,"// Four tranformations to do here:; // 1) Find loads that directly read from stores and promote them by; // replacing with mov instructions. If the store is wider than the load,; // the load will be replaced with a bitfield extract.; // e.g.,; // str w1, [x0, #4]; // ldrh w2, [x0, #6]; // ; becomes; // str w1, [x0, #4]; // lsr w2, w1, #16",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:186,Performance,load,load,186,"// Four tranformations to do here:; // 1) Find loads that directly read from stores and promote them by; // replacing with mov instructions. If the store is wider than the load,; // the load will be replaced with a bitfield extract.; // e.g.,; // str w1, [x0, #4]; // ldrh w2, [x0, #6]; // ; becomes; // str w1, [x0, #4]; // lsr w2, w1, #16",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:11,Performance,load,loads,11,"// 3) Find loads and stores that can be merged into a single load or store; // pair instruction.; // e.g.,; // ldr x0, [x2]; // ldr x1, [x2, #8]; // ; becomes; // ldp x0, x1, [x2]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:61,Performance,load,load,61,"// 3) Find loads and stores that can be merged into a single load or store; // pair instruction.; // e.g.,; // ldr x0, [x2]; // ldr x1, [x2, #8]; // ; becomes; // ldp x0, x1, [x2]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:25,Deployability,update,updates,25,"// 4) Find base register updates that can be merged into the load or store; // as a base-reg writeback.; // e.g.,; // ldr x0, [x2]; // add x2, x2, #4; // ; becomes; // ldr x0, [x2], #4",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:61,Performance,load,load,61,"// 4) Find base register updates that can be merged into the load or store; // as a base-reg writeback.; // e.g.,; // ldr x0, [x2]; // add x2, x2, #4; // ; becomes; // ldr x0, [x2], #4",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:136,Performance,optimiz,optimize,136,// Resize the modified and used register unit trackers. We do this once; // per function and then clear the register units each time we optimize a load; // or store.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:147,Performance,load,load,147,// Resize the modified and used register unit trackers. We do this once; // per function and then clear the register units each time we optimize a load; // or store.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:98,Usability,clear,clear,98,// Resize the modified and used register unit trackers. We do this once; // per function and then clear the register units each time we optimize a load; // or store.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:139,Energy Efficiency,schedul,scheduler,139,"// FIXME: Do we need/want a pre-alloc pass like ARM has to try to keep loads and; // stores near one another? Note: The pre-RA instruction scheduler already has; // hooks to try and schedule pairable loads/stores together to improve pairing; // opportunities. Thus, pre-RA pairing pass may not be worth the effort.; // FIXME: When pairing store instructions it's very possible for this pass to; // hoist a store with a KILL marker above another use (without a KILL marker).; // The resulting IR is invalid, but nothing uses the KILL markers after this; // pass, so it's never caused a problem in practice.; /// createAArch64LoadStoreOptimizationPass - returns an instance of the; /// load / store optimization pass.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:182,Energy Efficiency,schedul,schedule,182,"// FIXME: Do we need/want a pre-alloc pass like ARM has to try to keep loads and; // stores near one another? Note: The pre-RA instruction scheduler already has; // hooks to try and schedule pairable loads/stores together to improve pairing; // opportunities. Thus, pre-RA pairing pass may not be worth the effort.; // FIXME: When pairing store instructions it's very possible for this pass to; // hoist a store with a KILL marker above another use (without a KILL marker).; // The resulting IR is invalid, but nothing uses the KILL markers after this; // pass, so it's never caused a problem in practice.; /// createAArch64LoadStoreOptimizationPass - returns an instance of the; /// load / store optimization pass.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:71,Performance,load,loads,71,"// FIXME: Do we need/want a pre-alloc pass like ARM has to try to keep loads and; // stores near one another? Note: The pre-RA instruction scheduler already has; // hooks to try and schedule pairable loads/stores together to improve pairing; // opportunities. Thus, pre-RA pairing pass may not be worth the effort.; // FIXME: When pairing store instructions it's very possible for this pass to; // hoist a store with a KILL marker above another use (without a KILL marker).; // The resulting IR is invalid, but nothing uses the KILL markers after this; // pass, so it's never caused a problem in practice.; /// createAArch64LoadStoreOptimizationPass - returns an instance of the; /// load / store optimization pass.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:200,Performance,load,loads,200,"// FIXME: Do we need/want a pre-alloc pass like ARM has to try to keep loads and; // stores near one another? Note: The pre-RA instruction scheduler already has; // hooks to try and schedule pairable loads/stores together to improve pairing; // opportunities. Thus, pre-RA pairing pass may not be worth the effort.; // FIXME: When pairing store instructions it's very possible for this pass to; // hoist a store with a KILL marker above another use (without a KILL marker).; // The resulting IR is invalid, but nothing uses the KILL markers after this; // pass, so it's never caused a problem in practice.; /// createAArch64LoadStoreOptimizationPass - returns an instance of the; /// load / store optimization pass.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:684,Performance,load,load,684,"// FIXME: Do we need/want a pre-alloc pass like ARM has to try to keep loads and; // stores near one another? Note: The pre-RA instruction scheduler already has; // hooks to try and schedule pairable loads/stores together to improve pairing; // opportunities. Thus, pre-RA pairing pass may not be worth the effort.; // FIXME: When pairing store instructions it's very possible for this pass to; // hoist a store with a KILL marker above another use (without a KILL marker).; // The resulting IR is invalid, but nothing uses the KILL markers after this; // pass, so it's never caused a problem in practice.; /// createAArch64LoadStoreOptimizationPass - returns an instance of the; /// load / store optimization pass.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:697,Performance,optimiz,optimization,697,"// FIXME: Do we need/want a pre-alloc pass like ARM has to try to keep loads and; // stores near one another? Note: The pre-RA instruction scheduler already has; // hooks to try and schedule pairable loads/stores together to improve pairing; // opportunities. Thus, pre-RA pairing pass may not be worth the effort.; // FIXME: When pairing store instructions it's very possible for this pass to; // hoist a store with a KILL marker above another use (without a KILL marker).; // The resulting IR is invalid, but nothing uses the KILL markers after this; // pass, so it's never caused a problem in practice.; /// createAArch64LoadStoreOptimizationPass - returns an instance of the; /// load / store optimization pass.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:1003,Availability,fault,faulting,1003,"//===- AArch64LoopIdiomTransform.cpp - Loop idiom recognition -------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass implements a pass that recognizes certain loop idioms and; // transforms them into more optimized versions of the same loop. In cases; // where this happens, it can be a significant performance win.; //; // We currently only recognize one loop that finds the first mismatched byte; // in an array and returns the index, i.e. something like:; //; // while (++i != n) {; // if (a[i] != b[i]); // break;; // }; //; // In this example we can actually vectorize the loop despite the early exit,; // although the loop vectorizer does not support it. It requires some extra; // checks to deal with the possibility of faulting loads when crossing page; // boundaries. However, even with these checks it is still profitable to do the; // transformation.; //; //===----------------------------------------------------------------------===//; //; // TODO List:; //; // * Add support for the inverse case where we scan for a matching element.; // * Permit 64-bit induction variable types.; // * Recognize loops that increment the IV *after* comparing bytes.; // * Allow 32-bit sign-extends of the IV used by the GEP.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:1354,Modifiability,variab,variable,1354,"//===- AArch64LoopIdiomTransform.cpp - Loop idiom recognition -------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass implements a pass that recognizes certain loop idioms and; // transforms them into more optimized versions of the same loop. In cases; // where this happens, it can be a significant performance win.; //; // We currently only recognize one loop that finds the first mismatched byte; // in an array and returns the index, i.e. something like:; //; // while (++i != n) {; // if (a[i] != b[i]); // break;; // }; //; // In this example we can actually vectorize the loop despite the early exit,; // although the loop vectorizer does not support it. It requires some extra; // checks to deal with the possibility of faulting loads when crossing page; // boundaries. However, even with these checks it is still profitable to do the; // transformation.; //; //===----------------------------------------------------------------------===//; //; // TODO List:; //; // * Add support for the inverse case where we scan for a matching element.; // * Permit 64-bit induction variable types.; // * Recognize loops that increment the IV *after* comparing bytes.; // * Allow 32-bit sign-extends of the IV used by the GEP.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:1463,Modifiability,extend,extends,1463,"//===- AArch64LoopIdiomTransform.cpp - Loop idiom recognition -------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass implements a pass that recognizes certain loop idioms and; // transforms them into more optimized versions of the same loop. In cases; // where this happens, it can be a significant performance win.; //; // We currently only recognize one loop that finds the first mismatched byte; // in an array and returns the index, i.e. something like:; //; // while (++i != n) {; // if (a[i] != b[i]); // break;; // }; //; // In this example we can actually vectorize the loop despite the early exit,; // although the loop vectorizer does not support it. It requires some extra; // checks to deal with the possibility of faulting loads when crossing page; // boundaries. However, even with these checks it is still profitable to do the; // transformation.; //; //===----------------------------------------------------------------------===//; //; // TODO List:; //; // * Add support for the inverse case where we scan for a matching element.; // * Permit 64-bit induction variable types.; // * Recognize loops that increment the IV *after* comparing bytes.; // * Allow 32-bit sign-extends of the IV used by the GEP.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:481,Performance,optimiz,optimized,481,"//===- AArch64LoopIdiomTransform.cpp - Loop idiom recognition -------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass implements a pass that recognizes certain loop idioms and; // transforms them into more optimized versions of the same loop. In cases; // where this happens, it can be a significant performance win.; //; // We currently only recognize one loop that finds the first mismatched byte; // in an array and returns the index, i.e. something like:; //; // while (++i != n) {; // if (a[i] != b[i]); // break;; // }; //; // In this example we can actually vectorize the loop despite the early exit,; // although the loop vectorizer does not support it. It requires some extra; // checks to deal with the possibility of faulting loads when crossing page; // boundaries. However, even with these checks it is still profitable to do the; // transformation.; //; //===----------------------------------------------------------------------===//; //; // TODO List:; //; // * Add support for the inverse case where we scan for a matching element.; // * Permit 64-bit induction variable types.; // * Recognize loops that increment the IV *after* comparing bytes.; // * Allow 32-bit sign-extends of the IV used by the GEP.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:575,Performance,perform,performance,575,"//===- AArch64LoopIdiomTransform.cpp - Loop idiom recognition -------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass implements a pass that recognizes certain loop idioms and; // transforms them into more optimized versions of the same loop. In cases; // where this happens, it can be a significant performance win.; //; // We currently only recognize one loop that finds the first mismatched byte; // in an array and returns the index, i.e. something like:; //; // while (++i != n) {; // if (a[i] != b[i]); // break;; // }; //; // In this example we can actually vectorize the loop despite the early exit,; // although the loop vectorizer does not support it. It requires some extra; // checks to deal with the possibility of faulting loads when crossing page; // boundaries. However, even with these checks it is still profitable to do the; // transformation.; //; //===----------------------------------------------------------------------===//; //; // TODO List:; //; // * Add support for the inverse case where we scan for a matching element.; // * Permit 64-bit induction variable types.; // * Recognize loops that increment the IV *after* comparing bytes.; // * Allow 32-bit sign-extends of the IV used by the GEP.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:1012,Performance,load,loads,1012,"//===- AArch64LoopIdiomTransform.cpp - Loop idiom recognition -------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass implements a pass that recognizes certain loop idioms and; // transforms them into more optimized versions of the same loop. In cases; // where this happens, it can be a significant performance win.; //; // We currently only recognize one loop that finds the first mismatched byte; // in an array and returns the index, i.e. something like:; //; // while (++i != n) {; // if (a[i] != b[i]); // break;; // }; //; // In this example we can actually vectorize the loop despite the early exit,; // although the loop vectorizer does not support it. It requires some extra; // checks to deal with the possibility of faulting loads when crossing page; // boundaries. However, even with these checks it is still profitable to do the; // transformation.; //; //===----------------------------------------------------------------------===//; //; // TODO List:; //; // * Add support for the inverse case where we scan for a matching element.; // * Permit 64-bit induction variable types.; // * Recognize loops that increment the IV *after* comparing bytes.; // * Allow 32-bit sign-extends of the IV used by the GEP.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:312,Availability,fault,fault,312,"// Currently the transformation only works on scalable vector types, although; // there is no fundamental reason why it cannot be made to work for fixed; // width too.; // We also need to know the minimum page size for the target in order to; // generate runtime memory checks to ensure the vector version won't fault.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:46,Performance,scalab,scalable,46,"// Currently the transformation only works on scalable vector types, although; // there is no fundamental reason why it cannot be made to work for fixed; // width too.; // We also need to know the minimum page size for the target in order to; // generate runtime memory checks to ensure the vector version won't fault.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:170,Performance,load,load,170,"// The second block should contain 7 instructions, e.g.; //; // while.body:; // %idx = zext i32 %inc to i64; // %idx.a = getelementptr inbounds i8, ptr %a, i64 %idx; // %load.a = load i8, ptr %idx.a; // %idx.b = getelementptr inbounds i8, ptr %b, i64 %idx; // %load.b = load i8, ptr %idx.b; // %cmp.not.ld = icmp eq i8 %load.a, %load.b; // br i1 %cmp.not.ld, label %while.cond, label %while.end; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:179,Performance,load,load,179,"// The second block should contain 7 instructions, e.g.; //; // while.body:; // %idx = zext i32 %inc to i64; // %idx.a = getelementptr inbounds i8, ptr %a, i64 %idx; // %load.a = load i8, ptr %idx.a; // %idx.b = getelementptr inbounds i8, ptr %b, i64 %idx; // %load.b = load i8, ptr %idx.b; // %cmp.not.ld = icmp eq i8 %load.a, %load.b; // br i1 %cmp.not.ld, label %while.cond, label %while.end; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:261,Performance,load,load,261,"// The second block should contain 7 instructions, e.g.; //; // while.body:; // %idx = zext i32 %inc to i64; // %idx.a = getelementptr inbounds i8, ptr %a, i64 %idx; // %load.a = load i8, ptr %idx.a; // %idx.b = getelementptr inbounds i8, ptr %b, i64 %idx; // %load.b = load i8, ptr %idx.b; // %cmp.not.ld = icmp eq i8 %load.a, %load.b; // br i1 %cmp.not.ld, label %while.cond, label %while.end; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:270,Performance,load,load,270,"// The second block should contain 7 instructions, e.g.; //; // while.body:; // %idx = zext i32 %inc to i64; // %idx.a = getelementptr inbounds i8, ptr %a, i64 %idx; // %load.a = load i8, ptr %idx.a; // %idx.b = getelementptr inbounds i8, ptr %b, i64 %idx; // %load.b = load i8, ptr %idx.b; // %cmp.not.ld = icmp eq i8 %load.a, %load.b; // br i1 %cmp.not.ld, label %while.cond, label %while.end; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:320,Performance,load,load,320,"// The second block should contain 7 instructions, e.g.; //; // while.body:; // %idx = zext i32 %inc to i64; // %idx.a = getelementptr inbounds i8, ptr %a, i64 %idx; // %load.a = load i8, ptr %idx.a; // %idx.b = getelementptr inbounds i8, ptr %b, i64 %idx; // %load.b = load i8, ptr %idx.b; // %cmp.not.ld = icmp eq i8 %load.a, %load.b; // br i1 %cmp.not.ld, label %while.cond, label %while.end; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:329,Performance,load,load,329,"// The second block should contain 7 instructions, e.g.; //; // while.body:; // %idx = zext i32 %inc to i64; // %idx.a = getelementptr inbounds i8, ptr %a, i64 %idx; // %load.a = load i8, ptr %idx.a; // %idx.b = getelementptr inbounds i8, ptr %b, i64 %idx; // %load.b = load i8, ptr %idx.b; // %cmp.not.ld = icmp eq i8 %load.a, %load.b; // br i1 %cmp.not.ld, label %while.cond, label %while.end; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:41,Performance,load,load,41,// WhileBB should contain the pattern of load & compare instructions. Match; // the pattern and find the GEP instructions used by the loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:134,Performance,load,loads,134,// WhileBB should contain the pattern of load & compare instructions. Match; // the pattern and find the GEP instructions used by the loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:16,Performance,load,loading,16,// Check we are loading i8 values from two loop invariant pointers,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:43,Performance,Load,Load,43,// The index is incremented before the GEP/Load pair so we need to; // add 1 to the start value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:84,Modifiability,extend,extended,84,"// Create the blocks that we're going to need:; // 1. A block for checking the zero-extended length exceeds 0; // 2. A block to check that the start and end addresses of a given array; // lie on the same page.; // 3. The SVE loop preheader.; // 4. The first SVE loop block.; // 5. The SVE loop increment block.; // 6. A block we can jump to from the SVE loop when a mismatch is found.; // 7. The first block of the scalar loop itself, containing PHIs , loads; // and cmp.; // 8. A scalar loop increment block to increment the PHIs and go back; // around the loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:453,Performance,load,loads,453,"// Create the blocks that we're going to need:; // 1. A block for checking the zero-extended length exceeds 0; // 2. A block to check that the start and end addresses of a given array; // lie on the same page.; // 3. The SVE loop preheader.; // 4. The first SVE loop block.; // 5. The SVE loop increment block.; // 6. A block we can jump to from the SVE loop when a mismatch is found.; // 7. The first block of the scalar loop itself, containing PHIs , loads; // and cmp.; // 8. A scalar loop increment block to increment the PHIs and go back; // around the loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:3,Deployability,Update,Update,3,// Update the terminator added by SplitBlock to branch to the first block,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:3,Deployability,Update,Update,3,// Update LoopInfo with the new SVE & scalar loops.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:18,Modifiability,extend,extended,18,// Check the zero-extended iteration count > 0,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:151,Availability,fault,fault,151,"// The early exit in the original loop means that when performing vector; // loads we are potentially reading ahead of the early exit. So we could; // fault if crossing a page boundary. Therefore, we create runtime memory; // checks based on the minimum page size as follows:; // 1. Calculate the addresses of the first memory accesses in the loop,; // i.e. LhsStart and RhsStart.; // 2. Get the last accessed addresses in the loop, i.e. LhsEnd and RhsEnd.; // 3. Determine which pages correspond to all the memory accesses, i.e; // LhsStartPage, LhsEndPage, RhsStartPage, RhsEndPage.; // 4. If LhsStartPage == LhsEndPage and RhsStartPage == RhsEndPage, then; // we know we won't cross any page boundaries in the loop so we can; // enter the vector loop! Otherwise we fall back on the scalar loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:55,Performance,perform,performing,55,"// The early exit in the original loop means that when performing vector; // loads we are potentially reading ahead of the early exit. So we could; // fault if crossing a page boundary. Therefore, we create runtime memory; // checks based on the minimum page size as follows:; // 1. Calculate the addresses of the first memory accesses in the loop,; // i.e. LhsStart and RhsStart.; // 2. Get the last accessed addresses in the loop, i.e. LhsEnd and RhsEnd.; // 3. Determine which pages correspond to all the memory accesses, i.e; // LhsStartPage, LhsEndPage, RhsStartPage, RhsEndPage.; // 4. If LhsStartPage == LhsEndPage and RhsStartPage == RhsEndPage, then; // we know we won't cross any page boundaries in the loop so we can; // enter the vector loop! Otherwise we fall back on the scalar loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:77,Performance,load,loads,77,"// The early exit in the original loop means that when performing vector; // loads we are potentially reading ahead of the early exit. So we could; // fault if crossing a page boundary. Therefore, we create runtime memory; // checks based on the minimum page size as follows:; // 1. Calculate the addresses of the first memory accesses in the loop,; // i.e. LhsStart and RhsStart.; // 2. Get the last accessed addresses in the loop, i.e. LhsEnd and RhsEnd.; // 3. Determine which pages correspond to all the memory accesses, i.e; // LhsStartPage, LhsEndPage, RhsStartPage, RhsEndPage.; // 4. If LhsStartPage == LhsEndPage and RhsStartPage == RhsEndPage, then; // we know we won't cross any page boundaries in the loop so we can; // enter the vector loop! Otherwise we fall back on the scalar loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:327,Security,access,accesses,327,"// The early exit in the original loop means that when performing vector; // loads we are potentially reading ahead of the early exit. So we could; // fault if crossing a page boundary. Therefore, we create runtime memory; // checks based on the minimum page size as follows:; // 1. Calculate the addresses of the first memory accesses in the loop,; // i.e. LhsStart and RhsStart.; // 2. Get the last accessed addresses in the loop, i.e. LhsEnd and RhsEnd.; // 3. Determine which pages correspond to all the memory accesses, i.e; // LhsStartPage, LhsEndPage, RhsStartPage, RhsEndPage.; // 4. If LhsStartPage == LhsEndPage and RhsStartPage == RhsEndPage, then; // we know we won't cross any page boundaries in the loop so we can; // enter the vector loop! Otherwise we fall back on the scalar loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:401,Security,access,accessed,401,"// The early exit in the original loop means that when performing vector; // loads we are potentially reading ahead of the early exit. So we could; // fault if crossing a page boundary. Therefore, we create runtime memory; // checks based on the minimum page size as follows:; // 1. Calculate the addresses of the first memory accesses in the loop,; // i.e. LhsStart and RhsStart.; // 2. Get the last accessed addresses in the loop, i.e. LhsEnd and RhsEnd.; // 3. Determine which pages correspond to all the memory accesses, i.e; // LhsStartPage, LhsEndPage, RhsStartPage, RhsEndPage.; // 4. If LhsStartPage == LhsEndPage and RhsStartPage == RhsEndPage, then; // we know we won't cross any page boundaries in the loop so we can; // enter the vector loop! Otherwise we fall back on the scalar loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:515,Security,access,accesses,515,"// The early exit in the original loop means that when performing vector; // loads we are potentially reading ahead of the early exit. So we could; // fault if crossing a page boundary. Therefore, we create runtime memory; // checks based on the minimum page size as follows:; // 1. Calculate the addresses of the first memory accesses in the loop,; // i.e. LhsStart and RhsStart.; // 2. Get the last accessed addresses in the loop, i.e. LhsEnd and RhsEnd.; // 3. Determine which pages correspond to all the memory accesses, i.e; // LhsStartPage, LhsEndPage, RhsStartPage, RhsEndPage.; // 4. If LhsStartPage == LhsEndPage and RhsStartPage == RhsEndPage, then; // we know we won't cross any page boundaries in the loop so we can; // enter the vector loop! Otherwise we fall back on the scalar loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:82,Modifiability,extend,extend,82,"// Set up the SVE loop preheader, i.e. calculate initial loop predicate,; // zero-extend MaxLen to 64-bits, determine the number of vector elements; // processed in each iteration, etc.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:184,Modifiability,variab,variable,184,"// At this point we know two things must be true:; // 1. Start <= End; // 2. ExtMaxLen <= MinPageSize due to the page checks.; // Therefore, we know that we can use a 64-bit induction variable that; // starts from 0 -> ExtMaxLen and it will not overflow.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:78,Performance,load,loads,78,"// Set up the first SVE loop block by creating the PHIs, doing the vector; // loads and comparing the vectors.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:36,Performance,Load,Load,36,// Otherwise compare the values; // Load bytes from each array and compare them.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:53,Performance,load,loads,53,// Increment the pointer if this was done before the loads in the loop.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:54,Integrability,depend,depending,54,// Create the branch to either the end or found block depending on the value; // returned by the intrinsic.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:16,Integrability,depend,depended,16,// Any PHI that depended upon the result of the byte compare needs a new; // incoming value from CmpBB. This is because the original loop will get; // deleted.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LowerHomogeneousPrologEpilog.cpp:143,Performance,optimiz,optimized,143,"/// Lower a HOM_Prolog pseudo instruction into a helper call; /// or a sequence of homogeneous stores.; /// When a fp setup follows, it can be optimized.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LowerHomogeneousPrologEpilog.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LowerHomogeneousPrologEpilog.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LowerHomogeneousPrologEpilog.cpp:95,Performance,load,loads,95,"/// Lower a HOM_Epilog pseudo instruction into a helper call; /// or a sequence of homogeneous loads.; /// When a return follow, it can be optimized.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LowerHomogeneousPrologEpilog.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LowerHomogeneousPrologEpilog.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LowerHomogeneousPrologEpilog.cpp:139,Performance,optimiz,optimized,139,"/// Lower a HOM_Epilog pseudo instruction into a helper call; /// or a sequence of homogeneous loads.; /// When a return follow, it can be optimized.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LowerHomogeneousPrologEpilog.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LowerHomogeneousPrologEpilog.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LowerHomogeneousPrologEpilog.cpp:22,Safety,avoid,avoid,22,// Use ODR linkage to avoid duplication.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LowerHomogeneousPrologEpilog.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LowerHomogeneousPrologEpilog.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LowerHomogeneousPrologEpilog.cpp:11,Performance,load,load-pair,11,"/// Emit a load-pair instruction for frame-destroy.; /// If Reg2 is AArch64::NoRegister, emit LDR instead.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LowerHomogeneousPrologEpilog.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LowerHomogeneousPrologEpilog.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LowerHomogeneousPrologEpilog.cpp:125,Performance,load,loads,125,"/// Lower a HOM_Epilog pseudo instruction into a helper call while; /// creating the helper on demand. Or emit a sequence of loads in place when not; /// using a helper call.; ///; /// 1. With a helper including ret; /// HOM_Epilog x30, x29, x19, x20, x21, x22 ; MBBI; /// ret ; NextMBBI; /// =>; /// b _OUTLINED_FUNCTION_EPILOG_TAIL_x30x29x19x20x21x22; /// ... ; NextMBBI; ///; /// 2. With a helper; /// HOM_Epilog x30, x29, x19, x20, x21, x22; /// =>; /// bl _OUTLINED_FUNCTION_EPILOG_x30x29x19x20x21x22; ///; /// 3. Without a helper; /// HOM_Epilog x30, x29, x19, x20, x21, x22; /// =>; /// ldp x29, x30, [sp, #32]; /// ldp x20, x19, [sp, #16]; /// ldp x22, x21, [sp], #48",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LowerHomogeneousPrologEpilog.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LowerHomogeneousPrologEpilog.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LowerHomogeneousPrologEpilog.cpp:179,Deployability,update,updated,179,/// Process each machine instruction; /// @param MBB machine basic block; /// @param MBBI current instruction iterator; /// @param NextMBBI next instruction iterator which can be updated; /// @return True when IR is changed.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LowerHomogeneousPrologEpilog.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LowerHomogeneousPrologEpilog.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.cpp:62,Performance,optimiz,optimization,62,// TODO: skip functions that have no instrumented allocas for optimization,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.cpp:133,Integrability,depend,depending,133,"// BTI/PAuthLR may be set either on the function or the module. Set Bool from; // either the function attribute or module attribute, depending on what is; // set.; // Note: the module attributed is numeric (0 or 1) but the function attribute; // is stringy (""true"" or ""false"").",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.cpp:105,Safety,safe,safe,105,// The default stack probe size is 4096 if the function has no; // stack-probe-size attribute. This is a safe default because it is the; // smallest possible guard page size.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.cpp:9,Availability,down,down,9,// Round down to the stack alignment.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h:168,Usability,usab,usable,168,"/// Number of bytes of arguments this function has on the stack. If the callee; /// is expected to restore the argument stack this should be a multiple of 16,; /// all usable during a tail call.; ///; /// The alternative would forbid tail call optimisation in some cases: if we; /// want to transfer control from a function with 8-bytes of stack-argument; /// space to a function with 16-bytes then misalignment of this value would; /// make a stack adjustment necessary, which could not be undone by the; /// callee.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h:491,Usability,undo,undone,491,"/// Number of bytes of arguments this function has on the stack. If the callee; /// is expected to restore the argument stack this should be a multiple of 16,; /// all usable during a tail call.; ///; /// The alternative would forbid tail call optimisation in some cases: if we; /// want to transfer control from a function with 8-bytes of stack-argument; /// space to a function with 16-bytes then misalignment of this value would; /// make a stack adjustment necessary, which could not be undone by the; /// callee.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h:244,Availability,avail,available,244,/// Space just below incoming stack pointer reserved for arguments being; /// passed on the stack during a tail call. This will be the difference; /// between the largest tail call argument space needed in this function and; /// what's already available by reusing space of incoming arguments.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h:18,Security,access,accesses,18,/// Number of TLS accesses using the special (combinable); /// _TLS_MODULE_BASE_ symbol.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h:129,Security,authenticat,authentication,129,"/// SigningInstrOffset captures the offset of the PAC-RET signing instruction; /// within the prologue, so it can be re-used for authentication in the; /// epilogue when using PC as a second salt (FEAT_PAuth_LR)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h:33,Modifiability,extend,extended,33,"/// Whether this function has an extended frame record [Ctx, FP, LR]. If so,; /// bit 60 of the in-memory FP will be 1 to enable other tools to detect the; /// extended record.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h:160,Modifiability,extend,extended,160,"/// Whether this function has an extended frame record [Ctx, FP, LR]. If so,; /// bit 60 of the in-memory FP will be 1 to enable other tools to detect the; /// extended record.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h:144,Safety,detect,detect,144,"/// Whether this function has an extended frame record [Ctx, FP, LR]. If so,; /// bit 60 of the in-memory FP will be 1 to enable other tools to detect the; /// extended record.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h:21,Performance,Scalab,Scalable,21,/// The function has Scalable Vector or Scalable Predicate register argument; /// or return type,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h:40,Performance,Scalab,Scalable,40,/// The function has Scalable Vector or Scalable Predicate register argument; /// or return type,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h:81,Performance,cache,cached,81,// Make sure the calculated size derived from the CalleeSavedInfo; // equals the cached size that was calculated elsewhere (e.g. in; // determineCalleeSaves).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h:54,Performance,scalab,scalable,54,// Saves the CalleeSavedStackSize for SVE vectors in 'scalable bytes',MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineScheduler.cpp:40,Energy Efficiency,Schedul,Scheduler,40,"//===- AArch64MachineScheduler.cpp - MI Scheduler for AArch64 -------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineScheduler.h:53,Energy Efficiency,schedul,scheduler,53,"//===- AArch64MachineScheduler.h - Custom AArch64 MI scheduler --*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // Custom AArch64 MI scheduler.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineScheduler.h:401,Energy Efficiency,schedul,scheduler,401,"//===- AArch64MachineScheduler.h - Custom AArch64 MI scheduler --*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // Custom AArch64 MI scheduler.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineScheduler.h:62,Energy Efficiency,schedul,scheduling,62,/// A MachineSchedStrategy implementation for AArch64 post RA scheduling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MacroFusion.cpp:447,Energy Efficiency,schedul,scheduling,447,"//===- AArch64MacroFusion.cpp - AArch64 Macro Fusion ----------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This file contains the AArch64 implementation of the DAG scheduling; /// mutation to pair instructions back to back.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MacroFusion.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MacroFusion.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MacroFusion.cpp:32,Performance,load,loads,32,/// Fuse address generation and loads or stores.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MacroFusion.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MacroFusion.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MacroFusion.cpp:18,Testability,log,logic,18,// Arithmetic and logic.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MacroFusion.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MacroFusion.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MacroFusion.cpp:3,Testability,Log,Logic,3,// Logic,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MacroFusion.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MacroFusion.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MacroFusion.h:443,Energy Efficiency,schedul,scheduling,443,"//===- AArch64MacroFusion.h - AArch64 Macro Fusion ------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This file contains the AArch64 definition of the DAG scheduling; /// mutation to pair instructions back to back.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MacroFusion.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MacroFusion.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MCInstLower.cpp:27,Energy Efficiency,efficient,efficient,27,"// FIXME: We would like an efficient form for this, so we don't have to do a; // lot of extra uniquing.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MCInstLower.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MCInstLower.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MCInstLower.cpp:23,Security,access,access,23,// The general dynamic access sequence is used to get the; // address of _TLS_MODULE_BASE_.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MCInstLower.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MCInstLower.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MCInstLower.cpp:204,Deployability,patch,patch,204,// FIXME: Currently we only set VK_NC for MO_G3/MO_G2/MO_G1/MO_G0. This is; // because setting VK_NC for others would mean setting their respective; // RefFlags correctly. We should do this in a separate patch.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MCInstLower.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MCInstLower.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp:1096,Availability,redundant,redundant,1096,"der the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass performs below peephole optimizations on MIR level.; //; // 1. MOVi32imm + ANDWrr ==> ANDWri + ANDWri; // MOVi64imm + ANDXrr ==> ANDXri + ANDXri; //; // 2. MOVi32imm + ADDWrr ==> ADDWRi + ADDWRi; // MOVi64imm + ADDXrr ==> ANDXri + ANDXri; //; // 3. MOVi32imm + SUBWrr ==> SUBWRi + SUBWRi; // MOVi64imm + SUBXrr ==> SUBXri + SUBXri; //; // The mov pseudo instruction could be expanded to multiple mov instructions; // later. In this case, we could try to split the constant operand of mov; // instruction into two immediates which can be directly encoded into; // *Wri/*Xri instructions. It makes two AND/ADD/SUB instructions instead of; // multiple `mov` + `and/add/sub` instructions.; //; // 4. Remove redundant ORRWrs which is generated by zero-extend.; //; // %3:gpr32 = ORRWrs $wzr, %2, 0; // %4:gpr64 = SUBREG_TO_REG 0, %3, %subreg.sub_32; //; // If AArch64's 32-bit form of instruction defines the source operand of; // ORRWrs, we can remove the ORRWrs because the upper 32 bits of the source; // operand are set to zero.; //; // 5. %reg = INSERT_SUBREG %reg(tied-def 0), %subreg, subidx; // ==> %reg:subidx = SUBREG_TO_REG 0, %subreg, subidx; //; // 6. %intermediate:gpr32 = COPY %src:fpr128; // %dst:fpr128 = INSvi32gpr %dst_vec:fpr128, dst_index, %intermediate:gpr32; // ==> %dst:fpr128 = INSvi32lane %dst_vec:fpr128, dst_index, %src:fpr128, 0; //; // In cases where a source FPR is copied to a GPR in order to be copied; // to a destination FPR, we can directly copy the values between the FPRs,; // eliminating the use of the Integer unit. When we match a pattern of; // INSvi[X]gpr that is preceded by a chain of COPY instructions from a FPR; // source, we use the INSvi[X]lane to replace the COPY & INSvi[X]gpr; ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp:1140,Modifiability,extend,extend,1140,"der the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass performs below peephole optimizations on MIR level.; //; // 1. MOVi32imm + ANDWrr ==> ANDWri + ANDWri; // MOVi64imm + ANDXrr ==> ANDXri + ANDXri; //; // 2. MOVi32imm + ADDWrr ==> ADDWRi + ADDWRi; // MOVi64imm + ADDXrr ==> ANDXri + ANDXri; //; // 3. MOVi32imm + SUBWrr ==> SUBWRi + SUBWRi; // MOVi64imm + SUBXrr ==> SUBXri + SUBXri; //; // The mov pseudo instruction could be expanded to multiple mov instructions; // later. In this case, we could try to split the constant operand of mov; // instruction into two immediates which can be directly encoded into; // *Wri/*Xri instructions. It makes two AND/ADD/SUB instructions instead of; // multiple `mov` + `and/add/sub` instructions.; //; // 4. Remove redundant ORRWrs which is generated by zero-extend.; //; // %3:gpr32 = ORRWrs $wzr, %2, 0; // %4:gpr64 = SUBREG_TO_REG 0, %3, %subreg.sub_32; //; // If AArch64's 32-bit form of instruction defines the source operand of; // ORRWrs, we can remove the ORRWrs because the upper 32 bits of the source; // operand are set to zero.; //; // 5. %reg = INSERT_SUBREG %reg(tied-def 0), %subreg, subidx; // ==> %reg:subidx = SUBREG_TO_REG 0, %subreg, subidx; //; // 6. %intermediate:gpr32 = COPY %src:fpr128; // %dst:fpr128 = INSvi32gpr %dst_vec:fpr128, dst_index, %intermediate:gpr32; // ==> %dst:fpr128 = INSvi32lane %dst_vec:fpr128, dst_index, %src:fpr128, 0; //; // In cases where a source FPR is copied to a GPR in order to be copied; // to a destination FPR, we can directly copy the values between the FPRs,; // eliminating the use of the Integer unit. When we match a pattern of; // INSvi[X]gpr that is preceded by a chain of COPY instructions from a FPR; // source, we use the INSvi[X]lane to replace the COPY & INSvi[X]gpr; ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp:54,Performance,optimiz,optimization,54,"//===- AArch64MIPeepholeOpt.cpp - AArch64 MI peephole optimization pass ---===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass performs below peephole optimizations on MIR level.; //; // 1. MOVi32imm + ANDWrr ==> ANDWri + ANDWri; // MOVi64imm + ANDXrr ==> ANDXri + ANDXri; //; // 2. MOVi32imm + ADDWrr ==> ADDWRi + ADDWRi; // MOVi64imm + ADDXrr ==> ANDXri + ANDXri; //; // 3. MOVi32imm + SUBWrr ==> SUBWRi + SUBWRi; // MOVi64imm + SUBXrr ==> SUBXri + SUBXri; //; // The mov pseudo instruction could be expanded to multiple mov instructions; // later. In this case, we could try to split the constant operand of mov; // instruction into two immediates which can be directly encoded into; // *Wri/*Xri instructions. It makes two AND/ADD/SUB instructions instead of; // multiple `mov` + `and/add/sub` instructions.; //; // 4. Remove redundant ORRWrs which is generated by zero-extend.; //; // %3:gpr32 = ORRWrs $wzr, %2, 0; // %4:gpr64 = SUBREG_TO_REG 0, %3, %subreg.sub_32; //; // If AArch64's 32-bit form of instruction defines the source operand of; // ORRWrs, we can remove the ORRWrs because the upper 32 bits of the source; // operand are set to zero.; //; // 5. %reg = INSERT_SUBREG %reg(tied-def 0), %subreg, subidx; // ==> %reg:subidx = SUBREG_TO_REG 0, %subreg, subidx; //; // 6. %intermediate:gpr32 = COPY %src:fpr128; // %dst:fpr128 = INSvi32gpr %dst_vec:fpr128, dst_index, %intermediate:gpr32; // ==> %dst:fpr128 = INSvi32lane %dst_vec:fpr128, dst_index, %src:fpr128, 0; //; // In cases where a source FPR is copied to a GPR in order to be copied; // to a destination FPR, we can directly copy the values between the FPRs,; // eliminating the use of the Integer unit. When we match a pattern of; // INSvi[X]gpr that is preced",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp:393,Performance,perform,performs,393,"//===- AArch64MIPeepholeOpt.cpp - AArch64 MI peephole optimization pass ---===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass performs below peephole optimizations on MIR level.; //; // 1. MOVi32imm + ANDWrr ==> ANDWri + ANDWri; // MOVi64imm + ANDXrr ==> ANDXri + ANDXri; //; // 2. MOVi32imm + ADDWrr ==> ADDWRi + ADDWRi; // MOVi64imm + ADDXrr ==> ANDXri + ANDXri; //; // 3. MOVi32imm + SUBWrr ==> SUBWRi + SUBWRi; // MOVi64imm + SUBXrr ==> SUBXri + SUBXri; //; // The mov pseudo instruction could be expanded to multiple mov instructions; // later. In this case, we could try to split the constant operand of mov; // instruction into two immediates which can be directly encoded into; // *Wri/*Xri instructions. It makes two AND/ADD/SUB instructions instead of; // multiple `mov` + `and/add/sub` instructions.; //; // 4. Remove redundant ORRWrs which is generated by zero-extend.; //; // %3:gpr32 = ORRWrs $wzr, %2, 0; // %4:gpr64 = SUBREG_TO_REG 0, %3, %subreg.sub_32; //; // If AArch64's 32-bit form of instruction defines the source operand of; // ORRWrs, we can remove the ORRWrs because the upper 32 bits of the source; // operand are set to zero.; //; // 5. %reg = INSERT_SUBREG %reg(tied-def 0), %subreg, subidx; // ==> %reg:subidx = SUBREG_TO_REG 0, %subreg, subidx; //; // 6. %intermediate:gpr32 = COPY %src:fpr128; // %dst:fpr128 = INSvi32gpr %dst_vec:fpr128, dst_index, %intermediate:gpr32; // ==> %dst:fpr128 = INSvi32lane %dst_vec:fpr128, dst_index, %src:fpr128, 0; //; // In cases where a source FPR is copied to a GPR in order to be copied; // to a destination FPR, we can directly copy the values between the FPRs,; // eliminating the use of the Integer unit. When we match a pattern of; // INSvi[X]gpr that is preced",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp:417,Performance,optimiz,optimizations,417,"//===- AArch64MIPeepholeOpt.cpp - AArch64 MI peephole optimization pass ---===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass performs below peephole optimizations on MIR level.; //; // 1. MOVi32imm + ANDWrr ==> ANDWri + ANDWri; // MOVi64imm + ANDXrr ==> ANDXri + ANDXri; //; // 2. MOVi32imm + ADDWrr ==> ADDWRi + ADDWRi; // MOVi64imm + ADDXrr ==> ANDXri + ANDXri; //; // 3. MOVi32imm + SUBWrr ==> SUBWRi + SUBWRi; // MOVi64imm + SUBXrr ==> SUBXri + SUBXri; //; // The mov pseudo instruction could be expanded to multiple mov instructions; // later. In this case, we could try to split the constant operand of mov; // instruction into two immediates which can be directly encoded into; // *Wri/*Xri instructions. It makes two AND/ADD/SUB instructions instead of; // multiple `mov` + `and/add/sub` instructions.; //; // 4. Remove redundant ORRWrs which is generated by zero-extend.; //; // %3:gpr32 = ORRWrs $wzr, %2, 0; // %4:gpr64 = SUBREG_TO_REG 0, %3, %subreg.sub_32; //; // If AArch64's 32-bit form of instruction defines the source operand of; // ORRWrs, we can remove the ORRWrs because the upper 32 bits of the source; // operand are set to zero.; //; // 5. %reg = INSERT_SUBREG %reg(tied-def 0), %subreg, subidx; // ==> %reg:subidx = SUBREG_TO_REG 0, %subreg, subidx; //; // 6. %intermediate:gpr32 = COPY %src:fpr128; // %dst:fpr128 = INSvi32gpr %dst_vec:fpr128, dst_index, %intermediate:gpr32; // ==> %dst:fpr128 = INSvi32lane %dst_vec:fpr128, dst_index, %src:fpr128, 0; //; // In cases where a source FPR is copied to a GPR in order to be copied; // to a destination FPR, we can directly copy the values between the FPRs,; // eliminating the use of the Integer unit. When we match a pattern of; // INSvi[X]gpr that is preced",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp:1096,Safety,redund,redundant,1096,"der the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass performs below peephole optimizations on MIR level.; //; // 1. MOVi32imm + ANDWrr ==> ANDWri + ANDWri; // MOVi64imm + ANDXrr ==> ANDXri + ANDXri; //; // 2. MOVi32imm + ADDWrr ==> ADDWRi + ADDWRi; // MOVi64imm + ADDXrr ==> ANDXri + ANDXri; //; // 3. MOVi32imm + SUBWrr ==> SUBWRi + SUBWRi; // MOVi64imm + SUBXrr ==> SUBXri + SUBXri; //; // The mov pseudo instruction could be expanded to multiple mov instructions; // later. In this case, we could try to split the constant operand of mov; // instruction into two immediates which can be directly encoded into; // *Wri/*Xri instructions. It makes two AND/ADD/SUB instructions instead of; // multiple `mov` + `and/add/sub` instructions.; //; // 4. Remove redundant ORRWrs which is generated by zero-extend.; //; // %3:gpr32 = ORRWrs $wzr, %2, 0; // %4:gpr64 = SUBREG_TO_REG 0, %3, %subreg.sub_32; //; // If AArch64's 32-bit form of instruction defines the source operand of; // ORRWrs, we can remove the ORRWrs because the upper 32 bits of the source; // operand are set to zero.; //; // 5. %reg = INSERT_SUBREG %reg(tied-def 0), %subreg, subidx; // ==> %reg:subidx = SUBREG_TO_REG 0, %subreg, subidx; //; // 6. %intermediate:gpr32 = COPY %src:fpr128; // %dst:fpr128 = INSvi32gpr %dst_vec:fpr128, dst_index, %intermediate:gpr32; // ==> %dst:fpr128 = INSvi32lane %dst_vec:fpr128, dst_index, %src:fpr128, 0; //; // In cases where a source FPR is copied to a GPR in order to be copied; // to a destination FPR, we can directly copy the values between the FPRs,; // eliminating the use of the Integer unit. When we match a pattern of; // INSvi[X]gpr that is preceded by a chain of COPY instructions from a FPR; // source, we use the INSvi[X]lane to replace the COPY & INSvi[X]gpr; ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp:154,Performance,optimiz,optimization,154,"/// For instructions where an immediate operand could be split into two; /// separate immediate instructions, use the splitTwoPartImm two handle the; /// optimization.; ///; /// To implement, the following function types must be passed to; /// splitTwoPartImm. A SplitAndOpcFunc must be implemented that determines if; /// splitting the immediate is valid and returns the associated new opcode. A; /// BuildMIFunc must be implemented to build the two immediate instructions.; ///; /// Example Pattern (where IMM would require 2+ MOV instructions):; /// %dst = <Instr>rr %src IMM [...]; /// becomes:; /// %tmp = <Instr>ri %src (encode half IMM) [...]; /// %dst = <Instr>ri %tmp (encode half IMM) [...]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp:12,Availability,mask,mask,12,// Create a mask which is filled with one from the position of lowest bit set; // to the position of highest bit set.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp:12,Availability,mask,mask,12,// Create a mask which is filled with one outside the position of lowest bit; // set and the position of highest bit set.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp:40,Modifiability,extend,extend,40,"// Check this ORR comes from below zero-extend pattern.; //; // def : Pat<(i64 (zext GPR32:$src)),; // (SUBREG_TO_REG (i32 0), (ORRWrs WZR, GPR32:$src, 0), sub_32)>;",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp:339,Modifiability,extend,extend,339,"// From https://developer.arm.com/documentation/dui0801/b/BABBGCAC; //; // When you use the 32-bit form of an instruction, the upper 32 bits of the; // source registers are ignored and the upper 32 bits of the destination; // register are set to zero.; //; // If AArch64's 32-bit form of instruction defines the source operand of; // zero-extend, we do not need the zero-extend. Let's check the MI's opcode is; // real AArch64 instruction and if it is not, do not process the opcode; // conservatively.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp:371,Modifiability,extend,extend,371,"// From https://developer.arm.com/documentation/dui0801/b/BABBGCAC; //; // When you use the 32-bit form of an instruction, the upper 32 bits of the; // source registers are ignored and the upper 32 bits of the destination; // register are set to zero.; //; // If AArch64's 32-bit form of instruction defines the source operand of; // zero-extend, we do not need the zero-extend. Let's check the MI's opcode is; // real AArch64 instruction and if it is not, do not process the opcode; // conservatively.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp:50,Modifiability,extend,extend,50,"// Check this INSERT_SUBREG comes from below zero-extend pattern.; //; // From %reg = INSERT_SUBREG %reg(tied-def 0), %subreg, subidx; // To %reg:subidx = SUBREG_TO_REG 0, %subreg, subidx; //; // We're assuming the first operand to INSERT_SUBREG is irrelevant because a; // COPY would destroy the upper part of the register anyway",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp:339,Modifiability,extend,extend,339,"// From https://developer.arm.com/documentation/dui0801/b/BABBGCAC; //; // When you use the 32-bit form of an instruction, the upper 32 bits of the; // source registers are ignored and the upper 32 bits of the destination; // register are set to zero.; //; // If AArch64's 32-bit form of instruction defines the source operand of; // zero-extend, we do not need the zero-extend. Let's check the MI's opcode is; // real AArch64 instruction and if it is not, do not process the opcode; // conservatively.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp:371,Modifiability,extend,extend,371,"// From https://developer.arm.com/documentation/dui0801/b/BABBGCAC; //; // When you use the 32-bit form of an instruction, the upper 32 bits of the; // source registers are ignored and the upper 32 bits of the destination; // register are set to zero.; //; // If AArch64's 32-bit form of instruction defines the source operand of; // zero-extend, we do not need the zero-extend. Let's check the MI's opcode is; // real AArch64 instruction and if it is not, do not process the opcode; // conservatively.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp:93,Performance,optimiz,optimization,93,// Checks if the corresponding MOV immediate instruction is applicable for; // this peephole optimization.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp:15,Performance,perform,perform,15,// It is OK to perform this peephole optimization.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp:37,Performance,optimiz,optimization,37,// It is OK to perform this peephole optimization.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp:3,Performance,Perform,Perform,3,// Perform several essential checks against current MI.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp:260,Modifiability,extend,extended,260,"// For the 32 bit form of instruction, the upper 32 bits of the destination; // register are set to zero. If there is SUBREG_TO_REG, set the upper 32 bits; // of Imm to zero. This is essential if the Immediate value was a negative; // number since it was sign extended when we assign to the 64-bit Imm.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64MIPeepholeOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PBQPRegAlloc.cpp:676,Performance,perform,performing,676,"//===-- AArch64PBQPRegAlloc.cpp - AArch64 specific PBQP constraints -------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // This file contains the AArch64 / Cortex-A57 specific register allocation; // constraints for use by the PBQP register allocator.; //; // It is essentially a transcription of what is contained in; // AArch64A57FPLoadBalancing, which tries to use a balanced; // mix of odd and even D-registers when performing a critical sequence of; // independent, non-quadword FP/ASIMD floating-point multiply-accumulates.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PBQPRegAlloc.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PBQPRegAlloc.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PerfectShuffle.h:16,Availability,mask,mask,16,// Get the four mask elementd from the 2 inputs. Perfect shuffles encode undef; // elements with value 8.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PerfectShuffle.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PerfectShuffle.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp:11,Security,authenticat,authentication,11,// PAuthLR authentication instructions need to know the value of PC at the; // point of signing (PACI*).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp:100,Safety,safe,safely,100,"// The AUTIASP instruction assembles to a hint instruction before v8.3a so; // this instruction can safely used for any v8a architecture.; // From v8.3a onwards there are optimised authenticate LR and return; // instructions, namely RETA{A,B}, that can be used instead. In this case the; // DW_CFA_AARCH64_negate_ra_state can't be emitted.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp:181,Security,authenticat,authenticate,181,"// The AUTIASP instruction assembles to a hint instruction before v8.3a so; // this instruction can safely used for any v8a architecture.; // From v8.3a onwards there are optimised authenticate LR and return; // instructions, namely RETA{A,B}, that can be used instead. In this case the; // DW_CFA_AARCH64_negate_ra_state can't be emitted.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp:66,Availability,failure,failure,66,// The block that explicitly generates a break-point exception on failure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp:156,Safety,avoid,avoid,156,"// The following code may create a signing oracle:; //; // <authenticate LR>; // TCRETURN ; the callee may sign and spill the LR in its prologue; //; // To avoid generating a signing oracle, check the authenticated value; // before possibly re-signing it in the callee, as follows:; //; // <authenticate LR>; // <check if LR contains a valid address>; // b.<cond> break_block; // ret_block:; // TCRETURN; // break_block:; // brk <BrkOperand>; //; // or just; //; // <authenticate LR>; // ldr tmp, [lr]; // TCRETURN; // TmpReg is chosen assuming X16 and X17 are dead after TI.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp:60,Security,authenticat,authenticate,60,"// The following code may create a signing oracle:; //; // <authenticate LR>; // TCRETURN ; the callee may sign and spill the LR in its prologue; //; // To avoid generating a signing oracle, check the authenticated value; // before possibly re-signing it in the callee, as follows:; //; // <authenticate LR>; // <check if LR contains a valid address>; // b.<cond> break_block; // ret_block:; // TCRETURN; // break_block:; // brk <BrkOperand>; //; // or just; //; // <authenticate LR>; // ldr tmp, [lr]; // TCRETURN; // TmpReg is chosen assuming X16 and X17 are dead after TI.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp:201,Security,authenticat,authenticated,201,"// The following code may create a signing oracle:; //; // <authenticate LR>; // TCRETURN ; the callee may sign and spill the LR in its prologue; //; // To avoid generating a signing oracle, check the authenticated value; // before possibly re-signing it in the callee, as follows:; //; // <authenticate LR>; // <check if LR contains a valid address>; // b.<cond> break_block; // ret_block:; // TCRETURN; // break_block:; // brk <BrkOperand>; //; // or just; //; // <authenticate LR>; // ldr tmp, [lr]; // TCRETURN; // TmpReg is chosen assuming X16 and X17 are dead after TI.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp:291,Security,authenticat,authenticate,291,"// The following code may create a signing oracle:; //; // <authenticate LR>; // TCRETURN ; the callee may sign and spill the LR in its prologue; //; // To avoid generating a signing oracle, check the authenticated value; // before possibly re-signing it in the callee, as follows:; //; // <authenticate LR>; // <check if LR contains a valid address>; // b.<cond> break_block; // ret_block:; // TCRETURN; // break_block:; // brk <BrkOperand>; //; // or just; //; // <authenticate LR>; // ldr tmp, [lr]; // TCRETURN; // TmpReg is chosen assuming X16 and X17 are dead after TI.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp:467,Security,authenticat,authenticate,467,"// The following code may create a signing oracle:; //; // <authenticate LR>; // TCRETURN ; the callee may sign and spill the LR in its prologue; //; // To avoid generating a signing oracle, check the authenticated value; // before possibly re-signing it in the callee, as follows:; //; // <authenticate LR>; // <check if LR contains a valid address>; // b.<cond> break_block; // ret_block:; // TCRETURN; // break_block:; // brk <BrkOperand>; //; // or just; //; // <authenticate LR>; // ldr tmp, [lr]; // TCRETURN; // TmpReg is chosen assuming X16 and X17 are dead after TI.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h:22,Performance,perform,performed,22,"/// Variants of check performed on an authenticated pointer.; ///; /// In cases such as authenticating the LR value when performing a tail call; /// or when re-signing a signed pointer with a different signing schema,; /// a failed authentication may not generate an exception on its own and may; /// create an authentication or signing oracle if not checked explicitly.; ///; /// A number of check methods modify control flow in a similar way by; /// rewriting the code; ///; /// ```; /// <authenticate LR>; /// <more instructions>; /// ```; ///; /// as follows:; ///; /// ```; /// <authenticate LR>; /// <method-specific checker>; /// ret_block:; /// <more instructions>; /// ...; ///; /// break_block:; /// brk <code>; /// ```",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h:121,Performance,perform,performing,121,"/// Variants of check performed on an authenticated pointer.; ///; /// In cases such as authenticating the LR value when performing a tail call; /// or when re-signing a signed pointer with a different signing schema,; /// a failed authentication may not generate an exception on its own and may; /// create an authentication or signing oracle if not checked explicitly.; ///; /// A number of check methods modify control flow in a similar way by; /// rewriting the code; ///; /// ```; /// <authenticate LR>; /// <more instructions>; /// ```; ///; /// as follows:; ///; /// ```; /// <authenticate LR>; /// <method-specific checker>; /// ret_block:; /// <more instructions>; /// ...; ///; /// break_block:; /// brk <code>; /// ```",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h:38,Security,authenticat,authenticated,38,"/// Variants of check performed on an authenticated pointer.; ///; /// In cases such as authenticating the LR value when performing a tail call; /// or when re-signing a signed pointer with a different signing schema,; /// a failed authentication may not generate an exception on its own and may; /// create an authentication or signing oracle if not checked explicitly.; ///; /// A number of check methods modify control flow in a similar way by; /// rewriting the code; ///; /// ```; /// <authenticate LR>; /// <more instructions>; /// ```; ///; /// as follows:; ///; /// ```; /// <authenticate LR>; /// <method-specific checker>; /// ret_block:; /// <more instructions>; /// ...; ///; /// break_block:; /// brk <code>; /// ```",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h:88,Security,authenticat,authenticating,88,"/// Variants of check performed on an authenticated pointer.; ///; /// In cases such as authenticating the LR value when performing a tail call; /// or when re-signing a signed pointer with a different signing schema,; /// a failed authentication may not generate an exception on its own and may; /// create an authentication or signing oracle if not checked explicitly.; ///; /// A number of check methods modify control flow in a similar way by; /// rewriting the code; ///; /// ```; /// <authenticate LR>; /// <more instructions>; /// ```; ///; /// as follows:; ///; /// ```; /// <authenticate LR>; /// <method-specific checker>; /// ret_block:; /// <more instructions>; /// ...; ///; /// break_block:; /// brk <code>; /// ```",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h:232,Security,authenticat,authentication,232,"/// Variants of check performed on an authenticated pointer.; ///; /// In cases such as authenticating the LR value when performing a tail call; /// or when re-signing a signed pointer with a different signing schema,; /// a failed authentication may not generate an exception on its own and may; /// create an authentication or signing oracle if not checked explicitly.; ///; /// A number of check methods modify control flow in a similar way by; /// rewriting the code; ///; /// ```; /// <authenticate LR>; /// <more instructions>; /// ```; ///; /// as follows:; ///; /// ```; /// <authenticate LR>; /// <method-specific checker>; /// ret_block:; /// <more instructions>; /// ...; ///; /// break_block:; /// brk <code>; /// ```",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h:311,Security,authenticat,authentication,311,"/// Variants of check performed on an authenticated pointer.; ///; /// In cases such as authenticating the LR value when performing a tail call; /// or when re-signing a signed pointer with a different signing schema,; /// a failed authentication may not generate an exception on its own and may; /// create an authentication or signing oracle if not checked explicitly.; ///; /// A number of check methods modify control flow in a similar way by; /// rewriting the code; ///; /// ```; /// <authenticate LR>; /// <more instructions>; /// ```; ///; /// as follows:; ///; /// ```; /// <authenticate LR>; /// <method-specific checker>; /// ret_block:; /// <more instructions>; /// ...; ///; /// break_block:; /// brk <code>; /// ```",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h:491,Security,authenticat,authenticate,491,"/// Variants of check performed on an authenticated pointer.; ///; /// In cases such as authenticating the LR value when performing a tail call; /// or when re-signing a signed pointer with a different signing schema,; /// a failed authentication may not generate an exception on its own and may; /// create an authentication or signing oracle if not checked explicitly.; ///; /// A number of check methods modify control flow in a similar way by; /// rewriting the code; ///; /// ```; /// <authenticate LR>; /// <more instructions>; /// ```; ///; /// as follows:; ///; /// ```; /// <authenticate LR>; /// <method-specific checker>; /// ret_block:; /// <more instructions>; /// ...; ///; /// break_block:; /// brk <code>; /// ```",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h:584,Security,authenticat,authenticate,584,"/// Variants of check performed on an authenticated pointer.; ///; /// In cases such as authenticating the LR value when performing a tail call; /// or when re-signing a signed pointer with a different signing schema,; /// a failed authentication may not generate an exception on its own and may; /// create an authentication or signing oracle if not checked explicitly.; ///; /// A number of check methods modify control flow in a similar way by; /// rewriting the code; ///; /// ```; /// <authenticate LR>; /// <more instructions>; /// ```; ///; /// as follows:; ///; /// ```; /// <authenticate LR>; /// <method-specific checker>; /// ret_block:; /// <more instructions>; /// ...; ///; /// break_block:; /// brk <code>; /// ```",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h:4,Performance,Perform,Perform,4,/// Perform a load to a temporary register,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h:14,Performance,load,load,14,/// Perform a load to a temporary register,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h:45,Security,authenticat,authenticated,45,"/// Check by comparing bits 62 and 61 of the authenticated address.; ///; /// This method modifies control flow and inserts the following checker:; ///; /// ```; /// eor Xtmp, Xn, Xn, lsl #1; /// tbnz Xtmp, #62, break_block; /// ```",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h:27,Security,authenticat,authenticated,27,"/// Check by comparing the authenticated value with an XPAC-ed one without; /// using PAuth instructions not encoded as HINT. Can only be applied to LR.; ///; /// This method modifies control flow and inserts the following checker:; ///; /// ```; /// mov Xtmp, LR; /// xpaclri ; encoded as ""hint #7""; /// ; Note: at this point, the LR register contains the address as if; /// ; the authentication succeeded and the temporary register contains the; /// ; *real* result of authentication.; /// cmp Xtmp, LR; /// b.ne break_block; /// ```",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h:382,Security,authenticat,authentication,382,"/// Check by comparing the authenticated value with an XPAC-ed one without; /// using PAuth instructions not encoded as HINT. Can only be applied to LR.; ///; /// This method modifies control flow and inserts the following checker:; ///; /// ```; /// mov Xtmp, LR; /// xpaclri ; encoded as ""hint #7""; /// ; Note: at this point, the LR register contains the address as if; /// ; the authentication succeeded and the temporary register contains the; /// ; *real* result of authentication.; /// cmp Xtmp, LR; /// b.ne break_block; /// ```",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h:471,Security,authenticat,authentication,471,"/// Check by comparing the authenticated value with an XPAC-ed one without; /// using PAuth instructions not encoded as HINT. Can only be applied to LR.; ///; /// This method modifies control flow and inserts the following checker:; ///; /// ```; /// mov Xtmp, LR; /// xpaclri ; encoded as ""hint #7""; /// ; Note: at this point, the LR register contains the address as if; /// ; the authentication succeeded and the temporary register contains the; /// ; *real* result of authentication.; /// cmp Xtmp, LR; /// b.ne break_block; /// ```",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h:35,Security,authenticat,authentication,35,"/// Explicitly checks that pointer authentication succeeded.; ///; /// Assuming AuthenticatedReg contains a value returned by one of the AUT*; /// instructions, check the value using Method just before the instruction; /// pointed to by MBBI. If the check succeeds, execution proceeds to the; /// instruction pointed to by MBBI, otherwise a CPU exception is generated.; ///; /// Some of the methods may need to know if the pointer was authenticated; /// using an I-key or D-key and which register can be used as temporary.; /// If an explicit BRK instruction is used to generate an exception, BrkImm; /// specifies its immediate operand.; ///; /// \returns The machine basic block containing the code that is executed; /// after the check succeeds.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h:80,Security,Authenticat,AuthenticatedReg,80,"/// Explicitly checks that pointer authentication succeeded.; ///; /// Assuming AuthenticatedReg contains a value returned by one of the AUT*; /// instructions, check the value using Method just before the instruction; /// pointed to by MBBI. If the check succeeds, execution proceeds to the; /// instruction pointed to by MBBI, otherwise a CPU exception is generated.; ///; /// Some of the methods may need to know if the pointer was authenticated; /// using an I-key or D-key and which register can be used as temporary.; /// If an explicit BRK instruction is used to generate an exception, BrkImm; /// specifies its immediate operand.; ///; /// \returns The machine basic block containing the code that is executed; /// after the check succeeds.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h:435,Security,authenticat,authenticated,435,"/// Explicitly checks that pointer authentication succeeded.; ///; /// Assuming AuthenticatedReg contains a value returned by one of the AUT*; /// instructions, check the value using Method just before the instruction; /// pointed to by MBBI. If the check succeeds, execution proceeds to the; /// instruction pointed to by MBBI, otherwise a CPU exception is generated.; ///; /// Some of the methods may need to know if the pointer was authenticated; /// using an I-key or D-key and which register can be used as temporary.; /// If an explicit BRK instruction is used to generate an exception, BrkImm; /// specifies its immediate operand.; ///; /// \returns The machine basic block containing the code that is executed; /// after the check succeeds.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PointerAuth.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:516,Energy Efficiency,efficient,efficient,516,"//==- AArch64PromoteConstant.cpp - Promote constant to global for AArch64 --==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file implements the AArch64PromoteConstant pass which promotes constants; // to global variables when this is likely to be more efficient. Currently only; // types related to constant vector (i.e., constant vector, array of constant; // vectors, constant structure with a constant vector field, etc.) are promoted; // to global variables. Constant vectors are likely to be lowered in target; // constant pool during instruction selection already; therefore, the access; // will remain the same (memory load), but the structure types are not split; // into different constant pool accesses for each field. A bonus side effect is; // that created globals may be merged by the global merge pass.; //; // FIXME: This pass may be useful for other targets too.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:475,Modifiability,variab,variables,475,"//==- AArch64PromoteConstant.cpp - Promote constant to global for AArch64 --==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file implements the AArch64PromoteConstant pass which promotes constants; // to global variables when this is likely to be more efficient. Currently only; // types related to constant vector (i.e., constant vector, array of constant; // vectors, constant structure with a constant vector field, etc.) are promoted; // to global variables. Constant vectors are likely to be lowered in target; // constant pool during instruction selection already; therefore, the access; // will remain the same (memory load), but the structure types are not split; // into different constant pool accesses for each field. A bonus side effect is; // that created globals may be merged by the global merge pass.; //; // FIXME: This pass may be useful for other targets too.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:716,Modifiability,variab,variables,716,"//==- AArch64PromoteConstant.cpp - Promote constant to global for AArch64 --==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file implements the AArch64PromoteConstant pass which promotes constants; // to global variables when this is likely to be more efficient. Currently only; // types related to constant vector (i.e., constant vector, array of constant; // vectors, constant structure with a constant vector field, etc.) are promoted; // to global variables. Constant vectors are likely to be lowered in target; // constant pool during instruction selection already; therefore, the access; // will remain the same (memory load), but the structure types are not split; // into different constant pool accesses for each field. A bonus side effect is; // that created globals may be merged by the global merge pass.; //; // FIXME: This pass may be useful for other targets too.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:890,Performance,load,load,890,"//==- AArch64PromoteConstant.cpp - Promote constant to global for AArch64 --==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file implements the AArch64PromoteConstant pass which promotes constants; // to global variables when this is likely to be more efficient. Currently only; // types related to constant vector (i.e., constant vector, array of constant; // vectors, constant structure with a constant vector field, etc.) are promoted; // to global variables. Constant vectors are likely to be lowered in target; // constant pool during instruction selection already; therefore, the access; // will remain the same (memory load), but the structure types are not split; // into different constant pool accesses for each field. A bonus side effect is; // that created globals may be merged by the global merge pass.; //; // FIXME: This pass may be useful for other targets too.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:850,Security,access,access,850,"//==- AArch64PromoteConstant.cpp - Promote constant to global for AArch64 --==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file implements the AArch64PromoteConstant pass which promotes constants; // to global variables when this is likely to be more efficient. Currently only; // types related to constant vector (i.e., constant vector, array of constant; // vectors, constant structure with a constant vector field, etc.) are promoted; // to global variables. Constant vectors are likely to be lowered in target; // constant pool during instruction selection already; therefore, the access; // will remain the same (memory load), but the structure types are not split; // into different constant pool accesses for each field. A bonus side effect is; // that created globals may be merged by the global merge pass.; //; // FIXME: This pass may be useful for other targets too.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:968,Security,access,accesses,968,"//==- AArch64PromoteConstant.cpp - Promote constant to global for AArch64 --==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file implements the AArch64PromoteConstant pass which promotes constants; // to global variables when this is likely to be more efficient. Currently only; // types related to constant vector (i.e., constant vector, array of constant; // vectors, constant structure with a constant vector field, etc.) are promoted; // to global variables. Constant vectors are likely to be lowered in target; // constant pool during instruction selection already; therefore, the access; // will remain the same (memory load), but the structure types are not split; // into different constant pool accesses for each field. A bonus side effect is; // that created globals may be merged by the global merge pass.; //; // FIXME: This pass may be useful for other targets too.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:10,Testability,test,testing,10,// Stress testing mode - disable heuristics.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:46,Modifiability,variab,variables,46,"/// Promotes interesting constant into global variables.; /// The motivating example is:; /// static const uint16_t TableA[32] = {; /// 41944, 40330, 38837, 37450, 36158, 34953, 33826, 32768,; /// 31776, 30841, 29960, 29128, 28340, 27595, 26887, 26215,; /// 25576, 24967, 24386, 23832, 23302, 22796, 22311, 21846,; /// 21400, 20972, 20561, 20165, 19785, 19419, 19066, 18725,; /// };; ///; /// uint8x16x4_t LoadStatic(void) {; /// uint8x16x4_t ret;; /// ret.val[0] = vld1q_u16(TableA + 0);; /// ret.val[1] = vld1q_u16(TableA + 8);; /// ret.val[2] = vld1q_u16(TableA + 16);; /// ret.val[3] = vld1q_u16(TableA + 24);; /// return ret;; /// }; ///; /// The constants in this example are folded into the uses. Thus, 4 different; /// constants are created.; ///; /// As their type is vector the cheapest way to create them is to load them; /// for the memory.; ///; /// Therefore the final assembly final has 4 different loads. With this pass; /// enabled, only one load is issued for the constants.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:406,Performance,Load,LoadStatic,406,"/// Promotes interesting constant into global variables.; /// The motivating example is:; /// static const uint16_t TableA[32] = {; /// 41944, 40330, 38837, 37450, 36158, 34953, 33826, 32768,; /// 31776, 30841, 29960, 29128, 28340, 27595, 26887, 26215,; /// 25576, 24967, 24386, 23832, 23302, 22796, 22311, 21846,; /// 21400, 20972, 20561, 20165, 19785, 19419, 19066, 18725,; /// };; ///; /// uint8x16x4_t LoadStatic(void) {; /// uint8x16x4_t ret;; /// ret.val[0] = vld1q_u16(TableA + 0);; /// ret.val[1] = vld1q_u16(TableA + 8);; /// ret.val[2] = vld1q_u16(TableA + 16);; /// ret.val[3] = vld1q_u16(TableA + 24);; /// return ret;; /// }; ///; /// The constants in this example are folded into the uses. Thus, 4 different; /// constants are created.; ///; /// As their type is vector the cheapest way to create them is to load them; /// for the memory.; ///; /// Therefore the final assembly final has 4 different loads. With this pass; /// enabled, only one load is issued for the constants.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:822,Performance,load,load,822,"/// Promotes interesting constant into global variables.; /// The motivating example is:; /// static const uint16_t TableA[32] = {; /// 41944, 40330, 38837, 37450, 36158, 34953, 33826, 32768,; /// 31776, 30841, 29960, 29128, 28340, 27595, 26887, 26215,; /// 25576, 24967, 24386, 23832, 23302, 22796, 22311, 21846,; /// 21400, 20972, 20561, 20165, 19785, 19419, 19066, 18725,; /// };; ///; /// uint8x16x4_t LoadStatic(void) {; /// uint8x16x4_t ret;; /// ret.val[0] = vld1q_u16(TableA + 0);; /// ret.val[1] = vld1q_u16(TableA + 8);; /// ret.val[2] = vld1q_u16(TableA + 16);; /// ret.val[3] = vld1q_u16(TableA + 24);; /// return ret;; /// }; ///; /// The constants in this example are folded into the uses. Thus, 4 different; /// constants are created.; ///; /// As their type is vector the cheapest way to create them is to load them; /// for the memory.; ///; /// Therefore the final assembly final has 4 different loads. With this pass; /// enabled, only one load is issued for the constants.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:914,Performance,load,loads,914,"/// Promotes interesting constant into global variables.; /// The motivating example is:; /// static const uint16_t TableA[32] = {; /// 41944, 40330, 38837, 37450, 36158, 34953, 33826, 32768,; /// 31776, 30841, 29960, 29128, 28340, 27595, 26887, 26215,; /// 25576, 24967, 24386, 23832, 23302, 22796, 22311, 21846,; /// 21400, 20972, 20561, 20165, 19785, 19419, 19066, 18725,; /// };; ///; /// uint8x16x4_t LoadStatic(void) {; /// uint8x16x4_t ret;; /// ret.val[0] = vld1q_u16(TableA + 0);; /// ret.val[1] = vld1q_u16(TableA + 8);; /// ret.val[2] = vld1q_u16(TableA + 16);; /// ret.val[3] = vld1q_u16(TableA + 24);; /// return ret;; /// }; ///; /// The constants in this example are folded into the uses. Thus, 4 different; /// constants are created.; ///; /// As their type is vector the cheapest way to create them is to load them; /// for the memory.; ///; /// Therefore the final assembly final has 4 different loads. With this pass; /// enabled, only one load is issued for the constants.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:959,Performance,load,load,959,"/// Promotes interesting constant into global variables.; /// The motivating example is:; /// static const uint16_t TableA[32] = {; /// 41944, 40330, 38837, 37450, 36158, 34953, 33826, 32768,; /// 31776, 30841, 29960, 29128, 28340, 27595, 26887, 26215,; /// 25576, 24967, 24386, 23832, 23302, 22796, 22311, 21846,; /// 21400, 20972, 20561, 20165, 19785, 19419, 19066, 18725,; /// };; ///; /// uint8x16x4_t LoadStatic(void) {; /// uint8x16x4_t ret;; /// ret.val[0] = vld1q_u16(TableA + 0);; /// ret.val[1] = vld1q_u16(TableA + 8);; /// ret.val[2] = vld1q_u16(TableA + 16);; /// ret.val[3] = vld1q_u16(TableA + 24);; /// return ret;; /// }; ///; /// The constants in this example are folded into the uses. Thus, 4 different; /// constants are created.; ///; /// As their type is vector the cheapest way to create them is to load them; /// for the memory.; ///; /// Therefore the final assembly final has 4 different loads. With this pass; /// enabled, only one load is issued for the constants.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:86,Modifiability,variab,variables,86,/// Iterate over the functions and promote the interesting constants into; /// global variables with module scope.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:97,Modifiability,variab,variables,97,"/// Look for interesting constants used within the given function.; /// Promote them into global variables, load these global variables within; /// the related function, so that the number of inserted load is minimal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:126,Modifiability,variab,variables,126,"/// Look for interesting constants used within the given function.; /// Promote them into global variables, load these global variables within; /// the related function, so that the number of inserted load is minimal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:108,Performance,load,load,108,"/// Look for interesting constants used within the given function.; /// Promote them into global variables, load these global variables within; /// the related function, so that the number of inserted load is minimal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:201,Performance,load,load,201,"/// Look for interesting constants used within the given function.; /// Promote them into global variables, load these global variables within; /// the related function, so that the number of inserted load is minimal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:99,Deployability,update,update,99,/// Insert a definition of a new global variable at each point contained in; /// InsPtsPerFunc and update the related uses (also contained in; /// InsPtsPerFunc).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:40,Modifiability,variab,variable,40,/// Insert a definition of a new global variable at each point contained in; /// InsPtsPerFunc and update the related uses (also contained in; /// InsPtsPerFunc).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:47,Deployability,Update,Updates,47,"/// Do the constant promotion indicated by the Updates records, keeping track; /// of globals in PromotionCache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:239,Availability,mask,mask,239,"/// Check if the given use (Instruction + OpIdx) of Cst should be converted into; /// a load of a global variable initialized with Cst.; /// A use should be converted if it is legal to do so.; /// For instance, it is not legal to turn the mask operand of a shuffle vector; /// into a load of a global variable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:105,Modifiability,variab,variable,105,"/// Check if the given use (Instruction + OpIdx) of Cst should be converted into; /// a load of a global variable initialized with Cst.; /// A use should be converted if it is legal to do so.; /// For instance, it is not legal to turn the mask operand of a shuffle vector; /// into a load of a global variable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:301,Modifiability,variab,variable,301,"/// Check if the given use (Instruction + OpIdx) of Cst should be converted into; /// a load of a global variable initialized with Cst.; /// A use should be converted if it is legal to do so.; /// For instance, it is not legal to turn the mask operand of a shuffle vector; /// into a load of a global variable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:88,Performance,load,load,88,"/// Check if the given use (Instruction + OpIdx) of Cst should be converted into; /// a load of a global variable initialized with Cst.; /// A use should be converted if it is legal to do so.; /// For instance, it is not legal to turn the mask operand of a shuffle vector; /// into a load of a global variable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:284,Performance,load,load,284,"/// Check if the given use (Instruction + OpIdx) of Cst should be converted into; /// a load of a global variable initialized with Cst.; /// A use should be converted if it is legal to do so.; /// For instance, it is not legal to turn the mask operand of a shuffle vector; /// into a load of a global variable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:53,Availability,mask,mask,53,"// shufflevector instruction expects a const for the mask argument, i.e., the; // third argument. Do not promote this use in that case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:76,Modifiability,variab,variable,76,"/// Check if the given Cst should be converted into; /// a load of a global variable initialized with Cst.; /// A constant should be converted if it is likely that the materialization of; /// the constant will be tricky. Thus, we give up on zero or undef values.; ///; /// \todo Currently, accept only vector related types.; /// Also we give up on all simple vector type to keep the existing; /// behavior. Otherwise, we should push here all the check of the lowering of; /// BUILD_VECTOR. By giving up, we lose the potential benefit of merging; /// constant via global merge and the fact that the same constant is stored; /// only once with this method (versus, as many function that uses the constant; /// for the regular approach, even for float).; /// Again, the simplest solution would be to promote every; /// constant and rematerialize them when they are actually cheap to create.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:59,Performance,load,load,59,"/// Check if the given Cst should be converted into; /// a load of a global variable initialized with Cst.; /// A constant should be converted if it is likely that the materialization of; /// the constant will be tricky. Thus, we give up on zero or undef values.; ///; /// \todo Currently, accept only vector related types.; /// Also we give up on all simple vector type to keep the existing; /// behavior. Otherwise, we should push here all the check of the lowering of; /// BUILD_VECTOR. By giving up, we lose the potential benefit of merging; /// constant via global merge and the fact that the same constant is stored; /// only once with this method (versus, as many function that uses the constant; /// for the regular approach, even for float).; /// Again, the simplest solution would be to promote every; /// constant and rematerialize them when they are actually cheap to create.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:352,Usability,simpl,simple,352,"/// Check if the given Cst should be converted into; /// a load of a global variable initialized with Cst.; /// A constant should be converted if it is likely that the materialization of; /// the constant will be tricky. Thus, we give up on zero or undef values.; ///; /// \todo Currently, accept only vector related types.; /// Also we give up on all simple vector type to keep the existing; /// behavior. Otherwise, we should push here all the check of the lowering of; /// BUILD_VECTOR. By giving up, we lose the potential benefit of merging; /// constant via global merge and the fact that the same constant is stored; /// only once with this method (versus, as many function that uses the constant; /// for the regular approach, even for float).; /// Again, the simplest solution would be to promote every; /// constant and rematerialize them when they are actually cheap to create.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:767,Usability,simpl,simplest,767,"/// Check if the given Cst should be converted into; /// a load of a global variable initialized with Cst.; /// A constant should be converted if it is likely that the materialization of; /// the constant will be tricky. Thus, we give up on zero or undef values.; ///; /// \todo Currently, accept only vector related types.; /// Also we give up on all simple vector type to keep the existing; /// behavior. Otherwise, we should push here all the check of the lowering of; /// BUILD_VECTOR. By giving up, we lose the potential benefit of merging; /// constant via global merge and the fact that the same constant is stored; /// only once with this method (versus, as many function that uses the constant; /// for the regular approach, even for float).; /// Again, the simplest solution would be to promote every; /// constant and rematerialize them when they are actually cheap to create.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:180,Performance,load,load,180,"// FIXME: In some cases, it may be interesting to promote in memory; // a zero initialized constant.; // E.g., when the type of Cst require more instructions than the; // adrp/add/load sequence or when this sequence can be shared by several; // instances of Cst.; // Ideally, we could promote this into a global and rematerialize the constant; // when it was a bad idea.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:120,Testability,test,testing,120,"// When IPI.first is a terminator instruction, DT may think that; // the result is defined on the edge.; // Here we are testing the insertion point, not the definition.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:33,Modifiability,variab,variable,33,// Create the load of the global variable.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:14,Performance,load,load,14,// Create the load of the global variable.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:3,Deployability,Update,Update,3,// Update the dominated uses.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:153,Deployability,update,update,153,// Look for instructions using constant vector. Promote that constant to a; // global variable. Create as few loads of this variable as possible and; // update the uses accordingly.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:86,Modifiability,variab,variable,86,// Look for instructions using constant vector. Promote that constant to a; // global variable. Create as few loads of this variable as possible and; // update the uses accordingly.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:124,Modifiability,variab,variable,124,// Look for instructions using constant vector. Promote that constant to a; // global variable. Create as few loads of this variable as possible and; // update the uses accordingly.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:110,Performance,load,loads,110,// Look for instructions using constant vector. Promote that constant to a; // global variable. Create as few loads of this variable as possible and; // update the uses accordingly.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:94,Modifiability,variab,variable,94,"// Traverse the operand, looking for constant vectors. Replace them by a; // load of a global variable of constant vector type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:77,Performance,load,load,77,"// Traverse the operand, looking for constant vectors. Replace them by a; // load of a global variable of constant vector type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp:758,Availability,redundant,redundant,758,"//=- AArch64RedundantCopyElimination.cpp - Remove useless copy for AArch64 -=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; // This pass removes unnecessary copies/moves in BBs based on a dominating; // condition.; //; // We handle three cases:; // 1. For BBs that are targets of CBZ/CBNZ instructions, we know the value of; // the CBZ/CBNZ source register is zero on the taken/not-taken path. For; // instance, the copy instruction in the code below can be removed because; // the CBZW jumps to %bb.2 when w0 is zero.; //; // %bb.1:; // cbz w0, .LBB0_2; // .LBB0_2:; // mov w0, wzr ; <-- redundant; //; // 2. If the flag setting instruction defines a register other than WZR/XZR, we; // can remove a zero copy in some cases.; //; // %bb.0:; // subs w0, w1, w2; // str w0, [x1]; // b.ne .LBB0_2; // %bb.1:; // mov w0, wzr ; <-- redundant; // str w0, [x2]; // .LBB0_2; //; // 3. Finally, if the flag setting instruction is a comparison against a; // constant (i.e., ADDS[W|X]ri, SUBS[W|X]ri), we can remove a mov immediate; // in some cases.; //; // %bb.0:; // subs xzr, x0, #1; // b.eq .LBB0_1; // .LBB0_1:; // orr x0, xzr, #0x1 ; <-- redundant; //; // This pass should be run after register allocation.; //; // FIXME: This could also be extended to check the whole dominance subtree below; // the comparison if the compile time regression is acceptable.; //; // FIXME: Add support for handling CCMP instructions.; // FIXME: If the known register value is zero, we should be able to rewrite uses; // to use WZR/XZR directly in some cases.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp:997,Availability,redundant,redundant,997,"//=- AArch64RedundantCopyElimination.cpp - Remove useless copy for AArch64 -=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; // This pass removes unnecessary copies/moves in BBs based on a dominating; // condition.; //; // We handle three cases:; // 1. For BBs that are targets of CBZ/CBNZ instructions, we know the value of; // the CBZ/CBNZ source register is zero on the taken/not-taken path. For; // instance, the copy instruction in the code below can be removed because; // the CBZW jumps to %bb.2 when w0 is zero.; //; // %bb.1:; // cbz w0, .LBB0_2; // .LBB0_2:; // mov w0, wzr ; <-- redundant; //; // 2. If the flag setting instruction defines a register other than WZR/XZR, we; // can remove a zero copy in some cases.; //; // %bb.0:; // subs w0, w1, w2; // str w0, [x1]; // b.ne .LBB0_2; // %bb.1:; // mov w0, wzr ; <-- redundant; // str w0, [x2]; // .LBB0_2; //; // 3. Finally, if the flag setting instruction is a comparison against a; // constant (i.e., ADDS[W|X]ri, SUBS[W|X]ri), we can remove a mov immediate; // in some cases.; //; // %bb.0:; // subs xzr, x0, #1; // b.eq .LBB0_1; // .LBB0_1:; // orr x0, xzr, #0x1 ; <-- redundant; //; // This pass should be run after register allocation.; //; // FIXME: This could also be extended to check the whole dominance subtree below; // the comparison if the compile time regression is acceptable.; //; // FIXME: Add support for handling CCMP instructions.; // FIXME: If the known register value is zero, we should be able to rewrite uses; // to use WZR/XZR directly in some cases.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp:1304,Availability,redundant,redundant,1304,"//=- AArch64RedundantCopyElimination.cpp - Remove useless copy for AArch64 -=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; // This pass removes unnecessary copies/moves in BBs based on a dominating; // condition.; //; // We handle three cases:; // 1. For BBs that are targets of CBZ/CBNZ instructions, we know the value of; // the CBZ/CBNZ source register is zero on the taken/not-taken path. For; // instance, the copy instruction in the code below can be removed because; // the CBZW jumps to %bb.2 when w0 is zero.; //; // %bb.1:; // cbz w0, .LBB0_2; // .LBB0_2:; // mov w0, wzr ; <-- redundant; //; // 2. If the flag setting instruction defines a register other than WZR/XZR, we; // can remove a zero copy in some cases.; //; // %bb.0:; // subs w0, w1, w2; // str w0, [x1]; // b.ne .LBB0_2; // %bb.1:; // mov w0, wzr ; <-- redundant; // str w0, [x2]; // .LBB0_2; //; // 3. Finally, if the flag setting instruction is a comparison against a; // constant (i.e., ADDS[W|X]ri, SUBS[W|X]ri), we can remove a mov immediate; // in some cases.; //; // %bb.0:; // subs xzr, x0, #1; // b.eq .LBB0_1; // .LBB0_1:; // orr x0, xzr, #0x1 ; <-- redundant; //; // This pass should be run after register allocation.; //; // FIXME: This could also be extended to check the whole dominance subtree below; // the comparison if the compile time regression is acceptable.; //; // FIXME: Add support for handling CCMP instructions.; // FIXME: If the known register value is zero, we should be able to rewrite uses; // to use WZR/XZR directly in some cases.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp:1407,Modifiability,extend,extended,1407,"//=- AArch64RedundantCopyElimination.cpp - Remove useless copy for AArch64 -=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; // This pass removes unnecessary copies/moves in BBs based on a dominating; // condition.; //; // We handle three cases:; // 1. For BBs that are targets of CBZ/CBNZ instructions, we know the value of; // the CBZ/CBNZ source register is zero on the taken/not-taken path. For; // instance, the copy instruction in the code below can be removed because; // the CBZW jumps to %bb.2 when w0 is zero.; //; // %bb.1:; // cbz w0, .LBB0_2; // .LBB0_2:; // mov w0, wzr ; <-- redundant; //; // 2. If the flag setting instruction defines a register other than WZR/XZR, we; // can remove a zero copy in some cases.; //; // %bb.0:; // subs w0, w1, w2; // str w0, [x1]; // b.ne .LBB0_2; // %bb.1:; // mov w0, wzr ; <-- redundant; // str w0, [x2]; // .LBB0_2; //; // 3. Finally, if the flag setting instruction is a comparison against a; // constant (i.e., ADDS[W|X]ri, SUBS[W|X]ri), we can remove a mov immediate; // in some cases.; //; // %bb.0:; // subs xzr, x0, #1; // b.eq .LBB0_1; // .LBB0_1:; // orr x0, xzr, #0x1 ; <-- redundant; //; // This pass should be run after register allocation.; //; // FIXME: This could also be extended to check the whole dominance subtree below; // the comparison if the compile time regression is acceptable.; //; // FIXME: Add support for handling CCMP instructions.; // FIXME: If the known register value is zero, we should be able to rewrite uses; // to use WZR/XZR directly in some cases.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp:1652,Modifiability,rewrite,rewrite,1652,"//=- AArch64RedundantCopyElimination.cpp - Remove useless copy for AArch64 -=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; // This pass removes unnecessary copies/moves in BBs based on a dominating; // condition.; //; // We handle three cases:; // 1. For BBs that are targets of CBZ/CBNZ instructions, we know the value of; // the CBZ/CBNZ source register is zero on the taken/not-taken path. For; // instance, the copy instruction in the code below can be removed because; // the CBZW jumps to %bb.2 when w0 is zero.; //; // %bb.1:; // cbz w0, .LBB0_2; // .LBB0_2:; // mov w0, wzr ; <-- redundant; //; // 2. If the flag setting instruction defines a register other than WZR/XZR, we; // can remove a zero copy in some cases.; //; // %bb.0:; // subs w0, w1, w2; // str w0, [x1]; // b.ne .LBB0_2; // %bb.1:; // mov w0, wzr ; <-- redundant; // str w0, [x2]; // .LBB0_2; //; // 3. Finally, if the flag setting instruction is a comparison against a; // constant (i.e., ADDS[W|X]ri, SUBS[W|X]ri), we can remove a mov immediate; // in some cases.; //; // %bb.0:; // subs xzr, x0, #1; // b.eq .LBB0_1; // .LBB0_1:; // orr x0, xzr, #0x1 ; <-- redundant; //; // This pass should be run after register allocation.; //; // FIXME: This could also be extended to check the whole dominance subtree below; // the comparison if the compile time regression is acceptable.; //; // FIXME: Add support for handling CCMP instructions.; // FIXME: If the known register value is zero, we should be able to rewrite uses; // to use WZR/XZR directly in some cases.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp:758,Safety,redund,redundant,758,"//=- AArch64RedundantCopyElimination.cpp - Remove useless copy for AArch64 -=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; // This pass removes unnecessary copies/moves in BBs based on a dominating; // condition.; //; // We handle three cases:; // 1. For BBs that are targets of CBZ/CBNZ instructions, we know the value of; // the CBZ/CBNZ source register is zero on the taken/not-taken path. For; // instance, the copy instruction in the code below can be removed because; // the CBZW jumps to %bb.2 when w0 is zero.; //; // %bb.1:; // cbz w0, .LBB0_2; // .LBB0_2:; // mov w0, wzr ; <-- redundant; //; // 2. If the flag setting instruction defines a register other than WZR/XZR, we; // can remove a zero copy in some cases.; //; // %bb.0:; // subs w0, w1, w2; // str w0, [x1]; // b.ne .LBB0_2; // %bb.1:; // mov w0, wzr ; <-- redundant; // str w0, [x2]; // .LBB0_2; //; // 3. Finally, if the flag setting instruction is a comparison against a; // constant (i.e., ADDS[W|X]ri, SUBS[W|X]ri), we can remove a mov immediate; // in some cases.; //; // %bb.0:; // subs xzr, x0, #1; // b.eq .LBB0_1; // .LBB0_1:; // orr x0, xzr, #0x1 ; <-- redundant; //; // This pass should be run after register allocation.; //; // FIXME: This could also be extended to check the whole dominance subtree below; // the comparison if the compile time regression is acceptable.; //; // FIXME: Add support for handling CCMP instructions.; // FIXME: If the known register value is zero, we should be able to rewrite uses; // to use WZR/XZR directly in some cases.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp:997,Safety,redund,redundant,997,"//=- AArch64RedundantCopyElimination.cpp - Remove useless copy for AArch64 -=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; // This pass removes unnecessary copies/moves in BBs based on a dominating; // condition.; //; // We handle three cases:; // 1. For BBs that are targets of CBZ/CBNZ instructions, we know the value of; // the CBZ/CBNZ source register is zero on the taken/not-taken path. For; // instance, the copy instruction in the code below can be removed because; // the CBZW jumps to %bb.2 when w0 is zero.; //; // %bb.1:; // cbz w0, .LBB0_2; // .LBB0_2:; // mov w0, wzr ; <-- redundant; //; // 2. If the flag setting instruction defines a register other than WZR/XZR, we; // can remove a zero copy in some cases.; //; // %bb.0:; // subs w0, w1, w2; // str w0, [x1]; // b.ne .LBB0_2; // %bb.1:; // mov w0, wzr ; <-- redundant; // str w0, [x2]; // .LBB0_2; //; // 3. Finally, if the flag setting instruction is a comparison against a; // constant (i.e., ADDS[W|X]ri, SUBS[W|X]ri), we can remove a mov immediate; // in some cases.; //; // %bb.0:; // subs xzr, x0, #1; // b.eq .LBB0_1; // .LBB0_1:; // orr x0, xzr, #0x1 ; <-- redundant; //; // This pass should be run after register allocation.; //; // FIXME: This could also be extended to check the whole dominance subtree below; // the comparison if the compile time regression is acceptable.; //; // FIXME: Add support for handling CCMP instructions.; // FIXME: If the known register value is zero, we should be able to rewrite uses; // to use WZR/XZR directly in some cases.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp:1304,Safety,redund,redundant,1304,"//=- AArch64RedundantCopyElimination.cpp - Remove useless copy for AArch64 -=//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; // This pass removes unnecessary copies/moves in BBs based on a dominating; // condition.; //; // We handle three cases:; // 1. For BBs that are targets of CBZ/CBNZ instructions, we know the value of; // the CBZ/CBNZ source register is zero on the taken/not-taken path. For; // instance, the copy instruction in the code below can be removed because; // the CBZW jumps to %bb.2 when w0 is zero.; //; // %bb.1:; // cbz w0, .LBB0_2; // .LBB0_2:; // mov w0, wzr ; <-- redundant; //; // 2. If the flag setting instruction defines a register other than WZR/XZR, we; // can remove a zero copy in some cases.; //; // %bb.0:; // subs w0, w1, w2; // str w0, [x1]; // b.ne .LBB0_2; // %bb.1:; // mov w0, wzr ; <-- redundant; // str w0, [x2]; // .LBB0_2; //; // 3. Finally, if the flag setting instruction is a comparison against a; // constant (i.e., ADDS[W|X]ri, SUBS[W|X]ri), we can remove a mov immediate; // in some cases.; //; // %bb.0:; // subs xzr, x0, #1; // b.eq .LBB0_1; // .LBB0_1:; // orr x0, xzr, #0x1 ; <-- redundant; //; // This pass should be run after register allocation.; //; // FIXME: This could also be extended to check the whole dominance subtree below; // the comparison if the compile time regression is acceptable.; //; // FIXME: Add support for handling CCMP instructions.; // FIXME: If the known register value is zero, we should be able to rewrite uses; // to use WZR/XZR directly in some cases.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp:51,Availability,redundant,redundant,51,// OptBBClobberedRegs is used when optimizing away redundant copies/moves.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp:35,Performance,optimiz,optimizing,35,// OptBBClobberedRegs is used when optimizing away redundant copies/moves.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp:51,Safety,redund,redundant,51,// OptBBClobberedRegs is used when optimizing away redundant copies/moves.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp:306,Deployability,update,update,306,"// After calling knownRegValInBlock, FirstUse will either point to a CBZ/CBNZ; // or a compare (i.e., SUBS). In the latter case, we must take care when; // updating FirstUse when scanning for COPY instructions. In particular, if; // there's a COPY in between the compare and branch the COPY should not; // update FirstUse.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp:10,Availability,redundant,redundant,10,// Remove redundant copy/move instructions unless KnownReg is modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp:10,Safety,redund,redundant,10,// Remove redundant copy/move instructions unless KnownReg is modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp:3,Usability,Clear,Clear,3,"// Clear kills in the range where changes were made. This is conservative,; // but should be okay since kill markers are being phased out.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RedundantCopyElimination.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:86,Integrability,depend,depending,86,"// Darwin has its own CSR_AArch64_AAPCS_SaveList, which means most CSR save; // lists depending on that will need to have their Darwin variant as well.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:86,Availability,mask,mask,86,// See TargetRegisterInfo::getCallPreservedMask for how to interpret the; // register mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:33,Availability,mask,mask,33,"// This should return a register mask that is the same as that returned by; // getCallPreservedMask but that additionally preserves the register used for; // the first i64 argument (which must also be the register used to return a; // single i64 return value); //; // In case that the calling convention does not use the same register for; // both, the function should return NULL (does not currently apply)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:10,Safety,avoid,avoid,10,// FIXME: avoid re-calculating this every time.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:442,Availability,reliab,reliable,442,"// In the presence of variable sized objects or funclets, if the fixed stack; // size is large enough that referencing from the FP won't result in things; // being in range relatively often, we can use a base pointer to allow access; // from the other direction like the SP normally works.; //; // Furthermore, if both variable sized objects are present, and the; // stack needs to be dynamically re-aligned, the base pointer is the only; // reliable way to reference the locals.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:22,Modifiability,variab,variable,22,"// In the presence of variable sized objects or funclets, if the fixed stack; // size is large enough that referencing from the FP won't result in things; // being in range relatively often, we can use a base pointer to allow access; // from the other direction like the SP normally works.; //; // Furthermore, if both variable sized objects are present, and the; // stack needs to be dynamically re-aligned, the base pointer is the only; // reliable way to reference the locals.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:319,Modifiability,variab,variable,319,"// In the presence of variable sized objects or funclets, if the fixed stack; // size is large enough that referencing from the FP won't result in things; // being in range relatively often, we can use a base pointer to allow access; // from the other direction like the SP normally works.; //; // Furthermore, if both variable sized objects are present, and the; // stack needs to be dynamically re-aligned, the base pointer is the only; // reliable way to reference the locals.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:226,Security,access,access,226,"// In the presence of variable sized objects or funclets, if the fixed stack; // size is large enough that referencing from the FP won't result in things; // being in range relatively often, we can use a base pointer to allow access; // from the other direction like the SP normally works.; //; // Furthermore, if both variable sized objects are present, and the; // stack needs to be dynamically re-aligned, the base pointer is the only; // reliable way to reference the locals.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:20,Modifiability,variab,variable,20,"// Frames that have variable sized objects and scalable SVE objects,; // should always use a basepointer.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:47,Performance,scalab,scalable,47,"// Frames that have variable sized objects and scalable SVE objects,; // should always use a basepointer.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:427,Performance,load,load,427,"// Conservatively estimate whether the negative offset from the frame; // pointer will be sufficient to reach. If a function has a smallish; // frame, it's less likely to have lots of spills and callee saved; // space, so it's all more likely to be within range of the frame pointer.; // If it's wrong, we'll materialize the constant and still get to the; // object; it's just suboptimal. Negative offsets use the unscaled; // load/store instructions, which have a 9-bit signed immediate.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:200,Availability,reliab,reliably,200,"// This function indicates whether the emergency spillslot should be placed; // close to the beginning of the stackframe (closer to FP) or the end; // (closer to SP).; //; // The beginning works most reliably if we have a frame pointer.; // In the presence of any non-constant space between FP and locals,; // (e.g. in case of stack realignment or a scalable SVE area), it is; // better to use SP or BP.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:350,Performance,scalab,scalable,350,"// This function indicates whether the emergency spillslot should be placed; // close to the beginning of the stackframe (closer to FP) or the end; // (closer to SP).; //; // The beginning works most reliably if we have a frame pointer.; // In the presence of any non-constant space between FP and locals,; // (e.g. in case of stack realignment or a scalable SVE area), it is; // better to use SP or BP.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:12,Performance,load,load,12,"// It's the load/store FI references that cause issues, as it can be difficult; // to materialize the offset if it won't fit in the literal field. Estimate; // based on the size of the local frame and some conservative assumptions; // about the rest of the stack frame (note, this is pre-regalloc, so; // we don't know everything for certain yet) whether this offset is likely; // to be out of range of the immediate. Return true if so.; // We only generate virtual base registers for loads and stores, so; // return false for everything else.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:485,Performance,load,loads,485,"// It's the load/store FI references that cause issues, as it can be difficult; // to materialize the offset if it won't fit in the literal field. Estimate; // based on the size of the local frame and some conservative assumptions; // about the rest of the stack frame (note, this is pre-regalloc, so; // we don't know everything for certain yet) whether this offset is likely; // to be out of range of the immediate. Return true if so.; // We only generate virtual base registers for loads and stores, so; // return false for everything else.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:56,Modifiability,variab,variable,56,"// Without a virtual base register, if the function has variable sized; // objects, all fixed-size local references will be via the frame pointer,; // Approximate the offset and see if it's legal for the instruction.; // Note that the incoming offset is based on the SP value at function entry,; // so it'll be negative.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:138,Security,access,access,138,"// Estimate an offset from the stack pointer.; // The incoming offset is relating to the SP at the start of the function,; // but when we access the local it'll be relative to the SP after local; // allocation, so adjust our SP-relative offset by that allocation size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:52,Energy Efficiency,allocate,allocated,52,// Assume that we'll have at least some spill slots allocated.; // FIXME: This is a total SWAG number. We should run some statistics; // and pick a real one.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:94,Availability,avail,available,94,"// 128 bytes of spill slots; // If there is a frame pointer, try using it.; // The FP is only available if there is no dynamic realignment. We; // don't know for sure yet whether we'll need that, so we guess based; // on whether there are any local variables that would trigger it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:249,Modifiability,variab,variables,249,"// 128 bytes of spill slots; // If there is a frame pointer, try using it.; // The FP is only available if there is no dynamic realignment. We; // don't know for sure yet whether we'll need that, so we guess based; // on whether there are any local variables that would trigger it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:45,Energy Efficiency,allocate,allocate,45,// The offset likely isn't legal; we want to allocate a virtual base register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:16,Performance,scalab,scalable,16,"// The smallest scalable element supported by scaled SVE addressing; // modes are predicates, which are 2 scalable bytes in size. So the scalable; // byte offset must always be a multiple of 2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:106,Performance,scalab,scalable,106,"// The smallest scalable element supported by scaled SVE addressing; // modes are predicates, which are 2 scalable bytes in size. So the scalable; // byte offset must always be a multiple of 2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:137,Performance,scalab,scalable,137,"// The smallest scalable element supported by scaled SVE addressing; // modes are predicates, which are 2 scalable bytes in size. So the scalable; // byte offset must always be a multiple of 2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:54,Integrability,interface,interface,54,// Add fixed-sized offset using existing DIExpression interface.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:43,Deployability,patch,patchpoint,43,"// Special handling of dbg_value, stackmap patchpoint statepoint instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:9,Deployability,update,update,9,// Can't update to SP + offset in place. Precalculate the tagged pointer; // in a scratch register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:38,Deployability,toggle,toggle,38,"// For calls that temporarily have to toggle streaming mode as part of the; // call-sequence, we need to be more careful when coalescing copy instructions; // so that we don't end up coalescing the NEON/FP result or argument register; // with a whole Z-register, such that after coalescing the register allocator; // will try to spill/reload the entire Z register.; //; // We do this by checking if the node has any defs/uses that are; // COALESCER_BARRIER pseudos. These are 'nops' in practice, but they exist to; // instruct the coalescer to avoid coalescing the copy.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:544,Safety,avoid,avoid,544,"// For calls that temporarily have to toggle streaming mode as part of the; // call-sequence, we need to be more careful when coalescing copy instructions; // so that we don't end up coalescing the NEON/FP result or argument register; // with a whole Z-register, such that after coalescing the register allocator; // will try to spill/reload the entire Z register.; //; // We do this by checking if the node has any defs/uses that are; // COALESCER_BARRIER pseudos. These are 'nops' in practice, but they exist to; // instruct the coalescer to avoid coalescing the copy.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.h:118,Availability,mask,mask,118,"// Calls involved in thread-local variable lookup save more registers than; // normal calls, so they need a different mask to represent this.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.h:34,Modifiability,variab,variable,34,"// Calls involved in thread-local variable lookup save more registers than; // normal calls, so they need a different mask to represent this.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.h:58,Availability,mask,mask,58,/// getThisReturnPreservedMask - Returns a call preserved mask specific to the; /// case that 'returned' is on an i64 first argument if the calling convention; /// is one that can (partially) model this attribute with a preserved mask; /// (i.e. it is a calling convention that uses the same register for the first; /// i64 argument and an i64 return value); ///; /// Should return NULL in the case that the calling convention does not have; /// this property,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.h:230,Availability,mask,mask,230,/// getThisReturnPreservedMask - Returns a call preserved mask specific to the; /// case that 'returned' is on an i64 first argument if the calling convention; /// is one that can (partially) model this attribute with a preserved mask; /// (i.e. it is a calling convention that uses the same register for the first; /// i64 argument and an i64 return value); ///; /// Should return NULL in the case that the calling convention does not have; /// this property,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SelectionDAGInfo.cpp:3,Modifiability,Extend,Extend,3,"// Extend value to i64, if required.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SelectionDAGInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SelectionDAGInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:425,Energy Efficiency,efficient,efficient,425,"//; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass that performs optimization on SIMD instructions; // with high latency by splitting them into more efficient series of; // instructions.; //; // 1. Rewrite certain SIMD instructions with vector element due to their; // inefficiency on some targets.; //; // For example:; // fmla v0.4s, v1.4s, v2.s[1]; //; // Is rewritten into:; // dup v3.4s, v2.s[1]; // fmla v0.4s, v1.4s, v3.4s; //; // 2. Rewrite interleaved memory access instructions due to their; // inefficiency on some targets.; //; // For example:; // st2 {v0.4s, v1.4s}, addr; //; // Is rewritten into:; // zip1 v2.4s, v0.4s, v1.4s; // zip2 v3.4s, v0.4s, v1.4s; // stp q2, q3, addr; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:474,Modifiability,Rewrite,Rewrite,474,"//; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass that performs optimization on SIMD instructions; // with high latency by splitting them into more efficient series of; // instructions.; //; // 1. Rewrite certain SIMD instructions with vector element due to their; // inefficiency on some targets.; //; // For example:; // fmla v0.4s, v1.4s, v2.s[1]; //; // Is rewritten into:; // dup v3.4s, v2.s[1]; // fmla v0.4s, v1.4s, v3.4s; //; // 2. Rewrite interleaved memory access instructions due to their; // inefficiency on some targets.; //; // For example:; // st2 {v0.4s, v1.4s}, addr; //; // Is rewritten into:; // zip1 v2.4s, v0.4s, v1.4s; // zip2 v3.4s, v0.4s, v1.4s; // stp q2, q3, addr; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:717,Modifiability,Rewrite,Rewrite,717,"//; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass that performs optimization on SIMD instructions; // with high latency by splitting them into more efficient series of; // instructions.; //; // 1. Rewrite certain SIMD instructions with vector element due to their; // inefficiency on some targets.; //; // For example:; // fmla v0.4s, v1.4s, v2.s[1]; //; // Is rewritten into:; // dup v3.4s, v2.s[1]; // fmla v0.4s, v1.4s, v3.4s; //; // 2. Rewrite interleaved memory access instructions due to their; // inefficiency on some targets.; //; // For example:; // st2 {v0.4s, v1.4s}, addr; //; // Is rewritten into:; // zip1 v2.4s, v0.4s, v1.4s; // zip2 v3.4s, v0.4s, v1.4s; // stp q2, q3, addr; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:332,Performance,perform,performs,332,"//; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass that performs optimization on SIMD instructions; // with high latency by splitting them into more efficient series of; // instructions.; //; // 1. Rewrite certain SIMD instructions with vector element due to their; // inefficiency on some targets.; //; // For example:; // fmla v0.4s, v1.4s, v2.s[1]; //; // Is rewritten into:; // dup v3.4s, v2.s[1]; // fmla v0.4s, v1.4s, v3.4s; //; // 2. Rewrite interleaved memory access instructions due to their; // inefficiency on some targets.; //; // For example:; // st2 {v0.4s, v1.4s}, addr; //; // Is rewritten into:; // zip1 v2.4s, v0.4s, v1.4s; // zip2 v3.4s, v0.4s, v1.4s; // stp q2, q3, addr; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:341,Performance,optimiz,optimization,341,"//; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass that performs optimization on SIMD instructions; // with high latency by splitting them into more efficient series of; // instructions.; //; // 1. Rewrite certain SIMD instructions with vector element due to their; // inefficiency on some targets.; //; // For example:; // fmla v0.4s, v1.4s, v2.s[1]; //; // Is rewritten into:; // dup v3.4s, v2.s[1]; // fmla v0.4s, v1.4s, v3.4s; //; // 2. Rewrite interleaved memory access instructions due to their; // inefficiency on some targets.; //; // For example:; // st2 {v0.4s, v1.4s}, addr; //; // Is rewritten into:; // zip1 v2.4s, v0.4s, v1.4s; // zip2 v3.4s, v0.4s, v1.4s; // stp q2, q3, addr; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:389,Performance,latency,latency,389,"//; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass that performs optimization on SIMD instructions; // with high latency by splitting them into more efficient series of; // instructions.; //; // 1. Rewrite certain SIMD instructions with vector element due to their; // inefficiency on some targets.; //; // For example:; // fmla v0.4s, v1.4s, v2.s[1]; //; // Is rewritten into:; // dup v3.4s, v2.s[1]; // fmla v0.4s, v1.4s, v3.4s; //; // 2. Rewrite interleaved memory access instructions due to their; // inefficiency on some targets.; //; // For example:; // st2 {v0.4s, v1.4s}, addr; //; // Is rewritten into:; // zip1 v2.4s, v0.4s, v1.4s; // zip2 v3.4s, v0.4s, v1.4s; // stp q2, q3, addr; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:744,Security,access,access,744,"//; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass that performs optimization on SIMD instructions; // with high latency by splitting them into more efficient series of; // instructions.; //; // 1. Rewrite certain SIMD instructions with vector element due to their; // inefficiency on some targets.; //; // For example:; // fmla v0.4s, v1.4s, v2.s[1]; //; // Is rewritten into:; // dup v3.4s, v2.s[1]; // fmla v0.4s, v1.4s, v3.4s; //; // 2. Rewrite interleaved memory access instructions due to their; // inefficiency on some targets.; //; // For example:; // st2 {v0.4s, v1.4s}, addr; //; // Is rewritten into:; // zip1 v2.4s, v0.4s, v1.4s; // zip2 v3.4s, v0.4s, v1.4s; // stp q2, q3, addr; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:34,Performance,cache,cache,34,// The two maps below are used to cache decisions instead of recomputing:; // This is used to cache instruction replacement decisions within function; // units and across function units.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:94,Performance,cache,cache,94,// The two maps below are used to cache decisions instead of recomputing:; // This is used to cache instruction replacement decisions within function; // units and across function units.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:19,Performance,cache,cache,19,// This is used to cache the decision of whether to leave the interleaved; // store instructions replacement pass early or not for a particular target.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:54,Energy Efficiency,efficient,efficient,54,// A costly instruction is replaced in this work by N efficient instructions; // The maximum of N is curently 10 and it is for ST4 case.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:67,Energy Efficiency,efficient,efficient,67,"/// Based only on latency of instructions, determine if it is cost efficient; /// to replace the instruction InstDesc by the instructions stored in the; /// array InstDescRepl.; /// Return true if replacement is expected to be faster.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:18,Performance,latency,latency,18,"/// Based only on latency of instructions, determine if it is cost efficient; /// to replace the instruction InstDesc by the instructions stored in the; /// array InstDescRepl.; /// Return true if replacement is expected to be faster.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:61,Performance,optimiz,optimization,61,/// Determine if we need to exit the instruction replacement optimization; /// passes early. This makes sure that no compile time is spent in this pass; /// for targets with no need for any of these optimizations.; /// Return true if early exit of the pass is recommended.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:199,Performance,optimiz,optimizations,199,/// Determine if we need to exit the instruction replacement optimization; /// passes early. This makes sure that no compile time is spent in this pass; /// for targets with no need for any of these optimizations.; /// Return true if early exit of the pass is recommended.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:66,Energy Efficiency,efficient,efficient,66,/// Certain SIMD instructions with vector element operand are not efficient.; /// Rewrite them into SIMD instructions with vector operands. This rewrite; /// is driven by the latency of the instructions.; /// Return true if the SIMD instruction is modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:82,Modifiability,Rewrite,Rewrite,82,/// Certain SIMD instructions with vector element operand are not efficient.; /// Rewrite them into SIMD instructions with vector operands. This rewrite; /// is driven by the latency of the instructions.; /// Return true if the SIMD instruction is modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:145,Modifiability,rewrite,rewrite,145,/// Certain SIMD instructions with vector element operand are not efficient.; /// Rewrite them into SIMD instructions with vector operands. This rewrite; /// is driven by the latency of the instructions.; /// Return true if the SIMD instruction is modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:175,Performance,latency,latency,175,/// Certain SIMD instructions with vector element operand are not efficient.; /// Rewrite them into SIMD instructions with vector operands. This rewrite; /// is driven by the latency of the instructions.; /// Return true if the SIMD instruction is modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:4,Performance,Load,Load,4,/// Load/Store Interleaving instructions are not always beneficial.; /// Replace them by ZIP instructionand classical load/store.; /// Return true if the SIMD instruction is modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:118,Performance,load,load,118,/// Load/Store Interleaving instructions are not always beneficial.; /// Replace them by ZIP instructionand classical load/store.; /// Return true if the SIMD instruction is modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:67,Energy Efficiency,efficient,efficient,67,"/// Based only on latency of instructions, determine if it is cost efficient; /// to replace the instruction InstDesc by the instructions stored in the; /// array InstDescRepl.; /// Return true if replacement is expected to be faster.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:18,Performance,latency,latency,18,"/// Based only on latency of instructions, determine if it is cost efficient; /// to replace the instruction InstDesc by the instructions stored in the; /// array InstDescRepl.; /// Return true if replacement is expected to be faster.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:44,Availability,avail,available,44,"// Check if replacement decision is already available in the cached table.; // if so, return it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:61,Performance,cache,cached,61,"// Check if replacement decision is already available in the cached table.; // if so, return it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:199,Performance,optimiz,optimizations,199,/// Determine if we need to exit this pass for a kind of instruction replacement; /// early. This makes sure that no compile time is spent in this pass for; /// targets with no need for any of these optimizations beyond performing this; /// check.; /// Return true if early exit of this pass for a kind of instruction; /// replacement is recommended for a target.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:220,Performance,perform,performing,220,/// Determine if we need to exit this pass for a kind of instruction replacement; /// early. This makes sure that no compile time is spent in this pass for; /// targets with no need for any of these optimizations beyond performing this; /// check.; /// Return true if early exit of this pass for a kind of instruction; /// replacement is recommended for a target.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:12,Performance,optimiz,optimization,12,"// For this optimization, check by comparing the latency of a representative; // instruction to that of the replacement instructions.; // TODO: check for all concerned instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:49,Performance,latency,latency,49,"// For this optimization, check by comparing the latency of a representative; // instruction to that of the replacement instructions.; // TODO: check for all concerned instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:12,Performance,optimiz,optimization,12,"// For this optimization, check for all concerned instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:454,Availability,redundant,redundant,454,"/// Certain SIMD instructions with vector element operand are not efficient.; /// Rewrite them into SIMD instructions with vector operands. This rewrite; /// is driven by the latency of the instructions.; /// The instruction of concerns are for the time being FMLA, FMLS, FMUL,; /// and FMULX and hence they are hardcoded.; ///; /// For example:; /// fmla v0.4s, v1.4s, v2.s[1]; ///; /// Is rewritten into; /// dup v3.4s, v2.s[1] // DUP not necessary if redundant; /// fmla v0.4s, v1.4s, v3.4s; ///; /// Return true if the SIMD instruction is modified.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:66,Energy Efficiency,efficient,efficient,66,"/// Certain SIMD instructions with vector element operand are not efficient.; /// Rewrite them into SIMD instructions with vector operands. This rewrite; /// is driven by the latency of the instructions.; /// The instruction of concerns are for the time being FMLA, FMLS, FMUL,; /// and FMULX and hence they are hardcoded.; ///; /// For example:; /// fmla v0.4s, v1.4s, v2.s[1]; ///; /// Is rewritten into; /// dup v3.4s, v2.s[1] // DUP not necessary if redundant; /// fmla v0.4s, v1.4s, v3.4s; ///; /// Return true if the SIMD instruction is modified.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:82,Modifiability,Rewrite,Rewrite,82,"/// Certain SIMD instructions with vector element operand are not efficient.; /// Rewrite them into SIMD instructions with vector operands. This rewrite; /// is driven by the latency of the instructions.; /// The instruction of concerns are for the time being FMLA, FMLS, FMUL,; /// and FMULX and hence they are hardcoded.; ///; /// For example:; /// fmla v0.4s, v1.4s, v2.s[1]; ///; /// Is rewritten into; /// dup v3.4s, v2.s[1] // DUP not necessary if redundant; /// fmla v0.4s, v1.4s, v3.4s; ///; /// Return true if the SIMD instruction is modified.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:145,Modifiability,rewrite,rewrite,145,"/// Certain SIMD instructions with vector element operand are not efficient.; /// Rewrite them into SIMD instructions with vector operands. This rewrite; /// is driven by the latency of the instructions.; /// The instruction of concerns are for the time being FMLA, FMLS, FMUL,; /// and FMULX and hence they are hardcoded.; ///; /// For example:; /// fmla v0.4s, v1.4s, v2.s[1]; ///; /// Is rewritten into; /// dup v3.4s, v2.s[1] // DUP not necessary if redundant; /// fmla v0.4s, v1.4s, v3.4s; ///; /// Return true if the SIMD instruction is modified.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:175,Performance,latency,latency,175,"/// Certain SIMD instructions with vector element operand are not efficient.; /// Rewrite them into SIMD instructions with vector operands. This rewrite; /// is driven by the latency of the instructions.; /// The instruction of concerns are for the time being FMLA, FMLS, FMUL,; /// and FMULX and hence they are hardcoded.; ///; /// For example:; /// fmla v0.4s, v1.4s, v2.s[1]; ///; /// Is rewritten into; /// dup v3.4s, v2.s[1] // DUP not necessary if redundant; /// fmla v0.4s, v1.4s, v3.4s; ///; /// Return true if the SIMD instruction is modified.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:454,Safety,redund,redundant,454,"/// Certain SIMD instructions with vector element operand are not efficient.; /// Rewrite them into SIMD instructions with vector operands. This rewrite; /// is driven by the latency of the instructions.; /// The instruction of concerns are for the time being FMLA, FMLS, FMUL,; /// and FMULX and hence they are hardcoded.; ///; /// For example:; /// fmla v0.4s, v1.4s, v2.s[1]; ///; /// Is rewritten into; /// dup v3.4s, v2.s[1] // DUP not necessary if redundant; /// fmla v0.4s, v1.4s, v3.4s; ///; /// Return true if the SIMD instruction is modified.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:4,Performance,Load,Load,4,"/// Load/Store Interleaving instructions are not always beneficial.; /// Replace them by ZIP instructions and classical load/store.; ///; /// For example:; /// st2 {v0.4s, v1.4s}, addr; ///; /// Is rewritten into:; /// zip1 v2.4s, v0.4s, v1.4s; /// zip2 v3.4s, v0.4s, v1.4s; /// stp q2, q3, addr; //; /// For example:; /// st4 {v0.4s, v1.4s, v2.4s, v3.4s}, addr; ///; /// Is rewritten into:; /// zip1 v4.4s, v0.4s, v2.4s; /// zip2 v5.4s, v0.4s, v2.4s; /// zip1 v6.4s, v1.4s, v3.4s; /// zip2 v7.4s, v1.4s, v3.4s; /// zip1 v8.4s, v4.4s, v6.4s; /// zip2 v9.4s, v4.4s, v6.4s; /// zip1 v10.4s, v5.4s, v7.4s; /// zip2 v11.4s, v5.4s, v7.4s; /// stp q8, q9, addr; /// stp q10, q11, addr+32; ///; /// Currently only instructions related to ST2 and ST4 are considered.; /// Other may be added later.; /// Return true if the SIMD instruction is modified.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:120,Performance,load,load,120,"/// Load/Store Interleaving instructions are not always beneficial.; /// Replace them by ZIP instructions and classical load/store.; ///; /// For example:; /// st2 {v0.4s, v1.4s}, addr; ///; /// Is rewritten into:; /// zip1 v2.4s, v0.4s, v1.4s; /// zip2 v3.4s, v0.4s, v1.4s; /// stp q2, q3, addr; //; /// For example:; /// st4 {v0.4s, v1.4s, v2.4s, v3.4s}, addr; ///; /// Is rewritten into:; /// zip1 v4.4s, v0.4s, v2.4s; /// zip2 v5.4s, v0.4s, v2.4s; /// zip1 v6.4s, v1.4s, v3.4s; /// zip2 v7.4s, v1.4s, v3.4s; /// zip1 v8.4s, v4.4s, v6.4s; /// zip2 v9.4s, v4.4s, v6.4s; /// zip1 v10.4s, v5.4s, v7.4s; /// zip2 v11.4s, v5.4s, v7.4s; /// stp q8, q9, addr; /// stp q10, q11, addr+32; ///; /// Currently only instructions related to ST2 and ST4 are considered.; /// Other may be added later.; /// Return true if the SIMD instruction is modified.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:3,Security,Validat,Validation,3,// Validation check for the other arguments.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:76,Performance,optimiz,optimization,76,/// Returns an instance of the high cost ASIMD instruction replacement; /// optimization pass.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SLSHardening.cpp:194,Performance,optimiz,optimization,194,// FIXME: It probably would be possible to filter which thunks to produce; // based on which registers are actually used in BLR instructions in this; // function. But would that be a worthwhile optimization?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SLSHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SLSHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SLSHardening.cpp:330,Safety,avoid,avoid,330,"// Now copy the implicit operands from BLR to BL and copy other necessary; // info.; // However, both BLR and BL instructions implictly use SP and implicitly; // define LR. Blindly copying implicit operands would result in SP and LR; // operands to be present multiple times. While this may not be too much of; // an issue, let's avoid that for cleanliness, by removing those implicit; // operands from the BL created above before we copy over all implicit; // operands from the BLR.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SLSHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SLSHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:668,Availability,mask,mask,668,"//===- AArch64SpeculationHardening.cpp - Harden Against Missspeculation --===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass to insert code to mitigate against side channel; // vulnerabilities that may happen under control flow miss-speculation.; //; // The pass implements tracking of control flow miss-speculation into a ""taint""; // register. That taint register can then be used to mask off registers with; // sensitive data when executing under miss-speculation, a.k.a. ""transient; // execution"".; // This pass is aimed at mitigating against SpectreV1-style vulnarabilities.; //; // It also implements speculative load hardening, i.e. using the taint register; // to automatically mask off loaded data.; //; // As a possible follow-on improvement, also an intrinsics-based approach as; // explained at https://lwn.net/Articles/759423/ could be implemented on top of; // the current design.; //; // For AArch64, the following implementation choices are made to implement the; // tracking of control flow miss-speculation into a taint register:; // Some of these are different than the implementation choices made in; // the similar pass implemented in X86SpeculativeLoadHardening.cpp, as; // the instruction set characteristics result in different trade-offs.; // - The speculation hardening is done after register allocation. With a; // relative abundance of registers, one register is reserved (X16) to be; // the taint register. X16 is expected to not clash with other register; // reservation mechanisms with very high probability because:; // . The AArch64 ABI doesn't guarantee X16 to be retained across any call.; // . The only way to request X16 to be used as a programmer is through; // inline assembly. In",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:968,Availability,mask,mask,968,"//===- AArch64SpeculationHardening.cpp - Harden Against Missspeculation --===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass to insert code to mitigate against side channel; // vulnerabilities that may happen under control flow miss-speculation.; //; // The pass implements tracking of control flow miss-speculation into a ""taint""; // register. That taint register can then be used to mask off registers with; // sensitive data when executing under miss-speculation, a.k.a. ""transient; // execution"".; // This pass is aimed at mitigating against SpectreV1-style vulnarabilities.; //; // It also implements speculative load hardening, i.e. using the taint register; // to automatically mask off loaded data.; //; // As a possible follow-on improvement, also an intrinsics-based approach as; // explained at https://lwn.net/Articles/759423/ could be implemented on top of; // the current design.; //; // For AArch64, the following implementation choices are made to implement the; // tracking of control flow miss-speculation into a taint register:; // Some of these are different than the implementation choices made in; // the similar pass implemented in X86SpeculativeLoadHardening.cpp, as; // the instruction set characteristics result in different trade-offs.; // - The speculation hardening is done after register allocation. With a; // relative abundance of registers, one register is reserved (X16) to be; // the taint register. X16 is expected to not clash with other register; // reservation mechanisms with very high probability because:; // . The AArch64 ABI doesn't guarantee X16 to be retained across any call.; // . The only way to request X16 to be used as a programmer is through; // inline assembly. In",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:2240,Availability,mask,mask,2240,"of control flow miss-speculation into a taint register:; // Some of these are different than the implementation choices made in; // the similar pass implemented in X86SpeculativeLoadHardening.cpp, as; // the instruction set characteristics result in different trade-offs.; // - The speculation hardening is done after register allocation. With a; // relative abundance of registers, one register is reserved (X16) to be; // the taint register. X16 is expected to not clash with other register; // reservation mechanisms with very high probability because:; // . The AArch64 ABI doesn't guarantee X16 to be retained across any call.; // . The only way to request X16 to be used as a programmer is through; // inline assembly. In the rare case a function explicitly demands to; // use X16/W16, this pass falls back to hardening against speculation; // by inserting a DSB SYS/ISB barrier pair which will prevent control; // flow speculation.; // - It is easy to insert mask operations at this late stage as we have; // mask operations available that don't set flags.; // - The taint variable contains all-ones when no miss-speculation is detected,; // and contains all-zeros when miss-speculation is detected. Therefore, when; // masking, an AND instruction (which only changes the register to be masked,; // no other side effects) can easily be inserted anywhere that's needed.; // - The tracking of miss-speculation is done by using a data-flow conditional; // select instruction (CSEL) to evaluate the flags that were also used to; // make conditional branch direction decisions. Speculation of the CSEL; // instruction can be limited with a CSDB instruction - so the combination of; // CSEL + a later CSDB gives the guarantee that the flags as used in the CSEL; // aren't speculated. When conditional branch direction gets miss-speculated,; // the semantics of the inserted CSEL instruction is such that the taint; // register will contain all zero bits.; // One key requirement for this to work is t",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:2290,Availability,mask,mask,2290,"of control flow miss-speculation into a taint register:; // Some of these are different than the implementation choices made in; // the similar pass implemented in X86SpeculativeLoadHardening.cpp, as; // the instruction set characteristics result in different trade-offs.; // - The speculation hardening is done after register allocation. With a; // relative abundance of registers, one register is reserved (X16) to be; // the taint register. X16 is expected to not clash with other register; // reservation mechanisms with very high probability because:; // . The AArch64 ABI doesn't guarantee X16 to be retained across any call.; // . The only way to request X16 to be used as a programmer is through; // inline assembly. In the rare case a function explicitly demands to; // use X16/W16, this pass falls back to hardening against speculation; // by inserting a DSB SYS/ISB barrier pair which will prevent control; // flow speculation.; // - It is easy to insert mask operations at this late stage as we have; // mask operations available that don't set flags.; // - The taint variable contains all-ones when no miss-speculation is detected,; // and contains all-zeros when miss-speculation is detected. Therefore, when; // masking, an AND instruction (which only changes the register to be masked,; // no other side effects) can easily be inserted anywhere that's needed.; // - The tracking of miss-speculation is done by using a data-flow conditional; // select instruction (CSEL) to evaluate the flags that were also used to; // make conditional branch direction decisions. Speculation of the CSEL; // instruction can be limited with a CSDB instruction - so the combination of; // CSEL + a later CSDB gives the guarantee that the flags as used in the CSEL; // aren't speculated. When conditional branch direction gets miss-speculated,; // the semantics of the inserted CSEL instruction is such that the taint; // register will contain all zero bits.; // One key requirement for this to work is t",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:2306,Availability,avail,available,2306,"of control flow miss-speculation into a taint register:; // Some of these are different than the implementation choices made in; // the similar pass implemented in X86SpeculativeLoadHardening.cpp, as; // the instruction set characteristics result in different trade-offs.; // - The speculation hardening is done after register allocation. With a; // relative abundance of registers, one register is reserved (X16) to be; // the taint register. X16 is expected to not clash with other register; // reservation mechanisms with very high probability because:; // . The AArch64 ABI doesn't guarantee X16 to be retained across any call.; // . The only way to request X16 to be used as a programmer is through; // inline assembly. In the rare case a function explicitly demands to; // use X16/W16, this pass falls back to hardening against speculation; // by inserting a DSB SYS/ISB barrier pair which will prevent control; // flow speculation.; // - It is easy to insert mask operations at this late stage as we have; // mask operations available that don't set flags.; // - The taint variable contains all-ones when no miss-speculation is detected,; // and contains all-zeros when miss-speculation is detected. Therefore, when; // masking, an AND instruction (which only changes the register to be masked,; // no other side effects) can easily be inserted anywhere that's needed.; // - The tracking of miss-speculation is done by using a data-flow conditional; // select instruction (CSEL) to evaluate the flags that were also used to; // make conditional branch direction decisions. Speculation of the CSEL; // instruction can be limited with a CSDB instruction - so the combination of; // CSEL + a later CSDB gives the guarantee that the flags as used in the CSEL; // aren't speculated. When conditional branch direction gets miss-speculated,; // the semantics of the inserted CSEL instruction is such that the taint; // register will contain all zero bits.; // One key requirement for this to work is t",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:2501,Availability,mask,masking,2501,"ion hardening is done after register allocation. With a; // relative abundance of registers, one register is reserved (X16) to be; // the taint register. X16 is expected to not clash with other register; // reservation mechanisms with very high probability because:; // . The AArch64 ABI doesn't guarantee X16 to be retained across any call.; // . The only way to request X16 to be used as a programmer is through; // inline assembly. In the rare case a function explicitly demands to; // use X16/W16, this pass falls back to hardening against speculation; // by inserting a DSB SYS/ISB barrier pair which will prevent control; // flow speculation.; // - It is easy to insert mask operations at this late stage as we have; // mask operations available that don't set flags.; // - The taint variable contains all-ones when no miss-speculation is detected,; // and contains all-zeros when miss-speculation is detected. Therefore, when; // masking, an AND instruction (which only changes the register to be masked,; // no other side effects) can easily be inserted anywhere that's needed.; // - The tracking of miss-speculation is done by using a data-flow conditional; // select instruction (CSEL) to evaluate the flags that were also used to; // make conditional branch direction decisions. Speculation of the CSEL; // instruction can be limited with a CSDB instruction - so the combination of; // CSEL + a later CSDB gives the guarantee that the flags as used in the CSEL; // aren't speculated. When conditional branch direction gets miss-speculated,; // the semantics of the inserted CSEL instruction is such that the taint; // register will contain all zero bits.; // One key requirement for this to work is that the conditional branch is; // followed by an execution of the CSEL instruction, where the CSEL; // instruction needs to use the same flags status as the conditional branch.; // This means that the conditional branches must not be implemented as one; // of the AArch64 conditional branch",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:2568,Availability,mask,masked,2568,"ion hardening is done after register allocation. With a; // relative abundance of registers, one register is reserved (X16) to be; // the taint register. X16 is expected to not clash with other register; // reservation mechanisms with very high probability because:; // . The AArch64 ABI doesn't guarantee X16 to be retained across any call.; // . The only way to request X16 to be used as a programmer is through; // inline assembly. In the rare case a function explicitly demands to; // use X16/W16, this pass falls back to hardening against speculation; // by inserting a DSB SYS/ISB barrier pair which will prevent control; // flow speculation.; // - It is easy to insert mask operations at this late stage as we have; // mask operations available that don't set flags.; // - The taint variable contains all-ones when no miss-speculation is detected,; // and contains all-zeros when miss-speculation is detected. Therefore, when; // masking, an AND instruction (which only changes the register to be masked,; // no other side effects) can easily be inserted anywhere that's needed.; // - The tracking of miss-speculation is done by using a data-flow conditional; // select instruction (CSEL) to evaluate the flags that were also used to; // make conditional branch direction decisions. Speculation of the CSEL; // instruction can be limited with a CSDB instruction - so the combination of; // CSEL + a later CSDB gives the guarantee that the flags as used in the CSEL; // aren't speculated. When conditional branch direction gets miss-speculated,; // the semantics of the inserted CSEL instruction is such that the taint; // register will contain all zero bits.; // One key requirement for this to work is that the conditional branch is; // followed by an execution of the CSEL instruction, where the CSEL; // instruction needs to use the same flags status as the conditional branch.; // This means that the conditional branches must not be implemented as one; // of the AArch64 conditional branch",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:4531,Availability,mask,masking,4531," not be implemented as one; // of the AArch64 conditional branches that do not use the flags as input; // (CB(N)Z and TB(N)Z). This is implemented by ensuring in the instruction; // selectors to not produce these instructions when speculation hardening; // is enabled. This pass will assert if it does encounter such an instruction.; // - On function call boundaries, the miss-speculation state is transferred from; // the taint register X16 to be encoded in the SP register as value 0.; //; // For the aspect of automatically hardening loads, using the taint register,; // (a.k.a. speculative load hardening, see; // https://llvm.org/docs/SpeculativeLoadHardening.html), the following; // implementation choices are made for AArch64:; // - Many of the optimizations described at; // https://llvm.org/docs/SpeculativeLoadHardening.html to harden fewer; // loads haven't been implemented yet - but for some of them there are; // FIXMEs in the code.; // - loads that load into general purpose (X or W) registers get hardened by; // masking the loaded data. For loads that load into other registers, the; // address loaded from gets hardened. It is expected that hardening the; // loaded data may be more efficient; but masking data in registers other; // than X or W is not easy and may result in being slower than just; // hardening the X address register loaded from.; // - On AArch64, CSDB instructions are inserted between the masking of the; // register and its first use, to ensure there's no non-control-flow; // speculation that might undermine the hardening mechanism.; //; // Future extensions/improvements could be:; // - Implement this functionality using full speculation barriers, akin to the; // x86-slh-lfence option. This may be more useful for the intrinsics-based; // approach than for the SLH approach to masking.; // Note that this pass already inserts the full speculation barriers if the; // function for some niche reason makes use of X16/W16.; // - no indirect branch mispredict",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:4718,Availability,mask,masking,4718," produce these instructions when speculation hardening; // is enabled. This pass will assert if it does encounter such an instruction.; // - On function call boundaries, the miss-speculation state is transferred from; // the taint register X16 to be encoded in the SP register as value 0.; //; // For the aspect of automatically hardening loads, using the taint register,; // (a.k.a. speculative load hardening, see; // https://llvm.org/docs/SpeculativeLoadHardening.html), the following; // implementation choices are made for AArch64:; // - Many of the optimizations described at; // https://llvm.org/docs/SpeculativeLoadHardening.html to harden fewer; // loads haven't been implemented yet - but for some of them there are; // FIXMEs in the code.; // - loads that load into general purpose (X or W) registers get hardened by; // masking the loaded data. For loads that load into other registers, the; // address loaded from gets hardened. It is expected that hardening the; // loaded data may be more efficient; but masking data in registers other; // than X or W is not easy and may result in being slower than just; // hardening the X address register loaded from.; // - On AArch64, CSDB instructions are inserted between the masking of the; // register and its first use, to ensure there's no non-control-flow; // speculation that might undermine the hardening mechanism.; //; // Future extensions/improvements could be:; // - Implement this functionality using full speculation barriers, akin to the; // x86-slh-lfence option. This may be more useful for the intrinsics-based; // approach than for the SLH approach to masking.; // Note that this pass already inserts the full speculation barriers if the; // function for some niche reason makes use of X16/W16.; // - no indirect branch misprediction gets protected/instrumented; but this; // could be done for some indirect branches, such as switch jump tables.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:4930,Availability,mask,masking,4930," produce these instructions when speculation hardening; // is enabled. This pass will assert if it does encounter such an instruction.; // - On function call boundaries, the miss-speculation state is transferred from; // the taint register X16 to be encoded in the SP register as value 0.; //; // For the aspect of automatically hardening loads, using the taint register,; // (a.k.a. speculative load hardening, see; // https://llvm.org/docs/SpeculativeLoadHardening.html), the following; // implementation choices are made for AArch64:; // - Many of the optimizations described at; // https://llvm.org/docs/SpeculativeLoadHardening.html to harden fewer; // loads haven't been implemented yet - but for some of them there are; // FIXMEs in the code.; // - loads that load into general purpose (X or W) registers get hardened by; // masking the loaded data. For loads that load into other registers, the; // address loaded from gets hardened. It is expected that hardening the; // loaded data may be more efficient; but masking data in registers other; // than X or W is not easy and may result in being slower than just; // hardening the X address register loaded from.; // - On AArch64, CSDB instructions are inserted between the masking of the; // register and its first use, to ensure there's no non-control-flow; // speculation that might undermine the hardening mechanism.; //; // Future extensions/improvements could be:; // - Implement this functionality using full speculation barriers, akin to the; // x86-slh-lfence option. This may be more useful for the intrinsics-based; // approach than for the SLH approach to masking.; // Note that this pass already inserts the full speculation barriers if the; // function for some niche reason makes use of X16/W16.; // - no indirect branch misprediction gets protected/instrumented; but this; // could be done for some indirect branches, such as switch jump tables.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:5324,Availability,mask,masking,5324," produce these instructions when speculation hardening; // is enabled. This pass will assert if it does encounter such an instruction.; // - On function call boundaries, the miss-speculation state is transferred from; // the taint register X16 to be encoded in the SP register as value 0.; //; // For the aspect of automatically hardening loads, using the taint register,; // (a.k.a. speculative load hardening, see; // https://llvm.org/docs/SpeculativeLoadHardening.html), the following; // implementation choices are made for AArch64:; // - Many of the optimizations described at; // https://llvm.org/docs/SpeculativeLoadHardening.html to harden fewer; // loads haven't been implemented yet - but for some of them there are; // FIXMEs in the code.; // - loads that load into general purpose (X or W) registers get hardened by; // masking the loaded data. For loads that load into other registers, the; // address loaded from gets hardened. It is expected that hardening the; // loaded data may be more efficient; but masking data in registers other; // than X or W is not easy and may result in being slower than just; // hardening the X address register loaded from.; // - On AArch64, CSDB instructions are inserted between the masking of the; // register and its first use, to ensure there's no non-control-flow; // speculation that might undermine the hardening mechanism.; //; // Future extensions/improvements could be:; // - Implement this functionality using full speculation barriers, akin to the; // x86-slh-lfence option. This may be more useful for the intrinsics-based; // approach than for the SLH approach to masking.; // Note that this pass already inserts the full speculation barriers if the; // function for some niche reason makes use of X16/W16.; // - no indirect branch misprediction gets protected/instrumented; but this; // could be done for some indirect branches, such as switch jump tables.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:4703,Energy Efficiency,efficient,efficient,4703," produce these instructions when speculation hardening; // is enabled. This pass will assert if it does encounter such an instruction.; // - On function call boundaries, the miss-speculation state is transferred from; // the taint register X16 to be encoded in the SP register as value 0.; //; // For the aspect of automatically hardening loads, using the taint register,; // (a.k.a. speculative load hardening, see; // https://llvm.org/docs/SpeculativeLoadHardening.html), the following; // implementation choices are made for AArch64:; // - Many of the optimizations described at; // https://llvm.org/docs/SpeculativeLoadHardening.html to harden fewer; // loads haven't been implemented yet - but for some of them there are; // FIXMEs in the code.; // - loads that load into general purpose (X or W) registers get hardened by; // masking the loaded data. For loads that load into other registers, the; // address loaded from gets hardened. It is expected that hardening the; // loaded data may be more efficient; but masking data in registers other; // than X or W is not easy and may result in being slower than just; // hardening the X address register loaded from.; // - On AArch64, CSDB instructions are inserted between the masking of the; // register and its first use, to ensure there's no non-control-flow; // speculation that might undermine the hardening mechanism.; //; // Future extensions/improvements could be:; // - Implement this functionality using full speculation barriers, akin to the; // x86-slh-lfence option. This may be more useful for the intrinsics-based; // approach than for the SLH approach to masking.; // Note that this pass already inserts the full speculation barriers if the; // function for some niche reason makes use of X16/W16.; // - no indirect branch misprediction gets protected/instrumented; but this; // could be done for some indirect branches, such as switch jump tables.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:2354,Modifiability,variab,variable,2354,"e similar pass implemented in X86SpeculativeLoadHardening.cpp, as; // the instruction set characteristics result in different trade-offs.; // - The speculation hardening is done after register allocation. With a; // relative abundance of registers, one register is reserved (X16) to be; // the taint register. X16 is expected to not clash with other register; // reservation mechanisms with very high probability because:; // . The AArch64 ABI doesn't guarantee X16 to be retained across any call.; // . The only way to request X16 to be used as a programmer is through; // inline assembly. In the rare case a function explicitly demands to; // use X16/W16, this pass falls back to hardening against speculation; // by inserting a DSB SYS/ISB barrier pair which will prevent control; // flow speculation.; // - It is easy to insert mask operations at this late stage as we have; // mask operations available that don't set flags.; // - The taint variable contains all-ones when no miss-speculation is detected,; // and contains all-zeros when miss-speculation is detected. Therefore, when; // masking, an AND instruction (which only changes the register to be masked,; // no other side effects) can easily be inserted anywhere that's needed.; // - The tracking of miss-speculation is done by using a data-flow conditional; // select instruction (CSEL) to evaluate the flags that were also used to; // make conditional branch direction decisions. Speculation of the CSEL; // instruction can be limited with a CSDB instruction - so the combination of; // CSEL + a later CSDB gives the guarantee that the flags as used in the CSEL; // aren't speculated. When conditional branch direction gets miss-speculated,; // the semantics of the inserted CSEL instruction is such that the taint; // register will contain all zero bits.; // One key requirement for this to work is that the conditional branch is; // followed by an execution of the CSEL instruction, where the CSEL; // instruction needs to use the sa",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:901,Performance,load,load,901,"//===- AArch64SpeculationHardening.cpp - Harden Against Missspeculation --===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass to insert code to mitigate against side channel; // vulnerabilities that may happen under control flow miss-speculation.; //; // The pass implements tracking of control flow miss-speculation into a ""taint""; // register. That taint register can then be used to mask off registers with; // sensitive data when executing under miss-speculation, a.k.a. ""transient; // execution"".; // This pass is aimed at mitigating against SpectreV1-style vulnarabilities.; //; // It also implements speculative load hardening, i.e. using the taint register; // to automatically mask off loaded data.; //; // As a possible follow-on improvement, also an intrinsics-based approach as; // explained at https://lwn.net/Articles/759423/ could be implemented on top of; // the current design.; //; // For AArch64, the following implementation choices are made to implement the; // tracking of control flow miss-speculation into a taint register:; // Some of these are different than the implementation choices made in; // the similar pass implemented in X86SpeculativeLoadHardening.cpp, as; // the instruction set characteristics result in different trade-offs.; // - The speculation hardening is done after register allocation. With a; // relative abundance of registers, one register is reserved (X16) to be; // the taint register. X16 is expected to not clash with other register; // reservation mechanisms with very high probability because:; // . The AArch64 ABI doesn't guarantee X16 to be retained across any call.; // . The only way to request X16 to be used as a programmer is through; // inline assembly. In",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:977,Performance,load,loaded,977,"//===- AArch64SpeculationHardening.cpp - Harden Against Missspeculation --===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass to insert code to mitigate against side channel; // vulnerabilities that may happen under control flow miss-speculation.; //; // The pass implements tracking of control flow miss-speculation into a ""taint""; // register. That taint register can then be used to mask off registers with; // sensitive data when executing under miss-speculation, a.k.a. ""transient; // execution"".; // This pass is aimed at mitigating against SpectreV1-style vulnarabilities.; //; // It also implements speculative load hardening, i.e. using the taint register; // to automatically mask off loaded data.; //; // As a possible follow-on improvement, also an intrinsics-based approach as; // explained at https://lwn.net/Articles/759423/ could be implemented on top of; // the current design.; //; // For AArch64, the following implementation choices are made to implement the; // tracking of control flow miss-speculation into a taint register:; // Some of these are different than the implementation choices made in; // the similar pass implemented in X86SpeculativeLoadHardening.cpp, as; // the instruction set characteristics result in different trade-offs.; // - The speculation hardening is done after register allocation. With a; // relative abundance of registers, one register is reserved (X16) to be; // the taint register. X16 is expected to not clash with other register; // reservation mechanisms with very high probability because:; // . The AArch64 ABI doesn't guarantee X16 to be retained across any call.; // . The only way to request X16 to be used as a programmer is through; // inline assembly. In",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:4038,Performance,load,loads,4038,"SEL; // aren't speculated. When conditional branch direction gets miss-speculated,; // the semantics of the inserted CSEL instruction is such that the taint; // register will contain all zero bits.; // One key requirement for this to work is that the conditional branch is; // followed by an execution of the CSEL instruction, where the CSEL; // instruction needs to use the same flags status as the conditional branch.; // This means that the conditional branches must not be implemented as one; // of the AArch64 conditional branches that do not use the flags as input; // (CB(N)Z and TB(N)Z). This is implemented by ensuring in the instruction; // selectors to not produce these instructions when speculation hardening; // is enabled. This pass will assert if it does encounter such an instruction.; // - On function call boundaries, the miss-speculation state is transferred from; // the taint register X16 to be encoded in the SP register as value 0.; //; // For the aspect of automatically hardening loads, using the taint register,; // (a.k.a. speculative load hardening, see; // https://llvm.org/docs/SpeculativeLoadHardening.html), the following; // implementation choices are made for AArch64:; // - Many of the optimizations described at; // https://llvm.org/docs/SpeculativeLoadHardening.html to harden fewer; // loads haven't been implemented yet - but for some of them there are; // FIXMEs in the code.; // - loads that load into general purpose (X or W) registers get hardened by; // masking the loaded data. For loads that load into other registers, the; // address loaded from gets hardened. It is expected that hardening the; // loaded data may be more efficient; but masking data in registers other; // than X or W is not easy and may result in being slower than just; // hardening the X address register loaded from.; // - On AArch64, CSDB instructions are inserted between the masking of the; // register and its first use, to ensure there's no non-control-flow; // speculation th",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:4095,Performance,load,load,4095,"ulated,; // the semantics of the inserted CSEL instruction is such that the taint; // register will contain all zero bits.; // One key requirement for this to work is that the conditional branch is; // followed by an execution of the CSEL instruction, where the CSEL; // instruction needs to use the same flags status as the conditional branch.; // This means that the conditional branches must not be implemented as one; // of the AArch64 conditional branches that do not use the flags as input; // (CB(N)Z and TB(N)Z). This is implemented by ensuring in the instruction; // selectors to not produce these instructions when speculation hardening; // is enabled. This pass will assert if it does encounter such an instruction.; // - On function call boundaries, the miss-speculation state is transferred from; // the taint register X16 to be encoded in the SP register as value 0.; //; // For the aspect of automatically hardening loads, using the taint register,; // (a.k.a. speculative load hardening, see; // https://llvm.org/docs/SpeculativeLoadHardening.html), the following; // implementation choices are made for AArch64:; // - Many of the optimizations described at; // https://llvm.org/docs/SpeculativeLoadHardening.html to harden fewer; // loads haven't been implemented yet - but for some of them there are; // FIXMEs in the code.; // - loads that load into general purpose (X or W) registers get hardened by; // masking the loaded data. For loads that load into other registers, the; // address loaded from gets hardened. It is expected that hardening the; // loaded data may be more efficient; but masking data in registers other; // than X or W is not easy and may result in being slower than just; // hardening the X address register loaded from.; // - On AArch64, CSDB instructions are inserted between the masking of the; // register and its first use, to ensure there's no non-control-flow; // speculation that might undermine the hardening mechanism.; //; // Future extensions/impr",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:4254,Performance,optimiz,optimizations,4254,"/ One key requirement for this to work is that the conditional branch is; // followed by an execution of the CSEL instruction, where the CSEL; // instruction needs to use the same flags status as the conditional branch.; // This means that the conditional branches must not be implemented as one; // of the AArch64 conditional branches that do not use the flags as input; // (CB(N)Z and TB(N)Z). This is implemented by ensuring in the instruction; // selectors to not produce these instructions when speculation hardening; // is enabled. This pass will assert if it does encounter such an instruction.; // - On function call boundaries, the miss-speculation state is transferred from; // the taint register X16 to be encoded in the SP register as value 0.; //; // For the aspect of automatically hardening loads, using the taint register,; // (a.k.a. speculative load hardening, see; // https://llvm.org/docs/SpeculativeLoadHardening.html), the following; // implementation choices are made for AArch64:; // - Many of the optimizations described at; // https://llvm.org/docs/SpeculativeLoadHardening.html to harden fewer; // loads haven't been implemented yet - but for some of them there are; // FIXMEs in the code.; // - loads that load into general purpose (X or W) registers get hardened by; // masking the loaded data. For loads that load into other registers, the; // address loaded from gets hardened. It is expected that hardening the; // loaded data may be more efficient; but masking data in registers other; // than X or W is not easy and may result in being slower than just; // hardening the X address register loaded from.; // - On AArch64, CSDB instructions are inserted between the masking of the; // register and its first use, to ensure there's no non-control-flow; // speculation that might undermine the hardening mechanism.; //; // Future extensions/improvements could be:; // - Implement this functionality using full speculation barriers, akin to the; // x86-slh-lfence option.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:4357,Performance,load,loads,4357,"needs to use the same flags status as the conditional branch.; // This means that the conditional branches must not be implemented as one; // of the AArch64 conditional branches that do not use the flags as input; // (CB(N)Z and TB(N)Z). This is implemented by ensuring in the instruction; // selectors to not produce these instructions when speculation hardening; // is enabled. This pass will assert if it does encounter such an instruction.; // - On function call boundaries, the miss-speculation state is transferred from; // the taint register X16 to be encoded in the SP register as value 0.; //; // For the aspect of automatically hardening loads, using the taint register,; // (a.k.a. speculative load hardening, see; // https://llvm.org/docs/SpeculativeLoadHardening.html), the following; // implementation choices are made for AArch64:; // - Many of the optimizations described at; // https://llvm.org/docs/SpeculativeLoadHardening.html to harden fewer; // loads haven't been implemented yet - but for some of them there are; // FIXMEs in the code.; // - loads that load into general purpose (X or W) registers get hardened by; // masking the loaded data. For loads that load into other registers, the; // address loaded from gets hardened. It is expected that hardening the; // loaded data may be more efficient; but masking data in registers other; // than X or W is not easy and may result in being slower than just; // hardening the X address register loaded from.; // - On AArch64, CSDB instructions are inserted between the masking of the; // register and its first use, to ensure there's no non-control-flow; // speculation that might undermine the hardening mechanism.; //; // Future extensions/improvements could be:; // - Implement this functionality using full speculation barriers, akin to the; // x86-slh-lfence option. This may be more useful for the intrinsics-based; // approach than for the SLH approach to masking.; // Note that this pass already inserts the full speculat",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:4455,Performance,load,loads,4455," not be implemented as one; // of the AArch64 conditional branches that do not use the flags as input; // (CB(N)Z and TB(N)Z). This is implemented by ensuring in the instruction; // selectors to not produce these instructions when speculation hardening; // is enabled. This pass will assert if it does encounter such an instruction.; // - On function call boundaries, the miss-speculation state is transferred from; // the taint register X16 to be encoded in the SP register as value 0.; //; // For the aspect of automatically hardening loads, using the taint register,; // (a.k.a. speculative load hardening, see; // https://llvm.org/docs/SpeculativeLoadHardening.html), the following; // implementation choices are made for AArch64:; // - Many of the optimizations described at; // https://llvm.org/docs/SpeculativeLoadHardening.html to harden fewer; // loads haven't been implemented yet - but for some of them there are; // FIXMEs in the code.; // - loads that load into general purpose (X or W) registers get hardened by; // masking the loaded data. For loads that load into other registers, the; // address loaded from gets hardened. It is expected that hardening the; // loaded data may be more efficient; but masking data in registers other; // than X or W is not easy and may result in being slower than just; // hardening the X address register loaded from.; // - On AArch64, CSDB instructions are inserted between the masking of the; // register and its first use, to ensure there's no non-control-flow; // speculation that might undermine the hardening mechanism.; //; // Future extensions/improvements could be:; // - Implement this functionality using full speculation barriers, akin to the; // x86-slh-lfence option. This may be more useful for the intrinsics-based; // approach than for the SLH approach to masking.; // Note that this pass already inserts the full speculation barriers if the; // function for some niche reason makes use of X16/W16.; // - no indirect branch mispredict",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:4466,Performance,load,load,4466," not be implemented as one; // of the AArch64 conditional branches that do not use the flags as input; // (CB(N)Z and TB(N)Z). This is implemented by ensuring in the instruction; // selectors to not produce these instructions when speculation hardening; // is enabled. This pass will assert if it does encounter such an instruction.; // - On function call boundaries, the miss-speculation state is transferred from; // the taint register X16 to be encoded in the SP register as value 0.; //; // For the aspect of automatically hardening loads, using the taint register,; // (a.k.a. speculative load hardening, see; // https://llvm.org/docs/SpeculativeLoadHardening.html), the following; // implementation choices are made for AArch64:; // - Many of the optimizations described at; // https://llvm.org/docs/SpeculativeLoadHardening.html to harden fewer; // loads haven't been implemented yet - but for some of them there are; // FIXMEs in the code.; // - loads that load into general purpose (X or W) registers get hardened by; // masking the loaded data. For loads that load into other registers, the; // address loaded from gets hardened. It is expected that hardening the; // loaded data may be more efficient; but masking data in registers other; // than X or W is not easy and may result in being slower than just; // hardening the X address register loaded from.; // - On AArch64, CSDB instructions are inserted between the masking of the; // register and its first use, to ensure there's no non-control-flow; // speculation that might undermine the hardening mechanism.; //; // Future extensions/improvements could be:; // - Implement this functionality using full speculation barriers, akin to the; // x86-slh-lfence option. This may be more useful for the intrinsics-based; // approach than for the SLH approach to masking.; // Note that this pass already inserts the full speculation barriers if the; // function for some niche reason makes use of X16/W16.; // - no indirect branch mispredict",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:4543,Performance,load,loaded,4543," not be implemented as one; // of the AArch64 conditional branches that do not use the flags as input; // (CB(N)Z and TB(N)Z). This is implemented by ensuring in the instruction; // selectors to not produce these instructions when speculation hardening; // is enabled. This pass will assert if it does encounter such an instruction.; // - On function call boundaries, the miss-speculation state is transferred from; // the taint register X16 to be encoded in the SP register as value 0.; //; // For the aspect of automatically hardening loads, using the taint register,; // (a.k.a. speculative load hardening, see; // https://llvm.org/docs/SpeculativeLoadHardening.html), the following; // implementation choices are made for AArch64:; // - Many of the optimizations described at; // https://llvm.org/docs/SpeculativeLoadHardening.html to harden fewer; // loads haven't been implemented yet - but for some of them there are; // FIXMEs in the code.; // - loads that load into general purpose (X or W) registers get hardened by; // masking the loaded data. For loads that load into other registers, the; // address loaded from gets hardened. It is expected that hardening the; // loaded data may be more efficient; but masking data in registers other; // than X or W is not easy and may result in being slower than just; // hardening the X address register loaded from.; // - On AArch64, CSDB instructions are inserted between the masking of the; // register and its first use, to ensure there's no non-control-flow; // speculation that might undermine the hardening mechanism.; //; // Future extensions/improvements could be:; // - Implement this functionality using full speculation barriers, akin to the; // x86-slh-lfence option. This may be more useful for the intrinsics-based; // approach than for the SLH approach to masking.; // Note that this pass already inserts the full speculation barriers if the; // function for some niche reason makes use of X16/W16.; // - no indirect branch mispredict",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:4560,Performance,load,loads,4560,"input; // (CB(N)Z and TB(N)Z). This is implemented by ensuring in the instruction; // selectors to not produce these instructions when speculation hardening; // is enabled. This pass will assert if it does encounter such an instruction.; // - On function call boundaries, the miss-speculation state is transferred from; // the taint register X16 to be encoded in the SP register as value 0.; //; // For the aspect of automatically hardening loads, using the taint register,; // (a.k.a. speculative load hardening, see; // https://llvm.org/docs/SpeculativeLoadHardening.html), the following; // implementation choices are made for AArch64:; // - Many of the optimizations described at; // https://llvm.org/docs/SpeculativeLoadHardening.html to harden fewer; // loads haven't been implemented yet - but for some of them there are; // FIXMEs in the code.; // - loads that load into general purpose (X or W) registers get hardened by; // masking the loaded data. For loads that load into other registers, the; // address loaded from gets hardened. It is expected that hardening the; // loaded data may be more efficient; but masking data in registers other; // than X or W is not easy and may result in being slower than just; // hardening the X address register loaded from.; // - On AArch64, CSDB instructions are inserted between the masking of the; // register and its first use, to ensure there's no non-control-flow; // speculation that might undermine the hardening mechanism.; //; // Future extensions/improvements could be:; // - Implement this functionality using full speculation barriers, akin to the; // x86-slh-lfence option. This may be more useful for the intrinsics-based; // approach than for the SLH approach to masking.; // Note that this pass already inserts the full speculation barriers if the; // function for some niche reason makes use of X16/W16.; // - no indirect branch misprediction gets protected/instrumented; but this; // could be done for some indirect branches, such as ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:4571,Performance,load,load,4571,"input; // (CB(N)Z and TB(N)Z). This is implemented by ensuring in the instruction; // selectors to not produce these instructions when speculation hardening; // is enabled. This pass will assert if it does encounter such an instruction.; // - On function call boundaries, the miss-speculation state is transferred from; // the taint register X16 to be encoded in the SP register as value 0.; //; // For the aspect of automatically hardening loads, using the taint register,; // (a.k.a. speculative load hardening, see; // https://llvm.org/docs/SpeculativeLoadHardening.html), the following; // implementation choices are made for AArch64:; // - Many of the optimizations described at; // https://llvm.org/docs/SpeculativeLoadHardening.html to harden fewer; // loads haven't been implemented yet - but for some of them there are; // FIXMEs in the code.; // - loads that load into general purpose (X or W) registers get hardened by; // masking the loaded data. For loads that load into other registers, the; // address loaded from gets hardened. It is expected that hardening the; // loaded data may be more efficient; but masking data in registers other; // than X or W is not easy and may result in being slower than just; // hardening the X address register loaded from.; // - On AArch64, CSDB instructions are inserted between the masking of the; // register and its first use, to ensure there's no non-control-flow; // speculation that might undermine the hardening mechanism.; //; // Future extensions/improvements could be:; // - Implement this functionality using full speculation barriers, akin to the; // x86-slh-lfence option. This may be more useful for the intrinsics-based; // approach than for the SLH approach to masking.; // Note that this pass already inserts the full speculation barriers if the; // function for some niche reason makes use of X16/W16.; // - no indirect branch misprediction gets protected/instrumented; but this; // could be done for some indirect branches, such as ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:4614,Performance,load,loaded,4614,"input; // (CB(N)Z and TB(N)Z). This is implemented by ensuring in the instruction; // selectors to not produce these instructions when speculation hardening; // is enabled. This pass will assert if it does encounter such an instruction.; // - On function call boundaries, the miss-speculation state is transferred from; // the taint register X16 to be encoded in the SP register as value 0.; //; // For the aspect of automatically hardening loads, using the taint register,; // (a.k.a. speculative load hardening, see; // https://llvm.org/docs/SpeculativeLoadHardening.html), the following; // implementation choices are made for AArch64:; // - Many of the optimizations described at; // https://llvm.org/docs/SpeculativeLoadHardening.html to harden fewer; // loads haven't been implemented yet - but for some of them there are; // FIXMEs in the code.; // - loads that load into general purpose (X or W) registers get hardened by; // masking the loaded data. For loads that load into other registers, the; // address loaded from gets hardened. It is expected that hardening the; // loaded data may be more efficient; but masking data in registers other; // than X or W is not easy and may result in being slower than just; // hardening the X address register loaded from.; // - On AArch64, CSDB instructions are inserted between the masking of the; // register and its first use, to ensure there's no non-control-flow; // speculation that might undermine the hardening mechanism.; //; // Future extensions/improvements could be:; // - Implement this functionality using full speculation barriers, akin to the; // x86-slh-lfence option. This may be more useful for the intrinsics-based; // approach than for the SLH approach to masking.; // Note that this pass already inserts the full speculation barriers if the; // function for some niche reason makes use of X16/W16.; // - no indirect branch misprediction gets protected/instrumented; but this; // could be done for some indirect branches, such as ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:4679,Performance,load,loaded,4679," produce these instructions when speculation hardening; // is enabled. This pass will assert if it does encounter such an instruction.; // - On function call boundaries, the miss-speculation state is transferred from; // the taint register X16 to be encoded in the SP register as value 0.; //; // For the aspect of automatically hardening loads, using the taint register,; // (a.k.a. speculative load hardening, see; // https://llvm.org/docs/SpeculativeLoadHardening.html), the following; // implementation choices are made for AArch64:; // - Many of the optimizations described at; // https://llvm.org/docs/SpeculativeLoadHardening.html to harden fewer; // loads haven't been implemented yet - but for some of them there are; // FIXMEs in the code.; // - loads that load into general purpose (X or W) registers get hardened by; // masking the loaded data. For loads that load into other registers, the; // address loaded from gets hardened. It is expected that hardening the; // loaded data may be more efficient; but masking data in registers other; // than X or W is not easy and may result in being slower than just; // hardening the X address register loaded from.; // - On AArch64, CSDB instructions are inserted between the masking of the; // register and its first use, to ensure there's no non-control-flow; // speculation that might undermine the hardening mechanism.; //; // Future extensions/improvements could be:; // - Implement this functionality using full speculation barriers, akin to the; // x86-slh-lfence option. This may be more useful for the intrinsics-based; // approach than for the SLH approach to masking.; // Note that this pass already inserts the full speculation barriers if the; // function for some niche reason makes use of X16/W16.; // - no indirect branch misprediction gets protected/instrumented; but this; // could be done for some indirect branches, such as switch jump tables.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:4856,Performance,load,loaded,4856," produce these instructions when speculation hardening; // is enabled. This pass will assert if it does encounter such an instruction.; // - On function call boundaries, the miss-speculation state is transferred from; // the taint register X16 to be encoded in the SP register as value 0.; //; // For the aspect of automatically hardening loads, using the taint register,; // (a.k.a. speculative load hardening, see; // https://llvm.org/docs/SpeculativeLoadHardening.html), the following; // implementation choices are made for AArch64:; // - Many of the optimizations described at; // https://llvm.org/docs/SpeculativeLoadHardening.html to harden fewer; // loads haven't been implemented yet - but for some of them there are; // FIXMEs in the code.; // - loads that load into general purpose (X or W) registers get hardened by; // masking the loaded data. For loads that load into other registers, the; // address loaded from gets hardened. It is expected that hardening the; // loaded data may be more efficient; but masking data in registers other; // than X or W is not easy and may result in being slower than just; // hardening the X address register loaded from.; // - On AArch64, CSDB instructions are inserted between the masking of the; // register and its first use, to ensure there's no non-control-flow; // speculation that might undermine the hardening mechanism.; //; // Future extensions/improvements could be:; // - Implement this functionality using full speculation barriers, akin to the; // x86-slh-lfence option. This may be more useful for the intrinsics-based; // approach than for the SLH approach to masking.; // Note that this pass already inserts the full speculation barriers if the; // function for some niche reason makes use of X16/W16.; // - no indirect branch misprediction gets protected/instrumented; but this; // could be done for some indirect branches, such as switch jump tables.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:2409,Safety,detect,detected,2409,"e similar pass implemented in X86SpeculativeLoadHardening.cpp, as; // the instruction set characteristics result in different trade-offs.; // - The speculation hardening is done after register allocation. With a; // relative abundance of registers, one register is reserved (X16) to be; // the taint register. X16 is expected to not clash with other register; // reservation mechanisms with very high probability because:; // . The AArch64 ABI doesn't guarantee X16 to be retained across any call.; // . The only way to request X16 to be used as a programmer is through; // inline assembly. In the rare case a function explicitly demands to; // use X16/W16, this pass falls back to hardening against speculation; // by inserting a DSB SYS/ISB barrier pair which will prevent control; // flow speculation.; // - It is easy to insert mask operations at this late stage as we have; // mask operations available that don't set flags.; // - The taint variable contains all-ones when no miss-speculation is detected,; // and contains all-zeros when miss-speculation is detected. Therefore, when; // masking, an AND instruction (which only changes the register to be masked,; // no other side effects) can easily be inserted anywhere that's needed.; // - The tracking of miss-speculation is done by using a data-flow conditional; // select instruction (CSEL) to evaluate the flags that were also used to; // make conditional branch direction decisions. Speculation of the CSEL; // instruction can be limited with a CSDB instruction - so the combination of; // CSEL + a later CSDB gives the guarantee that the flags as used in the CSEL; // aren't speculated. When conditional branch direction gets miss-speculated,; // the semantics of the inserted CSEL instruction is such that the taint; // register will contain all zero bits.; // One key requirement for this to work is that the conditional branch is; // followed by an execution of the CSEL instruction, where the CSEL; // instruction needs to use the sa",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:2471,Safety,detect,detected,2471,"e similar pass implemented in X86SpeculativeLoadHardening.cpp, as; // the instruction set characteristics result in different trade-offs.; // - The speculation hardening is done after register allocation. With a; // relative abundance of registers, one register is reserved (X16) to be; // the taint register. X16 is expected to not clash with other register; // reservation mechanisms with very high probability because:; // . The AArch64 ABI doesn't guarantee X16 to be retained across any call.; // . The only way to request X16 to be used as a programmer is through; // inline assembly. In the rare case a function explicitly demands to; // use X16/W16, this pass falls back to hardening against speculation; // by inserting a DSB SYS/ISB barrier pair which will prevent control; // flow speculation.; // - It is easy to insert mask operations at this late stage as we have; // mask operations available that don't set flags.; // - The taint variable contains all-ones when no miss-speculation is detected,; // and contains all-zeros when miss-speculation is detected. Therefore, when; // masking, an AND instruction (which only changes the register to be masked,; // no other side effects) can easily be inserted anywhere that's needed.; // - The tracking of miss-speculation is done by using a data-flow conditional; // select instruction (CSEL) to evaluate the flags that were also used to; // make conditional branch direction decisions. Speculation of the CSEL; // instruction can be limited with a CSDB instruction - so the combination of; // CSEL + a later CSDB gives the guarantee that the flags as used in the CSEL; // aren't speculated. When conditional branch direction gets miss-speculated,; // the semantics of the inserted CSEL instruction is such that the taint; // register will contain all zero bits.; // One key requirement for this to work is that the conditional branch is; // followed by an execution of the CSEL instruction, where the CSEL; // instruction needs to use the sa",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:3785,Testability,assert,assert,3785," to; // make conditional branch direction decisions. Speculation of the CSEL; // instruction can be limited with a CSDB instruction - so the combination of; // CSEL + a later CSDB gives the guarantee that the flags as used in the CSEL; // aren't speculated. When conditional branch direction gets miss-speculated,; // the semantics of the inserted CSEL instruction is such that the taint; // register will contain all zero bits.; // One key requirement for this to work is that the conditional branch is; // followed by an execution of the CSEL instruction, where the CSEL; // instruction needs to use the same flags status as the conditional branch.; // This means that the conditional branches must not be implemented as one; // of the AArch64 conditional branches that do not use the flags as input; // (CB(N)Z and TB(N)Z). This is implemented by ensuring in the instruction; // selectors to not produce these instructions when speculation hardening; // is enabled. This pass will assert if it does encounter such an instruction.; // - On function call boundaries, the miss-speculation state is transferred from; // the taint register X16 to be encoded in the SP register as value 0.; //; // For the aspect of automatically hardening loads, using the taint register,; // (a.k.a. speculative load hardening, see; // https://llvm.org/docs/SpeculativeLoadHardening.html), the following; // implementation choices are made for AArch64:; // - Many of the optimizations described at; // https://llvm.org/docs/SpeculativeLoadHardening.html to harden fewer; // loads haven't been implemented yet - but for some of them there are; // FIXMEs in the code.; // - loads that load into general purpose (X or W) registers get hardened by; // masking the loaded data. For loads that load into other registers, the; // address loaded from gets hardened. It is expected that hardening the; // loaded data may be more efficient; but masking data in registers other; // than X or W is not easy and may result in being",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:215,Availability,avail,available,215,// Perform correct code generation around function calls and before returns.; // The below variables record the return/terminator instructions and the call; // instructions respectively; including which register is available as a; // temporary register just before the recorded instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:91,Modifiability,variab,variables,91,// Perform correct code generation around function calls and before returns.; // The below variables record the return/terminator instructions and the call; // instructions respectively; including which register is available as a; // temporary register just before the recorded instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:3,Performance,Perform,Perform,3,// Perform correct code generation around function calls and before returns.; // The below variables record the return/terminator instructions and the call; // instructions respectively; including which register is available as a; // temporary register just before the recorded instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:34,Availability,avail,available,34,"// if a temporary register is not available for at least one of the; // instructions for which we need to transfer taint to the stack pointer, we; // need to insert a full speculation barrier.; // TmpRegisterNotAvailableEverywhere tracks that condition.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:41,Availability,avail,available,41,// The RegScavenger represents registers available *after* the MI; // instruction pointed to by RS.getCurrentPosition().; // We need to have a register that is available *before* the MI is executed.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:160,Availability,avail,available,160,// The RegScavenger represents registers available *after* the MI; // instruction pointed to by RS.getCurrentPosition().; // We need to have a register that is available *before* the MI is executed.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:197,Energy Efficiency,schedul,scheduling,197,"// FIXME: The below just finds *a* unused register. Maybe code could be; // optimized more if this looks for the register that isn't used for the; // longest time around this place, to enable more scheduling freedom. Not; // sure if that would actually result in a big performance difference; // though. Maybe RegisterScavenger::findSurvivorBackwards has some logic; // already to do this - but it's unclear if that could easily be used here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:76,Performance,optimiz,optimized,76,"// FIXME: The below just finds *a* unused register. Maybe code could be; // optimized more if this looks for the register that isn't used for the; // longest time around this place, to enable more scheduling freedom. Not; // sure if that would actually result in a big performance difference; // though. Maybe RegisterScavenger::findSurvivorBackwards has some logic; // already to do this - but it's unclear if that could easily be used here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:269,Performance,perform,performance,269,"// FIXME: The below just finds *a* unused register. Maybe code could be; // optimized more if this looks for the register that isn't used for the; // longest time around this place, to enable more scheduling freedom. Not; // sure if that would actually result in a big performance difference; // though. Maybe RegisterScavenger::findSurvivorBackwards has some logic; // already to do this - but it's unclear if that could easily be used here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:360,Testability,log,logic,360,"// FIXME: The below just finds *a* unused register. Maybe code could be; // optimized more if this looks for the register that isn't used for the; // longest time around this place, to enable more scheduling freedom. Not; // sure if that would actually result in a big performance difference; // though. Maybe RegisterScavenger::findSurvivorBackwards has some logic; // already to do this - but it's unclear if that could easily be used here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:36,Availability,avail,available,36,"// When a temporary register is not available everywhere in this basic; // basic block where a propagate-taint-to-sp operation is needed, just; // emit a full speculation barrier at the start of this basic block, which; // renders the taint/speculation tracking in this basic block unnecessary.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:37,Safety,safe,safe,37,"// Make GPR register Reg speculation-safe by putting it through the; // SpeculationSafeValue pseudo instruction, if we can't prove that; // the value in the register has already been hardened.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:3,Performance,Load,Loads,3,"// Loads cannot directly load a value into the SP (nor WSP).; // Therefore, if Reg is SP or WSP, it is because the instruction loads from; // the stack through the stack pointer.; //; // Since the stack pointer is never dynamically controllable, don't harden it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:25,Performance,load,load,25,"// Loads cannot directly load a value into the SP (nor WSP).; // Therefore, if Reg is SP or WSP, it is because the instruction loads from; // the stack through the stack pointer.; //; // Since the stack pointer is never dynamically controllable, don't harden it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:127,Performance,load,loads,127,"// Loads cannot directly load a value into the SP (nor WSP).; // Therefore, if Reg is SP or WSP, it is because the instruction loads from; // the stack through the stack pointer.; //; // Since the stack pointer is never dynamically controllable, don't harden it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:15,Performance,load,loaded,15,// Only harden loaded values or addresses used in loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:50,Performance,load,loads,50,// Only harden loaded values or addresses used in loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:130,Availability,Mask,Masking,130,"// For general purpose register loads, harden the registers loaded into.; // For other loads, harden the address loaded from.; // Masking the loaded value is expected to result in less performance; // overhead, as the load can still execute speculatively in comparison to; // when the address loaded from gets masked. However, masking is only; // easy to do efficiently on GPR registers, so for loads into non-GPR; // registers (e.g. floating point loads), mask the address loaded from.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:310,Availability,mask,masked,310,"// For general purpose register loads, harden the registers loaded into.; // For other loads, harden the address loaded from.; // Masking the loaded value is expected to result in less performance; // overhead, as the load can still execute speculatively in comparison to; // when the address loaded from gets masked. However, masking is only; // easy to do efficiently on GPR registers, so for loads into non-GPR; // registers (e.g. floating point loads), mask the address loaded from.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:327,Availability,mask,masking,327,"// For general purpose register loads, harden the registers loaded into.; // For other loads, harden the address loaded from.; // Masking the loaded value is expected to result in less performance; // overhead, as the load can still execute speculatively in comparison to; // when the address loaded from gets masked. However, masking is only; // easy to do efficiently on GPR registers, so for loads into non-GPR; // registers (e.g. floating point loads), mask the address loaded from.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:457,Availability,mask,mask,457,"// For general purpose register loads, harden the registers loaded into.; // For other loads, harden the address loaded from.; // Masking the loaded value is expected to result in less performance; // overhead, as the load can still execute speculatively in comparison to; // when the address loaded from gets masked. However, masking is only; // easy to do efficiently on GPR registers, so for loads into non-GPR; // registers (e.g. floating point loads), mask the address loaded from.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:358,Energy Efficiency,efficient,efficiently,358,"// For general purpose register loads, harden the registers loaded into.; // For other loads, harden the address loaded from.; // Masking the loaded value is expected to result in less performance; // overhead, as the load can still execute speculatively in comparison to; // when the address loaded from gets masked. However, masking is only; // easy to do efficiently on GPR registers, so for loads into non-GPR; // registers (e.g. floating point loads), mask the address loaded from.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:32,Performance,load,loads,32,"// For general purpose register loads, harden the registers loaded into.; // For other loads, harden the address loaded from.; // Masking the loaded value is expected to result in less performance; // overhead, as the load can still execute speculatively in comparison to; // when the address loaded from gets masked. However, masking is only; // easy to do efficiently on GPR registers, so for loads into non-GPR; // registers (e.g. floating point loads), mask the address loaded from.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:60,Performance,load,loaded,60,"// For general purpose register loads, harden the registers loaded into.; // For other loads, harden the address loaded from.; // Masking the loaded value is expected to result in less performance; // overhead, as the load can still execute speculatively in comparison to; // when the address loaded from gets masked. However, masking is only; // easy to do efficiently on GPR registers, so for loads into non-GPR; // registers (e.g. floating point loads), mask the address loaded from.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:87,Performance,load,loads,87,"// For general purpose register loads, harden the registers loaded into.; // For other loads, harden the address loaded from.; // Masking the loaded value is expected to result in less performance; // overhead, as the load can still execute speculatively in comparison to; // when the address loaded from gets masked. However, masking is only; // easy to do efficiently on GPR registers, so for loads into non-GPR; // registers (e.g. floating point loads), mask the address loaded from.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:113,Performance,load,loaded,113,"// For general purpose register loads, harden the registers loaded into.; // For other loads, harden the address loaded from.; // Masking the loaded value is expected to result in less performance; // overhead, as the load can still execute speculatively in comparison to; // when the address loaded from gets masked. However, masking is only; // easy to do efficiently on GPR registers, so for loads into non-GPR; // registers (e.g. floating point loads), mask the address loaded from.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:142,Performance,load,loaded,142,"// For general purpose register loads, harden the registers loaded into.; // For other loads, harden the address loaded from.; // Masking the loaded value is expected to result in less performance; // overhead, as the load can still execute speculatively in comparison to; // when the address loaded from gets masked. However, masking is only; // easy to do efficiently on GPR registers, so for loads into non-GPR; // registers (e.g. floating point loads), mask the address loaded from.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:185,Performance,perform,performance,185,"// For general purpose register loads, harden the registers loaded into.; // For other loads, harden the address loaded from.; // Masking the loaded value is expected to result in less performance; // overhead, as the load can still execute speculatively in comparison to; // when the address loaded from gets masked. However, masking is only; // easy to do efficiently on GPR registers, so for loads into non-GPR; // registers (e.g. floating point loads), mask the address loaded from.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:218,Performance,load,load,218,"// For general purpose register loads, harden the registers loaded into.; // For other loads, harden the address loaded from.; // Masking the loaded value is expected to result in less performance; // overhead, as the load can still execute speculatively in comparison to; // when the address loaded from gets masked. However, masking is only; // easy to do efficiently on GPR registers, so for loads into non-GPR; // registers (e.g. floating point loads), mask the address loaded from.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:293,Performance,load,loaded,293,"// For general purpose register loads, harden the registers loaded into.; // For other loads, harden the address loaded from.; // Masking the loaded value is expected to result in less performance; // overhead, as the load can still execute speculatively in comparison to; // when the address loaded from gets masked. However, masking is only; // easy to do efficiently on GPR registers, so for loads into non-GPR; // registers (e.g. floating point loads), mask the address loaded from.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:395,Performance,load,loads,395,"// For general purpose register loads, harden the registers loaded into.; // For other loads, harden the address loaded from.; // Masking the loaded value is expected to result in less performance; // overhead, as the load can still execute speculatively in comparison to; // when the address loaded from gets masked. However, masking is only; // easy to do efficiently on GPR registers, so for loads into non-GPR; // registers (e.g. floating point loads), mask the address loaded from.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:449,Performance,load,loads,449,"// For general purpose register loads, harden the registers loaded into.; // For other loads, harden the address loaded from.; // Masking the loaded value is expected to result in less performance; // overhead, as the load can still execute speculatively in comparison to; // when the address loaded from gets masked. However, masking is only; // easy to do efficiently on GPR registers, so for loads into non-GPR; // registers (e.g. floating point loads), mask the address loaded from.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:474,Performance,load,loaded,474,"// For general purpose register loads, harden the registers loaded into.; // For other loads, harden the address loaded from.; // Masking the loaded value is expected to result in less performance; // overhead, as the load can still execute speculatively in comparison to; // when the address loaded from gets masked. However, masking is only; // easy to do efficiently on GPR registers, so for loads into non-GPR; // registers (e.g. floating point loads), mask the address loaded from.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:55,Availability,mask,mask,55,"// FIXME: it might be a worthwhile optimization to not mask loaded; // values if all the registers involved in address calculation are already; // hardened, leading to this load not able to execute on a miss-speculated; // path.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:35,Performance,optimiz,optimization,35,"// FIXME: it might be a worthwhile optimization to not mask loaded; // values if all the registers involved in address calculation are already; // hardened, leading to this load not able to execute on a miss-speculated; // path.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:60,Performance,load,loaded,60,"// FIXME: it might be a worthwhile optimization to not mask loaded; // values if all the registers involved in address calculation are already; // hardened, leading to this load not able to execute on a miss-speculated; // path.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:173,Performance,load,load,173,"// FIXME: it might be a worthwhile optimization to not mask loaded; // values if all the registers involved in address calculation are already; // hardened, leading to this load not able to execute on a miss-speculated; // path.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:188,Availability,mask,masked,188,// First remove registers from AlreadyMaskedRegisters if their value is; // updated by this instruction - it makes them contain a new value that is; // not guaranteed to already have been masked.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:76,Deployability,update,updated,76,// First remove registers from AlreadyMaskedRegisters if their value is; // updated by this instruction - it makes them contain a new value that is; // not guaranteed to already have been masked.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:10,Performance,load,loads,10,"// FIXME: loads from the stack with an immediate offset from the stack; // pointer probably shouldn't be hardened, which could result in a; // significant optimization. See section ""Dont check loads from; // compile-time constant stack offsets"", in; // https://llvm.org/docs/SpeculativeLoadHardening.html",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:155,Performance,optimiz,optimization,155,"// FIXME: loads from the stack with an immediate offset from the stack; // pointer probably shouldn't be hardened, which could result in a; // significant optimization. See section ""Dont check loads from; // compile-time constant stack offsets"", in; // https://llvm.org/docs/SpeculativeLoadHardening.html",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:194,Performance,load,loads,194,"// FIXME: loads from the stack with an immediate offset from the stack; // pointer probably shouldn't be hardened, which could result in a; // significant optimization. See section ""Dont check loads from; // compile-time constant stack offsets"", in; // https://llvm.org/docs/SpeculativeLoadHardening.html",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:10,Availability,mask,mask,10,// Do not mask a register that is not used further.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:167,Performance,optimiz,optimization,167,"// FIXME: For pre/post-increment addressing modes, the base register; // used in address calculation is also defined by this instruction.; // It might be a worthwhile optimization to not harden that; // base register increment/decrement when the increment/decrement is; // an immediate.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:8,Performance,load,loads,8,"// Some loads of floating point data have implicit defs/uses on a; // super register of that floating point data. Some examples:; // $s0 = LDRSui $sp, 22, implicit-def $q0; // $q0 = LD1i64 $q0, 1, renamable $x0; // We need to filter out these uses for non-GPR register which occur; // because the load partially fills a non-GPR register with the loaded; // data. Just skipping all non-GPR registers is safe (for now) as all; // AArch64 load instructions only use GPR registers to perform the; // address calculation. FIXME: However that might change once we can; // produce SVE gather instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:297,Performance,load,load,297,"// Some loads of floating point data have implicit defs/uses on a; // super register of that floating point data. Some examples:; // $s0 = LDRSui $sp, 22, implicit-def $q0; // $q0 = LD1i64 $q0, 1, renamable $x0; // We need to filter out these uses for non-GPR register which occur; // because the load partially fills a non-GPR register with the loaded; // data. Just skipping all non-GPR registers is safe (for now) as all; // AArch64 load instructions only use GPR registers to perform the; // address calculation. FIXME: However that might change once we can; // produce SVE gather instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:346,Performance,load,loaded,346,"// Some loads of floating point data have implicit defs/uses on a; // super register of that floating point data. Some examples:; // $s0 = LDRSui $sp, 22, implicit-def $q0; // $q0 = LD1i64 $q0, 1, renamable $x0; // We need to filter out these uses for non-GPR register which occur; // because the load partially fills a non-GPR register with the loaded; // data. Just skipping all non-GPR registers is safe (for now) as all; // AArch64 load instructions only use GPR registers to perform the; // address calculation. FIXME: However that might change once we can; // produce SVE gather instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:436,Performance,load,load,436,"// Some loads of floating point data have implicit defs/uses on a; // super register of that floating point data. Some examples:; // $s0 = LDRSui $sp, 22, implicit-def $q0; // $q0 = LD1i64 $q0, 1, renamable $x0; // We need to filter out these uses for non-GPR register which occur; // because the load partially fills a non-GPR register with the loaded; // data. Just skipping all non-GPR registers is safe (for now) as all; // AArch64 load instructions only use GPR registers to perform the; // address calculation. FIXME: However that might change once we can; // produce SVE gather instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:480,Performance,perform,perform,480,"// Some loads of floating point data have implicit defs/uses on a; // super register of that floating point data. Some examples:; // $s0 = LDRSui $sp, 22, implicit-def $q0; // $q0 = LD1i64 $q0, 1, renamable $x0; // We need to filter out these uses for non-GPR register which occur; // because the load partially fills a non-GPR register with the loaded; // data. Just skipping all non-GPR registers is safe (for now) as all; // AArch64 load instructions only use GPR registers to perform the; // address calculation. FIXME: However that might change once we can; // produce SVE gather instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:402,Safety,safe,safe,402,"// Some loads of floating point data have implicit defs/uses on a; // super register of that floating point data. Some examples:; // $s0 = LDRSui $sp, 22, implicit-def $q0; // $q0 = LD1i64 $q0, 1, renamable $x0; // We need to filter out these uses for non-GPR register which occur; // because the load partially fills a non-GPR register with the loaded; // data. Just skipping all non-GPR registers is safe (for now) as all; // AArch64 load instructions only use GPR registers to perform the; // address calculation. FIXME: However that might change once we can; // produce SVE gather instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:3,Availability,Mask,Mask,3,// Mask off with taint state.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:334,Availability,mask,masked,334,"// The following loop iterates over all instructions in the basic block,; // and performs 2 operations:; // 1. Insert a CSDB at this location if needed.; // 2. Expand the SpeculationSafeValuePseudo if the current instruction is; // one.; //; // The insertion of the CSDB is done as late as possible (i.e. just before; // the use of a masked register), in the hope that that will reduce the; // total number of CSDBs in a block when there are multiple masked registers; // in the block.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:451,Availability,mask,masked,451,"// The following loop iterates over all instructions in the basic block,; // and performs 2 operations:; // 1. Insert a CSDB at this location if needed.; // 2. Expand the SpeculationSafeValuePseudo if the current instruction is; // one.; //; // The insertion of the CSDB is done as late as possible (i.e. just before; // the use of a masked register), in the hope that that will reduce the; // total number of CSDBs in a block when there are multiple masked registers; // in the block.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:379,Energy Efficiency,reduce,reduce,379,"// The following loop iterates over all instructions in the basic block,; // and performs 2 operations:; // 1. Insert a CSDB at this location if needed.; // 2. Expand the SpeculationSafeValuePseudo if the current instruction is; // one.; //; // The insertion of the CSDB is done as late as possible (i.e. just before; // the use of a masked register), in the hope that that will reduce the; // total number of CSDBs in a block when there are multiple masked registers; // in the block.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:81,Performance,perform,performs,81,"// The following loop iterates over all instructions in the basic block,; // and performs 2 operations:; // 1. Insert a CSDB at this location if needed.; // 2. Expand the SpeculationSafeValuePseudo if the current instruction is; // one.; //; // The insertion of the CSDB is done as late as possible (i.e. just before; // the use of a masked register), in the hope that that will reduce the; // total number of CSDBs in a block when there are multiple masked registers; // in the block.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:85,Availability,mask,masked,85,// First check if a CSDB needs to be inserted due to earlier registers; // that were masked and that are used by the next instruction.; // Also emit the barrier on any potential control flow changes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTagging.cpp:167,Availability,avail,available,167,"// Look through 8-byte initializer list 16 bytes at a time;; // If one of the two 8-byte halfs is non-zero non-undef, emit STGP.; // Otherwise, emit zeroes up to next available item.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTagging.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTagging.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTagging.cpp:22,Deployability,update,update,22,"// memset(0) does not update Out[], therefore the tail can be either undef; // or zero.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTagging.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTagging.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTagging.cpp:63,Integrability,wrap,wrap,63,// Try sinking IRG as deep as possible to avoid hurting shrink wrap.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTagging.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTagging.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTagging.cpp:42,Safety,avoid,avoid,42,// Try sinking IRG as deep as possible to avoid hurting shrink wrap.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTagging.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTagging.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTaggingPreRA.cpp:513,Safety,safe,safe,513,"// This estimate can be improved if we had harder guarantees about stack frame; // layout. With LocalStackAllocation we can estimate SP offset to any; // preallocated slot. AArch64FrameLowering::orderFrameObjects could put tagged; // objects ahead of non-tagged ones, but that's not always desirable.; //; // Underestimating SP offset here may require the use of LDG to materialize; // the tagged address of the stack slot, along with a scratch register; // allocation (post-regalloc!).; //; // For now we do the safe thing here and require that the entire stack frame; // is within range of the shortest of the unchecked instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTaggingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTaggingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTaggingPreRA.cpp:105,Availability,avail,available,105,"// Pin one of the tagged slots to offset 0 from the tagged base pointer.; // This would make its address available in a virtual register (IRG's def), as; // opposed to requiring an ADDG instruction to materialize. This effectively; // eliminates a vreg (by replacing it with direct uses of IRG, which is usually; // live almost everywhere anyway), and therefore needs to happen before; // regalloc.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTaggingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTaggingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTaggingPreRA.cpp:493,Performance,Load,Load,493,"// Find the best (FI, Tag) pair to pin to offset 0.; // Looking at the possible uses of a tagged address, the advantage of pinning; // is:; // - COPY to physical register.; // Does not matter, this would trade a MOV instruction for an ADDG.; // - ST*G matter, but those mostly appear near the function prologue where all; // the tagged addresses need to be materialized anyway; also, counting ST*G; // uses would overweight large allocas that require more than one ST*G; // instruction.; // - Load/Store instructions in the address operand do not require a tagged; // pointer, so they also do not benefit. These operands have already been; // eliminated (see uncheckLoadsAndStores) so all remaining load/store; // instructions count.; // - Any other instruction may benefit from being pinned to offset 0.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTaggingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTaggingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTaggingPreRA.cpp:699,Performance,load,load,699,"// Find the best (FI, Tag) pair to pin to offset 0.; // Looking at the possible uses of a tagged address, the advantage of pinning; // is:; // - COPY to physical register.; // Does not matter, this would trade a MOV instruction for an ADDG.; // - ST*G matter, but those mostly appear near the function prologue where all; // the tagged addresses need to be materialized anyway; also, counting ST*G; // uses would overweight large allocas that require more than one ST*G; // instruction.; // - Load/Store instructions in the address operand do not require a tagged; // pointer, so they also do not benefit. These operands have already been; // eliminated (see uncheckLoadsAndStores) so all remaining load/store; // instructions count.; // - Any other instruction may benefit from being pinned to offset 0.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTaggingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTaggingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTaggingPreRA.cpp:122,Usability,simpl,simply,122,"// Swap tags between the victim and the highest scoring pair.; // If SwapWith is still (-1, -1), that's fine, too - we'll simply take tag for; // the highest score slot without changing anything else.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTaggingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTaggingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StorePairSuppress.cpp:520,Performance,load,loads,520,"//===--- AArch64StorePairSuppress.cpp --- Suppress store pair formation ---===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass identifies floating point stores that should not be combined into; // store pairs. Later we may do the same for floating point loads.; // ===---------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StorePairSuppress.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StorePairSuppress.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StorePairSuppress.cpp:386,Usability,simpl,simply,386,"/// Return true if an STP can be added to this block without increasing the; /// critical resource height. STP is good to form in Ld/St limited blocks and; /// bad to form in float-point limited blocks. This is true independent of the; /// critical path. If the critical path is longer than the resource height, the; /// extra vector ops can limit physreg renaming. Otherwise, it could simply; /// oversaturate the vector units.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StorePairSuppress.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StorePairSuppress.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StorePairSuppress.cpp:27,Energy Efficiency,schedul,scheduling,27,// Get the machine model's scheduling class for STPQi.; // Bypass TargetSchedule's SchedClass resolution since we only have an opcode.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StorePairSuppress.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StorePairSuppress.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StorePairSuppress.cpp:358,Performance,load,loads,358,"/// Return true if this is a floating-point store smaller than the V reg. On; /// cyclone, these require a vector shuffle before storing a pair.; /// Ideally we would call getMatchingPairOpcode() and have the machine model; /// tell us if it's profitable with no cpu knowledge here.; ///; /// FIXME: We plan to develop a decent Target abstraction for simple loads and; /// stores. Until then use a nasty switch similar to AArch64LoadStoreOptimizer.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StorePairSuppress.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StorePairSuppress.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StorePairSuppress.cpp:351,Usability,simpl,simple,351,"/// Return true if this is a floating-point store smaller than the V reg. On; /// cyclone, these require a vector shuffle before storing a pair.; /// Ideally we would call getMatchingPairOpcode() and have the machine model; /// tell us if it's profitable with no cpu knowledge here.; ///; /// FIXME: We plan to develop a decent Target abstraction for simple loads and; /// stores. Until then use a nasty switch similar to AArch64LoadStoreOptimizer.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StorePairSuppress.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StorePairSuppress.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StorePairSuppress.cpp:221,Safety,avoid,avoid,221,// Check for a sequence of stores to the same base address. We don't need to; // precisely determine whether a store pair can be formed. But we do want to; // filter out most situations where we can't form store pairs to avoid; // computing trace metrics in those cases.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StorePairSuppress.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StorePairSuppress.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp:46,Performance,perform,performance,46,// FIXME: remove this to enable 64-bit SLP if performance looks good.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp:46,Performance,perform,performance,46,// FIXME: remove this to enable 64-bit SLP if performance looks good.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp:46,Performance,perform,performance,46,// FIXME: remove this to enable 64-bit SLP if performance looks good.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp:46,Performance,perform,performance,46,// FIXME: remove this to enable 64-bit SLP if performance looks good.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp:46,Performance,perform,performance,46,// FIXME: remove this to enable 64-bit SLP if performance looks good.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp:46,Performance,perform,performance,46,// FIXME: remove this to enable 64-bit SLP if performance looks good.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp:44,Usability,simpl,simply,44,"// MachO large model always goes via a GOT, simply to get a single 8-byte; // absolute relocation on all global addresses.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp:117,Performance,load,loader,117,// All globals dynamically protected by MTE must have their address tags; // synthesized. This is done by having the loader stash the tag in the GOT; // entry. Force all tagged globals (even ones with internal linkage) through; // the GOT.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp:33,Security,access,accesses,33,"// The small code model's direct accesses use ADRP, which cannot; // necessarily produce the value 0 (if the code is above 4GB).; // Same for the tiny code model, where we have a pc relative LDR.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp:86,Availability,avail,available,86,"// MachO large model always goes via a GOT, because we don't have the; // relocations available to do anything else..",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp:48,Availability,avail,available,48,// NonLazyBind goes via GOT unless we know it's available locally.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp:92,Energy Efficiency,schedul,scheduling,92,// LNT run (at least on Cyclone) showed reasonably significant gains for; // bi-directional scheduling. 253.perlbmk.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp:29,Performance,latency,latency,29,"// Enabling or Disabling the latency heuristic is a close call: It seems to; // help nearly no benchmark on out-of-order architectures, on the other hand; // it regresses register pressure on a few benchmarking.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp:95,Testability,benchmark,benchmark,95,"// Enabling or Disabling the latency heuristic is a close call: It seems to; // help nearly no benchmark on out-of-order architectures, on the other hand; // it regresses register pressure on a few benchmarking.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp:198,Testability,benchmark,benchmarking,198,"// Enabling or Disabling the latency heuristic is a close call: It seems to; // help nearly no benchmark on out-of-order architectures, on the other hand; // it regresses register pressure on a few benchmarking.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp:571,Availability,failure,failure,571,"// If return address signing is enabled, tail calls are emitted as follows:; //; // ```; // <authenticate LR>; // <check LR>; // TCRETURN ; the callee may sign and spill the LR in its prologue; // ```; //; // LR may require explicit checking because if FEAT_FPAC is not implemented; // and LR was tampered with, then `<authenticate LR>` will not generate an; // exception on its own. Later, if the callee spills the signed LR value and; // neither FEAT_PAuth2 nor FEAT_EPAC are implemented, the valid PAC replaces; // the higher bits of LR thus hiding the authentication failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp:93,Security,authenticat,authenticate,93,"// If return address signing is enabled, tail calls are emitted as follows:; //; // ```; // <authenticate LR>; // <check LR>; // TCRETURN ; the callee may sign and spill the LR in its prologue; // ```; //; // LR may require explicit checking because if FEAT_FPAC is not implemented; // and LR was tampered with, then `<authenticate LR>` will not generate an; // exception on its own. Later, if the callee spills the signed LR value and; // neither FEAT_PAuth2 nor FEAT_EPAC are implemented, the valid PAC replaces; // the higher bits of LR thus hiding the authentication failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp:319,Security,authenticat,authenticate,319,"// If return address signing is enabled, tail calls are emitted as follows:; //; // ```; // <authenticate LR>; // <check LR>; // TCRETURN ; the callee may sign and spill the LR in its prologue; // ```; //; // LR may require explicit checking because if FEAT_FPAC is not implemented; // and LR was tampered with, then `<authenticate LR>` will not generate an; // exception on its own. Later, if the callee spills the signed LR value and; // neither FEAT_PAuth2 nor FEAT_EPAC are implemented, the valid PAC replaces; // the higher bits of LR thus hiding the authentication failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp:556,Security,authenticat,authentication,556,"// If return address signing is enabled, tail calls are emitted as follows:; //; // ```; // <authenticate LR>; // <check LR>; // TCRETURN ; the callee may sign and spill the LR in its prologue; // ```; //; // LR may require explicit checking because if FEAT_FPAC is not implemented; // and LR was tampered with, then `<authenticate LR>` will not generate an; // exception on its own. Later, if the callee spills the signed LR value and; // neither FEAT_PAuth2 nor FEAT_EPAC are implemented, the valid PAC replaces; // the higher bits of LR thus hiding the authentication failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp:78,Performance,perform,performance,78,"// At now, use None by default because checks may introduce an unexpected; // performance regression or incompatibility with execute-only mappings.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h:36,Availability,avail,available,36,// ReserveXRegister[i] - X#i is not available as a general purpose register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h:41,Availability,avail,available,41,// ReserveXRegisterForRA[i] - X#i is not available for register allocator.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h:39,Safety,Avoid,Avoid,39,/// Returns ARM processor family.; /// Avoid this function! CPU specifics should be kept local to this class; /// and preferably modeled with SubtargetFeatures or properties in; /// initializeProperties().,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h:160,Safety,safe,safe,160,"// Don't assume any minimum vector size when PSTATE.SM may not be 0, because; // we don't yet support streaming-compatible codegen support that we trust; // is safe for functions that may be executed in streaming-SVE mode.; // By returning '0' here, we disable vectorization.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h:103,Availability,error,error,103,/// This function is design to compatible with the function def in other; /// targets and escape build error about the virtual function def in base; /// class TargetSubtargetInfo. Updeate me if AArch64 target need to use it.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h:56,Modifiability,extend,extended,56,"/// Return whether FrameLowering should always set the ""extended frame; /// present"" bit in FP, or set it based on a symbol in the runtime.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h:83,Modifiability,extend,extended,83,"// Older OS versions (particularly system unwinders) are confused by the; // Swift extended frame, so when building code that might be run on them we; // must dynamically query the concurrency library to determine whether; // extended frames should be flagged as present.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h:226,Modifiability,extend,extended,226,"// Older OS versions (particularly system unwinders) are confused by the; // Swift extended frame, so when building code that might be run on them we; // must dynamically query the concurrency library to determine whether; // extended frames should be flagged as present.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h:181,Performance,concurren,concurrency,181,"// Older OS versions (particularly system unwinders) are confused by the; // Swift extended frame, so when building code that might be run on them we; // must dynamically query the concurrency library to determine whether; // extended frames should be flagged as present.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h:47,Availability,avail,available,47,// Prefer NEON unless larger SVE registers are available.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h:42,Performance,perform,performing,42,/// Choose a method of checking LR before performing a tail call.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h:37,Performance,load,load,37,"/// Pseudo value representing memory load performed to check an address.; ///; /// This load operation is solely used for its side-effects: if the address; /// is not mapped (or not readable), it triggers CPU exception, otherwise; /// execution proceeds and the value is not used.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h:42,Performance,perform,performed,42,"/// Pseudo value representing memory load performed to check an address.; ///; /// This load operation is solely used for its side-effects: if the address; /// is not mapped (or not readable), it triggers CPU exception, otherwise; /// execution proceeds and the value is not used.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h:88,Performance,load,load,88,"/// Pseudo value representing memory load performed to check an address.; ///; /// This load operation is solely used for its side-effects: if the address; /// is not mapped (or not readable), it triggers CPU exception, otherwise; /// execution proceeds and the value is not used.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:109,Integrability,interface,interface,109,//===----------------------------------------------------------------------===//; // AArch64 Lowering public interface.; //===----------------------------------------------------------------------===//,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:3,Security,Sanitiz,Sanitize,3,// Sanitize user input in case of no asserts,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:37,Testability,assert,asserts,37,// Sanitize user input in case of no asserts,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:86,Integrability,depend,depend,86,// This needs to be done before we create a new subtarget since any; // creation will depend on the TM and the code generation flags on the; // function that reside in TargetOptions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:32,Deployability,Configurat,Configuration,32,/// AArch64 Code Generator Pass Configuration Options.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:32,Modifiability,Config,Configuration,32,/// AArch64 Code Generator Pass Configuration Options.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:172,Usability,simpl,simplify,172,"// Cmpxchg instructions are often used with a subsequent comparison to; // determine whether it succeeded. We can exploit existing control-flow in; // ldrex/strex loops to simplify this, but it needs tidying up.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:28,Security,access,accesses,28,// Match interleaved memory accesses to ldN/stN intrinsics.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:8,Deployability,Pipeline,Pipeline,8,// Pass Pipeline Configuration,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:17,Deployability,Configurat,Configuration,17,// Pass Pipeline Configuration,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:17,Modifiability,Config,Configuration,17,// Pass Pipeline Configuration,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:27,Integrability,depend,depends,27,"// FIXME: On AArch64, this depends on the type.; // Basically, the addressable offsets are up to 4095 * Ty.getSizeInBytes().; // and the offset has to be a multiple of the related size in bytes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:262,Safety,safe,safe,262,// Merging of extern globals is enabled by default on non-Mach-O as we; // expect it to be generally either beneficial or harmless. On Mach-O it; // is disabled as we emit the .subsections_via_symbols directive which; // means that merging extern globals is not safe.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:138,Performance,perform,performance,138,// FIXME: extern global merging is only enabled when we optimise for size; // because there are some regressions with it also enabled for performance.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:42,Security,access,accesses,42,"// For ELF, cleanup any local-dynamic TLS accesses (i.e. combine as many; // references to _TLS_MODULE_BASE_ as possible.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:10,Availability,redundant,redundant,10,// Remove redundant copy instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:10,Safety,redund,redundant,10,// Remove redundant copy instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:11,Performance,perform,performance,11,// Improve performance for some FP/SIMD code for A57.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:51,Energy Efficiency,schedul,scheduling,51,// Expand some pseudo instructions to allow proper scheduling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:7,Performance,load,load,7,// Use load/store pair instructions when possible.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:242,Safety,avoid,avoid,242,"// The AArch64SpeculationHardeningPass destroys dominator tree and natural; // loop info, which is needed for the FalkorHWPFFixPass and also later on.; // Therefore, run the AArch64SpeculationHardeningPass before the; // FalkorHWPFFixPass to avoid recomputing dominator tree and natural loop; // info.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:159,Performance,load,load,159,"// Machine Block Placement might have created new opportunities when run; // at O3, where the Tail Duplication Threshold is set to 4 instructions.; // Run the load/store optimizer once more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:170,Performance,optimiz,optimizer,170,"// Machine Block Placement might have created new opportunities when run; // at O3, where the Tail Duplication Threshold is set to 4 instructions.; // Run the load/store optimizer once more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.h:8,Deployability,Pipeline,Pipeline,8,// Pass Pipeline Configuration,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.h:17,Deployability,Configurat,Configuration,17,// Pass Pipeline Configuration,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.h:17,Modifiability,Config,Configuration,17,// Pass Pipeline Configuration,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetObjectFile.cpp:130,Modifiability,variab,variables,130,// AARCH64 ELF ABI does not define static relocation type for TLS offset; // within a module. Do not generate AT_location for TLS variables.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetObjectFile.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetObjectFile.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetObjectFile.cpp:86,Security,access,accessed,86,// AArch64 does not use section-relative relocations so any global symbol must; // be accessed via at least a linker-private symbol.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetObjectFile.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetObjectFile.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:426,Modifiability,variab,variable,426,"// These bitfields will only ever be set to something non-zero in operator=,; // when setting the -sve-tail-folding option. This option should always be of; // the form (default|simple|all|disable)[+(Flag1|Flag2|etc)], where here; // InitialBits is one of (disabled|all|simple). EnableBits represents; // additional flags we're enabling, and DisableBits for those flags we're; // disabling. The default flag is tracked in the variable NeedsDefault, since; // at the time of setting the option we may not know what the default value; // for the CPU is.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:178,Usability,simpl,simple,178,"// These bitfields will only ever be set to something non-zero in operator=,; // when setting the -sve-tail-folding option. This option should always be of; // the form (default|simple|all|disable)[+(Flag1|Flag2|etc)], where here; // InitialBits is one of (disabled|all|simple). EnableBits represents; // additional flags we're enabling, and DisableBits for those flags we're; // disabling. The default flag is tracked in the variable NeedsDefault, since; // at the time of setting the option we may not know what the default value; // for the CPU is.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:270,Usability,simpl,simple,270,"// These bitfields will only ever be set to something non-zero in operator=,; // when setting the -sve-tail-folding option. This option should always be of; // the form (default|simple|all|disable)[+(Flag1|Flag2|etc)], where here; // InitialBits is one of (disabled|all|simple). EnableBits represents; // additional flags we're enabling, and DisableBits for those flags we're; // disabling. The default flag is tracked in the variable NeedsDefault, since; // at the time of setting the option we may not know what the default value; // for the CPU is.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:67,Availability,error,error,67,// If the user explicitly sets -sve-tail-folding= then treat as an error.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:134,Performance,scalab,scalable,134,// Experimental option that will only be fully functional when the cost-model; // and code-generator have been changed to avoid using scalable vector; // instructions that are not legal in streaming SVE mode.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:122,Safety,avoid,avoid,122,// Experimental option that will only be fully functional when the cost-model; // and code-generator have been changed to avoid using scalable vector; // instructions that are not legal in streaming SVE mode.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:75,Integrability,interface,interface,75,"// When inlining, we should consider the body of the function, not the; // interface.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:463,Safety,safe,safely,463,// We need to ensure that argument promotion does not attempt to promote; // pointers to fixed-length vector types larger than 128 bits like; // <8 x float> (and pointers to aggregate types which have such fixed-length; // vector type members) into the values of the pointees. Such vector types; // are used for SVE VLS but there is no ABI for SVE VLS arguments and the; // backend cannot lower such value arguments. The 128-bit fixed-length SVE; // types can be safely treated as 128-bit NEON types and they cannot be; // distinguished in IR.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:717,Safety,avoid,avoid,717,"// This function calculates a penalty for executing Call in F.; //; // There are two ways this function can be called:; // (1) F:; // call from F -> G (the call here is Call); //; // For (1), Call.getCaller() == F, so it will always return a high cost if; // a streaming-mode change is required (thus promoting the need to inline the; // function); //; // (2) F:; // call from F -> G (the call here is not Call); // G:; // call from G -> H (the call here is Call); //; // For (2), if after inlining the body of G into F the call to H requires a; // streaming-mode change, and the call to G from F would also require a; // streaming-mode change, then there is benefit to do the streaming-mode; // change only once and avoid inlining of G into F.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:8,Modifiability,extend,extend,8,// Sign-extend all constants to a multiple of 64-bit.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:82,Modifiability,extend,extend,82,"// This is a base cost of 1 for the vadd, plus 3 extract shifts if we; // need to extend the type, as it uses shr(qadd(shl, shl)).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:80,Safety,avoid,avoid,80,"// Costs for both fshl & fshr are the same, so just pass Intrinsic::fshl; // to avoid having to duplicate the costs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:29,Availability,redundant,redundant,29,/// The function will remove redundant reinterprets casting in the presence; /// of the control flow,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:29,Safety,redund,redundant,29,/// The function will remove redundant reinterprets casting in the presence; /// of the control flow,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:304,Integrability,depend,depending,304,"// The SIMD&FP variant of CLAST[AB] is significantly faster than the scalar; // integer variant across a variety of micro-architectures. Replace scalar; // integer CLAST[AB] intrinsic with optimal SIMD&FP variant. A simple; // bitcast-to-fp + clast[ab] + bitcast-to-int will cost a cycle or two more; // depending on the micro-architecture, but has been observed as generally; // being faster, particularly when the CLAST[AB] op is a loop-carried; // dependency.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:451,Integrability,depend,dependency,451,"// The SIMD&FP variant of CLAST[AB] is significantly faster than the scalar; // integer variant across a variety of micro-architectures. Replace scalar; // integer CLAST[AB] intrinsic with optimal SIMD&FP variant. A simple; // bitcast-to-fp + clast[ab] + bitcast-to-int will cost a cycle or two more; // depending on the micro-architecture, but has been observed as generally; // being faster, particularly when the CLAST[AB] op is a loop-carried; // dependency.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:216,Usability,simpl,simple,216,"// The SIMD&FP variant of CLAST[AB] is significantly faster than the scalar; // integer variant across a variety of micro-architectures. Replace scalar; // integer CLAST[AB] intrinsic with optimal SIMD&FP variant. A simple; // bitcast-to-fp + clast[ab] + bitcast-to-int will cost a cycle or two more; // depending on the micro-architecture, but has been observed as generally; // being faster, particularly when the CLAST[AB] op is a loop-carried; // dependency.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:60,Performance,optimiz,optimizePTestInstr,60,"// Replace rdffr with predicated rdffr.z intrinsic, so that optimizePTestInstr; // can work with RDFFR_PP for ptest elimination.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:72,Performance,optimiz,optimizations,72,"// PTEST_<FIRST|LAST>(X, X) is equivalent to PTEST_ANY(X, X).; // Later optimizations prefer this form.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:90,Modifiability,rewrite,rewrite,90,"// Transform PTEST_ANY(X=OP(PG,...), X) -> PTEST_ANY(PG, X)).; // Later optimizations may rewrite sequence to use the flag-setting variant; // of instruction X to remove PTEST.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:72,Performance,optimiz,optimizations,72,"// Transform PTEST_ANY(X=OP(PG,...), X) -> PTEST_ANY(PG, X)).; // Later optimizations may rewrite sequence to use the flag-setting variant; // of instruction X to remove PTEST.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:131,Performance,optimiz,optimizations,131,// Stop the combine when the flags on the inputs differ in case dropping; // flags would lead to us missing out on more beneficial optimizations.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:48,Performance,scalab,scalable,48,// Bail due to missing support for ISD::STRICT_ scalable vector operations.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:3,Usability,Simpl,Simplify,3,// Simplify operations where predicate has all inactive lanes or try to replace; // with _u form when all lanes are active,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:28,Performance,optimiz,optimization,28,"// TODO: this is naive. The optimization is still valid if DupPg; // 'encompasses' OpPredicate, not only if they're the same predicate.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:41,Modifiability,extend,extend,41,// Hi = uunpkhi(splat(X)) --> Hi = splat(extend(X)); // Lo = uunpklo(splat(X)) --> Lo = splat(extend(X)),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:94,Modifiability,extend,extend,94,// Hi = uunpkhi(splat(X)) --> Hi = splat(extend(X)); // Lo = uunpklo(splat(X)) --> Lo = splat(extend(X)),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:115,Performance,optimiz,optimization,115,"// Convert sve_tbl(OpVal sve_dup_x(SplatValue)) to; // splat_vector(extractelement(OpVal, SplatValue)) for further optimization.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:24,Availability,mask,masked,24,// Contiguous gather => masked load.; // (sve.ld1.gather.index Mask BasePtr (sve.index IndexBase 1)); // => (masked.load (gep BasePtr IndexBase) Align Mask zeroinitializer),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:63,Availability,Mask,Mask,63,// Contiguous gather => masked load.; // (sve.ld1.gather.index Mask BasePtr (sve.index IndexBase 1)); // => (masked.load (gep BasePtr IndexBase) Align Mask zeroinitializer),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:109,Availability,mask,masked,109,// Contiguous gather => masked load.; // (sve.ld1.gather.index Mask BasePtr (sve.index IndexBase 1)); // => (masked.load (gep BasePtr IndexBase) Align Mask zeroinitializer),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:151,Availability,Mask,Mask,151,// Contiguous gather => masked load.; // (sve.ld1.gather.index Mask BasePtr (sve.index IndexBase 1)); // => (masked.load (gep BasePtr IndexBase) Align Mask zeroinitializer),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:31,Performance,load,load,31,// Contiguous gather => masked load.; // (sve.ld1.gather.index Mask BasePtr (sve.index IndexBase 1)); // => (masked.load (gep BasePtr IndexBase) Align Mask zeroinitializer),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:116,Performance,load,load,116,// Contiguous gather => masked load.; // (sve.ld1.gather.index Mask BasePtr (sve.index IndexBase 1)); // => (masked.load (gep BasePtr IndexBase) Align Mask zeroinitializer),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:25,Availability,mask,masked,25,// Contiguous scatter => masked store.; // (sve.st1.scatter.index Value Mask BasePtr (sve.index IndexBase 1)); // => (masked.store Value (gep BasePtr IndexBase) Align Mask),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:72,Availability,Mask,Mask,72,// Contiguous scatter => masked store.; // (sve.st1.scatter.index Value Mask BasePtr (sve.index IndexBase 1)); // => (masked.store Value (gep BasePtr IndexBase) Align Mask),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:118,Availability,mask,masked,118,// Contiguous scatter => masked store.; // (sve.st1.scatter.index Value Mask BasePtr (sve.index IndexBase 1)); // => (masked.store Value (gep BasePtr IndexBase) Align Mask),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:167,Availability,Mask,Mask,167,// Contiguous scatter => masked store.; // (sve.st1.scatter.index Value Mask BasePtr (sve.index IndexBase 1)); // => (masked.store Value (gep BasePtr IndexBase) Align Mask),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:10,Usability,simpl,simplify,10,"// Try to simplify dupqlane patterns like dupqlane(f32 A, f32 B, f32 A, f32 B); // to dupqlane(f64(C)) where C is A concatenated with B",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:15,Usability,simpl,simplified,15,"// Rebuild the simplified chain of InsertElements. e.g. (a, b, a, b) as (a, b)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:13,Usability,simpl,simplified,13,"// Splat the simplified sequence, e.g. (f16 a, f16 b, f16 c, f16 d) as one i64; // value or (f16 a, f16 b) as one i32 value. This requires an InsertSubvector; // be bitcast to a type wide enough to fit the sequence, be splatted, and then; // be narrowed back to the original type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:26,Usability,simpl,simpler,26,// Convert SRSHL into the simpler LSL intrinsic when fed by an ABS intrinsic.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:165,Modifiability,extend,extend,165,"// Exit early if DstTy is not a vector type whose elements are one of [i16,; // i32, i64]. SVE doesn't generally have the same set of instructions to; // perform an extend with the add/sub/mul. There are SMULLB style; // instructions, but they operate on top/bottom, requiring some sort of lane; // interleaving to be used with zext/sext.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:154,Performance,perform,perform,154,"// Exit early if DstTy is not a vector type whose elements are one of [i16,; // i32, i64]. SVE doesn't generally have the same set of instructions to; // perform an extend with the add/sub/mul. There are SMULLB style; // instructions, but they operate on top/bottom, requiring some sort of lane; // interleaving to be used with zext/sext.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:256,Modifiability,extend,extending,256,"// Determine if the operation has a widening variant. We consider both the; // ""long"" (e.g., usubl) and ""wide"" (e.g., usubw) versions of the; // instructions.; //; // TODO: Add additional widening operations (e.g., shl, etc.) once we; // verify that their extending operands are eliminated during code; // generation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:81,Modifiability,extend,extend,81,"// USUBL(2), SSUBL(2), USUBW(2), SSUBW(2).; // The second operand needs to be an extend",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:51,Modifiability,extend,extends,51,"// SMULL(2), UMULL(2); // Both operands need to be extends of the same type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:73,Modifiability,extend,extends,73,"// s/urhadd instructions implement the following pattern, making the; // extends free:; // %x = add ((zext i8 -> i16), 1); // %y = (zext i8 -> i16); // trunc i16 (lshr (add %x, %y), 1) -> i8; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:15,Modifiability,extend,extends,15,// Ensure both extends are of the same type,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:75,Modifiability,extend,extends,75,"// For adds only count the second operand as free if both operands are; // extends but not the same operation. (i.e both operands are not free in; // add(sext, zext)).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:19,Performance,throughput,throughput,19,// TODO: Allow non-throughput costs that aren't binary.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:3,Modifiability,Extend,Extend,3,// Extend from nxvmf16 to nxvmf32.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:3,Modifiability,Extend,Extend,3,// Extend from nxvmf16 to nxvmf64.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:3,Modifiability,Extend,Extend,3,// Extend from nxvmf32 to nxvmf64.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:16,Modifiability,extend,extending,16,"// Add cost for extending to illegal -too wide- scalable vectors.; // zero/sign extend are implemented by multiple unpack operations,; // where each operation has a cost of 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:80,Modifiability,extend,extend,80,"// Add cost for extending to illegal -too wide- scalable vectors.; // zero/sign extend are implemented by multiple unpack operations,; // where each operation has a cost of 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:48,Performance,scalab,scalable,48,"// Add cost for extending to illegal -too wide- scalable vectors.; // zero/sign extend are implemented by multiple unpack operations,; // where each operation has a cost of 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:139,Availability,mask,masked,139,// The standard behaviour in the backend for these cases is to split the; // extend up into two parts:; // 1. Perform an extending load or masked load up to the legal type.; // 2. Extend the loaded data to the final type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:77,Modifiability,extend,extend,77,// The standard behaviour in the backend for these cases is to split the; // extend up into two parts:; // 1. Perform an extending load or masked load up to the legal type.; // 2. Extend the loaded data to the final type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:121,Modifiability,extend,extending,121,// The standard behaviour in the backend for these cases is to split the; // extend up into two parts:; // 1. Perform an extending load or masked load up to the legal type.; // 2. Extend the loaded data to the final type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:180,Modifiability,Extend,Extend,180,// The standard behaviour in the backend for these cases is to split the; // extend up into two parts:; // 1. Perform an extending load or masked load up to the legal type.; // 2. Extend the loaded data to the final type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:110,Performance,Perform,Perform,110,// The standard behaviour in the backend for these cases is to split the; // extend up into two parts:; // 1. Perform an extending load or masked load up to the legal type.; // 2. Extend the loaded data to the final type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:131,Performance,load,load,131,// The standard behaviour in the backend for these cases is to split the; // extend up into two parts:; // 1. Perform an extending load or masked load up to the legal type.; // 2. Extend the loaded data to the final type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:146,Performance,load,load,146,// The standard behaviour in the backend for these cases is to split the; // extend up into two parts:; // 1. Perform an extending load or masked load up to the legal type.; // 2. Extend the loaded data to the final type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:191,Performance,load,loaded,191,// The standard behaviour in the backend for these cases is to split the; // extend up into two parts:; // 1. Perform an extending load or masked load up to the legal type.; // 2. Extend the loaded data to the final type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:137,Availability,Mask,Masked,137,"// The BasicTTIImpl version only deals with CCH==TTI::CastContextHint::Normal,; // but we also want to include the TTI::CastContextHint::Masked case too.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:35,Modifiability,extend,extend,35,// Make sure we were given a valid extend opcode.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:10,Modifiability,extend,extending,10,"// We are extending an element we extract from a vector, so the source type; // of the extend is the element type of the vector.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:87,Modifiability,extend,extend,87,"// We are extending an element we extract from a vector, so the source type; // of the extend is the element type of the vector.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:18,Modifiability,extend,extends,18,// Sign- and zero-extends are for integer types only.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:70,Modifiability,extend,extend,70,// Get the cost for the extract. We compute the cost (if any) for the extend; // below.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:158,Modifiability,extend,extend,158,"// If the resulting type is still a vector and the destination type is legal,; // we may get the extension for free. If not, get the default cost for the; // extend.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:105,Modifiability,extend,extend,105,"// The destination type should be larger than the element type. If not, get; // the default cost for the extend.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:12,Modifiability,extend,extends,12,"// For sign-extends, we only need a smov, which performs the extension; // automatically.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:48,Performance,perform,performs,48,"// For sign-extends, we only need a smov, which performs the extension; // automatically.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:12,Modifiability,extend,extends,12,"// For zero-extends, the extend is performed automatically by a umov unless; // the destination type is i64 and the element type is i8 or i16.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:25,Modifiability,extend,extend,25,"// For zero-extends, the extend is performed automatically by a umov unless; // the destination type is i64 and the element type is i8 or i16.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:35,Performance,perform,performed,35,"// For zero-extends, the extend is performed automatically by a umov unless; // the destination type is i64 and the element type is i8 or i16.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:35,Modifiability,extend,extend,35,"// If we are unable to perform the extend for free, get the default cost.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:23,Performance,perform,perform,23,"// If we are unable to perform the extend for free, get the default cost.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:30,Safety,predict,predicted,30,// Branches are assumed to be predicted.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:180,Performance,load,load,180,"// This is recognising a LD1 single-element structure to one lane of one; // register instruction. I.e., if this is an `insertelement` instruction,; // and its second operand is a load, then we will generate a LD1, which; // are expensive instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:82,Usability,simpl,simplified,82,"// FIXME:; // If the extract-element and insert-element instructions could be; // simplified away (e.g., could be combined into users by looking at use-def; // context), they have no cost. This is not done in the first place for; // compile-time considerations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:51,Energy Efficiency,power,power-of-two,51,"// On AArch64, scalar signed division by constants power-of-two are; // normally expanded to the sequence ADD + CMP + SELECT + SRA.; // The OperandValue properties many not be same as that of previous; // operation; conservatively assume OP_None.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:15,Availability,avail,available,15,"// When SVE is available, then we can lower the v2i64 operation using; // the SVE mul instruction, which has a lower cost.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:19,Availability,avail,available,19,"// When SVE is not available, there is no MUL.2d instruction,; // which means mul <2 x i64> is expensive as elements are extracted; // from the vectors and the muls scalarized.; // As getScalarizationOverhead is a bit too pessimistic, we; // estimate the cost for a i64 vector directly here, which is:; // - four 2-cost i64 extracts,; // - two 2-cost i64 inserts, and; // - two 1-cost muls.; // So, for a v2i64 with LT.First = 1 the cost is 14, and for a v4i64 with; // LT.first = 2 the cost is 28. If both operands are extensions it will not; // need to scalarize so the cost can be cheaper (smull or umull).; // so the cost can be cheaper (smull or umull).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:275,Performance,throughput,throughput,275,// Address computations in vectorized code with non-consecutive addresses will; // likely result in more instructions compared to scalar code where the; // computation can more often be merged into the index mode. The resulting; // extra micro-ops can significantly decrease throughput.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:25,Performance,scalab,scalable,25,"// The base case handles scalable vectors fine for now, since it treats the; // cost as 1 * legalization cost.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:56,Performance,load,loads,56,// TODO: Add cost modeling for strict align. Misaligned loads expand to; // a bunch of instructions when strict align is enabled.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:129,Energy Efficiency,power,power,129,"// TODO: Though vector loads usually perform well on AArch64, in some targets; // they may wake up the FP unit, which raises the power consumption. Perhaps; // they could be used with no holds barred (-O3).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:135,Energy Efficiency,consumption,consumption,135,"// TODO: Though vector loads usually perform well on AArch64, in some targets; // they may wake up the FP unit, which raises the power consumption. Perhaps; // they could be used with no holds barred (-O3).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:23,Performance,load,loads,23,"// TODO: Though vector loads usually perform well on AArch64, in some targets; // they may wake up the FP unit, which raises the power consumption. Perhaps; // they could be used with no holds barred (-O3).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:37,Performance,perform,perform,37,"// TODO: Though vector loads usually perform well on AArch64, in some targets; // they may wake up the FP unit, which raises the power consumption. Perhaps; // they could be used with no holds barred (-O3).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:242,Availability,reliab,reliable,242,"// The code-generator is currently not able to handle scalable vectors; // of <vscale x 1 x eltty> yet, so return an invalid cost to avoid selecting; // it. This change will be removed when code-generation for these types is; // sufficiently reliable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:54,Performance,scalab,scalable,54,"// The code-generator is currently not able to handle scalable vectors; // of <vscale x 1 x eltty> yet, so return an invalid cost to avoid selecting; // it. This change will be removed when code-generation for these types is; // sufficiently reliable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:133,Safety,avoid,avoid,133,"// The code-generator is currently not able to handle scalable vectors; // of <vscale x 1 x eltty> yet, so return an invalid cost to avoid selecting; // it. This change will be removed when code-generation for these types is; // sufficiently reliable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:242,Availability,reliab,reliable,242,"// The code-generator is currently not able to handle scalable vectors; // of <vscale x 1 x eltty> yet, so return an invalid cost to avoid selecting; // it. This change will be removed when code-generation for these types is; // sufficiently reliable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:54,Performance,scalab,scalable,54,"// The code-generator is currently not able to handle scalable vectors; // of <vscale x 1 x eltty> yet, so return an invalid cost to avoid selecting; // it. This change will be removed when code-generation for these types is; // sufficiently reliable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:133,Safety,avoid,avoid,133,"// The code-generator is currently not able to handle scalable vectors; // of <vscale x 1 x eltty> yet, so return an invalid cost to avoid selecting; // it. This change will be removed when code-generation for these types is; // sufficiently reliable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:242,Availability,reliab,reliable,242,"// The code-generator is currently not able to handle scalable vectors; // of <vscale x 1 x eltty> yet, so return an invalid cost to avoid selecting; // it. This change will be removed when code-generation for these types is; // sufficiently reliable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:54,Performance,scalab,scalable,54,"// The code-generator is currently not able to handle scalable vectors; // of <vscale x 1 x eltty> yet, so return an invalid cost to avoid selecting; // it. This change will be removed when code-generation for these types is; // sufficiently reliable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:133,Safety,avoid,avoid,133,"// The code-generator is currently not able to handle scalable vectors; // of <vscale x 1 x eltty> yet, so return an invalid cost to avoid selecting; // it. This change will be removed when code-generation for these types is; // sufficiently reliable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:18,Performance,latency,latency,18,// TODO: consider latency as well for TCK_SizeAndLatency.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:31,Modifiability,extend,extending,31,// Check truncating stores and extending loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:41,Performance,load,loads,41,// Check truncating stores and extending loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:38,Performance,load,load,38,// v4i8 types are lowered to scalar a load/store and sshll/xtn.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:125,Availability,down,down,125,"// Check non-power-of-2 loads/stores for legal vector element types with; // NEON. Non-power-of-2 memory ops will get broken down to a set of; // operations on smaller power-of-2 ops, including ld1/st1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:13,Energy Efficiency,power,power-of-,13,"// Check non-power-of-2 loads/stores for legal vector element types with; // NEON. Non-power-of-2 memory ops will get broken down to a set of; // operations on smaller power-of-2 ops, including ld1/st1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:87,Energy Efficiency,power,power-of-,87,"// Check non-power-of-2 loads/stores for legal vector element types with; // NEON. Non-power-of-2 memory ops will get broken down to a set of; // operations on smaller power-of-2 ops, including ld1/st1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:168,Energy Efficiency,power,power-of-,168,"// Check non-power-of-2 loads/stores for legal vector element types with; // NEON. Non-power-of-2 memory ops will get broken down to a set of; // operations on smaller power-of-2 ops, including ld1/st1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:24,Performance,load,loads,24,"// Check non-power-of-2 loads/stores for legal vector element types with; // NEON. Non-power-of-2 memory ops will get broken down to a set of; // operations on smaller power-of-2 ops, including ld1/st1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:21,Availability,mask,masked,21,// Vectorization for masked interleaved accesses is only enabled for scalable; // VF.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:69,Performance,scalab,scalable,69,// Vectorization for masked interleaved accesses is only enabled for scalable; // VF.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:40,Security,access,accesses,40,// Vectorization for masked interleaved accesses is only enabled for scalable; // VF.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:74,Security,Access,Accesses,74,// ldN/stN only support legal vector types of size 64 or 128 in bits.; // Accesses having vector types that are a multiple of 128 bits can be; // matched to more than one ldN/stN instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:56,Performance,load,loads,56,"// For Falkor, we want to avoid having too many strided loads in a loop since; // that can exhaust the HW prefetcher resources. We adjust the unroller; // MaxCount preference below to attempt to ensure unrolling doesn't create too; // many strided loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:248,Performance,load,loads,248,"// For Falkor, we want to avoid having too many strided loads in a loop since; // that can exhaust the HW prefetcher resources. We adjust the unroller; // MaxCount preference below to attempt to ensure unrolling doesn't create too; // many strided loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:26,Safety,avoid,avoid,26,"// For Falkor, we want to avoid having too many strided loads in a loop since; // that can exhaust the HW prefetcher resources. We adjust the unroller; // MaxCount preference below to attempt to ensure unrolling doesn't create too; // many strided loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:90,Performance,load,loads,90,// FIXME? We could make this more precise by looking at the CFG and; // e.g. not counting loads in each side of an if-then-else diamond.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:44,Performance,load,load,44,"// FIXME? We could take pairing of unrolled load copies into account; // by looking at the AddRec, but we would probably have to limit this; // to loops with no stores or other memory optimization barriers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:184,Performance,optimiz,optimization,184,"// FIXME? We could take pairing of unrolled load copies into account; // by looking at the AddRec, but we would probably have to limit this; // to loops with no stores or other memory optimization barriers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:29,Performance,load,loads,29,// We've seen enough strided loads that seeing more won't make a; // difference.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:20,Energy Efficiency,power,power,20,// Pick the largest power of 2 unroll count that won't result in too many; // strided loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:86,Performance,load,loads,86,// Pick the largest power of 2 unroll count that won't result in too many; // strided loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:270,Modifiability,extend,extended,270,"/// See if \p I should be considered for address type promotion. We check if \p; /// I is a sext with right type and used in memory accesses. If it used in a; /// ""complex"" getelementptr, we allow it to be promoted without finding other; /// sext instructions that sign extended the same initial value. A getelementptr; /// is considered as ""complex"" if it has more than 2 operands.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:132,Security,access,accesses,132,"/// See if \p I should be considered for address type promotion. We check if \p; /// I is a sext with right type and used in memory accesses. If it used in a; /// ""complex"" getelementptr, we allow it to be promoted without finding other; /// sext instructions that sign extended the same initial value. A getelementptr; /// is considered as ""complex"" if it has more than 2 operands.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:359,Energy Efficiency,reduce,reduce-or,359,"// Horizontal adds can use the 'addv' instruction. We model the cost of these; // instructions as twice a normal vector add, plus 1 for each legalization; // step (LT.first). This is the only arithmetic vector reduction operation for; // which we have an instruction.; // OR, XOR and AND costs should match the codegen from:; // OR: llvm/test/CodeGen/AArch64/reduce-or.ll; // XOR: llvm/test/CodeGen/AArch64/reduce-xor.ll; // AND: llvm/test/CodeGen/AArch64/reduce-and.ll",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:407,Energy Efficiency,reduce,reduce-xor,407,"// Horizontal adds can use the 'addv' instruction. We model the cost of these; // instructions as twice a normal vector add, plus 1 for each legalization; // step (LT.first). This is the only arithmetic vector reduction operation for; // which we have an instruction.; // OR, XOR and AND costs should match the codegen from:; // OR: llvm/test/CodeGen/AArch64/reduce-or.ll; // XOR: llvm/test/CodeGen/AArch64/reduce-xor.ll; // AND: llvm/test/CodeGen/AArch64/reduce-and.ll",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:456,Energy Efficiency,reduce,reduce-and,456,"// Horizontal adds can use the 'addv' instruction. We model the cost of these; // instructions as twice a normal vector add, plus 1 for each legalization; // step (LT.first). This is the only arithmetic vector reduction operation for; // which we have an instruction.; // OR, XOR and AND costs should match the codegen from:; // OR: llvm/test/CodeGen/AArch64/reduce-or.ll; // XOR: llvm/test/CodeGen/AArch64/reduce-xor.ll; // AND: llvm/test/CodeGen/AArch64/reduce-and.ll",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:338,Testability,test,test,338,"// Horizontal adds can use the 'addv' instruction. We model the cost of these; // instructions as twice a normal vector add, plus 1 for each legalization; // step (LT.first). This is the only arithmetic vector reduction operation for; // which we have an instruction.; // OR, XOR and AND costs should match the codegen from:; // OR: llvm/test/CodeGen/AArch64/reduce-or.ll; // XOR: llvm/test/CodeGen/AArch64/reduce-xor.ll; // AND: llvm/test/CodeGen/AArch64/reduce-and.ll",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:386,Testability,test,test,386,"// Horizontal adds can use the 'addv' instruction. We model the cost of these; // instructions as twice a normal vector add, plus 1 for each legalization; // step (LT.first). This is the only arithmetic vector reduction operation for; // which we have an instruction.; // OR, XOR and AND costs should match the codegen from:; // OR: llvm/test/CodeGen/AArch64/reduce-or.ll; // XOR: llvm/test/CodeGen/AArch64/reduce-xor.ll; // AND: llvm/test/CodeGen/AArch64/reduce-and.ll",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:435,Testability,test,test,435,"// Horizontal adds can use the 'addv' instruction. We model the cost of these; // instructions as twice a normal vector add, plus 1 for each legalization; // step (LT.first). This is the only arithmetic vector reduction operation for; // which we have an instruction.; // OR, XOR and AND costs should match the codegen from:; // OR: llvm/test/CodeGen/AArch64/reduce-or.ll; // XOR: llvm/test/CodeGen/AArch64/reduce-xor.ll; // AND: llvm/test/CodeGen/AArch64/reduce-and.ll",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:242,Availability,reliab,reliable,242,"// The code-generator is currently not able to handle scalable vectors; // of <vscale x 1 x eltty> yet, so return an invalid cost to avoid selecting; // it. This change will be removed when code-generation for these types is; // sufficiently reliable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:54,Performance,scalab,scalable,54,"// The code-generator is currently not able to handle scalable vectors; // of <vscale x 1 x eltty> yet, so return an invalid cost to avoid selecting; // it. This change will be removed when code-generation for these types is; // sufficiently reliable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:133,Safety,avoid,avoid,133,"// The code-generator is currently not able to handle scalable vectors; // of <vscale x 1 x eltty> yet, so return an invalid cost to avoid selecting; // it. This change will be removed when code-generation for these types is; // sufficiently reliable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:86,Performance,perform,performed,86,// Predicated splice are promoted when lowering. See AArch64ISelLowering.cpp; // Cost performed on a promoted type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:16,Availability,Mask,Mask,16,"// If we have a Mask, and the LT is being legalized somehow, split the Mask; // into smaller vectors and sum the cost of each shuffle.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:71,Availability,Mask,Mask,71,"// If we have a Mask, and the LT is being legalized somehow, split the Mask; // into smaller vectors and sum the cost of each shuffle.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:22,Availability,mask,mask,22,// Split the existing mask into chunks of size LTNumElts. Track the source; // sub-vectors to ensure the result has at most 2 inputs.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:18,Availability,mask,mask,18,"// Add to the new mask. For the NumSources>2 case these are not correct,; // but are only used for the modular lane number.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:14,Availability,mask,mask,14,// If the sub-mask has at most 2 input sub-vectors then re-cost it using; // getShuffleCost. If not then cost it using the worst case.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:23,Performance,load,loads,23,"// Check for broadcast loads, which are supported by the LD1R instruction.; // In terms of code-size, the shuffle vector is free when a load + dup get; // folded into a LD1R. That's what we check and return here. For performance; // and reciprocal throughput, a LD1R is not completely free. In this case, we; // return the cost for the broadcast below (i.e. 1 for most/all types), so; // that we model the load + dup sequence slightly higher because LD1R is a; // high latency instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:136,Performance,load,load,136,"// Check for broadcast loads, which are supported by the LD1R instruction.; // In terms of code-size, the shuffle vector is free when a load + dup get; // folded into a LD1R. That's what we check and return here. For performance; // and reciprocal throughput, a LD1R is not completely free. In this case, we; // return the cost for the broadcast below (i.e. 1 for most/all types), so; // that we model the load + dup sequence slightly higher because LD1R is a; // high latency instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:217,Performance,perform,performance,217,"// Check for broadcast loads, which are supported by the LD1R instruction.; // In terms of code-size, the shuffle vector is free when a load + dup get; // folded into a LD1R. That's what we check and return here. For performance; // and reciprocal throughput, a LD1R is not completely free. In this case, we; // return the cost for the broadcast below (i.e. 1 for most/all types), so; // that we model the load + dup sequence slightly higher because LD1R is a; // high latency instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:248,Performance,throughput,throughput,248,"// Check for broadcast loads, which are supported by the LD1R instruction.; // In terms of code-size, the shuffle vector is free when a load + dup get; // folded into a LD1R. That's what we check and return here. For performance; // and reciprocal throughput, a LD1R is not completely free. In this case, we; // return the cost for the broadcast below (i.e. 1 for most/all types), so; // that we model the load + dup sequence slightly higher because LD1R is a; // high latency instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:406,Performance,load,load,406,"// Check for broadcast loads, which are supported by the LD1R instruction.; // In terms of code-size, the shuffle vector is free when a load + dup get; // folded into a LD1R. That's what we check and return here. For performance; // and reciprocal throughput, a LD1R is not completely free. In this case, we; // return the cost for the broadcast below (i.e. 1 for most/all types), so; // that we model the load + dup sequence slightly higher because LD1R is a; // high latency instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:469,Performance,latency,latency,469,"// Check for broadcast loads, which are supported by the LD1R instruction.; // In terms of code-size, the shuffle vector is free when a load + dup get; // folded into a LD1R. That's what we check and return here. For performance; // and reciprocal throughput, a LD1R is not completely free. In this case, we; // return the cost for the broadcast below (i.e. 1 for most/all types), so; // that we model the load + dup sequence slightly higher because LD1R is a; // high latency instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:47,Availability,Mask,Mask,47,"// If we have 4 elements for the shuffle and a Mask, get the cost straight; // from the perfect shuffle tables.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:34,Performance,perform,performed,34,// Broadcast shuffle kinds can be performed with 'dup'.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:34,Performance,perform,performed,34,// Transpose shuffle kinds can be performed with 'trn1/trn2' and; // 'zip1/zip2' instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:15,Performance,load,load,15,// constpool + load + tbl,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:15,Performance,load,load,15,// constpool + load + tbl,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:15,Performance,load,load,15,// constpool + load + tbl,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:15,Performance,load,load,15,// constpool + load + tbl,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:15,Performance,load,load,15,// constpool + load + tbl; // Reverse can be lowered with `rev`.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:31,Performance,scalab,scalable,31,// Broadcast shuffle kinds for scalable vectors,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:44,Performance,scalab,scalable,44,// Handle the cases for vector.reverse with scalable vectors,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:40,Performance,load,load,40,"// We call this to discover whether any load/store pointers in the loop have; // negative strides. This will require extra work to reverse the loop; // predicate, which may be expensive.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:242,Modifiability,extend,extend,242,"// Scaling factors are not free at all.; // Operands | Rt Latency; // -------------------------------------------; // Rt, [Xn, Xm] | 4; // -------------------------------------------; // Rt, [Xn, Xm, lsl #imm] | Rn: 4 Rm: 5; // Rt, [Xn, Wm, <extend> #imm] |",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:58,Performance,Latency,Latency,58,"// Scaling factors are not free at all.; // Operands | Rt Latency; // -------------------------------------------; // Rt, [Xn, Xm] | 4; // -------------------------------------------; // Rt, [Xn, Xm, lsl #imm] | Rn: 4 Rm: 5; // Rt, [Xn, Wm, <extend> #imm] |",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h:181,Availability,avail,available,181,// A helper function called by 'getVectorInstrCost'.; //; // 'Val' and 'Index' are forwarded from 'getVectorInstrCost'; 'HasRealUse'; // indicates whether the vector instruction is available in the input IR or; // just imaginary in vectorizer passes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h:157,Performance,scalab,scalable,157,/// Try to return an estimate cost factor that can be used as a multiplier; /// when scalarizing an operation for a vector with ElementCount \p VF.; /// For scalable vectors this currently takes the most pessimistic view based; /// upon the maximum possible value for vscale.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h:22,Safety,avoid,avoid,22,"// For fixed vectors, avoid scalarization if using SVE for them.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h:33,Availability,mask,masked,33,// Fall back to scalarization of masked operations.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h:49,Performance,load,load,49,// Return true if we can generate a `ld1r` splat load instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h:405,Energy Efficiency,power,power,405,"// NOTE: The logic below is mostly geared towards LV, which calls it with; // vectors with 2 elements. We might want to improve that, if other; // users show up.; // Nontemporal vector loads/stores can be directly lowered to LDNP/STNP, if; // the vector can be halved so that each half fits into a register. That's; // the case if the element type fits into a register and the number of; // elements is a power of 2 > 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h:185,Performance,load,loads,185,"// NOTE: The logic below is mostly geared towards LV, which calls it with; // vectors with 2 elements. We might want to improve that, if other; // users show up.; // Nontemporal vector loads/stores can be directly lowered to LDNP/STNP, if; // the vector can be halved so that each half fits into a register. That's; // the case if the element type fits into a register and the number of; // elements is a power of 2 > 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h:13,Testability,log,logic,13,"// NOTE: The logic below is mostly geared towards LV, which calls it with; // vectors with 2 elements. We might want to improve that, if other; // users show up.; // Nontemporal vector loads/stores can be directly lowered to LDNP/STNP, if; // the vector can be halved so that each half fits into a register. That's; // the case if the element type fits into a register and the number of; // elements is a power of 2 > 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h:116,Performance,load,load,116,"/// Return the cost of the scaling factor used in the addressing; /// mode represented by AM for this target, for a load/store; /// of the specified type.; /// If the AM is supported, the return value must be >= 0.; /// If the AM is not supported, it returns a negative value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SMEABIPass.cpp:246,Usability,clear,clear,246,//===----------------------------------------------------------------------===//; // Utility functions; //===----------------------------------------------------------------------===//; // Utility function to emit a call to __arm_tpidr2_save and clear TPIDR2_EL0.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SMEABIPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SMEABIPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SMEABIPass.cpp:42,Usability,clear,clearing,42,// A save to TPIDR2 should be followed by clearing TPIDR2_EL0.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SMEABIPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SMEABIPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SMEABIPass.cpp:260,Integrability,Interface,Interface,260,"/// This function generates code at the beginning and end of a function marked; /// with either `aarch64_pstate_za_new` or `aarch64_new_zt0`.; /// At the beginning of the function, the following code is generated:; /// - Commit lazy-save if active [Private-ZA Interface*]; /// - Enable PSTATE.ZA [Private-ZA Interface]; /// - Zero ZA [Has New ZA State]; /// - Zero ZT0 [Has New ZT0 State]; ///; /// * A function with new ZT0 state will not change ZA, so committing the; /// lazy-save is not strictly necessary. However, the lazy-save mechanism; /// may be active on entry to the function, with PSTATE.ZA set to 1. If; /// the new ZT0 function calls a function that does not share ZT0, we will; /// need to conditionally SMSTOP ZA before the call, setting PSTATE.ZA to 0.; /// For this reason, it's easier to always commit the lazy-save at the; /// beginning of the function regardless of whether it has ZA state.; ///; /// At the end of the function, PSTATE.ZA is disabled if the function has a; /// Private-ZA Interface. A function is considered to have a Private-ZA; /// interface if it does not share ZA or ZT0.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SMEABIPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SMEABIPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SMEABIPass.cpp:308,Integrability,Interface,Interface,308,"/// This function generates code at the beginning and end of a function marked; /// with either `aarch64_pstate_za_new` or `aarch64_new_zt0`.; /// At the beginning of the function, the following code is generated:; /// - Commit lazy-save if active [Private-ZA Interface*]; /// - Enable PSTATE.ZA [Private-ZA Interface]; /// - Zero ZA [Has New ZA State]; /// - Zero ZT0 [Has New ZT0 State]; ///; /// * A function with new ZT0 state will not change ZA, so committing the; /// lazy-save is not strictly necessary. However, the lazy-save mechanism; /// may be active on entry to the function, with PSTATE.ZA set to 1. If; /// the new ZT0 function calls a function that does not share ZT0, we will; /// need to conditionally SMSTOP ZA before the call, setting PSTATE.ZA to 0.; /// For this reason, it's easier to always commit the lazy-save at the; /// beginning of the function regardless of whether it has ZA state.; ///; /// At the end of the function, PSTATE.ZA is disabled if the function has a; /// Private-ZA Interface. A function is considered to have a Private-ZA; /// interface if it does not share ZA or ZT0.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SMEABIPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SMEABIPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SMEABIPass.cpp:1011,Integrability,Interface,Interface,1011,"/// This function generates code at the beginning and end of a function marked; /// with either `aarch64_pstate_za_new` or `aarch64_new_zt0`.; /// At the beginning of the function, the following code is generated:; /// - Commit lazy-save if active [Private-ZA Interface*]; /// - Enable PSTATE.ZA [Private-ZA Interface]; /// - Zero ZA [Has New ZA State]; /// - Zero ZT0 [Has New ZT0 State]; ///; /// * A function with new ZT0 state will not change ZA, so committing the; /// lazy-save is not strictly necessary. However, the lazy-save mechanism; /// may be active on entry to the function, with PSTATE.ZA set to 1. If; /// the new ZT0 function calls a function that does not share ZT0, we will; /// need to conditionally SMSTOP ZA before the call, setting PSTATE.ZA to 0.; /// For this reason, it's easier to always commit the lazy-save at the; /// beginning of the function regardless of whether it has ZA state.; ///; /// At the end of the function, PSTATE.ZA is disabled if the function has a; /// Private-ZA Interface. A function is considered to have a Private-ZA; /// interface if it does not share ZA or ZT0.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SMEABIPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SMEABIPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SMEABIPass.cpp:1073,Integrability,interface,interface,1073,"/// This function generates code at the beginning and end of a function marked; /// with either `aarch64_pstate_za_new` or `aarch64_new_zt0`.; /// At the beginning of the function, the following code is generated:; /// - Commit lazy-save if active [Private-ZA Interface*]; /// - Enable PSTATE.ZA [Private-ZA Interface]; /// - Zero ZA [Has New ZA State]; /// - Zero ZT0 [Has New ZT0 State]; ///; /// * A function with new ZT0 state will not change ZA, so committing the; /// lazy-save is not strictly necessary. However, the lazy-save mechanism; /// may be active on entry to the function, with PSTATE.ZA set to 1. If; /// the new ZT0 function calls a function that does not share ZT0, we will; /// need to conditionally SMSTOP ZA before the call, setting PSTATE.ZA to 0.; /// For this reason, it's easier to always commit the lazy-save at the; /// beginning of the function regardless of whether it has ZA state.; ///; /// At the end of the function, PSTATE.ZA is disabled if the function has a; /// Private-ZA Interface. A function is considered to have a Private-ZA; /// interface if it does not share ZA or ZT0.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SMEABIPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SMEABIPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:383,Performance,Perform,Performs,383,"//===----- SVEIntrinsicOpts - SVE ACLE Intrinsics Opts --------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // Performs general IR level optimizations on SVE intrinsics.; //; // This pass performs the following optimizations:; //; // - removes unnecessary ptrue intrinsics (llvm.aarch64.sve.ptrue), e.g:; // %1 = @llvm.aarch64.sve.ptrue.nxv4i1(i32 31); // %2 = @llvm.aarch64.sve.ptrue.nxv8i1(i32 31); // ; (%1 can be replaced with a reinterpret of %2); //; // - optimizes ptest intrinsics where the operands are being needlessly; // converted to and from svbool_t.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:409,Performance,optimiz,optimizations,409,"//===----- SVEIntrinsicOpts - SVE ACLE Intrinsics Opts --------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // Performs general IR level optimizations on SVE intrinsics.; //; // This pass performs the following optimizations:; //; // - removes unnecessary ptrue intrinsics (llvm.aarch64.sve.ptrue), e.g:; // %1 = @llvm.aarch64.sve.ptrue.nxv4i1(i32 31); // %2 = @llvm.aarch64.sve.ptrue.nxv8i1(i32 31); // ; (%1 can be replaced with a reinterpret of %2); //; // - optimizes ptest intrinsics where the operands are being needlessly; // converted to and from svbool_t.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:460,Performance,perform,performs,460,"//===----- SVEIntrinsicOpts - SVE ACLE Intrinsics Opts --------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // Performs general IR level optimizations on SVE intrinsics.; //; // This pass performs the following optimizations:; //; // - removes unnecessary ptrue intrinsics (llvm.aarch64.sve.ptrue), e.g:; // %1 = @llvm.aarch64.sve.ptrue.nxv4i1(i32 31); // %2 = @llvm.aarch64.sve.ptrue.nxv8i1(i32 31); // ; (%1 can be replaced with a reinterpret of %2); //; // - optimizes ptest intrinsics where the operands are being needlessly; // converted to and from svbool_t.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:483,Performance,optimiz,optimizations,483,"//===----- SVEIntrinsicOpts - SVE ACLE Intrinsics Opts --------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // Performs general IR level optimizations on SVE intrinsics.; //; // This pass performs the following optimizations:; //; // - removes unnecessary ptrue intrinsics (llvm.aarch64.sve.ptrue), e.g:; // %1 = @llvm.aarch64.sve.ptrue.nxv4i1(i32 31); // %2 = @llvm.aarch64.sve.ptrue.nxv8i1(i32 31); // ; (%1 can be replaced with a reinterpret of %2); //; // - optimizes ptest intrinsics where the operands are being needlessly; // converted to and from svbool_t.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:734,Performance,optimiz,optimizes,734,"//===----- SVEIntrinsicOpts - SVE ACLE Intrinsics Opts --------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // Performs general IR level optimizations on SVE intrinsics.; //; // This pass performs the following optimizations:; //; // - removes unnecessary ptrue intrinsics (llvm.aarch64.sve.ptrue), e.g:; // %1 = @llvm.aarch64.sve.ptrue.nxv4i1(i32 31); // %2 = @llvm.aarch64.sve.ptrue.nxv8i1(i32 31); // ; (%1 can be replaced with a reinterpret of %2); //; // - optimizes ptest intrinsics where the operands are being needlessly; // converted to and from svbool_t.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:42,Performance,optimiz,optimizations,42,"/// Operates at the function-scope. I.e., optimizations are applied local to; /// the functions themselves.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:81,Safety,safe,safe,81,"// Hoist MostEncompassingPTrue to the start of the basic block. It is always; // safe to do this, since ptrue intrinsic calls are guaranteed to have no; // predecessors.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:43,Availability,redundant,redundant,43,"/// The goal of this function is to remove redundant calls to the SVE ptrue; /// intrinsic in each basic block within the given functions.; ///; /// SVE ptrues have two representations in LLVM IR:; /// - a logical representation -- an arbitrary-width scalable vector of i1s,; /// i.e. <vscale x N x i1>.; /// - a physical representation (svbool, <vscale x 16 x i1>) -- a 16-element; /// scalable vector of i1s, i.e. <vscale x 16 x i1>.; ///; /// The SVE ptrue intrinsic is used to create a logical representation of an SVE; /// predicate. Suppose that we have two SVE ptrue intrinsic calls: P1 and P2. If; /// P1 creates a logical SVE predicate that is at least as wide as the logical; /// SVE predicate created by P2, then all of the bits that are true in the; /// physical representation of P2 are necessarily also true in the physical; /// representation of P1. P1 'encompasses' P2, therefore, the intrinsic call to; /// P2 is redundant and can be replaced by an SVE reinterpret of P1 via; /// convert.{to,from}.svbool.; ///; /// Currently, this pass only coalesces calls to SVE ptrue intrinsics; /// if they match the following conditions:; ///; /// - the call to the intrinsic uses either the SV_ALL or SV_POW2 patterns.; /// SV_ALL indicates that all bits of the predicate vector are to be set to; /// true. SV_POW2 indicates that all bits of the predicate vector up to the; /// largest power-of-two are to be set to true.; /// - the result of the call to the intrinsic is not promoted to a wider; /// predicate. In this case, keeping the extra ptrue leads to better codegen; /// -- coalescing here would create an irreducible chain of SVE reinterprets; /// via convert.{to,from}.svbool.; ///; /// EXAMPLE:; ///; /// %1 = <vscale x 8 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1, 1, 1, 1, 1>; /// ; Physical: <1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0>; /// ...; ///; /// %2 = <vscale x 4 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1>; /// ; Physical: <1, 0, 0, 0, 1, 0, 0, 0,",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:930,Availability,redundant,redundant,930,"/// The goal of this function is to remove redundant calls to the SVE ptrue; /// intrinsic in each basic block within the given functions.; ///; /// SVE ptrues have two representations in LLVM IR:; /// - a logical representation -- an arbitrary-width scalable vector of i1s,; /// i.e. <vscale x N x i1>.; /// - a physical representation (svbool, <vscale x 16 x i1>) -- a 16-element; /// scalable vector of i1s, i.e. <vscale x 16 x i1>.; ///; /// The SVE ptrue intrinsic is used to create a logical representation of an SVE; /// predicate. Suppose that we have two SVE ptrue intrinsic calls: P1 and P2. If; /// P1 creates a logical SVE predicate that is at least as wide as the logical; /// SVE predicate created by P2, then all of the bits that are true in the; /// physical representation of P2 are necessarily also true in the physical; /// representation of P1. P1 'encompasses' P2, therefore, the intrinsic call to; /// P2 is redundant and can be replaced by an SVE reinterpret of P1 via; /// convert.{to,from}.svbool.; ///; /// Currently, this pass only coalesces calls to SVE ptrue intrinsics; /// if they match the following conditions:; ///; /// - the call to the intrinsic uses either the SV_ALL or SV_POW2 patterns.; /// SV_ALL indicates that all bits of the predicate vector are to be set to; /// true. SV_POW2 indicates that all bits of the predicate vector up to the; /// largest power-of-two are to be set to true.; /// - the result of the call to the intrinsic is not promoted to a wider; /// predicate. In this case, keeping the extra ptrue leads to better codegen; /// -- coalescing here would create an irreducible chain of SVE reinterprets; /// via convert.{to,from}.svbool.; ///; /// EXAMPLE:; ///; /// %1 = <vscale x 8 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1, 1, 1, 1, 1>; /// ; Physical: <1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0>; /// ...; ///; /// %2 = <vscale x 4 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1>; /// ; Physical: <1, 0, 0, 0, 1, 0, 0, 0,",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:1393,Energy Efficiency,power,power-of-two,1393,"physical representation (svbool, <vscale x 16 x i1>) -- a 16-element; /// scalable vector of i1s, i.e. <vscale x 16 x i1>.; ///; /// The SVE ptrue intrinsic is used to create a logical representation of an SVE; /// predicate. Suppose that we have two SVE ptrue intrinsic calls: P1 and P2. If; /// P1 creates a logical SVE predicate that is at least as wide as the logical; /// SVE predicate created by P2, then all of the bits that are true in the; /// physical representation of P2 are necessarily also true in the physical; /// representation of P1. P1 'encompasses' P2, therefore, the intrinsic call to; /// P2 is redundant and can be replaced by an SVE reinterpret of P1 via; /// convert.{to,from}.svbool.; ///; /// Currently, this pass only coalesces calls to SVE ptrue intrinsics; /// if they match the following conditions:; ///; /// - the call to the intrinsic uses either the SV_ALL or SV_POW2 patterns.; /// SV_ALL indicates that all bits of the predicate vector are to be set to; /// true. SV_POW2 indicates that all bits of the predicate vector up to the; /// largest power-of-two are to be set to true.; /// - the result of the call to the intrinsic is not promoted to a wider; /// predicate. In this case, keeping the extra ptrue leads to better codegen; /// -- coalescing here would create an irreducible chain of SVE reinterprets; /// via convert.{to,from}.svbool.; ///; /// EXAMPLE:; ///; /// %1 = <vscale x 8 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1, 1, 1, 1, 1>; /// ; Physical: <1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0>; /// ...; ///; /// %2 = <vscale x 4 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1>; /// ; Physical: <1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0>; /// ...; ///; /// Here, %2 can be replaced by an SVE reinterpret of %1, giving, for instance:; ///; /// %1 = <vscale x 8 x i1> ptrue(i32 i31); /// %2 = <vscale x 16 x i1> convert.to.svbool(<vscale x 8 x i1> %1); /// %3 = <vscale x 4 x i1> convert.from.svbool(<vscale x 16 x i1> %2); ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:251,Performance,scalab,scalable,251,"/// The goal of this function is to remove redundant calls to the SVE ptrue; /// intrinsic in each basic block within the given functions.; ///; /// SVE ptrues have two representations in LLVM IR:; /// - a logical representation -- an arbitrary-width scalable vector of i1s,; /// i.e. <vscale x N x i1>.; /// - a physical representation (svbool, <vscale x 16 x i1>) -- a 16-element; /// scalable vector of i1s, i.e. <vscale x 16 x i1>.; ///; /// The SVE ptrue intrinsic is used to create a logical representation of an SVE; /// predicate. Suppose that we have two SVE ptrue intrinsic calls: P1 and P2. If; /// P1 creates a logical SVE predicate that is at least as wide as the logical; /// SVE predicate created by P2, then all of the bits that are true in the; /// physical representation of P2 are necessarily also true in the physical; /// representation of P1. P1 'encompasses' P2, therefore, the intrinsic call to; /// P2 is redundant and can be replaced by an SVE reinterpret of P1 via; /// convert.{to,from}.svbool.; ///; /// Currently, this pass only coalesces calls to SVE ptrue intrinsics; /// if they match the following conditions:; ///; /// - the call to the intrinsic uses either the SV_ALL or SV_POW2 patterns.; /// SV_ALL indicates that all bits of the predicate vector are to be set to; /// true. SV_POW2 indicates that all bits of the predicate vector up to the; /// largest power-of-two are to be set to true.; /// - the result of the call to the intrinsic is not promoted to a wider; /// predicate. In this case, keeping the extra ptrue leads to better codegen; /// -- coalescing here would create an irreducible chain of SVE reinterprets; /// via convert.{to,from}.svbool.; ///; /// EXAMPLE:; ///; /// %1 = <vscale x 8 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1, 1, 1, 1, 1>; /// ; Physical: <1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0>; /// ...; ///; /// %2 = <vscale x 4 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1>; /// ; Physical: <1, 0, 0, 0, 1, 0, 0, 0,",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:387,Performance,scalab,scalable,387,"/// The goal of this function is to remove redundant calls to the SVE ptrue; /// intrinsic in each basic block within the given functions.; ///; /// SVE ptrues have two representations in LLVM IR:; /// - a logical representation -- an arbitrary-width scalable vector of i1s,; /// i.e. <vscale x N x i1>.; /// - a physical representation (svbool, <vscale x 16 x i1>) -- a 16-element; /// scalable vector of i1s, i.e. <vscale x 16 x i1>.; ///; /// The SVE ptrue intrinsic is used to create a logical representation of an SVE; /// predicate. Suppose that we have two SVE ptrue intrinsic calls: P1 and P2. If; /// P1 creates a logical SVE predicate that is at least as wide as the logical; /// SVE predicate created by P2, then all of the bits that are true in the; /// physical representation of P2 are necessarily also true in the physical; /// representation of P1. P1 'encompasses' P2, therefore, the intrinsic call to; /// P2 is redundant and can be replaced by an SVE reinterpret of P1 via; /// convert.{to,from}.svbool.; ///; /// Currently, this pass only coalesces calls to SVE ptrue intrinsics; /// if they match the following conditions:; ///; /// - the call to the intrinsic uses either the SV_ALL or SV_POW2 patterns.; /// SV_ALL indicates that all bits of the predicate vector are to be set to; /// true. SV_POW2 indicates that all bits of the predicate vector up to the; /// largest power-of-two are to be set to true.; /// - the result of the call to the intrinsic is not promoted to a wider; /// predicate. In this case, keeping the extra ptrue leads to better codegen; /// -- coalescing here would create an irreducible chain of SVE reinterprets; /// via convert.{to,from}.svbool.; ///; /// EXAMPLE:; ///; /// %1 = <vscale x 8 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1, 1, 1, 1, 1>; /// ; Physical: <1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0>; /// ...; ///; /// %2 = <vscale x 4 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1>; /// ; Physical: <1, 0, 0, 0, 1, 0, 0, 0,",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:43,Safety,redund,redundant,43,"/// The goal of this function is to remove redundant calls to the SVE ptrue; /// intrinsic in each basic block within the given functions.; ///; /// SVE ptrues have two representations in LLVM IR:; /// - a logical representation -- an arbitrary-width scalable vector of i1s,; /// i.e. <vscale x N x i1>.; /// - a physical representation (svbool, <vscale x 16 x i1>) -- a 16-element; /// scalable vector of i1s, i.e. <vscale x 16 x i1>.; ///; /// The SVE ptrue intrinsic is used to create a logical representation of an SVE; /// predicate. Suppose that we have two SVE ptrue intrinsic calls: P1 and P2. If; /// P1 creates a logical SVE predicate that is at least as wide as the logical; /// SVE predicate created by P2, then all of the bits that are true in the; /// physical representation of P2 are necessarily also true in the physical; /// representation of P1. P1 'encompasses' P2, therefore, the intrinsic call to; /// P2 is redundant and can be replaced by an SVE reinterpret of P1 via; /// convert.{to,from}.svbool.; ///; /// Currently, this pass only coalesces calls to SVE ptrue intrinsics; /// if they match the following conditions:; ///; /// - the call to the intrinsic uses either the SV_ALL or SV_POW2 patterns.; /// SV_ALL indicates that all bits of the predicate vector are to be set to; /// true. SV_POW2 indicates that all bits of the predicate vector up to the; /// largest power-of-two are to be set to true.; /// - the result of the call to the intrinsic is not promoted to a wider; /// predicate. In this case, keeping the extra ptrue leads to better codegen; /// -- coalescing here would create an irreducible chain of SVE reinterprets; /// via convert.{to,from}.svbool.; ///; /// EXAMPLE:; ///; /// %1 = <vscale x 8 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1, 1, 1, 1, 1>; /// ; Physical: <1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0>; /// ...; ///; /// %2 = <vscale x 4 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1>; /// ; Physical: <1, 0, 0, 0, 1, 0, 0, 0,",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:930,Safety,redund,redundant,930,"/// The goal of this function is to remove redundant calls to the SVE ptrue; /// intrinsic in each basic block within the given functions.; ///; /// SVE ptrues have two representations in LLVM IR:; /// - a logical representation -- an arbitrary-width scalable vector of i1s,; /// i.e. <vscale x N x i1>.; /// - a physical representation (svbool, <vscale x 16 x i1>) -- a 16-element; /// scalable vector of i1s, i.e. <vscale x 16 x i1>.; ///; /// The SVE ptrue intrinsic is used to create a logical representation of an SVE; /// predicate. Suppose that we have two SVE ptrue intrinsic calls: P1 and P2. If; /// P1 creates a logical SVE predicate that is at least as wide as the logical; /// SVE predicate created by P2, then all of the bits that are true in the; /// physical representation of P2 are necessarily also true in the physical; /// representation of P1. P1 'encompasses' P2, therefore, the intrinsic call to; /// P2 is redundant and can be replaced by an SVE reinterpret of P1 via; /// convert.{to,from}.svbool.; ///; /// Currently, this pass only coalesces calls to SVE ptrue intrinsics; /// if they match the following conditions:; ///; /// - the call to the intrinsic uses either the SV_ALL or SV_POW2 patterns.; /// SV_ALL indicates that all bits of the predicate vector are to be set to; /// true. SV_POW2 indicates that all bits of the predicate vector up to the; /// largest power-of-two are to be set to true.; /// - the result of the call to the intrinsic is not promoted to a wider; /// predicate. In this case, keeping the extra ptrue leads to better codegen; /// -- coalescing here would create an irreducible chain of SVE reinterprets; /// via convert.{to,from}.svbool.; ///; /// EXAMPLE:; ///; /// %1 = <vscale x 8 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1, 1, 1, 1, 1>; /// ; Physical: <1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0>; /// ...; ///; /// %2 = <vscale x 4 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1>; /// ; Physical: <1, 0, 0, 0, 1, 0, 0, 0,",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:206,Testability,log,logical,206,"/// The goal of this function is to remove redundant calls to the SVE ptrue; /// intrinsic in each basic block within the given functions.; ///; /// SVE ptrues have two representations in LLVM IR:; /// - a logical representation -- an arbitrary-width scalable vector of i1s,; /// i.e. <vscale x N x i1>.; /// - a physical representation (svbool, <vscale x 16 x i1>) -- a 16-element; /// scalable vector of i1s, i.e. <vscale x 16 x i1>.; ///; /// The SVE ptrue intrinsic is used to create a logical representation of an SVE; /// predicate. Suppose that we have two SVE ptrue intrinsic calls: P1 and P2. If; /// P1 creates a logical SVE predicate that is at least as wide as the logical; /// SVE predicate created by P2, then all of the bits that are true in the; /// physical representation of P2 are necessarily also true in the physical; /// representation of P1. P1 'encompasses' P2, therefore, the intrinsic call to; /// P2 is redundant and can be replaced by an SVE reinterpret of P1 via; /// convert.{to,from}.svbool.; ///; /// Currently, this pass only coalesces calls to SVE ptrue intrinsics; /// if they match the following conditions:; ///; /// - the call to the intrinsic uses either the SV_ALL or SV_POW2 patterns.; /// SV_ALL indicates that all bits of the predicate vector are to be set to; /// true. SV_POW2 indicates that all bits of the predicate vector up to the; /// largest power-of-two are to be set to true.; /// - the result of the call to the intrinsic is not promoted to a wider; /// predicate. In this case, keeping the extra ptrue leads to better codegen; /// -- coalescing here would create an irreducible chain of SVE reinterprets; /// via convert.{to,from}.svbool.; ///; /// EXAMPLE:; ///; /// %1 = <vscale x 8 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1, 1, 1, 1, 1>; /// ; Physical: <1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0>; /// ...; ///; /// %2 = <vscale x 4 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1>; /// ; Physical: <1, 0, 0, 0, 1, 0, 0, 0,",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:490,Testability,log,logical,490,"/// The goal of this function is to remove redundant calls to the SVE ptrue; /// intrinsic in each basic block within the given functions.; ///; /// SVE ptrues have two representations in LLVM IR:; /// - a logical representation -- an arbitrary-width scalable vector of i1s,; /// i.e. <vscale x N x i1>.; /// - a physical representation (svbool, <vscale x 16 x i1>) -- a 16-element; /// scalable vector of i1s, i.e. <vscale x 16 x i1>.; ///; /// The SVE ptrue intrinsic is used to create a logical representation of an SVE; /// predicate. Suppose that we have two SVE ptrue intrinsic calls: P1 and P2. If; /// P1 creates a logical SVE predicate that is at least as wide as the logical; /// SVE predicate created by P2, then all of the bits that are true in the; /// physical representation of P2 are necessarily also true in the physical; /// representation of P1. P1 'encompasses' P2, therefore, the intrinsic call to; /// P2 is redundant and can be replaced by an SVE reinterpret of P1 via; /// convert.{to,from}.svbool.; ///; /// Currently, this pass only coalesces calls to SVE ptrue intrinsics; /// if they match the following conditions:; ///; /// - the call to the intrinsic uses either the SV_ALL or SV_POW2 patterns.; /// SV_ALL indicates that all bits of the predicate vector are to be set to; /// true. SV_POW2 indicates that all bits of the predicate vector up to the; /// largest power-of-two are to be set to true.; /// - the result of the call to the intrinsic is not promoted to a wider; /// predicate. In this case, keeping the extra ptrue leads to better codegen; /// -- coalescing here would create an irreducible chain of SVE reinterprets; /// via convert.{to,from}.svbool.; ///; /// EXAMPLE:; ///; /// %1 = <vscale x 8 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1, 1, 1, 1, 1>; /// ; Physical: <1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0>; /// ...; ///; /// %2 = <vscale x 4 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1>; /// ; Physical: <1, 0, 0, 0, 1, 0, 0, 0,",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:623,Testability,log,logical,623,"/// The goal of this function is to remove redundant calls to the SVE ptrue; /// intrinsic in each basic block within the given functions.; ///; /// SVE ptrues have two representations in LLVM IR:; /// - a logical representation -- an arbitrary-width scalable vector of i1s,; /// i.e. <vscale x N x i1>.; /// - a physical representation (svbool, <vscale x 16 x i1>) -- a 16-element; /// scalable vector of i1s, i.e. <vscale x 16 x i1>.; ///; /// The SVE ptrue intrinsic is used to create a logical representation of an SVE; /// predicate. Suppose that we have two SVE ptrue intrinsic calls: P1 and P2. If; /// P1 creates a logical SVE predicate that is at least as wide as the logical; /// SVE predicate created by P2, then all of the bits that are true in the; /// physical representation of P2 are necessarily also true in the physical; /// representation of P1. P1 'encompasses' P2, therefore, the intrinsic call to; /// P2 is redundant and can be replaced by an SVE reinterpret of P1 via; /// convert.{to,from}.svbool.; ///; /// Currently, this pass only coalesces calls to SVE ptrue intrinsics; /// if they match the following conditions:; ///; /// - the call to the intrinsic uses either the SV_ALL or SV_POW2 patterns.; /// SV_ALL indicates that all bits of the predicate vector are to be set to; /// true. SV_POW2 indicates that all bits of the predicate vector up to the; /// largest power-of-two are to be set to true.; /// - the result of the call to the intrinsic is not promoted to a wider; /// predicate. In this case, keeping the extra ptrue leads to better codegen; /// -- coalescing here would create an irreducible chain of SVE reinterprets; /// via convert.{to,from}.svbool.; ///; /// EXAMPLE:; ///; /// %1 = <vscale x 8 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1, 1, 1, 1, 1>; /// ; Physical: <1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0>; /// ...; ///; /// %2 = <vscale x 4 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1>; /// ; Physical: <1, 0, 0, 0, 1, 0, 0, 0,",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:677,Testability,log,logical,677,"/// The goal of this function is to remove redundant calls to the SVE ptrue; /// intrinsic in each basic block within the given functions.; ///; /// SVE ptrues have two representations in LLVM IR:; /// - a logical representation -- an arbitrary-width scalable vector of i1s,; /// i.e. <vscale x N x i1>.; /// - a physical representation (svbool, <vscale x 16 x i1>) -- a 16-element; /// scalable vector of i1s, i.e. <vscale x 16 x i1>.; ///; /// The SVE ptrue intrinsic is used to create a logical representation of an SVE; /// predicate. Suppose that we have two SVE ptrue intrinsic calls: P1 and P2. If; /// P1 creates a logical SVE predicate that is at least as wide as the logical; /// SVE predicate created by P2, then all of the bits that are true in the; /// physical representation of P2 are necessarily also true in the physical; /// representation of P1. P1 'encompasses' P2, therefore, the intrinsic call to; /// P2 is redundant and can be replaced by an SVE reinterpret of P1 via; /// convert.{to,from}.svbool.; ///; /// Currently, this pass only coalesces calls to SVE ptrue intrinsics; /// if they match the following conditions:; ///; /// - the call to the intrinsic uses either the SV_ALL or SV_POW2 patterns.; /// SV_ALL indicates that all bits of the predicate vector are to be set to; /// true. SV_POW2 indicates that all bits of the predicate vector up to the; /// largest power-of-two are to be set to true.; /// - the result of the call to the intrinsic is not promoted to a wider; /// predicate. In this case, keeping the extra ptrue leads to better codegen; /// -- coalescing here would create an irreducible chain of SVE reinterprets; /// via convert.{to,from}.svbool.; ///; /// EXAMPLE:; ///; /// %1 = <vscale x 8 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1, 1, 1, 1, 1>; /// ; Physical: <1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0>; /// ...; ///; /// %2 = <vscale x 4 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1>; /// ; Physical: <1, 0, 0, 0, 1, 0, 0, 0,",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:1771,Testability,Log,Logical,1771,"physical representation (svbool, <vscale x 16 x i1>) -- a 16-element; /// scalable vector of i1s, i.e. <vscale x 16 x i1>.; ///; /// The SVE ptrue intrinsic is used to create a logical representation of an SVE; /// predicate. Suppose that we have two SVE ptrue intrinsic calls: P1 and P2. If; /// P1 creates a logical SVE predicate that is at least as wide as the logical; /// SVE predicate created by P2, then all of the bits that are true in the; /// physical representation of P2 are necessarily also true in the physical; /// representation of P1. P1 'encompasses' P2, therefore, the intrinsic call to; /// P2 is redundant and can be replaced by an SVE reinterpret of P1 via; /// convert.{to,from}.svbool.; ///; /// Currently, this pass only coalesces calls to SVE ptrue intrinsics; /// if they match the following conditions:; ///; /// - the call to the intrinsic uses either the SV_ALL or SV_POW2 patterns.; /// SV_ALL indicates that all bits of the predicate vector are to be set to; /// true. SV_POW2 indicates that all bits of the predicate vector up to the; /// largest power-of-two are to be set to true.; /// - the result of the call to the intrinsic is not promoted to a wider; /// predicate. In this case, keeping the extra ptrue leads to better codegen; /// -- coalescing here would create an irreducible chain of SVE reinterprets; /// via convert.{to,from}.svbool.; ///; /// EXAMPLE:; ///; /// %1 = <vscale x 8 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1, 1, 1, 1, 1>; /// ; Physical: <1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0>; /// ...; ///; /// %2 = <vscale x 4 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1>; /// ; Physical: <1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0>; /// ...; ///; /// Here, %2 can be replaced by an SVE reinterpret of %1, giving, for instance:; ///; /// %1 = <vscale x 8 x i1> ptrue(i32 i31); /// %2 = <vscale x 16 x i1> convert.to.svbool(<vscale x 8 x i1> %1); /// %3 = <vscale x 4 x i1> convert.from.svbool(<vscale x 16 x i1> %2); ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:1938,Testability,Log,Logical,1938,"physical representation (svbool, <vscale x 16 x i1>) -- a 16-element; /// scalable vector of i1s, i.e. <vscale x 16 x i1>.; ///; /// The SVE ptrue intrinsic is used to create a logical representation of an SVE; /// predicate. Suppose that we have two SVE ptrue intrinsic calls: P1 and P2. If; /// P1 creates a logical SVE predicate that is at least as wide as the logical; /// SVE predicate created by P2, then all of the bits that are true in the; /// physical representation of P2 are necessarily also true in the physical; /// representation of P1. P1 'encompasses' P2, therefore, the intrinsic call to; /// P2 is redundant and can be replaced by an SVE reinterpret of P1 via; /// convert.{to,from}.svbool.; ///; /// Currently, this pass only coalesces calls to SVE ptrue intrinsics; /// if they match the following conditions:; ///; /// - the call to the intrinsic uses either the SV_ALL or SV_POW2 patterns.; /// SV_ALL indicates that all bits of the predicate vector are to be set to; /// true. SV_POW2 indicates that all bits of the predicate vector up to the; /// largest power-of-two are to be set to true.; /// - the result of the call to the intrinsic is not promoted to a wider; /// predicate. In this case, keeping the extra ptrue leads to better codegen; /// -- coalescing here would create an irreducible chain of SVE reinterprets; /// via convert.{to,from}.svbool.; ///; /// EXAMPLE:; ///; /// %1 = <vscale x 8 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1, 1, 1, 1, 1>; /// ; Physical: <1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0>; /// ...; ///; /// %2 = <vscale x 4 x i1> ptrue(i32 SV_ALL); /// ; Logical: <1, 1, 1, 1>; /// ; Physical: <1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0>; /// ...; ///; /// Here, %2 can be replaced by an SVE reinterpret of %1, giving, for instance:; ///; /// %1 = <vscale x 8 x i1> ptrue(i32 i31); /// %2 = <vscale x 16 x i1> convert.to.svbool(<vscale x 8 x i1> %1); /// %3 = <vscale x 4 x i1> convert.from.svbool(<vscale x 16 x i1> %2); ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:85,Performance,scalab,scalable,85,// This is done in SVEIntrinsicOpts rather than InstCombine so that we introduce; // scalable stores as late as possible,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:59,Performance,scalab,scalable,59,// The transform needs to know the exact runtime length of scalable vectors,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:85,Performance,scalab,scalable,85,// This is done in SVEIntrinsicOpts rather than InstCombine so that we introduce; // scalable loads as late as possible,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:94,Performance,load,loads,94,// This is done in SVEIntrinsicOpts rather than InstCombine so that we introduce; // scalable loads as late as possible,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:59,Performance,scalab,scalable,59,// The transform needs to know the exact runtime length of scalable vectors,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:43,Performance,load,load,43,// ..where the value inserted comes from a load..,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:13,Performance,load,loading,13,// ..that is loading a predicate vector sized worth of bits..,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp:77,Usability,simpl,simplification,77,"// Traverse the DT with an rpo walk so we see defs before uses, allowing; // simplification to be done incrementally.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/SVEIntrinsicOpts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:25,Availability,avail,available,25,// Initialize the set of available features.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:18,Modifiability,extend,extend,18,// Separate shift/extend operand.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:481,Deployability,update,update,481,"// In some cases the shift/extend needs to be explicitly parsed together; // with the register, rather than as a separate operand. This is needed; // for addressing modes where the instruction as a whole dictates the; // scaling/extend, rather than specific bits in the instruction.; // By parsing them as a single operand, we avoid the need to pass an; // extra operand in all CodeGen patterns (because all operands need to; // have an associated value), and we avoid the need to update TableGen to; // accept operands that have no associated bits in the instruction.; //; // An added benefit of parsing them together is that the assembler; // can give a sensible diagnostic if the scaling is not correct.; //; // The default is 'lsl #0' (HasExplicitAmount = false) if no; // ShiftExtend is specified.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:27,Modifiability,extend,extend,27,"// In some cases the shift/extend needs to be explicitly parsed together; // with the register, rather than as a separate operand. This is needed; // for addressing modes where the instruction as a whole dictates the; // scaling/extend, rather than specific bits in the instruction.; // By parsing them as a single operand, we avoid the need to pass an; // extra operand in all CodeGen patterns (because all operands need to; // have an associated value), and we avoid the need to update TableGen to; // accept operands that have no associated bits in the instruction.; //; // An added benefit of parsing them together is that the assembler; // can give a sensible diagnostic if the scaling is not correct.; //; // The default is 'lsl #0' (HasExplicitAmount = false) if no; // ShiftExtend is specified.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:229,Modifiability,extend,extend,229,"// In some cases the shift/extend needs to be explicitly parsed together; // with the register, rather than as a separate operand. This is needed; // for addressing modes where the instruction as a whole dictates the; // scaling/extend, rather than specific bits in the instruction.; // By parsing them as a single operand, we avoid the need to pass an; // extra operand in all CodeGen patterns (because all operands need to; // have an associated value), and we avoid the need to update TableGen to; // accept operands that have no associated bits in the instruction.; //; // An added benefit of parsing them together is that the assembler; // can give a sensible diagnostic if the scaling is not correct.; //; // The default is 'lsl #0' (HasExplicitAmount = false) if no; // ShiftExtend is specified.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:327,Safety,avoid,avoid,327,"// In some cases the shift/extend needs to be explicitly parsed together; // with the register, rather than as a separate operand. This is needed; // for addressing modes where the instruction as a whole dictates the; // scaling/extend, rather than specific bits in the instruction.; // By parsing them as a single operand, we avoid the need to pass an; // extra operand in all CodeGen patterns (because all operands need to; // have an associated value), and we avoid the need to update TableGen to; // accept operands that have no associated bits in the instruction.; //; // An added benefit of parsing them together is that the assembler; // can give a sensible diagnostic if the scaling is not correct.; //; // The default is 'lsl #0' (HasExplicitAmount = false) if no; // ShiftExtend is specified.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:463,Safety,avoid,avoid,463,"// In some cases the shift/extend needs to be explicitly parsed together; // with the register, rather than as a separate operand. This is needed; // for addressing modes where the instruction as a whole dictates the; // scaling/extend, rather than specific bits in the instruction.; // By parsing them as a single operand, we avoid the need to pass an; // extra operand in all CodeGen patterns (because all operands need to; // have an associated value), and we avoid the need to update TableGen to; // accept operands that have no associated bits in the instruction.; //; // An added benefit of parsing them together is that the assembler; // can give a sensible diagnostic if the scaling is not correct.; //; // The default is 'lsl #0' (HasExplicitAmount = false) if no; // ShiftExtend is specified.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:84,Testability,log,logical,84,// NOTE: Also used for isLogicalImmNot as anything that can be represented as; // a logical immediate can always be represented when inverted.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:3,Safety,Avoid,Avoid,3,// Avoid left shift by 64 directly.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:201,Usability,ux,uxtw,201,"// Give a more specific diagnostic when the user has explicitly typed in; // a shift-amount that does not match what is expected, but for which; // there is also an unscaled addressing mode (e.g. sxtw/uxtw).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:17,Modifiability,extend,extend,17,// Make sure the extend expects a 32-bit source register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:5,Testability,log,logical,5,"// A logical shifter is LSL, LSR, ASR or ROR.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:5,Testability,log,logical,5,"// A logical vector shifter is a left shift by 0, 8, 16, or 24.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:5,Testability,log,logical,5,// A logical vector shifter is a left shift by 0 or 8.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:5,Testability,log,logical,5,// A logical vector shifter is a left shift by 8 or 16.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:288,Safety,avoid,avoids,288,"// Fallback unscaled operands are for aliases of LDR/STR that fall back; // to LDUR/STUR when the offset is not legal for the former but is for; // the latter. As such, in addition to checking for being a legal unscaled; // address, also check that it is not a legal scaled address. This avoids; // ambiguity in the matcher.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:3,Security,Validat,Validation,3,"// Validation was handled during parsing, so we just verify that; // something didn't go haywire.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:3,Security,Validat,Validation,3,"// Validation was handled during parsing, so we just verify that; // something didn't go haywire.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:13,Performance,load,load,13,"// For 8-bit load/store instructions with a register offset, both the; // ""DoShift"" and ""NoShift"" variants have a shift of 0. Because of this,; // they're disambiguated by whether the shift was explicit or implicit rather; // than its size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:41,Security,hash,hash,41,"// Immediate case, with optional leading hash:",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:80,Security,hash,hash,80,// Either an identifier for named values or a 5-bit immediate.; // Eat optional hash.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:34,Security,validat,validate,34,/// tryParseAdrpLabel - Parse and validate a source label for the ADRP; /// instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:7,Security,hash,hash,7,// Eat hash token.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:33,Security,validat,validate,33,/// tryParseAdrLabel - Parse and validate a source label for the ADR; /// instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:7,Security,hash,hash,7,// Eat hash token.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:58,Availability,error,error,58,"// Operand should start from # or should be integer, emit error otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:4,Modifiability,extend,extend,4,"// ""extend"" type operations don't need an immediate, #0 is implicit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:62,Usability,simpl,simple,62,"/// parseSysAlias - The IC, DC, AT, and TLBI instructions are simple aliases for; /// the SYS instruction. Parse them specially so that we create a SYS MCInst.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:48,Usability,simpl,simple,48,/// parseSyspAlias - The TLBIP instructions are simple aliases for; /// the SYSP instruction. Parse them specially so that we create a SYSP MCInst.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:3,Integrability,Wrap,Wrapper,3,// Wrapper around parse function,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:40,Integrability,wrap,wraparound,40,// Register must be incremental (with a wraparound at last register).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:12,Modifiability,extend,extend,12,// No shift/extend is the default.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:107,Availability,error,error,107,"// If there wasn't a custom match, try the generic matcher below. Otherwise,; // there was a match, but an error occurred, in which case, just return that; // the operand parsing failed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:41,Modifiability,extend,extend,41,"// This could be an optional ""shift"" or ""extend"" operand.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:21,Availability,error,error,21,"// We always return 'error' for this, as we're done with this; // statement and don't need to match the 'instruction.""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:24,Safety,Predict,Prediction,24,"// IC, DC, AT, TLBI and Prediction invalidation instructions are aliases for; // the SYS instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:217,Performance,load,load,217,"// After successfully parsing some operands there are three special cases; // to consider (i.e. notional operands not separated by commas). Two are; // due to memory specifiers:; // + An RBrac will end an address for load/store/prefetch; // + An '!' will indicate a pre-indexed operation.; //; // And a further case is '}', which ends a group of tokens specifying the; // SME accumulator array 'ZA' or tile vector, i.e.; //; // '{ ZA }' or '{ <ZAt><HV>.<BHSDQ>[<Wv>, #<imm>] }'; //; // It's someone else's responsibility to make sure these tokens are sane; // in the given context!",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:139,Modifiability,extend,extended,139,// FIXME: This entire function is a giant hack to provide us with decent; // operand range validation/diagnostics until TableGen/MC can be extended; // to support autogeneration of this kind of validation.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:91,Security,validat,validation,91,// FIXME: This entire function is a giant hack to provide us with decent; // operand range validation/diagnostics until TableGen/MC can be extended; // to support autogeneration of this kind of validation.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:194,Security,validat,validation,194,// FIXME: This entire function is a giant hack to provide us with decent; // operand range validation/diagnostics until TableGen/MC can be extended; // to support autogeneration of this kind of validation.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:183,Availability,failure,failure,183,// A prefix only applies to the instruction following it. Here we extract; // prefix information for the next instruction before validating the current; // one so that in the case of failure we don't erronously continue using the; // current prefix.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:129,Security,validat,validating,129,// A prefix only applies to the instruction following it. Here we extract; // prefix information for the next instruction before validating the current; // one so that in the case of failure we don't erronously continue using the; // current prefix.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:10,Security,validat,validating,10,// Before validating the instruction in isolation we run through the rules; // applicable when it follows a prefix instruction.; // NOTE: brk & hlt can be prefixed but require no additional validation.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:190,Security,validat,validation,190,// Before validating the instruction in isolation we run through the rules; // applicable when it follows a prefix instruction.; // NOTE: brk & hlt can be prefixed but require no additional validation.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:119,Performance,load,load,119,// Check for indexed addressing modes w/ the base register being the; // same as a destination/source register or pair load where; // the Rt == Rt2. All of those are undefined behaviour.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:105,Integrability,depend,depending,105,"// Don't allow symbol refs in the immediate field otherwise; // Note: Loc.back() may be Loc[1] or Loc[2] depending on the number of; // operands of the original instruction (i.e. 'add w0, w1, borked' vs; // 'cmp w0, 'borked')",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:12,Security,validat,validate,12,// We don't validate more complex expressions here,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:169,Safety,safe,safe,169,// The Cyclone CPU and early successors didn't execute the zero-cycle zeroing; // instruction for FP registers correctly in some rare circumstances. Convert; // it to a safe instruction and warn (because silently changing someone's; // assembly is rude).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:37,Usability,ux,uxtw,37,// FIXME: Horrible hack for sxtw and uxtw with Wn src and Xd dst operands.; // InstAlias can't quite handle this since the reg classes aren't; // subclasses.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:23,Usability,ux,uxt,23,// FIXME: Likewise for uxt[bh] with a Xd dst operand,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:123,Availability,failure,failure,123,"// Now, both matches failed, and the long-form match failed on the mnemonic; // suffix token operand. The short-form match failure is probably more; // relevant: use it instead.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:3,Performance,Perform,Perform,3,// Perform range checking and other semantic validations,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:45,Security,validat,validations,45,// Perform range checking and other semantic validations,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:20,Availability,error,error,20,"// Special case the error message for the very common case where only; // a single subtarget feature is missing (neon, e.g.).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:26,Integrability,message,message,26,"// Special case the error message for the very common case where only; // a single subtarget feature is missing (neon, e.g.).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:2,Performance,Tune,TuneCPU,2,/*TuneCPU*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:2,Performance,Tune,TuneCPU,2,/*TuneCPU*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:80,Integrability,depend,depends,80,"/// ::= .loh <lohName | lohId> label1, ..., labelN; /// The number of arguments depends on the loh identifier.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:10,Usability,simpl,simple,10,// It's a simple symbol reference with no addend.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:12,Modifiability,extend,extend,12,// No shift/extend is the default.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:7,Security,hash,hash,7,// Eat hash; // Parse the immediate operand.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp:105,Availability,down,down,105,// Forward declare these because the autogenerated code will reference them.; // Definitions are further down.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp:7,Performance,Scalab,Scalable,7,"// For Scalable Matrix Extension (SME) instructions that have an implicit; // operand for the accumulator (ZA) or implicit immediate zero which isn't; // encoded, manually insert operand.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp:15,Testability,assert,asserted,15,// scale{5} is asserted as 1 in tblgen.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp:8,Modifiability,extend,extend,8,// Sign-extend 19-bit immediate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp:47,Modifiability,extend,extend,47,"// offset is a 9-bit signed immediate, so sign extend it to; // fill the unsigned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp:17,Performance,load,load,17,// You shouldn't load to the same register twice in an instruction...,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp:47,Modifiability,extend,extend,47,"// offset is a 7-bit signed immediate, so sign extend it to; // fill the unsigned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp:17,Performance,load,load,17,// You shouldn't load to the same register twice in an instruction...,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp:8,Modifiability,extend,extend,8,// Sign-extend the 21-bit immediate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp:8,Modifiability,extend,extend,8,// Sign-extend the 26-bit immediate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp:8,Modifiability,extend,extend,8,// Sign-extend 14-bit immediate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp:38,Modifiability,extend,extend,38,"// Imm is a signed immediate, so sign extend it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64ExternalSymbolizer.cpp:969,Performance,load,load,969,"/// tryAddingSymbolicOperand - tryAddingSymbolicOperand trys to add a symbolic; /// operand in place of the immediate Value in the MCInst. The immediate; /// Value has not had any PC adjustment made by the caller. If the instruction; /// is a branch that adds the PC to the immediate Value then isBranch is; /// Success, else Fail. If GetOpInfo is non-null, then it is called to get any; /// symbolic information at the Address for this instrution. If that returns; /// non-zero then the symbolic information it returns is used to create an; /// MCExpr and that is added as an operand to the MCInst. If GetOpInfo(); /// returns zero and isBranch is Success then a symbol look up for; /// Address + Value is done and if a symbol is found an MCExpr is created with; /// that, else an MCExpr with Address + Value is created. If GetOpInfo(); /// returns zero and isBranch is Fail then the Opcode of the MCInst is; /// tested and for ADRP an other instructions that help to load of pointers; /// a symbol look up is done to see it is returns a specific reference type; /// to add to the comment stream. This function returns Success if it adds; /// an operand to the MCInst and Fail otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64ExternalSymbolizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64ExternalSymbolizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64ExternalSymbolizer.cpp:914,Testability,test,tested,914,"/// tryAddingSymbolicOperand - tryAddingSymbolicOperand trys to add a symbolic; /// operand in place of the immediate Value in the MCInst. The immediate; /// Value has not had any PC adjustment made by the caller. If the instruction; /// is a branch that adds the PC to the immediate Value then isBranch is; /// Success, else Fail. If GetOpInfo is non-null, then it is called to get any; /// symbolic information at the Address for this instrution. If that returns; /// non-zero then the symbolic information it returns is used to create an; /// MCExpr and that is added as an operand to the MCInst. If GetOpInfo(); /// returns zero and isBranch is Success then a symbol look up for; /// Address + Value is done and if a symbol is found an MCExpr is created with; /// that, else an MCExpr with Address + Value is created. If GetOpInfo(); /// returns zero and isBranch is Fail then the Opcode of the MCInst is; /// tested and for ADRP an other instructions that help to load of pointers; /// a symbol look up is done to see it is returns a specific reference type; /// to add to the comment stream. This function returns Success if it adds; /// an operand to the MCInst and Fail otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64ExternalSymbolizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64ExternalSymbolizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64ExternalSymbolizer.cpp:157,Integrability,interface,interface,157,// FIXME: This method shares a lot of code with; // MCExternalSymbolizer::tryAddingSymbolicOperand. It may be possible; // refactor the MCExternalSymbolizer interface to allow more of this; // implementation to be shared.; //,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64ExternalSymbolizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64ExternalSymbolizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64ExternalSymbolizer.cpp:123,Modifiability,refactor,refactor,123,// FIXME: This method shares a lot of code with; // MCExternalSymbolizer::tryAddingSymbolicOperand. It may be possible; // refactor the MCExternalSymbolizer interface to allow more of this; // implementation to be shared.; //,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64ExternalSymbolizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64ExternalSymbolizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp:34,Modifiability,extend,extend,34,"// For varargs, we always want to extend them to 8 bytes, in which case; // we disable setting a max.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp:37,Energy Efficiency,allocate,allocated,37,// The store does not cover the full allocated stack slot.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp:3,Performance,Cache,Cache,3,// Cache the SP register vreg if we need it more than once in this call site.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp:63,Modifiability,extend,extended,63,// i1 is a special case because SDAG i1 true is naturally zero extended; // when widened using ANYEXT. We need to do it explicitly here.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp:24,Modifiability,extend,extending,24,// Some types will need extending as specified by the CC.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp:17,Modifiability,extend,extend,17,"// Instead of an extend, we might have a vector type which needs; // padding with more elements, e.g. <2 x half> -> <4 x half>.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp:20,Modifiability,extend,extend,20,// Just do a vector extend.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp:12,Modifiability,extend,extend,12,// A scalar extend.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp:25,Modifiability,extend,extended,25,// i1 arguments are zero-extended to i8 by the caller. Emit a; // hint to reflect this.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp:39,Testability,Assert,AssertZExt,39,"// Lower i1 argument as i8, and insert AssertZExt + Trunc later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp:65,Performance,optimiz,optimize,65,// Must pass all target-independent checks in order to tail call optimize.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp:215,Testability,assert,assert,215,"// We don't have -tailcallopt, so we're allowed to change the ABI (sibcall).; // Try to find cases where we can do that.; // I want anyone implementing a new calling convention to think long and hard; // about this assert.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp:75,Safety,safe,safe,75,// Verify that the incoming and outgoing arguments from the callee are; // safe to tail call.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp:45,Availability,mask,mask,45,"// For 'this' returns, use the X0-preserving mask if applicable",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp:160,Availability,avail,available,160,"// This will be 0 for sibcalls, potentially nonzero for tail calls produced; // by -tailcallopt. For sibcalls, the memory operands for the call are; // already available in the caller's incoming argument space.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp:3,Deployability,Update,Update,3,// Update the required reserved area if this is the tail call requiring the; // most argument stack space.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp:31,Modifiability,extend,extend,31,// AAPCS requires that we zero-extend i1 to 8 bits by the caller.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp:68,Modifiability,extend,extend,68,"// We cannot use a ZExt ArgInfo flag here, because it will; // zero-extend the argument to i32 instead of just i8.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64GlobalISelUtils.cpp:467,Deployability,pipeline,pipeline,467,"//===- AArch64GlobalISelUtils.cpp --------------------------------*- C++ -*-==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; /// \file Implementations of AArch64-specific helper functions used in the; /// GlobalISel pipeline.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64GlobalISelUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64GlobalISelUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64GlobalISelUtils.cpp:22,Availability,mask,mask,22,"// All of the compare-mask comparisons are ordered, but we can switch; // between the two by a double inversion. E.g. ULE == !OGT.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64GlobalISelUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64GlobalISelUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64GlobalISelUtils.h:457,Deployability,pipeline,pipeline,457,"//===- AArch64GlobalISelUtils.h ----------------------------------*- C++ -*-==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; /// \file APIs for AArch64-specific helper functions used in the GlobalISel; /// pipeline.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64GlobalISelUtils.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64GlobalISelUtils.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:75,Performance,cache,cache,75,"// hasFnAttribute() is expensive to call on every BRCOND selection, so; // cache it here for each run of the selector.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:119,Performance,load,load,119,"/// Emit a sequence of instructions representing a constant \p CV for a; /// vector register \p Dst. (E.g. a MOV, or a load from a constant pool.); ///; /// \returns the last instruction in the sequence on success, and nullptr; /// otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:37,Performance,load,load,37,"/// Helper function to select vector load intrinsics like; /// @llvm.aarch64.neon.ld2.*, @llvm.aarch64.neon.ld4.*, etc.; /// \p Opc is the opcode that the selected instruction should use.; /// \p NumVecs is the number of vector destinations for the instruction.; /// \p I is the original G_INTRINSIC_W_SIDE_EFFECTS instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:145,Performance,optimiz,optimize,145,"/// Emit a TB(N)Z instruction which tests \p Bit in \p TestReg.; /// \p IsNegative is true if the test should be ""not zero"".; /// This will also optimize the test bit instruction when possible.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:36,Testability,test,tests,36,"/// Emit a TB(N)Z instruction which tests \p Bit in \p TestReg.; /// \p IsNegative is true if the test should be ""not zero"".; /// This will also optimize the test bit instruction when possible.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:55,Testability,Test,TestReg,55,"/// Emit a TB(N)Z instruction which tests \p Bit in \p TestReg.; /// \p IsNegative is true if the test should be ""not zero"".; /// This will also optimize the test bit instruction when possible.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:98,Testability,test,test,98,"/// Emit a TB(N)Z instruction which tests \p Bit in \p TestReg.; /// \p IsNegative is true if the test should be ""not zero"".; /// This will also optimize the test bit instruction when possible.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:158,Testability,test,test,158,"/// Emit a TB(N)Z instruction which tests \p Bit in \p TestReg.; /// \p IsNegative is true if the test should be ""not zero"".; /// This will also optimize the test bit instruction when possible.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:101,Modifiability,extend,extend,101,"/// Returns a \p ComplexRendererFns which contains a base, offset, and whether; /// or not a shift + extend should be folded into an addressing mode. Returns; /// None when this is not profitable or possible.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:13,Modifiability,extend,extend,13,"/// Given an extend instruction, determine the correct shift-extend type for; /// that instruction.; ///; /// If the instruction is going to be used in a load or store, pass; /// \p IsLoadStore = true.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:61,Modifiability,extend,extend,61,"/// Given an extend instruction, determine the correct shift-extend type for; /// that instruction.; ///; /// If the instruction is going to be used in a load or store, pass; /// \p IsLoadStore = true.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:154,Performance,load,load,154,"/// Given an extend instruction, determine the correct shift-extend type for; /// that instruction.; ///; /// If the instruction is going to be used in a load or store, pass; /// \p IsLoadStore = true.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:3,Performance,Optimiz,Optimization,3,// Optimization methods.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:30,Performance,load,load,30,/// Return true if \p MI is a load or store of \p NumBytes bytes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:163,Modifiability,extend,extended,163,"/// Returns true if \p MI is guaranteed to have the high-half of a 64-bit; /// register zeroed out. In other words, the result of MI has been explicitly; /// zero extended.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:8,Performance,cache,cached,8,"// Some cached values used during selection.; // We use LR as a live-in register, and we keep track of it here as it can be; // clobbered by calls.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:199,Availability,avail,available,199,"/// Create a REG_SEQUENCE instruction using the registers in \p Regs.; /// Helper function for functions like createDTuple and createQTuple.; ///; /// \p RegClassIDs - The list of register class IDs available for some tuple of; /// a scalar class. E.g. QQRegClassID, QQQRegClassID, QQQQRegClassID. This is; /// expected to contain between 2 and 4 tuple classes.; ///; /// \p SubRegs - The list of subregister classes associated with each register; /// class ID in \p RegClassIDs. E.g., QQRegClassID should use the qsub0; /// subregister class. The index of each subregister class is expected to; /// correspond with the index of each register class.; ///; /// \returns Either the destination register of REG_SEQUENCE instruction that; /// was created, or the 0th element of \p Regs if \p Regs contains a single; /// element.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:155,Security,access,access,155,"/// Select the AArch64 opcode for the G_LOAD or G_STORE operation \p GenericOpc,; /// appropriate for the (value) register bank \p RegBankID and of memory access; /// size \p OpSize. This returns the variant with the base+unsigned-immediate; /// addressing mode (e.g., LDRXui).; /// \returns \p GenericOpc if the combination is unsupported.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:97,Usability,simpl,simple,97,// FIXME: We need some sort of API in RBI/TRI to allow generic code to; // constrain operands of simple instructions given a TargetRegisterClass; // and LLT,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:72,Performance,perform,perform,72,// If the source register is bigger than the destination we need to; // perform a subregister copy.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:50,Availability,down,down,50,// If this a GPR ZEXT that we want to just reduce down into a copy.; // The sizes will be mismatched with the source < 32b but that's ok.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:43,Energy Efficiency,reduce,reduce,43,// If this a GPR ZEXT that we want to just reduce down into a copy.; // The sizes will be mismatched with the source < 32b but that's ok.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:52,Testability,test,test,52,/// Return a register which can be used as a bit to test in a TB(N)Z.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:58,Modifiability,extend,extended,58,"// (tbz (any_ext x), b) -> (tbz x, b) if we don't use the extended bits.; //; // (tbz (trunc x), b) -> (tbz x, b) is always safe, because the bit number; // on the truncated x is the same as the bit number on x.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:124,Safety,safe,safe,124,"// (tbz (any_ext x), b) -> (tbz x, b) if we don't use the extended bits.; //; // (tbz (trunc x), b) -> (tbz x, b) is always safe, because the bit number; // on the truncated x is the same as the bit number on x.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:14,Performance,optimiz,optimize,14,// Attempt to optimize the test bit by walking over instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:27,Testability,test,test,27,// Attempt to optimize the test bit by walking over instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:12,Testability,test,test,12,"// When the test register is a 64-bit register, we have to narrow to make; // TBNZW work.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:524,Availability,mask,mask,524,"// Given something like this:; //; // %x = ...Something...; // %one = G_CONSTANT i64 1; // %zero = G_CONSTANT i64 0; // %and = G_AND %x, %one; // %cmp = G_ICMP intpred(ne), %and, %zero; // %cmp_trunc = G_TRUNC %cmp; // G_BRCOND %cmp_trunc, %bb.3; //; // We want to try and fold the AND into the G_BRCOND and produce either a; // TBNZ (when we have intpred(ne)) or a TBZ (when we have intpred(eq)).; //; // In this case, we'd get; //; // TBNZ %x %bb.3; //; // Check if the AND has a constant on its RHS which we can use as a mask.; // If it's a power of 2, then it's the same as checking a specific bit.; // (e.g, ANDing with 8 == ANDing with 000...100 == testing if bit 3 is set)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:544,Energy Efficiency,power,power,544,"// Given something like this:; //; // %x = ...Something...; // %one = G_CONSTANT i64 1; // %zero = G_CONSTANT i64 0; // %and = G_AND %x, %one; // %cmp = G_ICMP intpred(ne), %and, %zero; // %cmp_trunc = G_TRUNC %cmp; // G_BRCOND %cmp_trunc, %bb.3; //; // We want to try and fold the AND into the G_BRCOND and produce either a; // TBNZ (when we have intpred(ne)) or a TBZ (when we have intpred(eq)).; //; // In this case, we'd get; //; // TBNZ %x %bb.3; //; // Check if the AND has a constant on its RHS which we can use as a mask.; // If it's a power of 2, then it's the same as checking a specific bit.; // (e.g, ANDing with 8 == ANDing with 000...100 == testing if bit 3 is set)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:655,Testability,test,testing,655,"// Given something like this:; //; // %x = ...Something...; // %one = G_CONSTANT i64 1; // %zero = G_CONSTANT i64 0; // %and = G_AND %x, %one; // %cmp = G_ICMP intpred(ne), %and, %zero; // %cmp_trunc = G_TRUNC %cmp; // G_BRCOND %cmp_trunc, %bb.3; //; // We want to try and fold the AND into the G_BRCOND and produce either a; // TBNZ (when we have intpred(ne)) or a TBZ (when we have intpred(eq)).; //; // In this case, we'd get; //; // TBNZ %x %bb.3; //; // Check if the AND has a constant on its RHS which we can use as a mask.; // If it's a power of 2, then it's the same as checking a specific bit.; // (e.g, ANDing with 8 == ANDing with 000...100 == testing if bit 3 is set)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:14,Performance,optimiz,optimize,14,"// Attempt to optimize the G_BRCOND + G_ICMP into a TB(N)Z/CB(N)Z.; //; // Speculation tracking/SLH assumes that optimized TB(N)Z/CB(N)Z; // instructions will not be produced, as they are conditional branch; // instructions that do not set flags.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:113,Performance,optimiz,optimized,113,"// Attempt to optimize the G_BRCOND + G_ICMP into a TB(N)Z/CB(N)Z.; //; // Speculation tracking/SLH assumes that optimized TB(N)Z/CB(N)Z; // instructions will not be produced, as they are conditional branch; // instructions that do not set flags.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:235,Availability,redundant,redundant,235,"// When we can emit a TB(N)Z, prefer that.; //; // Handle non-commutative condition codes first.; // Note that we don't want to do this when we have a G_AND because it can; // become a tst. The tst will make the test bit in the TB(N)Z redundant.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:235,Safety,redund,redundant,235,"// When we can emit a TB(N)Z, prefer that.; //; // Handle non-commutative condition codes first.; // Note that we don't want to do this when we have a G_AND because it can; // become a tst. The tst will make the test bit in the TB(N)Z redundant.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:212,Testability,test,test,212,"// When we can emit a TB(N)Z, prefer that.; //; // Handle non-commutative condition codes first.; // Note that we don't want to do this when we have a G_AND because it can; // become a tst. The tst will make the test bit in the TB(N)Z redundant.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:55,Testability,test,test,55,"// When we have a greater-than comparison, we can just test if the msb is; // zero.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:52,Testability,test,test,52,"// When we have a less than comparison, we can just test if the msb is not; // zero.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:87,Testability,test,test,87,"// Inversely, if we have a signed greater-than-or-equal comparison to zero,; // we can test if the msb is zero.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:269,Availability,redundant,redundant,269,"// If there's a G_AND feeding into this branch, try to fold it away by; // emitting a TB(N)Z instead.; //; // Note: If we have LT, then it *is* possible to fold, but it wouldn't be; // beneficial. When we have an AND and LT, we need a TST/ANDS, so folding; // would be redundant.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:269,Safety,redund,redundant,269,"// If there's a G_AND feeding into this branch, try to fold it away by; // emitting a TB(N)Z instead.; //; // Note: If we have LT, then it *is* possible to fold, but it wouldn't be; // beneficial. When we have an AND and LT, we need a TST/ANDS, so folding; // would be redundant.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:12,Performance,optimiz,optimize,12,// Couldn't optimize. Emit a compare + a Bcc.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:41,Performance,optimiz,optimized,41,"// Speculation tracking/SLH assumes that optimized TB(N)Z/CB(N)Z; // instructions will not be produced, as they are conditional branch; // instructions that do not set flags.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:95,Safety,detect,detect,95,"/// Returns the element immediate value of a vector shift operand if found.; /// This needs to detect a splat-like operation, e.g. a G_BUILD_VECTOR.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:14,Performance,load,loads,14,"// For scalar loads of pointers, we try to convert the dest type from p0; // to s64 so that our imported patterns can match. Like with the G_PTR_ADD; // conversion, this should be ok because all users should have been; // selected already, so the type doesn't matter for them.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:284,Performance,load,loads,284,"/// This lowering tries to look for G_PTR_ADD instructions and then converts; /// them to a standard G_ADD with a COPY on the source.; ///; /// The motivation behind this is to expose the add semantics to the imported; /// tablegen patterns. We shouldn't need to check for uses being loads/stores,; /// because the selector works bottom up, uses before defs. By the time we; /// end up trying to select a G_PTR_ADD, we should have already attempted to; /// fold this into addressing modes and were therefore unsuccessful.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:177,Security,expose,expose,177,"/// This lowering tries to look for G_PTR_ADD instructions and then converts; /// them to a standard G_ADD with a COPY on the source.; ///; /// The motivation behind this is to expose the add semantics to the imported; /// tablegen patterns. We shouldn't need to check for uses being loads/stores,; /// because the selector works bottom up, uses before defs. By the time we; /// end up trying to select a G_PTR_ADD, we should have already attempted to; /// fold this into addressing modes and were therefore unsuccessful.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:52,Performance,optimiz,optimization,52,// Also take the opportunity here to try to do some optimization.; // Try to convert this into a G_SUB if the offset is a 0-x negate idiom.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:86,Performance,load,load,86,"// Before selecting a DUP instruction, check if it is better selected as a; // MOV or load from a constant pool.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:93,Modifiability,extend,extend,93,// Check for i64 sext(i32 vector_extract) prior to tablegen to select SMOV; // over a normal extend.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:136,Availability,Mask,MaskSrc,136,// Look for operations that take the lower `Width=Size-ShiftImm` bits of; // `ShiftSrc` and insert them into the upper `Width` bits of `MaskSrc` via; // shifting and masking that we can replace with a BFI (encoded as a BFM).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:166,Availability,mask,masking,166,// Look for operations that take the lower `Width=Size-ShiftImm` bits of; // `ShiftSrc` and insert them into the upper `Width` bits of `MaskSrc` via; // shifting and masking that we can replace with a BFI (encoded as a BFM).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:294,Integrability,rout,routines,294,"// There may be patterns where the importer can't deal with them optimally,; // but does select it to a suboptimal sequence so our custom C++ selection; // code later never has a chance to work on it. Therefore, we have an early; // selection attempt here to give priority to certain selection routines; // over the imported ones.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:281,Energy Efficiency,schedul,scheduling,281,"// This op may have been separated from it's ADRP companion by the localizer; // or some other code motion pass. Given that many CPUs will try to; // macro fuse these operations anyway, select this into a MOVaddr pseudo; // which will later be expanded into an ADRP+ADD pair after scheduling.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:10,Availability,Redundant,Redundant,10,"// FIXME: Redundant check, but even less readable when factored out.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:10,Safety,Redund,Redundant,10,"// FIXME: Redundant check, but even less readable when factored out.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:53,Performance,load,load,53,"// For 16, 64, and 128b values, emit a constant pool load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:84,Performance,load,load,84,"// If TLI says that this fpimm is illegal, then we'll expand to a; // constant pool load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:21,Modifiability,extend,extending,21,"// If this is an any-extending load from the FPR bank, split it into a regular; // load + extend.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:90,Modifiability,extend,extend,90,"// If this is an any-extending load from the FPR bank, split it into a regular; // load + extend.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:31,Performance,load,load,31,"// If this is an any-extending load from the FPR bank, split it into a regular; // load + extend.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:83,Performance,load,load,83,"// If this is an any-extending load from the FPR bank, split it into a regular; // load + extend.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:31,Modifiability,extend,extend,31,// Generate a SUBREG_TO_REG to extend it.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:96,Deployability,update,updated,96,"// Helper lambda for partially selecting I. Either returns the original; // instruction with an updated opcode, or a new instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:41,Performance,load,load,41,// If we have a ZEXTLOAD then change the load's type to be a narrower reg; // and zero_extend with SUBREG_TO_REG.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:80,Modifiability,extend,extended,80,"// SEXT_INREG has the same src reg size as dst, the size of the value to be; // extended is encoded in the imm.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:121,Availability,redundant,redundant,121,"// First check if we're extending the result of a load which has a dest type; // smaller than 32 bits, then this zext is redundant. GPR32 is the smallest; // GPR register on AArch64 and all loads which are smaller automatically; // zero-extend the upper bits. E.g.; // %v(s8) = G_LOAD %p, :: (load 1); // %v2(s32) = G_ZEXT %v(s8)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:24,Modifiability,extend,extending,24,"// First check if we're extending the result of a load which has a dest type; // smaller than 32 bits, then this zext is redundant. GPR32 is the smallest; // GPR register on AArch64 and all loads which are smaller automatically; // zero-extend the upper bits. E.g.; // %v(s8) = G_LOAD %p, :: (load 1); // %v2(s32) = G_ZEXT %v(s8)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:237,Modifiability,extend,extend,237,"// First check if we're extending the result of a load which has a dest type; // smaller than 32 bits, then this zext is redundant. GPR32 is the smallest; // GPR register on AArch64 and all loads which are smaller automatically; // zero-extend the upper bits. E.g.; // %v(s8) = G_LOAD %p, :: (load 1); // %v2(s32) = G_ZEXT %v(s8)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:50,Performance,load,load,50,"// First check if we're extending the result of a load which has a dest type; // smaller than 32 bits, then this zext is redundant. GPR32 is the smallest; // GPR register on AArch64 and all loads which are smaller automatically; // zero-extend the upper bits. E.g.; // %v(s8) = G_LOAD %p, :: (load 1); // %v2(s32) = G_ZEXT %v(s8)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:190,Performance,load,loads,190,"// First check if we're extending the result of a load which has a dest type; // smaller than 32 bits, then this zext is redundant. GPR32 is the smallest; // GPR register on AArch64 and all loads which are smaller automatically; // zero-extend the upper bits. E.g.; // %v(s8) = G_LOAD %p, :: (load 1); // %v2(s32) = G_ZEXT %v(s8)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:293,Performance,load,load,293,"// First check if we're extending the result of a load which has a dest type; // smaller than 32 bits, then this zext is redundant. GPR32 is the smallest; // GPR register on AArch64 and all loads which are smaller automatically; // zero-extend the upper bits. E.g.; // %v(s8) = G_LOAD %p, :: (load 1); // %v2(s32) = G_ZEXT %v(s8)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:121,Safety,redund,redundant,121,"// First check if we're extending the result of a load which has a dest type; // smaller than 32 bits, then this zext is redundant. GPR32 is the smallest; // GPR register on AArch64 and all loads which are smaller automatically; // zero-extend the upper bits. E.g.; // %v(s8) = G_LOAD %p, :: (load 1); // %v2(s32) = G_ZEXT %v(s8)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:17,Safety,avoid,avoid,17,// FIXME: Can we avoid manually doing this?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:175,Performance,optimiz,optimizer,175,"// Imported SelectionDAG rules can handle every bitcast except those that; // bitcast from a type to the same type. Ideally, these shouldn't occur; // but we might not run an optimizer that deletes them. The other exception; // is bitcasts involving pointer types, as SelectionDAG has no knowledge; // of them.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:76,Performance,optimiz,optimizations,76,"// Make sure to use an unused vreg instead of wzr, so that the peephole; // optimizations will be able to optimize these.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:106,Performance,optimiz,optimize,106,"// Make sure to use an unused vreg instead of wzr, so that the peephole; // optimizations will be able to optimize these.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:299,Availability,robust,robust,299,"// When the scalar of G_DUP is an s8/s16 gpr, they can't be selected by; // imported patterns. Do it manually here. Avoiding generating s16 gpr is; // difficult because at RBS we may end up pessimizing the fpr case if we; // decided to add an anyextend to fix this. Manual selection is the most; // robust solution for now.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:116,Safety,Avoid,Avoiding,116,"// When the scalar of G_DUP is an s8/s16 gpr, they can't be selected by; // imported patterns. Do it manually here. Avoiding generating s16 gpr is; // difficult because at RBS we may end up pessimizing the fpr case if we; // decided to add an anyextend to fix this. Manual selection is the most; // robust solution for now.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:61,Deployability,update,update,61,"// New instruction uses the copied registers because it must update them.; // The defs are not used since they don't exist in G_MEM*. They are still; // tied.; // Note: order of operands is different from G_MEMSET, G_MEMCPY, G_MEMMOVE",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:18,Performance,perform,perform,18,"// No. We have to perform subregister inserts. For each insert, create an; // implicit def and a subregister insert, and save the register we create.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:99,Performance,Perform,Perform,99,"// Now that we've created any necessary subregister inserts, we can; // create the copies.; //; // Perform the first copy separately as a subregister copy.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:8,Performance,perform,perform,8,"// Now, perform the remaining copies as vector lane copies.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:7,Performance,load,load,7,// Use load(literal) for tiny code model.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:16,Testability,log,logical,16,// ANDS needs a logical immediate for its immediate form. Check if we can; // fold one in.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:223,Integrability,depend,depends,223,"// If the previous instruction will already produce the correct carry, do not; // emit a carry generating instruction. E.g. for G_UADDE/G_USUBE sequences; // generated during legalization of wide add/sub. This optimization depends on; // these sequences not being interrupted by other instructions.; // We have to select the previous instruction before the carry-using; // instruction is deleted by the calling function, otherwise the previous; // instruction might become dead and would get deleted.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:210,Performance,optimiz,optimization,210,"// If the previous instruction will already produce the correct carry, do not; // emit a carry generating instruction. E.g. for G_UADDE/G_USUBE sequences; // generated during legalization of wide add/sub. This optimization depends on; // these sequences not being interrupted by other instructions.; // We have to select the previous instruction before the carry-using; // instruction is deleted by the calling function, otherwise the previous; // instruction might become dead and would get deleted.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:225,Testability,test,tests,225,/// Returns true if @p Val is a tree of AND/OR/CMP operations that can be; /// expressed as a conjunction.; /// \param CanNegate Set to true if we can negate the whole sub-tree just by; /// changing the conditions on the CMP tests.; /// (this means we can call emitConjunctionRec() with; /// Negate==true on this sub-tree); /// \param MustBeFirst Set to true if this subtree needs to be negated and we; /// cannot do the negation naturally. We are required to; /// emit the subtree first in this case.; /// \param WillNegate Is true if are called when the result of this; /// subexpression must be negated. This happens when the; /// outer expression is an OR. We can use this fact to know; /// that we have a double negation (or (or ...) ...) that; /// can be implemented for free.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:24,Performance,optimiz,optimization,24,// TODO: emit CMN as an optimization.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:43,Testability,test,tested,43,// Some floating point conditions can't be tested with a single condition; // code. Construct an additional comparison in this case.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:76,Performance,optimiz,optimized,76,"// For now, any undef indexes we'll just assume to be 0. This should be; // optimized in future, e.g. to select DUP etc.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:26,Performance,load,load,26,// Use a constant pool to load the index vector for TBL.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:21,Performance,load,load,21,"// The constant pool load will be 64 bits, so need to convert to FPR128 reg.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:41,Integrability,depend,depending,41,"// We may need to generate one of these, depending on the type and sign of the; // input:; // DstReg = SMOV Src0, Lane;; // NewReg = UMOV Src0, Lane; DstReg = SUBREG_TO_REG NewReg, sub_32;",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:3,Performance,Perform,Perform,3,// Perform the lane insert.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:25,Performance,perform,perform,25,"// If we had to widen to perform the insert, then we have to demote back to; // the original size to get the result we want.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:27,Performance,load,load,27,"// Make sure to select the load with the MemTy as the dest type, and then; // insert into X reg if needed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:100,Performance,load,load,100,"// Check if we're building a constant vector, in which case we want to; // generate a constant pool load instead of a vector insert sequence.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:29,Performance,optimiz,optimized,29,"// Until we port more of the optimized selections, for now just use a vector; // insert sequence.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:76,Modifiability,refactor,refactor,76,// Emit the subreg copies and immediately select them.; // FIXME: We should refactor our copy code into an emitCopy helper and; // clean up uses of this pattern elsewhere in the selector.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:172,Deployability,update,updated,172,"// Transform; // %dst:gpr(p0) = \; // G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aarch64.mops.memset.tag),; // \ %dst:gpr(p0), %val:gpr(s64), %n:gpr(s64); // where %dst is updated, into; // %Rd:GPR64common, %Rn:GPR64) = \; // MOPSMemorySetTaggingPseudo \; // %Rd:GPR64common, %Rn:GPR64, %Rm:GPR64; // where Rd and Rn are tied.; // It is expected that %val has been extended to s64 in legalization.; // Note that the order of the size/value operands are swapped.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:365,Modifiability,extend,extended,365,"// Transform; // %dst:gpr(p0) = \; // G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aarch64.mops.memset.tag),; // \ %dst:gpr(p0), %val:gpr(s64), %n:gpr(s64); // where %dst is updated, into; // %Rd:GPR64common, %Rn:GPR64) = \; // MOPSMemorySetTaggingPseudo \; // %Rd:GPR64common, %Rn:GPR64, %Rm:GPR64; // where Rd and Rn are tied.; // It is expected that %val has been extended to s64 in legalization.; // Note that the order of the size/value operands are swapped.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:142,Deployability,update,updated,142,// MOPSMemorySetTaggingPseudo has two defs; the intrinsic call has only one.; // Therefore an additional virtual register is requried for the updated size; // operand. This value is not accessible via the semantics of the intrinsic.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:186,Security,access,accessible,186,// MOPSMemorySetTaggingPseudo has two defs; the intrinsic call has only one.; // Therefore an additional virtual register is requried for the updated size; // operand. This value is not accessible via the semantics of the intrinsic.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:28,Testability,assert,assert,28,// FIXME: Should this be an assert?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:50,Modifiability,extend,extended,50,"/// Return true if it is worth folding MI into an extended register. That is,; /// if it's safe to pull it into the addressing mode of a load or store as a; /// shift.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:137,Performance,load,load,137,"/// Return true if it is worth folding MI into an extended register. That is,; /// if it's safe to pull it into the addressing mode of a load or store as a; /// shift.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:91,Safety,safe,safe,91,"/// Return true if it is worth folding MI into an extended register. That is,; /// if it's safe to pull it into the addressing mode of a load or store as a; /// shift.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:48,Performance,optimiz,optimizing,48,"// Always fold if there is one use, or if we're optimizing for size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:18,Safety,avoid,avoid,18,// It's better to avoid folding and recomputing shifts when we don't have a; // fastpath.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:81,Energy Efficiency,power,power,81,"// Since we're going to pull this into a shift, the constant value must be; // a power of 2. If we got a multiply, then we need to check this.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:9,Energy Efficiency,power,power,9,"// Got a power of 2. So, the amount we'll shift is the log base-2 of that.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:55,Testability,log,log,55,"// Got a power of 2. So, the amount we'll shift is the log base-2 of that.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:40,Modifiability,extend,extend,40,"// Check if the offset is defined by an extend, unless we looked through a; // G_ZEXT earlier.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:215,Performance,load,load,215,"/// This is used for computing addresses like this:; ///; /// ldr x1, [x2, x3, lsl #3]; ///; /// Where x2 is the base register, and x3 is an offset register. The shift-left; /// is a constant value specific to this load instruction. That is, we'll never; /// see anything other than a 3 here (which corresponds to the size of the; /// element being loaded.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:349,Performance,load,loaded,349,"/// This is used for computing addresses like this:; ///; /// ldr x1, [x2, x3, lsl #3]; ///; /// Where x2 is the base register, and x3 is an offset register. The shift-left; /// is a constant value specific to this load instruction. That is, we'll never; /// see anything other than a 3 here (which corresponds to the size of the; /// element being loaded.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:130,Performance,load,loads,130,/// This is intended to be equivalent to selectAddrModeXRO in; /// AArch64ISelDAGtoDAG. It's used for selecting X register offset loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:47,Performance,load,load,47,// Skip immediates that can be selected in the load/store addresing; // mode.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:194,Modifiability,extend,extend,194,"/// This is used for computing addresses like this:; ///; /// ldr x0, [xBase, wOffset, sxtw #LegalShiftVal]; ///; /// Where we have a 64-bit base register, a 32-bit offset register, and an; /// extend (which may or may not be signed).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:70,Modifiability,extend,extend,70,"// The first case is the same as selectAddrModeXRO, except we need an extend.; // In this case, we try to find a shift and extend, and fold them into the; // addressing mode.; //; // E.g.; //; // off_reg = G_Z/S/ANYEXT ext_reg; // val = G_CONSTANT LegalShiftVal; // shift = G_SHL off_reg val; // ptr = G_PTR_ADD base_reg shift; // x = G_LOAD ptr; //; // In this case we can get a load like this:; //; // ldr x0, [base_reg, ext_reg, sxtw #LegalShiftVal]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:123,Modifiability,extend,extend,123,"// The first case is the same as selectAddrModeXRO, except we need an extend.; // In this case, we try to find a shift and extend, and fold them into the; // addressing mode.; //; // E.g.; //; // off_reg = G_Z/S/ANYEXT ext_reg; // val = G_CONSTANT LegalShiftVal; // shift = G_SHL off_reg val; // ptr = G_PTR_ADD base_reg shift; // x = G_LOAD ptr; //; // In this case we can get a load like this:; //; // ldr x0, [base_reg, ext_reg, sxtw #LegalShiftVal]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:380,Performance,load,load,380,"// The first case is the same as selectAddrModeXRO, except we need an extend.; // In this case, we try to find a shift and extend, and fold them into the; // addressing mode.; //; // E.g.; //; // off_reg = G_Z/S/ANYEXT ext_reg; // val = G_CONSTANT LegalShiftVal; // shift = G_SHL off_reg val; // ptr = G_PTR_ADD base_reg shift; // x = G_LOAD ptr; //; // In this case we can get a load like this:; //; // ldr x0, [base_reg, ext_reg, sxtw #LegalShiftVal]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:23,Modifiability,extend,extend,23,// Check if this is an extend. We'll get an extend type if it is.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:44,Modifiability,extend,extend,44,// Check if this is an extend. We'll get an extend type if it is.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:19,Modifiability,extend,extend,19,// Handle explicit extend instructions first.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:72,Availability,mask,mask,72,// Don't have an explicit extend. Try to handle a G_AND with a constant mask; // on the RHS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:26,Modifiability,extend,extend,26,// Don't have an explicit extend. Try to handle a G_AND with a constant mask; // on the RHS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:15,Modifiability,extend,extended,15,"/// Select an ""extended register"" operand. This operand folds in an extend; /// followed by an optional left shift.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:68,Modifiability,extend,extend,68,"/// Select an ""extended register"" operand. This operand folds in an extend; /// followed by an optional left shift.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:39,Modifiability,extend,extend,39,// Check if we can fold a shift and an extend.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:20,Modifiability,extend,extend,20,// Look for a valid extend instruction on the LHS of the shift.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:43,Modifiability,extend,extend,43,// Didn't get a shift. Try just folding an extend.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:109,Modifiability,extend,extend,109,"// If we have a 32 bit instruction which zeroes out the high half of a; // register, we get an implicit zero extend for free. Check if we have one.; // FIXME: We actually emit the extend right now even though we don't have; // to.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:180,Modifiability,extend,extend,180,"// If we have a 32 bit instruction which zeroes out the high half of a; // register, we get an implicit zero extend for free. Check if we have one.; // FIXME: We actually emit the extend right now even though we don't have; // to.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:3,Performance,Perform,Perform,3,// Perform fixups on the given PHI instruction's operands to force them all; // to be the same as the destination regbank.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:599,Usability,simpl,simplicity,599,"// We need to do some work here if the operand types are < 16 bit and they; // are split across fpr/gpr banks. Since all types <32b on gpr; // end up being assigned gpr32 regclasses, we can end up with PHIs here; // which try to select between a gpr32 and an fpr16. Ideally RBS shouldn't; // be selecting heterogenous regbanks for operands if possible, but we; // still need to be able to deal with it here.; //; // To fix this, if we have a gpr-bank operand < 32b in size and at least; // one other operand is on the fpr bank, then we add cross-bank copies; // to homogenize the operand banks. For simplicity the bank that we choose; // to settle on is whatever bank the def operand has. For example:; //; // %endbb:; // %dst:gpr(s16) = G_PHI %in1:gpr(s16), %bb1, %in2:fpr(s16), %bb2; // =>; // %bb2:; // ...; // %in2_copy:gpr(s16) = COPY %in2:fpr(s16); // ...; // %endbb:; // %dst:gpr(s16) = G_PHI %in1:gpr(s16), %bb1, %in2_copy:gpr(s16), %bb2",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:159,Safety,detect,detect,159,// Making clamping conditional on CSSC extension as without legal types we; // lower to CMP which can fold one of the two sxtb's we'd otherwise need; // if we detect a type smaller than 32-bit.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:21,Modifiability,extend,extending,21,// Atomics have zero extending behavior.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:9,Modifiability,extend,extends,9,// These extends are also legal,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:9,Modifiability,extend,extending,9,// Clamp extending load results to 32-bits.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:19,Performance,load,load,19,// Clamp extending load results to 32-bits.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:3,Modifiability,Extend,Extending,3,// Extending to a scalar s128 needs narrowing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:28,Modifiability,EXTEND,EXTEND,28,// Tries to convert a large EXTEND into two smaller EXTENDs,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:52,Modifiability,EXTEND,EXTENDs,52,// Tries to convert a large EXTEND into two smaller EXTENDs,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:155,Testability,test,tests,155,// FIXME: This is wrong since G_BITCAST is not allowed to change the; // number of bits but it's what the previous code described and fixing; // it breaks tests.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:51,Availability,avail,available,51,// For fadd reductions we have pairwise operations available. We treat the; // usual legal types as legal and handle the lowering to pairwise instructions; // later.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:16,Availability,down,down,16,// Try to break down into smaller vectors as long as they're at least 64; // bits. This lets us use vector operations for some parts of the; // reduction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:18,Performance,perform,perform,18,// We can usually perform 64b vector operations.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:3,Security,Access,Access,3,// Access to floating-point environment.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:58,Performance,optimiz,optimizer,58,// Lower non-constant shifts and leave zero shifts to the optimizer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:142,Performance,optimiz,optimize,142,"// We do this custom legalization to convert G_GLOBAL_VALUE into target ADRP +; // G_ADD_LOW instructions.; // By splitting this here, we can optimize accesses in the small code model by; // folding in the G_ADD_LOW into the load/store offset.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:225,Performance,load,load,225,"// We do this custom legalization to convert G_GLOBAL_VALUE into target ADRP +; // G_ADD_LOW instructions.; // By splitting this here, we can optimize accesses in the small code model by; // folding in the G_ADD_LOW into the load/store offset.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:151,Security,access,accesses,151,"// We do this custom legalization to convert G_GLOBAL_VALUE into target ADRP +; // G_ADD_LOW instructions.; // By splitting this here, we can optimize accesses in the small code model by; // folding in the G_ADD_LOW into the load/store offset.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:816,Performance,load,loaded,816,"// MO_TAGGED on the page indicates a tagged address. Set the tag now. We do so; // by creating a MOVK that sets bits 48-63 of the register to (global address; // + 0x100000000 - PC) >> 48. The additional 0x100000000 offset here is to; // prevent an incorrect tag being generated during relocation when the; // global appears before the code section. Without the offset, a global at; // `0x0f00'0000'0000'1000` (i.e. at `0x1000` with tag `0xf`) that's referenced; // by code at `0x2000` would result in `0x0f00'0000'0000'1000 - 0x2000 =; // 0x0eff'ffff'ffff'f000`, meaning the tag would be incorrectly set to `0xe`; // instead of `0xf`.; // This assumes that we're in the small code model so we can assume a binary; // size of <= 4GB, which makes the untagged PC relative offset positive. The; // binary must also be loaded into address range [0, 2^48). Both of these; // properties need to be ensured at runtime when using tagged addresses.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:3,Performance,Load,Load,3,// Load/Store bit,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:3,Performance,Cache,Cache,3,// Cache level bits,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:37,Performance,load,loads,37,"// Here we just try to handle vector loads/stores where our value type might; // have pointer elements, which the SelectionDAG importer can't handle. To; // allow the existing patterns for s64 to fire for p0, we just try to bitcast; // the value to use s64 types.; // Custom legalization requires the instruction, if not deleted, must be fully; // legalized. In order to allow further legalization of the inst, we create; // a new instruction and erase the existing one.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:13,Performance,load,loads,13,"// For LSE2, loads/stores should have been converted to monotonic and had; // a fence inserted after them.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:66,Availability,avail,available,66,"// When there is no integer popcount instruction (FEAT_CSSC isn't available),; // it can be more efficiently lowered to the following sequence that uses; // AdvSIMD registers/instructions as long as the copies to/from the AdvSIMD; // registers are cheap.; // FMOV D0, X0 // copy 64-bit int to vector, high bits zero'd; // CNT V0.8B, V0.8B // 8xbyte pop-counts; // ADDV B0, V0.8B // sum 8xbyte pop-counts; // UMOV X0, V0.B[0] // copy byte result back to integer reg; //; // For 128 bit vector popcounts, we lower to the following sequence:; // cnt.16b v0, v0 // v8s16, v4s32, v2s64; // uaddlp.8h v0, v0 // v8s16, v4s32, v2s64; // uaddlp.4s v0, v0 // v4s32, v2s64; // uaddlp.2d v0, v0 // v2s64; //; // For 64 bit vector popcounts, we lower to the following sequence:; // cnt.8b v0, v0 // v4s16, v2s32; // uaddlp.4h v0, v0 // v4s16, v2s32; // uaddlp.2s v0, v0 // v2s32",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:97,Energy Efficiency,efficient,efficiently,97,"// When there is no integer popcount instruction (FEAT_CSSC isn't available),; // it can be more efficiently lowered to the following sequence that uses; // AdvSIMD registers/instructions as long as the copies to/from the AdvSIMD; // registers are cheap.; // FMOV D0, X0 // copy 64-bit int to vector, high bits zero'd; // CNT V0.8B, V0.8B // 8xbyte pop-counts; // ADDV B0, V0.8B // sum 8xbyte pop-counts; // UMOV X0, V0.B[0] // copy byte result back to integer reg; //; // For 128 bit vector popcounts, we lower to the following sequence:; // cnt.16b v0, v0 // v8s16, v4s32, v2s64; // uaddlp.8h v0, v0 // v8s16, v4s32, v2s64; // uaddlp.4s v0, v0 // v4s32, v2s64; // uaddlp.2d v0, v0 // v2s64; //; // For 64 bit vector popcounts, we lower to the following sequence:; // cnt.8b v0, v0 // v4s16, v2s32; // uaddlp.4h v0, v0 // v4s16, v2s32; // uaddlp.2s v0, v0 // v2s32",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:28,Availability,mask,mask,28,// We want to materialize a mask with the high bit set.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:50,Availability,mask,mask,50,"// AdvSIMD immediate moves cannot materialize out mask in a single; // instruction for 64-bit elements. Instead, materialize zero and then; // negate it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:17,Availability,mask,mask,17,// Construct the mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:46,Performance,cache,cache,46,// The locality degree is the opposite of the cache speed.; // Put the number the other way around.; // The encoding starts at 0 for level 1,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64O0PreLegalizerCombiner.cpp:38,Performance,optimiz,optimizations,38,// Try to inline memcpy type calls if optimizations are enabled.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64O0PreLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64O0PreLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp:81,Modifiability,Rewrite,Rewrite,81,"/// This combine tries do what performExtractVectorEltCombine does in SDAG.; /// Rewrite for pairwise fadd pattern; /// (s32 (g_extract_vector_elt; /// (g_fadd (vXs32 Other); /// (g_vector_shuffle (vXs32 Other) undef <1,X,...> )) 0)); /// ->; /// (s32 (g_fadd (g_extract_vector_elt (vXs32 Other) 0); /// (g_extract_vector_elt (vXs32 Other) 1))",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp:31,Performance,perform,performExtractVectorEltCombine,31,"/// This combine tries do what performExtractVectorEltCombine does in SDAG.; /// Rewrite for pairwise fadd pattern; /// (s32 (g_extract_vector_elt; /// (g_fadd (vXs32 Other); /// (g_vector_shuffle (vXs32 Other) undef <1,X,...> )) 0)); /// ->; /// (s32 (g_fadd (g_extract_vector_elt (vXs32 Other) 0); /// (g_extract_vector_elt (vXs32 Other) 1))",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp:18,Modifiability,extend,extended,18,// TODO: check if extended build vector as well.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp:18,Modifiability,extend,extended,18,// TODO: check if extended build vector as well.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp:13,Performance,optimiz,optimizations,13,// The below optimizations require a constant RHS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp:82,Energy Efficiency,power,power,82,"// The following code is ported from AArch64ISelLowering.; // Multiplication of a power of two plus/minus one can be done more; // cheaply as shift+add/sub. For now, this is true unilaterally. If; // future CPUs have a cheaper MADD instruction, this may need to be; // gated on a subtarget feature. For Cyclone, 32-bit MADD is 4 cycles and; // 64-bit is 5 cycles, so this is always a win.; // More aggressively, some multiplications N0 * C can be lowered to; // shift+add+shift if the constant C = A * B where A = 2^N + 1 and B = 2^M,; // e.g. 6=3*2=(2+1)*2.; // TODO: consider lowering more cases, e.g. C = 14, -6, -14 or even 45; // which equals to (1+2)*16-(1+2).; // TrailingZeroes is used to test if the mul can be lowered to; // shift+add+shift.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp:697,Testability,test,test,697,"// The following code is ported from AArch64ISelLowering.; // Multiplication of a power of two plus/minus one can be done more; // cheaply as shift+add/sub. For now, this is true unilaterally. If; // future CPUs have a cheaper MADD instruction, this may need to be; // gated on a subtarget feature. For Cyclone, 32-bit MADD is 4 cycles and; // 64-bit is 5 cycles, so this is always a win.; // More aggressively, some multiplications N0 * C can be lowered to; // shift+add+shift if the constant C = A * B where A = 2^N + 1 and B = 2^M,; // e.g. 6=3*2=(2+1)*2.; // TODO: consider lowering more cases, e.g. C = 14, -6, -14 or even 45; // which equals to (1+2)*16-(1+2).; // TrailingZeroes is used to test if the mul can be lowered to; // shift+add+shift.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp:82,Performance,perform,performance,82,"/// Match a 128b store of zero and split it into two 64 bit stores, for; /// size/performance reasons.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp:59,Performance,cache,cache,59,// The G_PTR_ADD that's used by the store. We keep this to cache the; // MachineInstr def.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp:21,Integrability,depend,depend,21,"// Size savings will depend on whether we can fold the offset, as an; // immediate of an ADD.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp:86,Modifiability,rewrite,rewrite,86,// We have a series of consecutive stores. Factor out the common base; // pointer and rewrite the offsets.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp:244,Performance,optimiz,optimization,244,"// This combine needs to run after all reassociations/folds on pointer; // addressing have been done, specifically those that combine two G_PTR_ADDs; // with constant offsets into a single G_PTR_ADD with a combined offset.; // The goal of this optimization is to undo that combine in the case where; // doing so has prevented the formation of pair stores due to illegal; // addressing modes of STP. The reason that we do it here is because; // it's much easier to undo the transformation of a series consecutive; // mem ops, than it is to detect when doing it would be a bad idea looking; // at a single G_PTR_ADD in the reassociation/ptradd_immed_chain combine.; //; // An example:; // G_STORE %11:_(<2 x s64>), %base:_(p0) :: (store (<2 x s64>), align 1); // %off1:_(s64) = G_CONSTANT i64 4128; // %p1:_(p0) = G_PTR_ADD %0:_, %off1:_(s64); // G_STORE %11:_(<2 x s64>), %p1:_(p0) :: (store (<2 x s64>), align 1); // %off2:_(s64) = G_CONSTANT i64 4144; // %p2:_(p0) = G_PTR_ADD %0:_, %off2:_(s64); // G_STORE %11:_(<2 x s64>), %p2:_(p0) :: (store (<2 x s64>), align 1); // %off3:_(s64) = G_CONSTANT i64 4160; // %p3:_(p0) = G_PTR_ADD %0:_, %off3:_(s64); // G_STORE %11:_(<2 x s64>), %17:_(p0) :: (store (<2 x s64>), align 1)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp:539,Safety,detect,detect,539,"// This combine needs to run after all reassociations/folds on pointer; // addressing have been done, specifically those that combine two G_PTR_ADDs; // with constant offsets into a single G_PTR_ADD with a combined offset.; // The goal of this optimization is to undo that combine in the case where; // doing so has prevented the formation of pair stores due to illegal; // addressing modes of STP. The reason that we do it here is because; // it's much easier to undo the transformation of a series consecutive; // mem ops, than it is to detect when doing it would be a bad idea looking; // at a single G_PTR_ADD in the reassociation/ptradd_immed_chain combine.; //; // An example:; // G_STORE %11:_(<2 x s64>), %base:_(p0) :: (store (<2 x s64>), align 1); // %off1:_(s64) = G_CONSTANT i64 4128; // %p1:_(p0) = G_PTR_ADD %0:_, %off1:_(s64); // G_STORE %11:_(<2 x s64>), %p1:_(p0) :: (store (<2 x s64>), align 1); // %off2:_(s64) = G_CONSTANT i64 4144; // %p2:_(p0) = G_PTR_ADD %0:_, %off2:_(s64); // G_STORE %11:_(<2 x s64>), %p2:_(p0) :: (store (<2 x s64>), align 1); // %off3:_(s64) = G_CONSTANT i64 4160; // %p3:_(p0) = G_PTR_ADD %0:_, %off3:_(s64); // G_STORE %11:_(<2 x s64>), %17:_(p0) :: (store (<2 x s64>), align 1)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp:263,Usability,undo,undo,263,"// This combine needs to run after all reassociations/folds on pointer; // addressing have been done, specifically those that combine two G_PTR_ADDs; // with constant offsets into a single G_PTR_ADD with a combined offset.; // The goal of this optimization is to undo that combine in the case where; // doing so has prevented the formation of pair stores due to illegal; // addressing modes of STP. The reason that we do it here is because; // it's much easier to undo the transformation of a series consecutive; // mem ops, than it is to detect when doing it would be a bad idea looking; // at a single G_PTR_ADD in the reassociation/ptradd_immed_chain combine.; //; // An example:; // G_STORE %11:_(<2 x s64>), %base:_(p0) :: (store (<2 x s64>), align 1); // %off1:_(s64) = G_CONSTANT i64 4128; // %p1:_(p0) = G_PTR_ADD %0:_, %off1:_(s64); // G_STORE %11:_(<2 x s64>), %p1:_(p0) :: (store (<2 x s64>), align 1); // %off2:_(s64) = G_CONSTANT i64 4144; // %p2:_(p0) = G_PTR_ADD %0:_, %off2:_(s64); // G_STORE %11:_(<2 x s64>), %p2:_(p0) :: (store (<2 x s64>), align 1); // %off3:_(s64) = G_CONSTANT i64 4160; // %p3:_(p0) = G_PTR_ADD %0:_, %off3:_(s64); // G_STORE %11:_(<2 x s64>), %17:_(p0) :: (store (<2 x s64>), align 1)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp:464,Usability,undo,undo,464,"// This combine needs to run after all reassociations/folds on pointer; // addressing have been done, specifically those that combine two G_PTR_ADDs; // with constant offsets into a single G_PTR_ADD with a combined offset.; // The goal of this optimization is to undo that combine in the case where; // doing so has prevented the formation of pair stores due to illegal; // addressing modes of STP. The reason that we do it here is because; // it's much easier to undo the transformation of a series consecutive; // mem ops, than it is to detect when doing it would be a bad idea looking; // at a single G_PTR_ADD in the reassociation/ptradd_immed_chain combine.; //; // An example:; // G_STORE %11:_(<2 x s64>), %base:_(p0) :: (store (<2 x s64>), align 1); // %off1:_(s64) = G_CONSTANT i64 4128; // %p1:_(p0) = G_PTR_ADD %0:_, %off1:_(s64); // G_STORE %11:_(<2 x s64>), %p1:_(p0) :: (store (<2 x s64>), align 1); // %off2:_(s64) = G_CONSTANT i64 4144; // %p2:_(p0) = G_PTR_ADD %0:_, %off2:_(s64); // G_STORE %11:_(<2 x s64>), %p2:_(p0) :: (store (<2 x s64>), align 1); // %off3:_(s64) = G_CONSTANT i64 4160; // %p3:_(p0) = G_PTR_ADD %0:_, %off3:_(s64); // G_STORE %11:_(<2 x s64>), %17:_(p0) :: (store (<2 x s64>), align 1)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp:15,Performance,load,load,15,"// If we see a load, then we keep track of any values defined by it.; // In the following example, STP formation will fail anyway because; // the latter store is using a load result that appears after the; // the prior store. In this situation if we factor out the offset then; // we increase code size for no benefit.; // G_STORE %v1:_(s64), %base:_(p0) :: (store (s64)); // %v2:_(s64) = G_LOAD %ldptr:_(p0) :: (load (s64)); // G_STORE %v2:_(s64), %base:_(p0) :: (store (s64))",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp:170,Performance,load,load,170,"// If we see a load, then we keep track of any values defined by it.; // In the following example, STP formation will fail anyway because; // the latter store is using a load result that appears after the; // the prior store. In this situation if we factor out the offset then; // we increase code size for no benefit.; // G_STORE %v1:_(s64), %base:_(p0) :: (store (s64)); // %v2:_(s64) = G_LOAD %ldptr:_(p0) :: (load (s64)); // G_STORE %v2:_(s64), %base:_(p0) :: (store (s64))",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp:413,Performance,load,load,413,"// If we see a load, then we keep track of any values defined by it.; // In the following example, STP formation will fail anyway because; // the latter store is using a load result that appears after the; // the prior store. In this situation if we factor out the offset then; // we increase code size for no benefit.; // G_STORE %v1:_(s64), %base:_(p0) :: (store (s64)); // %v2:_(s64) = G_LOAD %ldptr:_(p0) :: (load (s64)); // G_STORE %v2:_(s64), %base:_(p0) :: (store (s64))",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp:34,Performance,load,load,34,"// Check if this store is using a load result that appears after the; // last store. If so, bail out.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp:13,Performance,load,load,13,// Reset the load value tracking.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp:77,Performance,optimiz,optimize,77,"// The store isn't a valid to consider for the prior sequence,; // so try to optimize what we have so far and start a new sequence.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp:640,Performance,optimiz,optimization,640,"//=== AArch64PostLegalizerLowering.cpp --------------------------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; ///; /// \file; /// Post-legalization lowering for instructions.; ///; /// This is used to offload pattern matching from the selector.; ///; /// For example, this combiner will notice that a G_SHUFFLE_VECTOR is actually; /// a G_ZIP, G_UZP, etc.; ///; /// General optimization combines should be handled by either the; /// AArch64PostLegalizerCombiner or the AArch64PreLegalizerCombiner.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp:43,Availability,mask,mask,43,/// Determines if \p M is a shuffle vector mask for a TRN of \p NumElts.; /// Whether or not G_TRN1 or G_TRN2 should be used is stored in \p WhichResult.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp:54,Availability,mask,mask,54,/// Check if a G_EXT instruction can handle a shuffle mask \p M when the vector; /// sources of the shuffle are different.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp:334,Availability,mask,mask,334,"// The index of an EXT is the first element if it is not UNDEF.; // Watch out for the beginning UNDEFs. The EXT index should be the expected; // value of the first element. E.g.; // <-1, -1, 3, ...> is treated as <1, 2, 3, ...>.; // <-1, -1, 0, 1, ...> is treated as <2*NumElts-2, 2*NumElts-1, 0, 1, ...>.; // ExpectedElt is the last mask index plus 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp:297,Availability,mask,mask,297,"// There are two difference cases requiring to reverse input vectors.; // For example, for vector <4 x i32> we have the following cases,; // Case 1: shufflevector(<4 x i32>,<4 x i32>,<-1, -1, -1, 0>); // Case 2: shufflevector(<4 x i32>,<4 x i32>,<-1, -1, 7, 0>); // For both cases, we finally use mask <5, 6, 7, 0>, which requires; // to reverse two input vectors.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp:43,Availability,mask,mask,43,/// Determines if \p M is a shuffle vector mask for a UZP of \p NumElts.; /// Whether or not G_UZP1 or G_UZP2 should be used is stored in \p WhichResult.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp:34,Availability,mask,mask,34,/// \return true if \p M is a zip mask for a shuffle vector of \p NumElts.; /// Whether or not G_ZIP1 or G_ZIP2 should be used is stored in \p WhichResult.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp:81,Availability,mask,mask,81,/// Helper function for matchINS.; ///; /// \returns a value when \p M is an ins mask for \p NumInputElements.; ///; /// First element of the returned pair is true when the produced; /// G_INSERT_VECTOR_ELT destination should be the LHS of the G_SHUFFLE_VECTOR.; ///; /// Second element is the destination lane for the G_INSERT_VECTOR_ELT.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp:3,Testability,Test,Test,3,"// Test if the LHS is a BUILD_VECTOR. If it is, then we can just reference the; // lane's definition directly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp:54,Availability,mask,mask,54,// Check if an EXT instruction can handle the shuffle mask when the vector; // sources of the shuffle are the same.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp:39,Integrability,wrap,wraps,39,"// Increment the expected index. If it wraps around, just follow it; // back to index zero and keep going.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp:36,Availability,mask,mask,36,"/// Match a G_SHUFFLE_VECTOR with a mask which corresponds to a; /// G_INSERT_VECTOR_ELT and G_EXTRACT_VECTOR_ELT pair.; ///; /// e.g.; /// %shuf = G_SHUFFLE_VECTOR %left, %right, shufflemask(0, 0); ///; /// Can be represented as; ///; /// %extract = G_EXTRACT_VECTOR_ELT %left, 0; /// %ins = G_INSERT_VECTOR_ELT %left, %extract, 1; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp:200,Deployability,update,updated,200,/// Determine if it is possible to modify the \p RHS and predicate \p P of a; /// G_ICMP instruction such that the right-hand side is an arithmetic immediate.; ///; /// \returns A pair containing the updated immediate and predicate which may; /// be used to optimize the instruction.; ///; /// \note This assumes that the comparison has been legalized.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp:258,Performance,optimiz,optimize,258,/// Determine if it is possible to modify the \p RHS and predicate \p P of a; /// G_ICMP instruction such that the right-hand side is an arithmetic immediate.; ///; /// \returns A pair containing the updated immediate and predicate which may; /// be used to optimize the instruction.; ///; /// \note This assumes that the comparison has been legalized.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp:54,Deployability,update,updated,54,"// Check if the new constant is valid, and return the updated constant and; // predicate if it is.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp:47,Deployability,update,update,47,/// Determine whether or not it is possible to update the RHS and predicate of; /// a G_ICMP instruction such that the RHS will be selected as an arithmetic; /// immediate.; ///; /// \p MI - The G_ICMP instruction; /// \p MatchInfo - The new RHS immediate and predicate on success; ///; /// See tryAdjustICmpImmAndPred for valid transformations.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp:27,Modifiability,extend,extend,27,// Check if we can fold an extend and a shift.; // FIXME: This is duplicated with the selector. (See:; // selectArithExtendedRegister),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp:97,Modifiability,extend,extend,97,"// Swap the operands if it would introduce a profitable folding opportunity.; // (e.g. a shift + extend).; //; // For example:; // lsl w13, w11, #1; // cmp w13, w12; // can be turned into:; // cmp w12, w11, lsl #1; // Don't swap if there's a constant on the RHS, because we know we can fold; // that.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp:59,Usability,simpl,simplify,59,"// Instead of having an apply function, just build here to simplify things.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp:31,Modifiability,EXTEND,EXTENDED,31,"// If the source operands were EXTENDED before, then {U/S}MULL can be used",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp:31,Modifiability,EXTEND,EXTENDED,31,"// If the source operands were EXTENDED before, then {U/S}MULL can be used",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostSelectOptimize.cpp:461,Deployability,pipeline,pipeline,461,"//=== AArch64PostSelectOptimize.cpp ---------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass does post-instruction-selection optimizations in the GlobalISel; // pipeline, before the rest of codegen runs.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostSelectOptimize.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostSelectOptimize.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostSelectOptimize.cpp:425,Performance,optimiz,optimizations,425,"//=== AArch64PostSelectOptimize.cpp ---------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass does post-instruction-selection optimizations in the GlobalISel; // pipeline, before the rest of codegen runs.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostSelectOptimize.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostSelectOptimize.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostSelectOptimize.cpp:163,Performance,optimiz,optimizations,163,"// If we find a dead NZCV implicit-def, we; // - try to convert the operation to a non-flag-setting equivalent; // - or mark the def as dead to aid later peephole optimizations.; // Use cases:; // 1); // Consider the following code:; // FCMPSrr %0, %1, implicit-def $nzcv; // %sel1:gpr32 = CSELWr %_, %_, 12, implicit $nzcv; // %sub:gpr32 = SUBSWrr %_, %_, implicit-def $nzcv; // FCMPSrr %0, %1, implicit-def $nzcv; // %sel2:gpr32 = CSELWr %_, %_, 12, implicit $nzcv; // This kind of code where we have 2 FCMPs each feeding a CSEL can happen; // when we have a single IR fcmp being used by two selects. During selection,; // to ensure that there can be no clobbering of nzcv between the fcmp and the; // csel, we have to generate an fcmp immediately before each csel is; // selected.; // However, often we can essentially CSE these together later in MachineCSE.; // This doesn't work though if there are unrelated flag-setting instructions; // in between the two FCMPs. In this case, the SUBS defines NZCV; // but it doesn't have any users, being overwritten by the second FCMP.; //; // 2); // The instruction selector always emits the flag-setting variant of ADC/SBC; // while selecting G_UADDE/G_SADDE/G_USUBE/G_SSUBE. If the carry-out of these; // instructions is never used, we can switch to the non-flag-setting variant.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostSelectOptimize.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostSelectOptimize.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostSelectOptimize.cpp:83,Performance,optimiz,optimizations,83,"// Otherwise, we just set the nzcv imp-def operand to be dead, so the; // peephole optimizations can optimize them further.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostSelectOptimize.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostSelectOptimize.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostSelectOptimize.cpp:101,Performance,optimiz,optimize,101,"// Otherwise, we just set the nzcv imp-def operand to be dead, so the; // peephole optimizations can optimize them further.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostSelectOptimize.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostSelectOptimize.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp:66,Safety,avoid,avoid,66,// Require that the new offset is larger than the existing one to avoid; // infinite loops.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp:108,Safety,avoid,avoid,108,"// Check whether folding this offset is legal. It must not go out of bounds of; // the referenced object to avoid violating the code model, and must be; // smaller than 2^20 because this is the largest offset expressible in all; // object formats. (The IMAGE_REL_ARM64_PAGEBASE_REL21 relocation in COFF; // stores an immediate signed 21 bit offset.); //; // This check also prevents us from folding negative offsets, which will end; // up being treated in the same way as large positive ones. They could also; // cause code model violations, and aren't really common enough to matter.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp:149,Performance,perform,performVecReduceAddCombine,149,"// Combines vecreduce_add(mul(ext(x), ext(y))) -> vecreduce_add(udot(x, y)); // Or vecreduce_add(ext(x)) -> vecreduce_add(udot(x, 1)); // Similar to performVecReduceAddCombine in SelectionDAG",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp:18,Modifiability,variab,variables,18,// Initialise the variables,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp:87,Modifiability,extend,extend,87,// Matches {U/S}ADDV(ext(x)) => {U/S}ADDLV(x); // Ensure that the type coming from the extend instruction is the right size,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp:39,Modifiability,extend,extend,39,// Check if the last instruction is an extend,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp:67,Modifiability,extend,extend,67,"// If the number of elements is too small to build an instruction, extend; // its size before applying addlv",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp:520,Availability,error,error,520,"// Try simplify G_UADDO with 8 or 16 bit operands to wide G_ADD and TBNZ if; // result is only used in the no-overflow case. It is restricted to cases; // where we know that the high-bits of the operands are 0. If there's an; // overflow, then the 9th or 17th bit must be set, which can be checked; // using TBNZ.; //; // Change (for UADDOs on 8 and 16 bits):; //; // %z0 = G_ASSERT_ZEXT _; // %op0 = G_TRUNC %z0; // %z1 = G_ASSERT_ZEXT _; // %op1 = G_TRUNC %z1; // %val, %cond = G_UADDO %op0, %op1; // G_BRCOND %cond, %error.bb; //; // error.bb:; // (no successors and no uses of %val); //; // To:; //; // %z0 = G_ASSERT_ZEXT _; // %z1 = G_ASSERT_ZEXT _; // %add = G_ADD %z0, %z1; // %val = G_TRUNC %add; // %bit = G_AND %add, 1 << scalar-size-in-bits(%op1); // %cond = G_ICMP NE, %bit, 0; // G_BRCOND %cond, %error.bb",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp:537,Availability,error,error,537,"// Try simplify G_UADDO with 8 or 16 bit operands to wide G_ADD and TBNZ if; // result is only used in the no-overflow case. It is restricted to cases; // where we know that the high-bits of the operands are 0. If there's an; // overflow, then the 9th or 17th bit must be set, which can be checked; // using TBNZ.; //; // Change (for UADDOs on 8 and 16 bits):; //; // %z0 = G_ASSERT_ZEXT _; // %op0 = G_TRUNC %z0; // %z1 = G_ASSERT_ZEXT _; // %op1 = G_TRUNC %z1; // %val, %cond = G_UADDO %op0, %op1; // G_BRCOND %cond, %error.bb; //; // error.bb:; // (no successors and no uses of %val); //; // To:; //; // %z0 = G_ASSERT_ZEXT _; // %z1 = G_ASSERT_ZEXT _; // %add = G_ADD %z0, %z1; // %val = G_TRUNC %add; // %bit = G_AND %add, 1 << scalar-size-in-bits(%op1); // %cond = G_ICMP NE, %bit, 0; // G_BRCOND %cond, %error.bb",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp:811,Availability,error,error,811,"// Try simplify G_UADDO with 8 or 16 bit operands to wide G_ADD and TBNZ if; // result is only used in the no-overflow case. It is restricted to cases; // where we know that the high-bits of the operands are 0. If there's an; // overflow, then the 9th or 17th bit must be set, which can be checked; // using TBNZ.; //; // Change (for UADDOs on 8 and 16 bits):; //; // %z0 = G_ASSERT_ZEXT _; // %op0 = G_TRUNC %z0; // %z1 = G_ASSERT_ZEXT _; // %op1 = G_TRUNC %z1; // %val, %cond = G_UADDO %op0, %op1; // G_BRCOND %cond, %error.bb; //; // error.bb:; // (no successors and no uses of %val); //; // To:; //; // %z0 = G_ASSERT_ZEXT _; // %z1 = G_ASSERT_ZEXT _; // %add = G_ADD %z0, %z1; // %val = G_TRUNC %add; // %bit = G_AND %add, 1 << scalar-size-in-bits(%op1); // %cond = G_ICMP NE, %bit, 0; // G_BRCOND %cond, %error.bb",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp:7,Usability,simpl,simplify,7,"// Try simplify G_UADDO with 8 or 16 bit operands to wide G_ADD and TBNZ if; // result is only used in the no-overflow case. It is restricted to cases; // where we know that the high-bits of the operands are 0. If there's an; // overflow, then the 9th or 17th bit must be set, which can be checked; // using TBNZ.; //; // Change (for UADDOs on 8 and 16 bits):; //; // %z0 = G_ASSERT_ZEXT _; // %op0 = G_TRUNC %z0; // %z1 = G_ASSERT_ZEXT _; // %op1 = G_TRUNC %z1; // %val, %cond = G_UADDO %op0, %op1; // G_BRCOND %cond, %error.bb; //; // error.bb:; // (no successors and no uses of %val); //; // To:; //; // %z0 = G_ASSERT_ZEXT _; // %z1 = G_ASSERT_ZEXT _; // %add = G_ADD %z0, %z1; // %val = G_TRUNC %add; // %bit = G_AND %add, 1 << scalar-size-in-bits(%op1); // %cond = G_ICMP NE, %bit, 0; // G_BRCOND %cond, %error.bb",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp:102,Modifiability,extend,extended,102,"// First check that the G_TRUNC feeding the G_UADDO are no-ops, because the; // inputs have been zero-extended.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp:41,Deployability,update,update,41,// Emit check of the 9th or 17th bit and update users (the branch). This will; // later be folded to TBNZ.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp:3,Deployability,Update,Update,3,"// Update ZEXts users of the result value. Because all uses are in the; // no-overflow case, we know that the top bits are 0 and we can ignore ZExts.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp:38,Performance,optimiz,optimizations,38,// Try to inline memcpy type calls if optimizations are enabled.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp:267,Energy Efficiency,schedul,scheduling,267,// What do we do with different size?; // copy are same size.; // Will introduce other hooks for different size:; // * extract cost.; // * build_sequence cost.; // Copy from (resp. to) GPR to (resp. from) FPR involves FMOV.; // FIXME: This should be deduced from the scheduling model.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp:55,Performance,load,loads,55,// GMemOperation because we also want to match indexed loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp:76,Performance,load,loading,76,// Look at the first element of the struct to determine the type we are; // loading,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp:19,Testability,log,logic,19,// Try the default logic for non-generic instructions that are either copies; // or already have some operands assigned to banks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp:85,Performance,tune,tune,85,// Some of the floating-point instructions have mixed GPR and FPR operands:; // fine-tune the computed mapping.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp:25,Performance,load,load,25,// We want to select dup(load) into LD1R.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp:307,Energy Efficiency,schedul,scheduling,307,"// Loading in vector unit is slightly more expensive.; // This is actually only true for the LD1R and co instructions,; // but anyway for the fast mode this number does not matter and; // for the greedy mode the cost of the cross bank copy will; // offset this number.; // FIXME: Should be derived from the scheduling model.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp:3,Performance,Load,Loading,3,"// Loading in vector unit is slightly more expensive.; // This is actually only true for the LD1R and co instructions,; // but anyway for the fast mode this number does not matter and; // for the greedy mode the cost of the cross bank copy will; // offset this number.; // FIXME: Should be derived from the scheduling model.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp:32,Performance,load,load,32,// Try to guess the type of the load from the MMO.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp:17,Performance,load,load,17,"// Check if that load feeds fp instructions.; // In that case, we want the default mapping to be on FPR; // instead of blind map every scalar to GPR.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp:96,Performance,load,load,96,"// If we have at least one direct use in a FP instruction,; // assume this was a floating point load in the IR. If it was; // not, we would have had a bitcast before reaching that; // instruction.; //; // Int->FP conversion operations are also captured in; // onlyDefinesFP().",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp:103,Deployability,update,update,103,"// Check if we know that the intrinsic has any constraints on its register; // banks. If it does, then update the mapping accordingly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.h:210,Energy Efficiency,allocate,allocated,210,"/// Get the instruction mapping for G_FPEXT.; ///; /// \pre (DstSize, SrcSize) pair is one of the following:; /// (32, 16), (64, 16), (64, 32), (128, 64); ///; /// \return An InstructionMapping with statically allocated OperandsMapping.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.h:211,Energy Efficiency,allocate,allocated,211,/// Get an instruction mapping where all the operands map to; /// the same register bank and have similar size.; ///; /// \pre MI.getNumOperands() <= 3; ///; /// \return An InstructionMappings with a statically allocated; /// OperandsMapping.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.h:25,Performance,load,load,25,/// \returns true if the load \p MI is likely loading from a floating-point; /// type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.h:46,Performance,load,loading,46,/// \returns true if the load \p MI is likely loading from a floating-point; /// type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:85,Modifiability,Extend,Extends,85,//===----------------------------------------------------------------------===//; // Extends; //; /// getArithShiftValue - get the arithmetic shift value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:32,Modifiability,extend,extend,32,/// getExtendType - Extract the extend type for operands of arithmetic ops.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:17,Modifiability,extend,extend,17,/// Mapping from extend bits to required operation:; /// shifter: 000 ==> uxtb; /// 001 ==> uxth; /// 010 ==> uxtw; /// 011 ==> uxtx; /// 100 ==> sxtb; /// 101 ==> sxth; /// 110 ==> sxtw; /// 111 ==> sxtx,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:74,Usability,ux,uxtb,74,/// Mapping from extend bits to required operation:; /// shifter: 000 ==> uxtb; /// 001 ==> uxth; /// 010 ==> uxtw; /// 011 ==> uxtx; /// 100 ==> sxtb; /// 101 ==> sxth; /// 110 ==> sxtw; /// 111 ==> sxtx,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:92,Usability,ux,uxth,92,/// Mapping from extend bits to required operation:; /// shifter: 000 ==> uxtb; /// 001 ==> uxth; /// 010 ==> uxtw; /// 011 ==> uxtx; /// 100 ==> sxtb; /// 101 ==> sxth; /// 110 ==> sxtw; /// 111 ==> sxtx,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:110,Usability,ux,uxtw,110,/// Mapping from extend bits to required operation:; /// shifter: 000 ==> uxtb; /// 001 ==> uxth; /// 010 ==> uxtw; /// 011 ==> uxtx; /// 100 ==> sxtb; /// 101 ==> sxth; /// 110 ==> sxtw; /// 111 ==> sxtx,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:128,Usability,ux,uxtx,128,/// Mapping from extend bits to required operation:; /// shifter: 000 ==> uxtb; /// 001 ==> uxth; /// 010 ==> uxtw; /// 011 ==> uxtx; /// 100 ==> sxtb; /// 101 ==> sxth; /// 110 ==> sxtw; /// 111 ==> sxtx,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:35,Modifiability,extend,extend,35,/// getArithExtendImm - Encode the extend type and shift amount for an; /// arithmetic instruction:; /// imm: 3-bit extend amount; /// {5-3} = shifter; /// {2-0} = imm3,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:116,Modifiability,extend,extend,116,/// getArithExtendImm - Encode the extend type and shift amount for an; /// arithmetic instruction:; /// imm: 3-bit extend amount; /// {5-3} = shifter; /// {2-0} = imm3,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:58,Performance,load,load,58,"/// getMemDoShift - Extract the ""do shift"" flag value for load/store; /// instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:32,Modifiability,extend,extend,32,/// getExtendType - Extract the extend type for the offset operand of; /// loads/stores.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:75,Performance,load,loads,75,/// getExtendType - Extract the extend type for the offset operand of; /// loads/stores.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:30,Modifiability,extend,extend,30,/// getExtendImm - Encode the extend type and amount for a load/store inst:; /// doshift: should the offset be scaled by the access size; /// shifter: 000 ==> uxtb; /// 001 ==> uxth; /// 010 ==> uxtw; /// 011 ==> uxtx; /// 100 ==> sxtb; /// 101 ==> sxth; /// 110 ==> sxtw; /// 111 ==> sxtx; /// {3-1} = shifter; /// {0} = doshift,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:59,Performance,load,load,59,/// getExtendImm - Encode the extend type and amount for a load/store inst:; /// doshift: should the offset be scaled by the access size; /// shifter: 000 ==> uxtb; /// 001 ==> uxth; /// 010 ==> uxtw; /// 011 ==> uxtx; /// 100 ==> sxtb; /// 101 ==> sxth; /// 110 ==> sxtw; /// 111 ==> sxtx; /// {3-1} = shifter; /// {0} = doshift,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:125,Security,access,access,125,/// getExtendImm - Encode the extend type and amount for a load/store inst:; /// doshift: should the offset be scaled by the access size; /// shifter: 000 ==> uxtb; /// 001 ==> uxth; /// 010 ==> uxtw; /// 011 ==> uxtx; /// 100 ==> sxtb; /// 101 ==> sxth; /// 110 ==> sxtw; /// 111 ==> sxtx; /// {3-1} = shifter; /// {0} = doshift,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:159,Usability,ux,uxtb,159,/// getExtendImm - Encode the extend type and amount for a load/store inst:; /// doshift: should the offset be scaled by the access size; /// shifter: 000 ==> uxtb; /// 001 ==> uxth; /// 010 ==> uxtw; /// 011 ==> uxtx; /// 100 ==> sxtb; /// 101 ==> sxth; /// 110 ==> sxtw; /// 111 ==> sxtx; /// {3-1} = shifter; /// {0} = doshift,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:177,Usability,ux,uxth,177,/// getExtendImm - Encode the extend type and amount for a load/store inst:; /// doshift: should the offset be scaled by the access size; /// shifter: 000 ==> uxtb; /// 001 ==> uxth; /// 010 ==> uxtw; /// 011 ==> uxtx; /// 100 ==> sxtb; /// 101 ==> sxth; /// 110 ==> sxtw; /// 111 ==> sxtx; /// {3-1} = shifter; /// {0} = doshift,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:195,Usability,ux,uxtw,195,/// getExtendImm - Encode the extend type and amount for a load/store inst:; /// doshift: should the offset be scaled by the access size; /// shifter: 000 ==> uxtb; /// 001 ==> uxth; /// 010 ==> uxtw; /// 011 ==> uxtx; /// 100 ==> sxtb; /// 101 ==> sxth; /// 110 ==> sxtw; /// 111 ==> sxtx; /// {3-1} = shifter; /// {0} = doshift,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:213,Usability,ux,uxtx,213,/// getExtendImm - Encode the extend type and amount for a load/store inst:; /// doshift: should the offset be scaled by the access size; /// shifter: 000 ==> uxtb; /// 001 ==> uxth; /// 010 ==> uxtw; /// 011 ==> uxtx; /// 100 ==> sxtb; /// 101 ==> sxth; /// 110 ==> sxtw; /// 111 ==> sxtx; /// {3-1} = shifter; /// {0} = doshift,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:112,Testability,log,logical,112,"/// processLogicalImmediate - Determine if an immediate value can be encoded; /// as the immediate operand of a logical instruction for the given register; /// size. If so, return true with ""encoding"" set to the encoded value in; /// the form N:immr:imms.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:31,Deployability,toggle,toggle,31,// Extract the seventh bit and toggle it to create the N field.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:69,Testability,log,logical,69,/// isLogicalImmediate - Return true if the immediate is valid for a logical; /// immediate instruction of the given register size. Return false otherwise.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:70,Testability,log,logical,70,/// encodeLogicalImmediate - Return the encoded immediate value for a logical; /// immediate instruction of the given register size.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:38,Testability,log,logical,38,"/// decodeLogicalImmediate - Decode a logical immediate value in the form; /// ""N:immr:imms"" (where the immr and imms fields are each 6 bits) into the; /// integer value it represents with regSize bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:56,Testability,log,logical,56,"/// isValidDecodeLogicalImmediate - Check to see if the logical immediate value; /// in the form ""N:immr:imms"" (where the immr and imms fields are each 6 bits); /// is a valid encoding for an integer value with regSize bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:13,Testability,log,logical,13,// undefined logical immediate encoding,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:13,Testability,log,logical,13,// undefined logical immediate encoding,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:13,Testability,log,logical,13,// undefined logical immediate encoding,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:44,Performance,optimiz,optimization,44,// The MSVC compiler 19.37 for ARM64 has an optimization bug that; // causes an incorrect behavior with the orignal version. Work around; // by using a slightly different variation.; // https://developercommunity.visualstudio.com/t/C-ARM64-compiler-optimization-bug/10481261,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h:249,Performance,optimiz,optimization-bug,249,// The MSVC compiler 19.37 for ARM64 has an optimization bug that; // causes an incorrect behavior with the orignal version. Work around; // by using a slightly different variation.; // https://developercommunity.visualstudio.com/t/C-ARM64-compiler-optimization-bug/10481261,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AddressingModes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AsmBackend.cpp:57,Availability,mask,mask,57,"// For each byte of the fragment that the fixup touches, mask in the; // bits from the fixup value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AsmBackend.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AsmBackend.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AsmBackend.cpp:68,Testability,log,logic,68,"// FIXME: This isn't correct for AArch64. Just moving the ""generic"" logic; // into the targets for now.; //; // Relax if the value is too big for a (signed) i8.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AsmBackend.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AsmBackend.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AsmBackend.cpp:170,Integrability,depend,depending,170,"// The ADRP instruction adds some multiple of 0x1000 to the current PC &; // ~0xfff. This means that the required offset to reach a symbol can vary by; // up to one step depending on where the ADRP is in memory. For example:; //; // ADRP x0, there; // there:; //; // If the ADRP occurs at address 0xffc then ""there"" will be at 0x1000 and; // we'll need that as an offset. At any other address ""there"" will be in the; // same page as the ADRP and the instruction should encode 0x0. Assuming the; // section isn't 0x1000-aligned, we therefore need to delegate this decision; // to the linker -- a relocation!",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AsmBackend.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AsmBackend.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AsmBackend.cpp:31,Availability,avail,available,31,"/// No compact unwind encoding available. Instead the low 23-bits of; /// the compact unwind encoding is the offset of the DWARF FDE in the; /// __eh_frame section. This mode is never used in object files. It is only; /// generated by the linker in final linked images, which have only DWARF info; /// for a function.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AsmBackend.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AsmBackend.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64ELFObjectWriter.cpp:32,Testability,test,tested,32,"// ILP32 case not reached here, tested with isNonILP32reloc",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64ELFObjectWriter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64ELFObjectWriter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64ELFStreamer.cpp:4,Modifiability,Extend,Extend,4,"/// Extend the generic ELFStreamer class so that it can emit mapping symbols at; /// the appropriate points in the object files. These symbols are defined in the; /// AArch64 ELF ABI:; /// infocenter.arm.com/help/topic/com.arm.doc.ihi0056a/IHI0056A_aaelf64.pdf; ///; /// In brief: $x or $d should be emitted at the start of each contiguous region; /// of A64 code or data in a section. In practice, this emission does not rely; /// on explicit assembler directives but on inherent properties of the; /// directives doing the emission (e.g. "".byte"" is data, ""add x0, x0, x0"" an; /// instruction).; ///; /// As a result this system is orthogonal to the DataRegion infrastructure used; /// by MachO. Beware!",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64ELFStreamer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64ELFStreamer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64FixupKinds.h:30,Performance,load,load,30,// unsigned 12-bit fixups for load and store instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64FixupKinds.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64FixupKinds.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64FixupKinds.h:138,Performance,load,loads,138,"// The high 19 bits of a 21-bit pc-relative immediate. Same encoding as; // fixup_aarch64_pcrel_adrhi, except this is used by pc-relative loads and; // generates relocations directly when necessary.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64FixupKinds.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64FixupKinds.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64FixupKinds.h:84,Security,authenticat,authentication,84,"// The high 16 bits of a 18-bit unsigned PC-relative immediate. Used by; // pointer authentication, only within a function, so no relocation can be; // generated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64FixupKinds.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64FixupKinds.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp:19,Safety,Predict,Prediction,19,"// Maybe IC, maybe Prediction Restriction",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp:3,Safety,Predict,Prediction,3,// Prediction Restriction aliases,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp:75,Usability,UX,UXTW,75,"// If the destination or first source register operand is [W]SP, print; // UXTW/UXTX as LSL, and if the shift amount is also zero, print nothing at; // all.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp:80,Usability,UX,UXTX,80,"// If the destination or first source register operand is [W]SP, print; // UXTW/UXTX as LSL, and if the shift amount is also zero, print nothing at; // all.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp:15,Usability,ux,uxtw,15,"// sxtw, sxtx, uxtw or lsl (== uxtx)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp:31,Usability,ux,uxtx,31,"// sxtw, sxtx, uxtw or lsl (== uxtx)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp:20,Integrability,wrap,wrap,20,// Vector lists can wrap around.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp:20,Integrability,wrap,wrap,20,// Vector lists can wrap around.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp:20,Integrability,wrap,wrap,20,// Vector lists can wrap around.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp:94,Integrability,wrap,wrap-around,94,// Do not print the range when the last register is lower than the first.; // Because it is a wrap-around register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp:3,Integrability,wrap,wrap-around,3,// wrap-around sve register,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp:27,Usability,simpl,simply,27,// If the branch target is simply an address then print it in hex.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MachObjectWriter.cpp:113,Usability,Clear,Clear,113,// ADRP fixups use relocations for the whole symbol value and only; // put the addend in the instruction itself. Clear out any value the; // generic code figured out from the sybmol definition.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MachObjectWriter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MachObjectWriter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MachObjectWriter.cpp:153,Availability,error,error,153,"// AArch64 always uses external relocations. If there is no symbol to use as; // a base address (a local symbol with no preceding non-local symbol),; // error out.; //; // FIXME: We should probably just synthesize an external symbol and use; // that.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MachObjectWriter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MachObjectWriter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MachObjectWriter.cpp:85,Availability,error,error,85,"// Make sure that the symbol is actually in a section here. If it isn't,; // emit an error and exit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MachObjectWriter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MachObjectWriter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MachObjectWriter.cpp:22,Modifiability,variab,variable,22,// If the symbol is a variable it can either be in a section and; // we have a base or it is absolute and should have been expanded.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MachObjectWriter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MachObjectWriter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp:97,Performance,load,load,97,"/// getLdStUImm12OpValue - Return encoding info for 12-bit unsigned immediate; /// attached to a load, store or prfm instruction. If operand requires a; /// relocation, record it and return zero in that part of the encoding.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp:71,Security,authenticat,authentication,71,/// getPAuthPCRelOpValue - Return the encoded value for a pointer; /// authentication pc-relative operand.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp:59,Performance,load,load-literal,59,/// getLoadLiteralOpValue - Return the encoded value for a load-literal; /// pc-relative address.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp:61,Modifiability,extend,extend,61,"/// getMemExtendOpValue - Return the encoded value for a reg-extend load/store; /// instruction: bit 0 is whether a shift is present, bit 1 is whether the; /// operation is a sign extend (as opposed to a zero extend).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp:180,Modifiability,extend,extend,180,"/// getMemExtendOpValue - Return the encoded value for a reg-extend load/store; /// instruction: bit 0 is whether a shift is present, bit 1 is whether the; /// operation is a sign extend (as opposed to a zero extend).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp:209,Modifiability,extend,extend,209,"/// getMemExtendOpValue - Return the encoded value for a reg-extend load/store; /// instruction: bit 0 is whether a shift is present, bit 1 is whether the; /// operation is a sign extend (as opposed to a zero extend).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp:68,Performance,load,load,68,"/// getMemExtendOpValue - Return the encoded value for a reg-extend load/store; /// instruction: bit 0 is whether a shift is present, bit 1 is whether the; /// operation is a sign extend (as opposed to a zero extend).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp:64,Testability,test,test-bit-and,64,/// getTestBranchTargetOpValue - Return the encoded value for a test-bit-and-; /// branch target.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp:71,Security,authenticat,authentication,71,/// getPAuthPCRelOpValue - Return the encoded value for a pointer; /// authentication pc-relative operand.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp:59,Performance,load,load-literal,59,/// getLoadLiteralOpValue - Return the encoded value for a load-literal; /// pc-relative address.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp:64,Testability,test,test-bit-and,64,/// getTestBranchTargetOpValue - Return the encoded value for a test-bit-and-; /// branch target.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp:3,Testability,Test,Test,3,// Test shift,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp:3,Testability,Test,Test,3,// Test immediate,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCExpr.h:81,Performance,perform,performed,81,"// Symbol locations specifying (roughly speaking) what calculation should be; // performed to construct the final address for the relocated; // symbol. E.g. direct, via the GOT, ...",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCExpr.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCExpr.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCExpr.h:76,Performance,perform,perform,76,// Whether the final relocation is a checked one (where a linker should; // perform a range-check on the final address) or not. Note that this field; // is unfortunately sometimes omitted from the assembly syntax. E.g. :lo12:; // on its own is a non-checked relocation. We side with ELF on being; // explicit about this!,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCExpr.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCExpr.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCExpr.h:18,Security,Access,Accessors,18,/// @}; /// @name Accessors; /// @{; /// Get the kind of this expression.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCExpr.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCExpr.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCTargetDesc.cpp:2,Performance,Tune,TuneCPU,2,/*TuneCPU*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCTargetDesc.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCTargetDesc.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h:282,Usability,Simpl,Simple,282,"/// An enum to describe what types of loops we should attempt to tail-fold:; /// Disabled: None; /// Reductions: Loops containing reductions; /// Recurrences: Loops with first-order recurrences, i.e. that would; /// require a SVE splice instruction; /// Reverse: Reverse loops; /// Simple: Loops that are not reversed and don't contain reductions; /// or first-order recurrences.; /// All: All",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h:147,Testability,stub,stub,147,"/// MO_COFFSTUB - On a symbol operand ""FOO"", this indicates that the; /// reference is actually to the "".refptr.FOO"" symbol. This is used for; /// stub symbols on windows.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h:46,Security,access,accessed,46,"/// MO_TLS - Indicates that the operand being accessed is some kind of; /// thread-local symbol. On Darwin, only one type of thread-local access; /// exists (pre linker-relaxation), but on ELF the TLSModel used for the; /// referee will affect interpretation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h:138,Security,access,access,138,"/// MO_TLS - Indicates that the operand being accessed is some kind of; /// thread-local symbol. On Darwin, only one type of thread-local access; /// exists (pre linker-relaxation), but on ELF the TLSModel used for the; /// referee will affect interpretation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h:111,Testability,stub,stub,111,"/// MO_DLLIMPORT - On a symbol operand, this represents that the reference; /// to the symbol is for an import stub. This is used for DLL import; /// storage class indication on Windows.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h:266,Performance,load,load,266,"/// MO_TAGGED - With MO_PAGE, indicates that the page includes a memory tag; /// in bits 56-63.; /// On a FrameIndex operand, indicates that the underlying memory is tagged; /// with an unknown tag value (MTE); this needs to be lowered either to an; /// SP-relative load or store instruction (which do not check tags), or to; /// an LDG instruction to obtain the tag value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h:127,Security,Authenticat,Authentication,127,// end namespace AArch64II; //===----------------------------------------------------------------------===//; // v8.3a Pointer Authentication; //,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h:212,Availability,redundant,redundant,212,"// The number of bits in a SVE register is architecturally defined; // to be a multiple of this value. If <M x t> has this number of bits,; // a <n x M x t> vector can be stored in a SVE register without any; // redundant bits. If <M x t> has this number of bits divided by P,; // a <n x M x t> vector is stored in a SVE register by placing index i; // in index i*P of a <n x (M*P) x t> vector. The other elements of the; // <n x (M*P) x t> vector (such as index 1) are undefined.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h:212,Safety,redund,redundant,212,"// The number of bits in a SVE register is architecturally defined; // to be a multiple of this value. If <M x t> has this number of bits,; // a <n x M x t> vector can be stored in a SVE register without any; // redundant bits. If <M x t> has this number of bits divided by P,; // a <n x M x t> vector is stored in a SVE register by placing index i; // in index i*P of a <n x (M*P) x t> vector. The other elements of the; // <n x (M*P) x t> vector (such as index 1) are undefined.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.h:170,Integrability,interface,interfaces,170,/// SMEAttrs is a utility class to parse the SME ACLE attributes on functions.; /// It helps determine a function's requirements for PSTATE.ZA and PSTATE.SM. It; /// has interfaces to query whether a streaming mode change or lazy-save; /// mechanism is required when going from one function to another (e.g. through; /// a call).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.h:20,Integrability,rout,routines,20,// Used for SME ABI routines to avoid lazy saves,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.h:32,Safety,avoid,avoid,32,// Used for SME ABI routines to avoid lazy saves,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.h:3,Integrability,Interface,Interfaces,3,// Interfaces to query PSTATE.SM,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.h:3,Integrability,Interface,Interfaces,3,// Interfaces to query PSTATE.ZA,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.h:3,Integrability,Interface,Interfaces,3,// Interfaces to query ZT0 State,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPU.h:43,Performance,optimiz,optimizer,43,// DPP/Iterative option enables the atomic optimizer with given strategy; // whereas None disables the atomic optimizer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPU.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPU.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPU.h:110,Performance,optimiz,optimizer,110,// DPP/Iterative option enables the atomic optimizer with given strategy; // whereas None disables the atomic optimizer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPU.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPU.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.cpp:38,Usability,simpl,simplify,38,// Canonicalize the location order to simplify the following alias check.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.cpp:206,Modifiability,variab,variables,206,"// If a generic pointer is loaded from the constant address space, it; // could only be a GLOBAL or CONSTANT one as that address space is solely; // prepared on the host side, where only GLOBAL or CONSTANT variables are; // visible. Note that this even holds for regular functions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.cpp:27,Performance,load,loaded,27,"// If a generic pointer is loaded from the constant address space, it; // could only be a GLOBAL or CONSTANT one as that address space is solely; // prepared on the host side, where only GLOBAL or CONSTANT variables are; // visible. Note that this even holds for regular functions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.cpp:71,Modifiability,variab,variables,71,"// In the kernel function, kernel arguments won't alias to (local); // variables in shared or private address space.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.cpp:48,Modifiability,variab,variable,48,"// TODO: In the regular function, if that local variable in the; // location B is not captured, that argument pointer won't alias to it; // as well.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.h:6,Usability,simpl,simple,6,/// A simple AA result that uses TBAA metadata to answer queries.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.h:11,Integrability,wrap,wrapper,11,/// Legacy wrapper pass to provide the AMDGPUAAResult object.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.h:3,Integrability,Wrap,Wrapper,3,// Wrapper around ExternalAAWrapperPass so that the default constructor gets the; // callback.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAlwaysInlinePass.cpp:98,Availability,error,error,98,"// FIXME: This is a horrible hack. We should always respect noinline,; // and just let us hit the error when we can't handle this.; //; // Unfortunately, clang adds noinline to all functions at -O0. We have; // to override this here until that's fixed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAlwaysInlinePass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAlwaysInlinePass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAlwaysInlinePass.cpp:206,Energy Efficiency,allocate,allocated,206,"// Always force inlining of any function that uses an LDS global address. This; // is something of a workaround because we don't have a way of supporting LDS; // objects defined in functions. LDS is always allocated by a kernel, and it; // is difficult to manage LDS usage if a function may be used by multiple; // kernels.; //; // OpenCL doesn't allow declaring LDS in non-kernels, so in practice this; // should only appear when IPO passes manages to move LDs defined in a kernel; // into a single user function.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAlwaysInlinePass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAlwaysInlinePass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAnnotateKernelFeatures.cpp:561,Performance,optimiz,optimizations,561,"//===- AMDGPUAnnotateKernelFeaturesPass.cpp -------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass propagates the uniform-work-group-size attribute from; /// kernels to leaf functions when possible. It also adds additional attributes; /// to hint ABI lowering optimizations later.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAnnotateKernelFeatures.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAnnotateKernelFeatures.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAnnotateKernelFeatures.cpp:78,Security,access,accessed,78,// TODO: We could refine this to captured pointers that could possibly be; // accessed by flat instructions. For now this is mostly a poor way of; // estimating whether there are calls before argument lowering.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAnnotateKernelFeatures.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAnnotateKernelFeatures.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp:53,Energy Efficiency,allocate,allocated,53,// CP microcode requires the kernel descriptor to be allocated on 64 byte; // alignment.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp:7,Modifiability,variab,variables,7,// LDS variables aren't emitted in HSA or PAL yet.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp:102,Performance,cache,caches,102,"// Pad with s_code_end to help tools and guard against instruction prefetch; // causing stale data in caches. Arguably this should be done by the linker,; // which is why this isn't done for Mesa.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp:73,Integrability,depend,depending,73,"// In the beginning all features are either 'Any' or 'NotSupported',; // depending on global target features. This will cover empty modules.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp:10,Energy Efficiency,allocate,allocated,10,// LDS is allocated in 64 dword blocks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp:10,Energy Efficiency,allocate,allocated,10,// LDS is allocated in 128 dword blocks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp:14,Energy Efficiency,allocate,allocated,14,// Scratch is allocated in 64-dword or 256-dword blocks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp:105,Energy Efficiency,allocate,allocated,105,"// The private segment wave byte offset is the last of the system SGPRs. We; // initially assumed it was allocated, and may have used it. It shouldn't harm; // anything to disable it if we know the stack isn't used here. We may still; // have emitted code reading it to initialize scratch, but if that's unused; // reading garbage should be OK.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp:45,Testability,log,log,45,// kernarg_segment_alignment is specified as log of the alignment.; // The minimum alignment is 16.; // FIXME: The metadata treats the minimum as 4?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp:294,Safety,avoid,avoid,294,"// FIXME: Formatting here is pretty nasty because clang does not accept; // newlines from diagnostics. This forces us to emit multiple diagnostic; // remarks to simulate newlines. If and when clang does accept newlines, this; // formatting should be aggregated into one remark with newlines to avoid; // printing multiple diagnostic location and diag opts.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.h:4,Integrability,Wrap,Wrapper,4,/// Wrapper for MCInstLowering.lowerOperand() for the tblgen'erated; /// pseudo lowering.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.h:47,Usability,simpl,simple,47,/// tblgen'erated driver function for lowering simple MI->MC pseudo; /// instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAsmPrinter.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:632,Energy Efficiency,reduce,reduced,632,"//===-- AMDGPUAtomicOptimizer.cpp -----------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass optimizes atomic operations by using a single lane of a wavefront; /// to perform the atomic operation, thus reducing contention on that memory; /// location.; /// Atomic optimizer uses following strategies to compute scan and reduced; /// values; /// 1. DPP -; /// This is the most efficient implementation for scan. DPP uses Whole Wave; /// Mode (WWM); /// 2. Iterative -; // An alternative implementation iterates over all active lanes; /// of Wavefront using llvm.cttz and performs scan using readlane & writelane; /// intrinsics; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:688,Energy Efficiency,efficient,efficient,688,"//===-- AMDGPUAtomicOptimizer.cpp -----------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass optimizes atomic operations by using a single lane of a wavefront; /// to perform the atomic operation, thus reducing contention on that memory; /// location.; /// Atomic optimizer uses following strategies to compute scan and reduced; /// values; /// 1. DPP -; /// This is the most efficient implementation for scan. DPP uses Whole Wave; /// Mode (WWM); /// 2. Iterative -; // An alternative implementation iterates over all active lanes; /// of Wavefront using llvm.cttz and performs scan using readlane & writelane; /// intrinsics; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:405,Performance,optimiz,optimizes,405,"//===-- AMDGPUAtomicOptimizer.cpp -----------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass optimizes atomic operations by using a single lane of a wavefront; /// to perform the atomic operation, thus reducing contention on that memory; /// location.; /// Atomic optimizer uses following strategies to compute scan and reduced; /// values; /// 1. DPP -; /// This is the most efficient implementation for scan. DPP uses Whole Wave; /// Mode (WWM); /// 2. Iterative -; // An alternative implementation iterates over all active lanes; /// of Wavefront using llvm.cttz and performs scan using readlane & writelane; /// intrinsics; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:479,Performance,perform,perform,479,"//===-- AMDGPUAtomicOptimizer.cpp -----------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass optimizes atomic operations by using a single lane of a wavefront; /// to perform the atomic operation, thus reducing contention on that memory; /// location.; /// Atomic optimizer uses following strategies to compute scan and reduced; /// values; /// 1. DPP -; /// This is the most efficient implementation for scan. DPP uses Whole Wave; /// Mode (WWM); /// 2. Iterative -; // An alternative implementation iterates over all active lanes; /// of Wavefront using llvm.cttz and performs scan using readlane & writelane; /// intrinsics; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:576,Performance,optimiz,optimizer,576,"//===-- AMDGPUAtomicOptimizer.cpp -----------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass optimizes atomic operations by using a single lane of a wavefront; /// to perform the atomic operation, thus reducing contention on that memory; /// location.; /// Atomic optimizer uses following strategies to compute scan and reduced; /// values; /// 1. DPP -; /// This is the most efficient implementation for scan. DPP uses Whole Wave; /// Mode (WWM); /// 2. Iterative -; // An alternative implementation iterates over all active lanes; /// of Wavefront using llvm.cttz and performs scan using readlane & writelane; /// intrinsics; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:882,Performance,perform,performs,882,"//===-- AMDGPUAtomicOptimizer.cpp -----------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass optimizes atomic operations by using a single lane of a wavefront; /// to perform the atomic operation, thus reducing contention on that memory; /// location.; /// Atomic optimizer uses following strategies to compute scan and reduced; /// values; /// 1. DPP -; /// This is the most efficient implementation for scan. DPP uses Whole Wave; /// Mode (WWM); /// 2. Iterative -; // An alternative implementation iterates over all active lanes; /// of Wavefront using llvm.cttz and performs scan using readlane & writelane; /// intrinsics; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:126,Performance,optimiz,optimize,126,"// If the pointer operand is divergent, then each lane is doing an atomic; // operation on a different address, and we cannot optimize that.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:170,Availability,avail,available,170,"// If the value operand is divergent, each lane is contributing a different; // value to the atomic calculation. We can only optimize divergent values if; // we have DPP available on our subtarget, and the atomic operation is 32; // bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:125,Performance,optimiz,optimize,125,"// If the value operand is divergent, each lane is contributing a different; // value to the atomic calculation. We can only optimize divergent values if; // we have DPP available on our subtarget, and the atomic operation is 32; // bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:26,Performance,optimiz,optimize,26,"// If we get here, we can optimize the atomic using a single wavefront-wide; // atomic operation to do the calculation for the entire wavefront, so; // remember the instruction so we can come back to it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:170,Availability,avail,available,170,"// If the value operand is divergent, each lane is contributing a different; // value to the atomic calculation. We can only optimize divergent values if; // we have DPP available on our subtarget, and the atomic operation is 32; // bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:125,Performance,optimiz,optimize,125,"// If the value operand is divergent, each lane is contributing a different; // value to the atomic calculation. We can only optimize divergent values if; // we have DPP available on our subtarget, and the atomic operation is 32; // bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:78,Performance,optimiz,optimize,78,"// If any of the other arguments to the intrinsic are divergent, we can't; // optimize the operation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:26,Performance,optimiz,optimize,26,"// If we get here, we can optimize the atomic using a single wavefront-wide; // atomic operation to do the calculation for the entire wavefront, so; // remember the instruction so we can come back to it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:3,Energy Efficiency,Reduce,Reduce,3,// Reduce within each row of 16 lanes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:3,Energy Efficiency,Reduce,Reduce,3,// Reduce within each pair of rows (i.e. 32 lanes).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:3,Energy Efficiency,Reduce,Reduce,3,// Reduce across the upper and lower 32 lanes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:269,Deployability,update,update,269,"// Use the builder to create an exclusive scan and compute the final reduced; // value using an iterative approach. This provides an alternative; // implementation to DPP which uses WMM for scan computations. This API iterate; // over active lanes to read, compute and update the value using; // readlane and writelane intrinsics.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:69,Energy Efficiency,reduce,reduced,69,"// Use the builder to create an exclusive scan and compute the final reduced; // value using an iterative approach. This provides an alternative; // implementation to DPP which uses WMM for scan computations. This API iterate; // over active lanes to read, compute and update the value using; // readlane and writelane intrinsics.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:3,Performance,Perform,Perform,3,// Perform writelane if intermediate scan results are required later in the; // kernel computations,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:58,Availability,mask,mask,58,"// If we are in a pixel shader, because of how we have to mask out helper; // lane invocations, we need to record the entry and exit BB's.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:67,Integrability,wrap,wrap,67,"// If we're optimizing an atomic within a pixel shader, we need to wrap the; // entire atomic operation in a helper-lane check. We do not want any helper; // lanes that are around only for the purposes of derivatives to take part; // in any cross-lane communication, and we use a branch on whether the lane is; // live to do this.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:12,Performance,optimiz,optimizing,12,"// If we're optimizing an atomic within a pixel shader, we need to wrap the; // entire atomic operation in a helper-lane check. We do not want any helper; // lanes that are around only for the purposes of derivatives to take part; // in any cross-lane communication, and we use a branch on whether the lane is; // live to do this.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:80,Energy Efficiency,reduce,reduce,80,// This is the value in the atomic operation we need to combine in order to; // reduce the number of atomic operations.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:90,Energy Efficiency,reduce,reduced,90,"// For atomic sub, perform scan with add operation and allow one lane to; // subtract the reduced value later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:19,Performance,perform,perform,19,"// For atomic sub, perform scan with add operation and allow one lane to; // subtract the reduced value later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:146,Performance,perform,performance,146,"// On GFX10 the permlanex16 instruction helps us build a reduction; // without too many readlanes and writelanes, which are generally bad; // for performance.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:84,Deployability,update,update,84,"// At this point, we have split the I's block to allow one lane in wavefront; // to update the precomputed reduced value. Also, completed the codegen for; // new control flow i.e. iterative loop which perform reduction and scan using; // ComputeLoop and ComputeEnd.; // For the new control flow, we need to move branch instruction i.e.; // terminator created during SplitBlockAndInsertIfThen from I's block to; // ComputeEnd block. We also need to set up predecessor to next block when; // single lane done updating the final reduced value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:107,Energy Efficiency,reduce,reduced,107,"// At this point, we have split the I's block to allow one lane in wavefront; // to update the precomputed reduced value. Also, completed the codegen for; // new control flow i.e. iterative loop which perform reduction and scan using; // ComputeLoop and ComputeEnd.; // For the new control flow, we need to move branch instruction i.e.; // terminator created during SplitBlockAndInsertIfThen from I's block to; // ComputeEnd block. We also need to set up predecessor to next block when; // single lane done updating the final reduced value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:526,Energy Efficiency,reduce,reduced,526,"// At this point, we have split the I's block to allow one lane in wavefront; // to update the precomputed reduced value. Also, completed the codegen for; // new control flow i.e. iterative loop which perform reduction and scan using; // ComputeLoop and ComputeEnd.; // For the new control flow, we need to move branch instruction i.e.; // terminator created during SplitBlockAndInsertIfThen from I's block to; // ComputeEnd block. We also need to set up predecessor to next block when; // single lane done updating the final reduced value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:201,Performance,perform,perform,201,"// At this point, we have split the I's block to allow one lane in wavefront; // to update the precomputed reduced value. Also, completed the codegen for; // new control flow i.e. iterative loop which perform reduction and scan using; // ComputeLoop and ComputeEnd.; // For the new control flow, we need to move branch instruction i.e.; // terminator created during SplitBlockAndInsertIfThen from I's block to; // ComputeEnd block. We also need to set up predecessor to next block when; // single lane done updating the final reduced value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:3,Deployability,Update,Update,3,// Update the dominator tree for new control flow.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:66,Availability,mask,mask,66,// Need a final PHI to reconverge to above the helper lane branch mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp:72,Security,access,access,72,"// Need queue_ptr anyway. But under V5, we also need implicitarg_ptr to access; // queue_ptr.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp:50,Security,access,access,50,"// Under V5, we need implicitarg_ptr + offsets to access private_base or; // shared_base. For pre-V5, however, need to access them through queue_ptr +; // offsets.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp:119,Security,access,access,119,"// Under V5, we need implicitarg_ptr + offsets to access private_base or; // shared_base. For pre-V5, however, need to access them through queue_ptr +; // offsets.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp:3,Security,Sanitiz,Sanitizers,3,// Sanitizers require the hostcall buffer passed in the implicit arguments.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp:49,Performance,queue,queue,49,/// Check if the ConstantExpr \p CE requires the queue pointer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp:21,Security,access,access,21,/// Get the constant access bitmap for \p C.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp:36,Performance,queue,queue,36,/// Returns true if \p Fn needs the queue pointer because of \p C.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp:48,Performance,queue,queue,48,/// Used to determine if the Constant needs the queue pointer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp:60,Security,sanitiz,sanitizers,60,"// If the function requires the implicit arg pointer due to sanitizers,; // assume it's needed even if explicitly marked as not requiring it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp:50,Security,access,access,50,"// Under V5, we need implicitarg_ptr + offsets to access private_base or; // shared_base. We do not actually need queue_ptr.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp:113,Performance,queue,queue,113,"// `checkForAllInstructions` is much more cheaper than going through all; // instructions, try it first.; // The queue pointer is not needed if aperture regs is present.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp:32,Performance,queue,queue,32,"// If we found that we need the queue pointer, nothing else to do.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp:207,Performance,load,load,207,// Check if this is a call to the implicitarg_ptr builtin and it; // is used to retrieve the hostcall pointer. The implicit arg for; // hostcall is not used only if every use of the implicitarg_ptr; // is a load that clearly does not retrieve any byte of the; // hostcall pointer. We check this by tracing all the uses of the; // initial call to the implicitarg_ptr intrinsic.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp:217,Usability,clear,clearly,217,// Check if this is a call to the implicitarg_ptr builtin and it; // is used to retrieve the hostcall pointer. The implicit arg for; // hostcall is not used only if every use of the implicitarg_ptr; // is a load that clearly does not retrieve any byte of the; // hostcall pointer. We check this by tracing all the uses of the; // initial call to the implicitarg_ptr intrinsic.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp:4,Integrability,Wrap,Wrapper,4,/// Wrapper around extendRegister to ensure we extend to a full 32-bit register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp:19,Modifiability,extend,extendRegister,19,/// Wrapper around extendRegister to ensure we extend to a full 32-bit register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp:47,Modifiability,extend,extend,47,/// Wrapper around extendRegister to ensure we extend to a full 32-bit register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp:75,Modifiability,extend,extend,75,// 16-bit types are reported as legal for 32-bit registers. We need to; // extend and do a 32-bit copy to avoid the verifier complaining about it.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp:106,Safety,avoid,avoid,106,// 16-bit types are reported as legal for 32-bit registers. We need to; // extend and do a 32-bit copy to avoid the verifier complaining about it.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp:111,Testability,Assert,Assert,111,"// If this is a scalar return, insert a readfirstlane just in case the value; // ends up in a VGPR.; // FIXME: Assert this is a shader return.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp:109,Safety,avoid,avoid,109,"// 16-bit types are reported as legal for 32-bit registers. We need to do; // a 32-bit copy, and truncate to avoid the verifier complaining about it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp:3,Performance,Cache,Cache,3,// Cache the SP register vreg if we need it more than once in this call site.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp:16,Security,access,accessed,16,"// The stack is accessed unswizzled, so we can use a regular copy.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp:3,Energy Efficiency,Allocate,Allocate,3,// Allocate special inputs passed in user SGPRs.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp:15,Availability,down,down,15,// TODO: Align down to dword alignment and extract bits for extending loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp:60,Modifiability,extend,extending,60,// TODO: Align down to dword alignment and extract bits for extending loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp:70,Performance,load,loads,70,// TODO: Align down to dword alignment and extract bits for extending loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp:112,Safety,avoid,avoid,112,// The infrastructure for normal calling convention lowering is essentially; // useless for kernels. We want to avoid any kind of legalization or argument; // splitting.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp:101,Energy Efficiency,allocate,allocate,101,"// We may have proven the input wasn't needed, although the ABI is; // requiring it. We just need to allocate the register appropriately.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp:65,Performance,optimiz,optimize,65,// Must pass all target-independent checks in order to tail call optimize.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp:75,Safety,safe,safe,75,// Verify that the incoming and outgoing arguments from the callee are; // safe to tail call.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp:56,Availability,mask,mask,56,"// If this is a chain call, we need to pass in the EXEC mask.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp:160,Availability,avail,available,160,"// This will be 0 for sibcalls, potentially nonzero for tail calls produced; // by -tailcallopt. For sibcalls, the memory operands for the call are; // already available in the caller's incoming argument space.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp:21,Energy Efficiency,allocate,allocate,21,"// With a fixed ABI, allocate fixed registers before user arguments.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp:21,Energy Efficiency,allocate,allocate,21,"// With a fixed ABI, allocate fixed registers before user arguments.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:423,Performance,optimiz,optimizations,423,"//===-- AMDGPUCodeGenPrepare.cpp ------------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass does misc. AMDGPU optimizations on IR before instruction; /// selection.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:98,Testability,test,testing,98,// Leave all division operations as they are. This supersedes ExpandDiv64InIR; // and is used for testing the legalizer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:47,Testability,test,test,47,// Disable processing of fdiv so we can better test the backend implementations.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:4,Integrability,Wrap,Wrapper,4,/// Wrapper to pass all the arguments to computeKnownFPClass,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:225,Modifiability,extend,extending,225,"/// Promotes uniform binary operation \p I to equivalent 32 bit binary; /// operation.; ///; /// \details \p I's base element bit width must be greater than 1 and less; /// than or equal 16. Promotion is done by sign or zero extending operands to; /// 32 bits, replacing \p I with equivalent 32 bit binary operation, and; /// truncating the result of 32 bit binary operation back to \p I's original; /// type. Division operation is not promoted.; ///; /// \returns True if \p I is promoted to equivalent 32 bit binary operation,; /// false otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:209,Modifiability,extend,extending,209,"/// Promotes uniform 'icmp' operation \p I to 32 bit 'icmp' operation.; ///; /// \details \p I's base element bit width must be greater than 1 and less; /// than or equal 16. Promotion is done by sign or zero extending operands to; /// 32 bits, and replacing \p I with 32 bit 'icmp' operation.; ///; /// \returns True.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:218,Modifiability,extend,extending,218,"/// Promotes uniform 'select' operation \p I to 32 bit 'select'; /// operation.; ///; /// \details \p I's base element bit width must be greater than 1 and less; /// than or equal 16. Promotion is done by sign or zero extending operands to; /// 32 bits, replacing \p I with 32 bit 'select' operation, and truncating the; /// result of 32 bit 'select' operation back to \p I's original type.; ///; /// \returns True.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:218,Modifiability,extend,extending,218,"/// Promotes uniform 'bitreverse' intrinsic \p I to 32 bit 'bitreverse'; /// intrinsic.; ///; /// \details \p I's base element bit width must be greater than 1 and less; /// than or equal 16. Promotion is done by zero extending the operand to 32; /// bits, replacing \p I with 32 bit 'bitreverse' intrinsic, shifting the; /// result of 32 bit 'bitreverse' intrinsic to the right with zero fill (the; /// shift amount is 32 minus \p I's base element bit width), and truncating; /// the result of the shift operation back to \p I's original type.; ///; /// \returns True.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:140,Modifiability,extend,extending,140,/// \returns The minimum number of bits needed to store the value of \Op as an; /// unsigned integer. Truncating to this size and then zero-extending to; /// the original will not change the value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:137,Modifiability,extend,extending,137,/// \returns The minimum number of bits needed to store the value of \Op as a; /// signed integer. Truncating to this size and then sign-extending to; /// the original size will not change the value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:122,Testability,assert,asserting,122,/// Replace mul instructions with llvm.amdgcn.mul.u24 or llvm.amdgcn.mul.s24.; /// SelectionDAG has an issue where an and asserting the bits are known,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:4,Performance,Perform,Perform,4,"/// Perform same function as equivalently named function in DAGCombiner. Since; /// we expand some divisions here, we need to perform this before obscuring.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:126,Performance,perform,perform,126,"/// Perform same function as equivalently named function in DAGCombiner. Since; /// we expand some divisions here, we need to perform this before obscuring.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:19,Performance,load,load,19,"/// Widen a scalar load.; ///; /// \details \p Widen scalar load for uniform, small type loads from constant; // memory / to a full 32-bits and then truncate the input to allow a scalar; // load instead of a vector load.; //; /// \returns True.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:60,Performance,load,load,60,"/// Widen a scalar load.; ///; /// \details \p Widen scalar load for uniform, small type loads from constant; // memory / to a full 32-bits and then truncate the input to allow a scalar; // load instead of a vector load.; //; /// \returns True.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:89,Performance,load,loads,89,"/// Widen a scalar load.; ///; /// \details \p Widen scalar load for uniform, small type loads from constant; // memory / to a full 32-bits and then truncate the input to allow a scalar; // load instead of a vector load.; //; /// \returns True.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:190,Performance,load,load,190,"/// Widen a scalar load.; ///; /// \details \p Widen scalar load for uniform, small type loads from constant; // memory / to a full 32-bits and then truncate the input to allow a scalar; // load instead of a vector load.; //; /// \returns True.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:215,Performance,load,load,215,"/// Widen a scalar load.; ///; /// \details \p Widen scalar load for uniform, small type loads from constant; // memory / to a full 32-bits and then truncate the input to allow a scalar; // load instead of a vector load.; //; /// \returns True.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:120,Safety,safe,safe,120,"// If we have have to work around the fract/frexp bug, we're worse off than; // using the fdiv.fast expansion. The full safe expansion is faster if we have; // fast FMA.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:28,Safety,avoid,avoid,28,"// We're scaling the LHS to avoid a denormal input, and scale the denominator; // to avoid large values underflowing the result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:85,Safety,avoid,avoid,85,"// We're scaling the LHS to avoid a denormal input, and scale the denominator; // to avoid large values underflowing the result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:13,Integrability,contract,contraction,13,// The rsqrt contraction increases accuracy from ~2ulp to ~1ulp.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:13,Integrability,contract,contraction,13,// The rsqrt contraction increases accuracy from ~2ulp to ~1ulp.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:152,Deployability,a/b,a/b,152,"// Optimize fdiv with rcp:; //; // 1/x -> rcp(x) when rcp is sufficiently accurate or inaccurate rcp is; // allowed with unsafe-fp-math or afn.; //; // a/b -> a*rcp(b) when arcp is allowed, and we only need provide ULP 1.0",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:3,Performance,Optimiz,Optimize,3,"// Optimize fdiv with rcp:; //; // 1/x -> rcp(x) when rcp is sufficiently accurate or inaccurate rcp is; // allowed with unsafe-fp-math or afn.; //; // a/b -> a*rcp(b) when arcp is allowed, and we only need provide ULP 1.0",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:121,Safety,unsafe,unsafe-fp-math,121,"// Optimize fdiv with rcp:; //; // 1/x -> rcp(x) when rcp is sufficiently accurate or inaccurate rcp is; // allowed with unsafe-fp-math or afn.; //; // a/b -> a*rcp(b) when arcp is allowed, and we only need provide ULP 1.0",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:112,Availability,error,error,112,"// v_rcp_f32 and v_rsq_f32 do not support denormals, and according to; // the CI documentation has a worst case error of 1 ulp.; // OpenCL requires <= 2.5 ulp for 1.0 / x, so it should always be OK; // to use it as long as we aren't trying to use denormals.; //; // v_rcp_f16 and v_rsq_f16 DO support denormals.; // NOTE: v_sqrt and v_rcp will be combined to v_rsq later. So we don't; // insert rsq intrinsic here.; // 1.0 / x -> rcp(x)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:121,Safety,avoid,avoid,121,"// TODO: If the input isn't denormal, and we know the input exponent isn't; // big enough to introduce a denormal we can avoid the scaling.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:42,Safety,avoid,avoid,42,// x / y -> x * (1.0 / y); // TODO: Could avoid denormal scaling and use raw rcp if we knew the output; // will never underflow.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:36,Deployability,a/b,a/b,36,"// optimize with fdiv.fast:; //; // a/b -> fdiv.fast(a, b) when !fpmath >= 2.5ulp with denormals flushed.; //; // 1/x -> fdiv.fast(1,x) when !fpmath >= 2.5ulp.; //; // NOTE: optimizeWithRcp should be tried first because rcp is the preference.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:3,Performance,optimiz,optimize,3,"// optimize with fdiv.fast:; //; // a/b -> fdiv.fast(a, b) when !fpmath >= 2.5ulp with denormals flushed.; //; // 1/x -> fdiv.fast(1,x) when !fpmath >= 2.5ulp.; //; // NOTE: optimizeWithRcp should be tried first because rcp is the preference.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:174,Performance,optimiz,optimizeWithRcp,174,"// optimize with fdiv.fast:; //; // a/b -> fdiv.fast(a, b) when !fpmath >= 2.5ulp with denormals flushed.; //; // 1/x -> fdiv.fast(1,x) when !fpmath >= 2.5ulp.; //; // NOTE: optimizeWithRcp should be tried first because rcp is the preference.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:274,Deployability,a/b,a/b,274,"// Optimizations is performed based on fpmath, fast math flags as well as; // denormals to optimize fdiv with either rcp or fdiv.fast.; //; // With rcp:; // 1/x -> rcp(x) when rcp is sufficiently accurate or inaccurate rcp is; // allowed with unsafe-fp-math or afn.; //; // a/b -> a*rcp(b) when inaccurate rcp is allowed with unsafe-fp-math or afn.; //; // With fdiv.fast:; // a/b -> fdiv.fast(a, b) when !fpmath >= 2.5ulp with denormals flushed.; //; // 1/x -> fdiv.fast(1,x) when !fpmath >= 2.5ulp.; //; // NOTE: rcp is the preference in cases that both are legal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:377,Deployability,a/b,a/b,377,"// Optimizations is performed based on fpmath, fast math flags as well as; // denormals to optimize fdiv with either rcp or fdiv.fast.; //; // With rcp:; // 1/x -> rcp(x) when rcp is sufficiently accurate or inaccurate rcp is; // allowed with unsafe-fp-math or afn.; //; // a/b -> a*rcp(b) when inaccurate rcp is allowed with unsafe-fp-math or afn.; //; // With fdiv.fast:; // a/b -> fdiv.fast(a, b) when !fpmath >= 2.5ulp with denormals flushed.; //; // 1/x -> fdiv.fast(1,x) when !fpmath >= 2.5ulp.; //; // NOTE: rcp is the preference in cases that both are legal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:3,Performance,Optimiz,Optimizations,3,"// Optimizations is performed based on fpmath, fast math flags as well as; // denormals to optimize fdiv with either rcp or fdiv.fast.; //; // With rcp:; // 1/x -> rcp(x) when rcp is sufficiently accurate or inaccurate rcp is; // allowed with unsafe-fp-math or afn.; //; // a/b -> a*rcp(b) when inaccurate rcp is allowed with unsafe-fp-math or afn.; //; // With fdiv.fast:; // a/b -> fdiv.fast(a, b) when !fpmath >= 2.5ulp with denormals flushed.; //; // 1/x -> fdiv.fast(1,x) when !fpmath >= 2.5ulp.; //; // NOTE: rcp is the preference in cases that both are legal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:20,Performance,perform,performed,20,"// Optimizations is performed based on fpmath, fast math flags as well as; // denormals to optimize fdiv with either rcp or fdiv.fast.; //; // With rcp:; // 1/x -> rcp(x) when rcp is sufficiently accurate or inaccurate rcp is; // allowed with unsafe-fp-math or afn.; //; // a/b -> a*rcp(b) when inaccurate rcp is allowed with unsafe-fp-math or afn.; //; // With fdiv.fast:; // a/b -> fdiv.fast(a, b) when !fpmath >= 2.5ulp with denormals flushed.; //; // 1/x -> fdiv.fast(1,x) when !fpmath >= 2.5ulp.; //; // NOTE: rcp is the preference in cases that both are legal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:91,Performance,optimiz,optimize,91,"// Optimizations is performed based on fpmath, fast math flags as well as; // denormals to optimize fdiv with either rcp or fdiv.fast.; //; // With rcp:; // 1/x -> rcp(x) when rcp is sufficiently accurate or inaccurate rcp is; // allowed with unsafe-fp-math or afn.; //; // a/b -> a*rcp(b) when inaccurate rcp is allowed with unsafe-fp-math or afn.; //; // With fdiv.fast:; // a/b -> fdiv.fast(a, b) when !fpmath >= 2.5ulp with denormals flushed.; //; // 1/x -> fdiv.fast(1,x) when !fpmath >= 2.5ulp.; //; // NOTE: rcp is the preference in cases that both are legal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:243,Safety,unsafe,unsafe-fp-math,243,"// Optimizations is performed based on fpmath, fast math flags as well as; // denormals to optimize fdiv with either rcp or fdiv.fast.; //; // With rcp:; // 1/x -> rcp(x) when rcp is sufficiently accurate or inaccurate rcp is; // allowed with unsafe-fp-math or afn.; //; // a/b -> a*rcp(b) when inaccurate rcp is allowed with unsafe-fp-math or afn.; //; // With fdiv.fast:; // a/b -> fdiv.fast(a, b) when !fpmath >= 2.5ulp with denormals flushed.; //; // 1/x -> fdiv.fast(1,x) when !fpmath >= 2.5ulp.; //; // NOTE: rcp is the preference in cases that both are legal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:326,Safety,unsafe,unsafe-fp-math,326,"// Optimizations is performed based on fpmath, fast math flags as well as; // denormals to optimize fdiv with either rcp or fdiv.fast.; //; // With rcp:; // 1/x -> rcp(x) when rcp is sufficiently accurate or inaccurate rcp is; // allowed with unsafe-fp-math or afn.; //; // a/b -> a*rcp(b) when inaccurate rcp is allowed with unsafe-fp-math or afn.; //; // With fdiv.fast:; // a/b -> fdiv.fast(a, b) when !fpmath >= 2.5ulp with denormals flushed.; //; // 1/x -> fdiv.fast(1,x) when !fpmath >= 2.5ulp.; //; // NOTE: rcp is the preference in cases that both are legal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:34,Safety,unsafe,unsafe-fp-math,34,"// Inaccurate rcp is allowed with unsafe-fp-math or afn.; //; // Defer to codegen to handle this.; //; // TODO: Decide on an interpretation for interactions between afn + arcp +; // !fpmath, and make it consistent between here and codegen. For now, defer; // expansion of afn to codegen. The current interpretation is so aggressive we; // don't need any pre-consideration here when we have better information. A; // more conservative interpretation could use handling here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:207,Availability,failure,failure,207,/// Figure out how many bits are really needed for this division. \p AtLeast is; /// an optimization hint to bypass the second ComputeNumSignBits call if we the; /// first one is insufficient. Returns -1 on failure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:88,Performance,optimiz,optimization,88,/// Figure out how many bits are really needed for this division. \p AtLeast is; /// an optimization hint to bypass the second ComputeNumSignBits call if we the; /// first one is insufficient. Returns -1 on failure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:3,Modifiability,Extend,Extend,3,// Extend in register from the number of bits this divide really is.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:181,Performance,optimiz,optimizations,181,"// Try to recognize special cases the DAG will emit special, better expansions; // than the general expansion we do here.; // TODO: It would be better to just directly handle those optimizations here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:118,Energy Efficiency,power,powers,118,"// TODO: Sdiv check for not exact for some reason.; // If there's no wider mulhi, there's only a better expansion for powers of; // two.; // TODO: Should really know for each vector element.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:59,Energy Efficiency,power,power,59,"// fold (udiv x, (shl c, y)) -> x >>u (log2(c)+y) iff c is power of 2",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:21,Performance,optimiz,optimization,21,// Keep it for later optimization.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:21,Performance,optimiz,optimization,21,// Keep it for later optimization.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:94,Safety,sanity check,sanity check,94,// Non constant index/out of bounds index -> folding is unlikely.; // The latter is more of a sanity check because canonical IR should just; // have replaced those with poison.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:16,Performance,cache,cache,16,// Check in the cache first.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:285,Safety,avoid,avoid,285,"// We consider PHI nodes as part of ""chains"", so given a PHI node I, we; // recursively consider all its users and incoming values that are also PHI; // nodes. We then make a decision about all of those PHIs at once. Either they; // all get broken up, or none of them do. That way, we avoid cases where a; // single PHI is/is not broken and we end up reforming/exploding a vector; // multiple times, or even worse, doing it in a loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:252,Performance,perform,performance,252,"// To consider a PHI profitable to break, we need to see some interesting; // incoming values. At least 2/3rd (rounded up) of all PHIs in the worklist; // must have one to consider all PHIs breakable.; //; // This threshold has been determined through performance testing.; //; // Note that the computation below is equivalent to; //; // (unsigned)ceil((K / 3.0) * 2); //; // It's simply written this way to avoid mixing integral/FP arithmetic.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:408,Safety,avoid,avoid,408,"// To consider a PHI profitable to break, we need to see some interesting; // incoming values. At least 2/3rd (rounded up) of all PHIs in the worklist; // must have one to consider all PHIs breakable.; //; // This threshold has been determined through performance testing.; //; // Note that the computation below is equivalent to; //; // (unsigned)ceil((K / 3.0) * 2); //; // It's simply written this way to avoid mixing integral/FP arithmetic.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:264,Testability,test,testing,264,"// To consider a PHI profitable to break, we need to see some interesting; // incoming values. At least 2/3rd (rounded up) of all PHIs in the worklist; // must have one to consider all PHIs breakable.; //; // This threshold has been determined through performance testing.; //; // Note that the computation below is equivalent to; //; // (unsigned)ceil((K / 3.0) * 2); //; // It's simply written this way to avoid mixing integral/FP arithmetic.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:381,Usability,simpl,simply,381,"// To consider a PHI profitable to break, we need to see some interesting; // incoming values. At least 2/3rd (rounded up) of all PHIs in the worklist; // must have one to consider all PHIs breakable.; //; // This threshold has been determined through performance testing.; //; // Note that the computation below is equivalent to; //; // (unsigned)ceil((K / 3.0) * 2); //; // It's simply written this way to avoid mixing integral/FP arithmetic.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:93,Usability,clear,clear,93,"// Don't break PHIs that have no interesting incoming values. That is, where; // there is no clear opportunity to fold the ""extractelement"" instructions; // we would add.; //; // Note: IC does not run after this pass, so we're only interested in the; // foldings that the DAG combiner can do.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:1160,Availability,error,error,1160,"/// Slice \p Inc according to the information contained within this slice.; /// This is cached, so if called multiple times for the same \p BB & \p Inc; /// pair, it returns the same Sliced value as well.; ///; /// Note this *intentionally* does not return the same value for, say,; /// [%bb.0, %0] & [%bb.1, %0] as:; /// - It could cause issues with dominance (e.g. if bb.1 is seen first, then; /// the value in bb.1 may not be reachable from bb.0 if it's its; /// predecessor.); /// - We also want to make our extract instructions as local as possible so; /// the DAG has better chances of folding them out. Duplicating them like; /// that is beneficial in that regard.; ///; /// This is both a minor optimization to avoid creating duplicate; /// instructions, but also a requirement for correctness. It is not forbidden; /// for a PHI node to have the same [BB, Val] pair multiple times. If we; /// returned a new value each time, those previously identical pairs would all; /// have different incoming values (from the same block) and it'd cause a ""PHI; /// node has multiple entries for the same basic block with different incoming; /// values!"" verifier error.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:88,Performance,cache,cached,88,"/// Slice \p Inc according to the information contained within this slice.; /// This is cached, so if called multiple times for the same \p BB & \p Inc; /// pair, it returns the same Sliced value as well.; ///; /// Note this *intentionally* does not return the same value for, say,; /// [%bb.0, %0] & [%bb.1, %0] as:; /// - It could cause issues with dominance (e.g. if bb.1 is seen first, then; /// the value in bb.1 may not be reachable from bb.0 if it's its; /// predecessor.); /// - We also want to make our extract instructions as local as possible so; /// the DAG has better chances of folding them out. Duplicating them like; /// that is beneficial in that regard.; ///; /// This is both a minor optimization to avoid creating duplicate; /// instructions, but also a requirement for correctness. It is not forbidden; /// for a PHI node to have the same [BB, Val] pair multiple times. If we; /// returned a new value each time, those previously identical pairs would all; /// have different incoming values (from the same block) and it'd cause a ""PHI; /// node has multiple entries for the same basic block with different incoming; /// values!"" verifier error.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:703,Performance,optimiz,optimization,703,"/// Slice \p Inc according to the information contained within this slice.; /// This is cached, so if called multiple times for the same \p BB & \p Inc; /// pair, it returns the same Sliced value as well.; ///; /// Note this *intentionally* does not return the same value for, say,; /// [%bb.0, %0] & [%bb.1, %0] as:; /// - It could cause issues with dominance (e.g. if bb.1 is seen first, then; /// the value in bb.1 may not be reachable from bb.0 if it's its; /// predecessor.); /// - We also want to make our extract instructions as local as possible so; /// the DAG has better chances of folding them out. Duplicating them like; /// that is beneficial in that regard.; ///; /// This is both a minor optimization to avoid creating duplicate; /// instructions, but also a requirement for correctness. It is not forbidden; /// for a PHI node to have the same [BB, Val] pair multiple times. If we; /// returned a new value each time, those previously identical pairs would all; /// have different incoming values (from the same block) and it'd cause a ""PHI; /// node has multiple entries for the same basic block with different incoming; /// values!"" verifier error.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:719,Safety,avoid,avoid,719,"/// Slice \p Inc according to the information contained within this slice.; /// This is cached, so if called multiple times for the same \p BB & \p Inc; /// pair, it returns the same Sliced value as well.; ///; /// Note this *intentionally* does not return the same value for, say,; /// [%bb.0, %0] & [%bb.1, %0] as:; /// - It could cause issues with dominance (e.g. if bb.1 is seen first, then; /// the value in bb.1 may not be reachable from bb.0 if it's its; /// predecessor.); /// - We also want to make our extract instructions as local as possible so; /// the DAG has better chances of folding them out. Duplicating them like; /// that is beneficial in that regard.; ///; /// This is both a minor optimization to avoid creating duplicate; /// instructions, but also a requirement for correctness. It is not forbidden; /// for a PHI node to have the same [BB, Val] pair multiple times. If we; /// returned a new value each time, those previously identical pairs would all; /// have different incoming values (from the same block) and it'd cause a ""PHI; /// node has multiple entries for the same basic block with different incoming; /// values!"" verifier error.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:495,Performance,optimiz,optimization,495,"// Break-up fixed-vector PHIs into smaller pieces.; // Default threshold is 32, so it breaks up any vector that's >32 bits into; // its elements, or into 32-bit pieces (for 8/16 bit elts).; //; // This is only helpful for DAGISel because it doesn't handle large PHIs as; // well as GlobalISel. DAGISel lowers PHIs by using CopyToReg/CopyFromReg.; // With large, odd-sized PHIs we may end up needing many `build_vector`; // operations with most elements being ""undef"". This inhibits a lot of; // optimization opportunities and can result in unreasonably high register; // pressure and the inevitable stack spilling.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp:82,Performance,optimiz,optimized,82,// Match pattern for fract intrinsic in contexts where the nan check has been; // optimized out (and hope the knowledge the source can't be nan wasn't lost).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCodeGenPrepare.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCombinerHelper.cpp:262,Safety,avoid,avoid,262,"// Some users (such as 3-operand FMA/MAD) must use a VOP3 encoding, and thus; // it is truly free to use a source modifier in all cases. If there are; // multiple users but for each one will necessitate using VOP3, there will be; // a code size increase. Try to avoid increasing code size unless we know it; // will save on the instruction count.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCombinerHelper.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCombinerHelper.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCombinerHelper.cpp:68,Availability,down,down,68,"// If the input has multiple uses and we can either fold the negate down, or; // the other uses cannot, give up. This both prevents unprofitable; // transformations and infinite loops: we won't repeatedly try to fold around; // a negate that has no 'good' form.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCombinerHelper.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCombinerHelper.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCombinerHelper.cpp:62,Safety,avoid,avoid,62,// TODO: Should return converted value / extension source and avoid introducing; // intermediate fptruncs in the apply function.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCombinerHelper.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUCombinerHelper.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp:414,Energy Efficiency,schedul,scheduling,414,"//===--- AMDGPUExportClusting.cpp - AMDGPU Export Clustering -------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This file contains a DAG scheduling mutation to cluster shader; /// exports.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp:83,Performance,perform,performance,83,// Position exports should occur as soon as possible in the shader; // for optimal performance. This moves position exports before; // other exports while preserving the order within different export; // types (pos or other).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp:12,Integrability,depend,dependencies,12,// Copy all dependencies to the head of the chain to avoid any; // computation being inserted into the chain.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp:53,Safety,avoid,avoid,53,// Copy all dependencies to the head of the chain to avoid any; // computation being inserted into the chain.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp:42,Integrability,depend,dependencies,42,// If we remove a barrier we need to copy dependencies; // from the predecessor to maintain order.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp:164,Energy Efficiency,schedul,scheduling,164,"// Pass through DAG gathering a list of exports and removing barrier edges; // creating dependencies on exports. Freeing exports of successor edges; // allows more scheduling freedom, and nothing should be order dependent; // on exports. Edges will be added later to order the exports.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp:88,Integrability,depend,dependencies,88,"// Pass through DAG gathering a list of exports and removing barrier edges; // creating dependencies on exports. Freeing exports of successor edges; // allows more scheduling freedom, and nothing should be order dependent; // on exports. Edges will be added later to order the exports.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp:212,Integrability,depend,dependent,212,"// Pass through DAG gathering a list of exports and removing barrier edges; // creating dependencies on exports. Freeing exports of successor edges; // allows more scheduling freedom, and nothing should be order dependent; // on exports. Edges will be added later to order the exports.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUExportClustering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUFrameLowering.cpp:383,Integrability,Interface,Interface,383,"//===----------------------- AMDGPUFrameLowering.cpp ----------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //==-----------------------------------------------------------------------===//; //; // Interface to describe a layout of a stack frame on a AMDGPU target machine.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUFrameLowering.cpp:414,Integrability,depend,depending,414,"// XXX: Hardcoding to 1 for now.; //; // I think the StackWidth should be stored as metadata associated with the; // MachineFunction. This metadata can either be added by a frontend, or; // calculated by a R600 specific LLVM IR pass.; //; // The StackWidth determines how stack objects are laid out in memory.; // For a vector stack variable, like: int4 stack[2], the data will be stored; // in the following ways depending on the StackWidth.; //; // StackWidth = 1:; //; // T0.X = stack[0].x; // T1.X = stack[0].y; // T2.X = stack[0].z; // T3.X = stack[0].w; // T4.X = stack[1].x; // T5.X = stack[1].y; // T6.X = stack[1].z; // T7.X = stack[1].w; //; // StackWidth = 2:; //; // T0.X = stack[0].x; // T0.Y = stack[0].y; // T1.X = stack[0].z; // T1.Y = stack[0].w; // T2.X = stack[1].x; // T2.Y = stack[1].y; // T3.X = stack[1].z; // T3.Y = stack[1].w; //; // StackWidth = 4:; // T0.X = stack[0].x; // T0.Y = stack[0].y; // T0.Z = stack[0].z; // T0.W = stack[0].w; // T1.X = stack[1].x; // T1.Y = stack[1].y; // T1.Z = stack[1].z; // T1.W = stack[1].w",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUFrameLowering.cpp:333,Modifiability,variab,variable,333,"// XXX: Hardcoding to 1 for now.; //; // I think the StackWidth should be stored as metadata associated with the; // MachineFunction. This metadata can either be added by a frontend, or; // calculated by a R600 specific LLVM IR pass.; //; // The StackWidth determines how stack objects are laid out in memory.; // For a vector stack variable, like: int4 stack[2], the data will be stored; // in the following ways depending on the StackWidth.; //; // StackWidth = 1:; //; // T0.X = stack[0].x; // T1.X = stack[0].y; // T2.X = stack[0].z; // T3.X = stack[0].w; // T4.X = stack[1].x; // T5.X = stack[1].y; // T6.X = stack[1].z; // T7.X = stack[1].w; //; // StackWidth = 2:; //; // T0.X = stack[0].x; // T0.Y = stack[0].y; // T1.X = stack[0].z; // T1.Y = stack[0].w; // T2.X = stack[1].x; // T2.Y = stack[1].y; // T3.X = stack[1].z; // T3.Y = stack[1].w; //; // StackWidth = 4:; // T0.X = stack[0].x; // T0.Y = stack[0].y; // T0.Z = stack[0].z; // T0.W = stack[0].w; // T1.X = stack[1].x; // T1.Y = stack[1].y; // T1.Z = stack[1].z; // T1.W = stack[1].w",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUFrameLowering.h:395,Integrability,Interface,Interface,395,"//===--------------------- AMDGPUFrameLowering.h ----------------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Interface to describe a layout of a stack frame on an AMDGPU target.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUFrameLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUFrameLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUGlobalISelDivergenceLowering.cpp:450,Availability,mask,mask,450,"//===-- AMDGPUGlobalISelDivergenceLowering.cpp ----------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// GlobalISel pass that selects divergent i1 phis as lane mask phis.; /// Lane mask merging uses same algorithm as SDAG in SILowerI1Copies.; /// Handles all cases of temporal divergence.; /// For divergent non-phi i1 and uniform i1 uses outside of the cycle this pass; /// currently depends on LCSSA to insert phis with one incoming.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUGlobalISelDivergenceLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUGlobalISelDivergenceLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUGlobalISelDivergenceLowering.cpp:471,Availability,mask,mask,471,"//===-- AMDGPUGlobalISelDivergenceLowering.cpp ----------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// GlobalISel pass that selects divergent i1 phis as lane mask phis.; /// Lane mask merging uses same algorithm as SDAG in SILowerI1Copies.; /// Handles all cases of temporal divergence.; /// For divergent non-phi i1 and uniform i1 uses outside of the cycle this pass; /// currently depends on LCSSA to insert phis with one incoming.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUGlobalISelDivergenceLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUGlobalISelDivergenceLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUGlobalISelDivergenceLowering.cpp:675,Integrability,depend,depends,675,"//===-- AMDGPUGlobalISelDivergenceLowering.cpp ----------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// GlobalISel pass that selects divergent i1 phis as lane mask phis.; /// Lane mask merging uses same algorithm as SDAG in SILowerI1Copies.; /// Handles all cases of temporal divergence.; /// For divergent non-phi i1 and uniform i1 uses outside of the cycle this pass; /// currently depends on LCSSA to insert phis with one incoming.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUGlobalISelDivergenceLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUGlobalISelDivergenceLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUGlobalISelUtils.cpp:76,Integrability,wrap,wraparound,76,"// A 32-bit (address + offset) should not cause unsigned 32-bit integer; // wraparound, because s_load instructions perform the addition in 64 bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUGlobalISelUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUGlobalISelUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUGlobalISelUtils.cpp:116,Performance,perform,perform,116,"// A 32-bit (address + offset) should not cause unsigned 32-bit integer; // wraparound, because s_load instructions perform the addition in 64 bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUGlobalISelUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUGlobalISelUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUGlobalISelUtils.cpp:41,Usability,simpl,simply,41,"// If Base was int converted to pointer, simply return int and offset.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUGlobalISelUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUGlobalISelUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUHSAMetadataStreamer.cpp:17,Performance,queue,queue,17,"// Emit ""default queue"" and ""completion action"" arguments if enqueue kernel is; // used, otherwise emit dummy ""none"" arguments.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUHSAMetadataStreamer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUHSAMetadataStreamer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:399,Energy Efficiency,schedul,schedule,399,"//===--- AMDGPUIGroupLP.cpp - AMDGPU IGroupLP ------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // \file This file defines a set of schedule DAG mutations that can be used to; // override default scheduler behavior to enforce specific scheduling patterns.; // They should be used in cases where runtime performance considerations such as; // inter-wavefront interactions, mean that compile-time heuristics cannot; // predict the optimal instruction ordering, or in kernels where optimum; // instruction scheduling is important enough to warrant manual intervention.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:463,Energy Efficiency,schedul,scheduler,463,"//===--- AMDGPUIGroupLP.cpp - AMDGPU IGroupLP ------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // \file This file defines a set of schedule DAG mutations that can be used to; // override default scheduler behavior to enforce specific scheduling patterns.; // They should be used in cases where runtime performance considerations such as; // inter-wavefront interactions, mean that compile-time heuristics cannot; // predict the optimal instruction ordering, or in kernels where optimum; // instruction scheduling is important enough to warrant manual intervention.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:502,Energy Efficiency,schedul,scheduling,502,"//===--- AMDGPUIGroupLP.cpp - AMDGPU IGroupLP ------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // \file This file defines a set of schedule DAG mutations that can be used to; // override default scheduler behavior to enforce specific scheduling patterns.; // They should be used in cases where runtime performance considerations such as; // inter-wavefront interactions, mean that compile-time heuristics cannot; // predict the optimal instruction ordering, or in kernels where optimum; // instruction scheduling is important enough to warrant manual intervention.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:770,Energy Efficiency,schedul,scheduling,770,"//===--- AMDGPUIGroupLP.cpp - AMDGPU IGroupLP ------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // \file This file defines a set of schedule DAG mutations that can be used to; // override default scheduler behavior to enforce specific scheduling patterns.; // They should be used in cases where runtime performance considerations such as; // inter-wavefront interactions, mean that compile-time heuristics cannot; // predict the optimal instruction ordering, or in kernels where optimum; // instruction scheduling is important enough to warrant manual intervention.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:570,Performance,perform,performance,570,"//===--- AMDGPUIGroupLP.cpp - AMDGPU IGroupLP ------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // \file This file defines a set of schedule DAG mutations that can be used to; // override default scheduler behavior to enforce specific scheduling patterns.; // They should be used in cases where runtime performance considerations such as; // inter-wavefront interactions, mean that compile-time heuristics cannot; // predict the optimal instruction ordering, or in kernels where optimum; // instruction scheduling is important enough to warrant manual intervention.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:684,Safety,predict,predict,684,"//===--- AMDGPUIGroupLP.cpp - AMDGPU IGroupLP ------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // \file This file defines a set of schedule DAG mutations that can be used to; // override default scheduler behavior to enforce specific scheduling patterns.; // They should be used in cases where runtime performance considerations such as; // inter-wavefront interactions, mean that compile-time heuristics cannot; // predict the optimal instruction ordering, or in kernels where optimum; // instruction scheduling is important enough to warrant manual intervention.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:21,Availability,mask,mask,21,// Components of the mask that determines which instruction types may be may be; // classified into a SchedGroup.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:172,Performance,Cache,Cache,172,// InstructionRule class is used to enact a filter which determines whether or; // not an SU maps to a given SchedGroup. It contains complementary data; // structures (e.g Cache) to help those filters.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:16,Availability,avail,available,16,// A cache made available to the Filter to store SUnits for subsequent; // invocations of the Filter,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:5,Performance,cache,cache,5,// A cache made available to the Filter to store SUnits for subsequent; // invocations of the Filter,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:79,Energy Efficiency,schedul,scheduler,79,// Classify instructions into groups to enable fine tuned control over the; // scheduler. These groups may be more specific than current SchedModel; // instruction classes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:52,Performance,tune,tuned,52,// Classify instructions into groups to enable fine tuned control over the; // scheduler. These groups may be more specific than current SchedModel; // instruction classes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:3,Availability,Mask,Mask,3,// Mask that defines which instruction types can be classified into this; // SchedGroup. The instruction types correspond to the mask from SCHED_BARRIER; // and SCHED_GROUP_BARRIER.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:129,Availability,mask,mask,129,// Mask that defines which instruction types can be classified into this; // SchedGroup. The instruction types correspond to the mask from SCHED_BARRIER; // and SCHED_GROUP_BARRIER.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:25,Integrability,synchroniz,synchronize,25,// SchedGroups will only synchronize with other SchedGroups that have the same; // SyncID.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:11,Integrability,depend,dependencies,11,"// Add DAG dependencies from all SUnits in this SchedGroup and this SU. If; // MakePred is true, SU will be a predecessor of the SUnits in this; // SchedGroup, otherwise SU will be a successor.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:11,Integrability,depend,dependencies,11,"// Add DAG dependencies and track which edges are added, and the count of; // missed edges",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:11,Integrability,depend,dependencies,11,// Add DAG dependencies from all SUnits in this SchedGroup and this SU.; // Use the predicate to determine whether SU should be a predecessor (P =; // true) or a successor (P = false) of this SchedGroup.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:11,Integrability,depend,dependencies,11,// Add DAG dependencies such that SUnits in this group shall be ordered; // before SUnits in OtherGroup.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:209,Deployability,Pipeline,Pipeline,209,"// Append a constraint that SUs must meet in order to fit into this; // SchedGroup. Since many rules involve the relationship between a SchedGroup; // and the SUnits in other SchedGroups, rules are checked at Pipeline Solve; // time (rather than SchedGroup init time.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:73,Deployability,Pipeline,PipelineInstrs,73,"// Add instructions to the SchedGroup bottom up starting from RIter.; // PipelineInstrs is a set of instructions that should not be added to the; // SchedGroup even when the other conditions for adding it are satisfied.; // RIter will be added to the SchedGroup as well, and dependencies will be; // added so that RIter will always be scheduled at the end of the group.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:335,Energy Efficiency,schedul,scheduled,335,"// Add instructions to the SchedGroup bottom up starting from RIter.; // PipelineInstrs is a set of instructions that should not be added to the; // SchedGroup even when the other conditions for adding it are satisfied.; // RIter will be added to the SchedGroup as well, and dependencies will be; // added so that RIter will always be scheduled at the end of the group.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:275,Integrability,depend,dependencies,275,"// Add instructions to the SchedGroup bottom up starting from RIter.; // PipelineInstrs is a set of instructions that should not be added to the; // SchedGroup even when the other conditions for adding it are satisfied.; // RIter will be added to the SchedGroup as well, and dependencies will be; // added so that RIter will always be scheduled at the end of the group.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:7,Deployability,Pipeline,PipelineSolver,7,"// The PipelineSolver is used to assign SUnits to SchedGroups in a pipeline; // in non-trivial cases. For example, if the requested pipeline is; // {VMEM_READ, VALU, MFMA, VMEM_READ} and we encounter a VMEM_READ instruction; // in the DAG, then we will have an instruction that can not be trivially; // assigned to a SchedGroup. The PipelineSolver class implements two algorithms; // to find a good solution to the pipeline -- a greedy algorithm and an exact; // algorithm. The exact algorithm has an exponential time complexity and should; // only be used for small sized problems or medium sized problems where an exact; // solution is highly desired.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:67,Deployability,pipeline,pipeline,67,"// The PipelineSolver is used to assign SUnits to SchedGroups in a pipeline; // in non-trivial cases. For example, if the requested pipeline is; // {VMEM_READ, VALU, MFMA, VMEM_READ} and we encounter a VMEM_READ instruction; // in the DAG, then we will have an instruction that can not be trivially; // assigned to a SchedGroup. The PipelineSolver class implements two algorithms; // to find a good solution to the pipeline -- a greedy algorithm and an exact; // algorithm. The exact algorithm has an exponential time complexity and should; // only be used for small sized problems or medium sized problems where an exact; // solution is highly desired.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:132,Deployability,pipeline,pipeline,132,"// The PipelineSolver is used to assign SUnits to SchedGroups in a pipeline; // in non-trivial cases. For example, if the requested pipeline is; // {VMEM_READ, VALU, MFMA, VMEM_READ} and we encounter a VMEM_READ instruction; // in the DAG, then we will have an instruction that can not be trivially; // assigned to a SchedGroup. The PipelineSolver class implements two algorithms; // to find a good solution to the pipeline -- a greedy algorithm and an exact; // algorithm. The exact algorithm has an exponential time complexity and should; // only be used for small sized problems or medium sized problems where an exact; // solution is highly desired.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:333,Deployability,Pipeline,PipelineSolver,333,"// The PipelineSolver is used to assign SUnits to SchedGroups in a pipeline; // in non-trivial cases. For example, if the requested pipeline is; // {VMEM_READ, VALU, MFMA, VMEM_READ} and we encounter a VMEM_READ instruction; // in the DAG, then we will have an instruction that can not be trivially; // assigned to a SchedGroup. The PipelineSolver class implements two algorithms; // to find a good solution to the pipeline -- a greedy algorithm and an exact; // algorithm. The exact algorithm has an exponential time complexity and should; // only be used for small sized problems or medium sized problems where an exact; // solution is highly desired.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:415,Deployability,pipeline,pipeline,415,"// The PipelineSolver is used to assign SUnits to SchedGroups in a pipeline; // in non-trivial cases. For example, if the requested pipeline is; // {VMEM_READ, VALU, MFMA, VMEM_READ} and we encounter a VMEM_READ instruction; // in the DAG, then we will have an instruction that can not be trivially; // assigned to a SchedGroup. The PipelineSolver class implements two algorithms; // to find a good solution to the pipeline -- a greedy algorithm and an exact; // algorithm. The exact algorithm has an exponential time complexity and should; // only be used for small sized problems or medium sized problems where an exact; // solution is highly desired.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:23,Deployability,pipeline,pipeline,23,// The current working pipeline,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:7,Deployability,pipeline,pipeline,7,// The pipeline that has the best solution found so far,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:16,Deployability,pipeline,pipeline,16,// Index to the pipeline that is currently being fitted,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:25,Deployability,pipeline,pipeline,25,// The first non trivial pipeline,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:3,Deployability,Update,Update,3,// Update indices to fit next conflicting instruction,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:42,Deployability,pipeline,pipeline,42,// Link the SchedGroups in the best found pipeline.; // Tmplated against the SchedGroup iterator (either reverse or forward).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:57,Deployability,pipeline,pipeline,57,"// Add the edges from the SU to the other SchedGroups in pipeline, and; // return the number of edges missed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:13,Deployability,pipeline,pipeline,13,"/// Link the pipeline as if \p SU was in the SchedGroup with ID \p SGID. It; /// returns the cost (in terms of missed pipeline edges), and tracks the edges; /// added in \p AddedEdges",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:118,Deployability,pipeline,pipeline,118,"/// Link the pipeline as if \p SU was in the SchedGroup with ID \p SGID. It; /// returns the cost (in terms of missed pipeline edges), and tracks the edges; /// added in \p AddedEdges",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:171,Deployability,configurat,configuration,171,"// For IsBottomUp, the first SchedGroup in SyncPipeline contains the; // instructions that are the ultimate successors in the resultant mutation.; // Therefore, in such a configuration, the SchedGroups occurring before the; // candidate SGID are successors of the candidate SchedGroup, thus the current; // SU should be linked as a predecessor to SUs in those SchedGroups. The; // opposite is true if !IsBottomUp. IsBottomUp occurs in the case of multiple; // SCHED_GROUP_BARRIERS, or if a user specifies IGLP_OPT SchedGroups using; // IsBottomUp (in reverse).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:171,Modifiability,config,configuration,171,"// For IsBottomUp, the first SchedGroup in SyncPipeline contains the; // instructions that are the ultimate successors in the resultant mutation.; // Therefore, in such a configuration, the SchedGroups occurring before the; // candidate SGID are successors of the candidate SchedGroup, thus the current; // SU should be linked as a predecessor to SUs in those SchedGroups. The; // opposite is true if !IsBottomUp. IsBottomUp occurs in the case of multiple; // SCHED_GROUP_BARRIERS, or if a user specifies IGLP_OPT SchedGroups using; // IsBottomUp (in reverse).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:49,Testability,test,testing,49,// Only remove the edges that we have added when testing; // the fit.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:31,Deployability,pipeline,pipeline,31,// Advance to next non-trivial pipeline,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:30,Deployability,pipeline,pipeline,30,// Go to previous non-trivial pipeline,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:11,Deployability,pipeline,pipeline,11,"// Try the pipeline where the current instruction is omitted; // Potentially if we omit a problematic instruction from the pipeline,; // all the other instructions can nicely fit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:123,Deployability,pipeline,pipeline,123,"// Try the pipeline where the current instruction is omitted; // Potentially if we omit a problematic instruction from the pipeline,; // all the other instructions can nicely fit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:100,Availability,down,down,100,"// Since we have added the potential SchedGroups from bottom up, but; // traversed the DAG from top down, parse over the groups from last to; // first. If we fail to do this for the greedy algorithm, the solution will; // likely not be good in more complex cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:20,Energy Efficiency,schedul,scheduling,20,// Implement a IGLP scheduling strategy.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:56,Energy Efficiency,Schedul,ScheduleDAG,56,// Returns true if this strategy should be applied to a ScheduleDAG.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:24,Performance,load,load,24,// Whether the combined load width of group is 128 bits,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:83,Integrability,depend,dependencies,83,"// Get the count of DS_WRITES with V_PERM predecessors which; // have loop carried dependencies (WAR) on the same VMEM_READs.; // We consider partial overlap as a miss -- in other words,; // for a given DS_W, we only consider another DS_W as matching; // if there is a corresponding (in terms of the VMEM_R it uses) V_PERM pred; // for every V_PERM pred of this DS_W.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:53,Energy Efficiency,Schedul,Schedule,53,// Phase 2a: Loop carried dependency with V_PERM; // Schedule VPerm & DS_WRITE as closely as possible to the VMEM_READ they; // depend on. Interleave MFMA to keep XDL unit busy throughout.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:26,Integrability,depend,dependency,26,// Phase 2a: Loop carried dependency with V_PERM; // Schedule VPerm & DS_WRITE as closely as possible to the VMEM_READ they; // depend on. Interleave MFMA to keep XDL unit busy throughout.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:128,Integrability,depend,depend,128,// Phase 2a: Loop carried dependency with V_PERM; // Schedule VPerm & DS_WRITE as closely as possible to the VMEM_READ they; // depend on. Interleave MFMA to keep XDL unit busy throughout.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:56,Energy Efficiency,Schedul,Schedule,56,// Phase 2b: Loop carried dependency without V_PERM; // Schedule DS_WRITE as closely as possible to the VMEM_READ they depend on.; // Interleave MFMA to keep XDL unit busy throughout.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:26,Integrability,depend,dependency,26,// Phase 2b: Loop carried dependency without V_PERM; // Schedule DS_WRITE as closely as possible to the VMEM_READ they depend on.; // Interleave MFMA to keep XDL unit busy throughout.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:119,Integrability,depend,depend,119,// Phase 2b: Loop carried dependency without V_PERM; // Schedule DS_WRITE as closely as possible to the VMEM_READ they depend on.; // Interleave MFMA to keep XDL unit busy throughout.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:105,Energy Efficiency,Schedul,Schedule,105,"// Phase 2c: Loop carried dependency with V_PERM, VMEM_READs are; // ultimately used by two DS_WRITE; // Schedule VPerm & DS_WRITE as closely as possible to the VMEM_READ they; // depend on. Interleave MFMA to keep XDL unit busy throughout.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:26,Integrability,depend,dependency,26,"// Phase 2c: Loop carried dependency with V_PERM, VMEM_READs are; // ultimately used by two DS_WRITE; // Schedule VPerm & DS_WRITE as closely as possible to the VMEM_READ they; // depend on. Interleave MFMA to keep XDL unit busy throughout.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:180,Integrability,depend,depend,180,"// Phase 2c: Loop carried dependency with V_PERM, VMEM_READs are; // ultimately used by two DS_WRITE; // Schedule VPerm & DS_WRITE as closely as possible to the VMEM_READ they; // depend on. Interleave MFMA to keep XDL unit busy throughout.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:25,Availability,mask,mask,25,"// Use a SCHED_BARRIER's mask to identify instruction SchedGroups that should; // not be reordered accross the SCHED_BARRIER. This is used for the base; // SCHED_BARRIER, and not SCHED_GROUP_BARRIER. The difference is that; // SCHED_BARRIER will always block all instructions that can be classified; // into a particular SchedClass, whereas SCHED_GROUP_BARRIER has a fixed size; // and may only synchronize with some SchedGroups. Returns the inverse of; // Mask. SCHED_BARRIER's mask describes which instruction types should be; // allowed to be scheduled across it. Invert the mask to get the; // SchedGroupMask of instructions that should be barred.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:457,Availability,Mask,Mask,457,"// Use a SCHED_BARRIER's mask to identify instruction SchedGroups that should; // not be reordered accross the SCHED_BARRIER. This is used for the base; // SCHED_BARRIER, and not SCHED_GROUP_BARRIER. The difference is that; // SCHED_BARRIER will always block all instructions that can be classified; // into a particular SchedClass, whereas SCHED_GROUP_BARRIER has a fixed size; // and may only synchronize with some SchedGroups. Returns the inverse of; // Mask. SCHED_BARRIER's mask describes which instruction types should be; // allowed to be scheduled across it. Invert the mask to get the; // SchedGroupMask of instructions that should be barred.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:479,Availability,mask,mask,479,"// Use a SCHED_BARRIER's mask to identify instruction SchedGroups that should; // not be reordered accross the SCHED_BARRIER. This is used for the base; // SCHED_BARRIER, and not SCHED_GROUP_BARRIER. The difference is that; // SCHED_BARRIER will always block all instructions that can be classified; // into a particular SchedClass, whereas SCHED_GROUP_BARRIER has a fixed size; // and may only synchronize with some SchedGroups. Returns the inverse of; // Mask. SCHED_BARRIER's mask describes which instruction types should be; // allowed to be scheduled across it. Invert the mask to get the; // SchedGroupMask of instructions that should be barred.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:578,Availability,mask,mask,578,"// Use a SCHED_BARRIER's mask to identify instruction SchedGroups that should; // not be reordered accross the SCHED_BARRIER. This is used for the base; // SCHED_BARRIER, and not SCHED_GROUP_BARRIER. The difference is that; // SCHED_BARRIER will always block all instructions that can be classified; // into a particular SchedClass, whereas SCHED_GROUP_BARRIER has a fixed size; // and may only synchronize with some SchedGroups. Returns the inverse of; // Mask. SCHED_BARRIER's mask describes which instruction types should be; // allowed to be scheduled across it. Invert the mask to get the; // SchedGroupMask of instructions that should be barred.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:546,Energy Efficiency,schedul,scheduled,546,"// Use a SCHED_BARRIER's mask to identify instruction SchedGroups that should; // not be reordered accross the SCHED_BARRIER. This is used for the base; // SCHED_BARRIER, and not SCHED_GROUP_BARRIER. The difference is that; // SCHED_BARRIER will always block all instructions that can be classified; // into a particular SchedClass, whereas SCHED_GROUP_BARRIER has a fixed size; // and may only synchronize with some SchedGroups. Returns the inverse of; // Mask. SCHED_BARRIER's mask describes which instruction types should be; // allowed to be scheduled across it. Invert the mask to get the; // SchedGroupMask of instructions that should be barred.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:395,Integrability,synchroniz,synchronize,395,"// Use a SCHED_BARRIER's mask to identify instruction SchedGroups that should; // not be reordered accross the SCHED_BARRIER. This is used for the base; // SCHED_BARRIER, and not SCHED_GROUP_BARRIER. The difference is that; // SCHED_BARRIER will always block all instructions that can be classified; // into a particular SchedClass, whereas SCHED_GROUP_BARRIER has a fixed size; // and may only synchronize with some SchedGroups. Returns the inverse of; // Mask. SCHED_BARRIER's mask describes which instruction types should be; // allowed to be scheduled across it. Invert the mask to get the; // SchedGroupMask of instructions that should be barred.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:26,Deployability,Pipeline,PipelineSolver,26,"// The order in which the PipelineSolver should process the candidate; // SchedGroup for a PipelineInstr. BOTTOM_UP will try to add SUs to the last; // created SchedGroup first, and will consider that as the ultimate; // predecessor group when linking. TOP_DOWN instead links and processes the; // first created SchedGroup first.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:91,Deployability,Pipeline,PipelineInstr,91,"// The order in which the PipelineSolver should process the candidate; // SchedGroup for a PipelineInstr. BOTTOM_UP will try to add SUs to the last; // created SchedGroup first, and will consider that as the ultimate; // predecessor group when linking. TOP_DOWN instead links and processes the; // first created SchedGroup first.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:42,Integrability,depend,dependency,42,"// tryAddEdge returns false if there is a dependency that makes adding; // the A->B edge impossible, otherwise it returns true;",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:3,Deployability,Pipeline,PipelineSolver,3,// PipelineSolver performs the mutation by adding the edges it; // determined as the best,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:18,Performance,perform,performs,18,// PipelineSolver performs the mutation by adding the edges it; // determined as the best,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:10,Availability,mask,mask,10,// Invert mask and erase bits for types of instructions that are implied to be; // allowed past the SCHED_BARRIER.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:122,Energy Efficiency,schedul,scheduling,122,"/// \p IsReentry specifes whether or not this is a reentry into the; /// IGroupLPDAGMutation. Since there may be multiple scheduling passes on the; /// same scheduling region (e.g. pre and post-RA scheduling / multiple; /// scheduling ""phases""), we can reenter this mutation framework more than once; /// for a given region.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:157,Energy Efficiency,schedul,scheduling,157,"/// \p IsReentry specifes whether or not this is a reentry into the; /// IGroupLPDAGMutation. Since there may be multiple scheduling passes on the; /// same scheduling region (e.g. pre and post-RA scheduling / multiple; /// scheduling ""phases""), we can reenter this mutation framework more than once; /// for a given region.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:197,Energy Efficiency,schedul,scheduling,197,"/// \p IsReentry specifes whether or not this is a reentry into the; /// IGroupLPDAGMutation. Since there may be multiple scheduling passes on the; /// same scheduling region (e.g. pre and post-RA scheduling / multiple; /// scheduling ""phases""), we can reenter this mutation framework more than once; /// for a given region.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp:224,Energy Efficiency,schedul,scheduling,224,"/// \p IsReentry specifes whether or not this is a reentry into the; /// IGroupLPDAGMutation. Since there may be multiple scheduling passes on the; /// same scheduling region (e.g. pre and post-RA scheduling / multiple; /// scheduling ""phases""), we can reenter this mutation framework more than once; /// for a given region.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUIGroupLP.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp:769,Energy Efficiency,reduce,reduced,769,"//===- AMDGPUImageIntrinsicOptimizer.cpp ----------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass tries to combine multiple image_load intrinsics with dim=2dmsaa; // or dim=2darraymsaa into a single image_msaa_load intrinsic if:; //; // - they refer to the same vaddr except for sample_id,; // - they use a constant sample_id and they fall into the same group,; // - they have the same dmask and the number of intrinsics and the number of; // vaddr/vdata dword transfers is reduced by the combine.; //; // Examples for the tradeoff (all are assuming 2DMsaa for vaddr):; //; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | popcount | a16 | d16 | #load | vaddr / | #msaa_load | vaddr / | combine? |; // | (dmask) | | | | vdata | | vdata | |; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | 1 | 0 | 0 | 4 | 12 / 4 | 1 | 3 / 4 | yes |; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | 1 | 0 | 0 | 2 | 6 / 2 | 1 | 3 / 4 | yes? |; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | 2 | 0 | 0 | 4 | 12 / 8 | 2 | 6 / 8 | yes |; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | 2 | 0 | 0 | 2 | 6 / 4 | 2 | 6 / 8 | no |; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | 1 | 0 | 1 | 2 | 6 / 2 | 1 | 3 / 2 | yes |; // +----------+-----+-----+-------+---------+------------+---------+----------+; //; // Some cases are of questionable benefit, like the one marked with ""yes?""; // above: fewer intrinsics and fewer vaddr and fewer total transfers between SP; // and TX, but higher vdata. We start b",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp:979,Performance,load,load,979,"s tries to combine multiple image_load intrinsics with dim=2dmsaa; // or dim=2darraymsaa into a single image_msaa_load intrinsic if:; //; // - they refer to the same vaddr except for sample_id,; // - they use a constant sample_id and they fall into the same group,; // - they have the same dmask and the number of intrinsics and the number of; // vaddr/vdata dword transfers is reduced by the combine.; //; // Examples for the tradeoff (all are assuming 2DMsaa for vaddr):; //; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | popcount | a16 | d16 | #load | vaddr / | #msaa_load | vaddr / | combine? |; // | (dmask) | | | | vdata | | vdata | |; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | 1 | 0 | 0 | 4 | 12 / 4 | 1 | 3 / 4 | yes |; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | 1 | 0 | 0 | 2 | 6 / 2 | 1 | 3 / 4 | yes? |; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | 2 | 0 | 0 | 4 | 12 / 8 | 2 | 6 / 8 | yes |; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | 2 | 0 | 0 | 2 | 6 / 4 | 2 | 6 / 8 | no |; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | 1 | 0 | 1 | 2 | 6 / 2 | 1 | 3 / 2 | yes |; // +----------+-----+-----+-------+---------+------------+---------+----------+; //; // Some cases are of questionable benefit, like the one marked with ""yes?""; // above: fewer intrinsics and fewer vaddr and fewer total transfers between SP; // and TX, but higher vdata. We start by erring on the side of converting these; // to MSAA_LOAD.; //; // clang-format off; //; // This pass will combine intrinsics such as (not neccessarily consecutive):; // call float @llvm.amdgcn.image.load.2dmsaa.f32.i32(i32 1, i32 %s, i32 %t, i32 0, <8 x i32> %rsrc, i32 0, i32 0); // call float @llvm.amdgcn.image.load.2dmsaa.f32.i32(i32 1, i32 %s, i32 %t, i32 1, <8 x i32> %rsrc, i32 0, i",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp:2201,Performance,load,load,2201,"// +----------+-----+-----+-------+---------+------------+---------+----------+; // | 1 | 0 | 0 | 2 | 6 / 2 | 1 | 3 / 4 | yes? |; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | 2 | 0 | 0 | 4 | 12 / 8 | 2 | 6 / 8 | yes |; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | 2 | 0 | 0 | 2 | 6 / 4 | 2 | 6 / 8 | no |; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | 1 | 0 | 1 | 2 | 6 / 2 | 1 | 3 / 2 | yes |; // +----------+-----+-----+-------+---------+------------+---------+----------+; //; // Some cases are of questionable benefit, like the one marked with ""yes?""; // above: fewer intrinsics and fewer vaddr and fewer total transfers between SP; // and TX, but higher vdata. We start by erring on the side of converting these; // to MSAA_LOAD.; //; // clang-format off; //; // This pass will combine intrinsics such as (not neccessarily consecutive):; // call float @llvm.amdgcn.image.load.2dmsaa.f32.i32(i32 1, i32 %s, i32 %t, i32 0, <8 x i32> %rsrc, i32 0, i32 0); // call float @llvm.amdgcn.image.load.2dmsaa.f32.i32(i32 1, i32 %s, i32 %t, i32 1, <8 x i32> %rsrc, i32 0, i32 0); // call float @llvm.amdgcn.image.load.2dmsaa.f32.i32(i32 1, i32 %s, i32 %t, i32 2, <8 x i32> %rsrc, i32 0, i32 0); // call float @llvm.amdgcn.image.load.2dmsaa.f32.i32(i32 1, i32 %s, i32 %t, i32 3, <8 x i32> %rsrc, i32 0, i32 0); // ==>; // call <4 x float> @llvm.amdgcn.image.msaa.load.2dmsaa.v4f32.i32(i32 1, i32 %s, i32 %t, i32 0, <8 x i32> %rsrc, i32 0, i32 0); //; // clang-format on; //; // Future improvements:; //; // - We may occasionally not want to do the combine if it increases the maximum; // register pressure.; //; // - Ensure clausing when multiple MSAA_LOAD are generated.; //; // Note: Even though the image_msaa_load intrinsic already exists on gfx10, this; // combine only applies to gfx11, due to a limitation in gfx10: the gfx10; // IMAGE_MSAA_LOAD only works correctly with single-c",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp:2316,Performance,load,load,2316," / 4 | yes? |; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | 2 | 0 | 0 | 4 | 12 / 8 | 2 | 6 / 8 | yes |; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | 2 | 0 | 0 | 2 | 6 / 4 | 2 | 6 / 8 | no |; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | 1 | 0 | 1 | 2 | 6 / 2 | 1 | 3 / 2 | yes |; // +----------+-----+-----+-------+---------+------------+---------+----------+; //; // Some cases are of questionable benefit, like the one marked with ""yes?""; // above: fewer intrinsics and fewer vaddr and fewer total transfers between SP; // and TX, but higher vdata. We start by erring on the side of converting these; // to MSAA_LOAD.; //; // clang-format off; //; // This pass will combine intrinsics such as (not neccessarily consecutive):; // call float @llvm.amdgcn.image.load.2dmsaa.f32.i32(i32 1, i32 %s, i32 %t, i32 0, <8 x i32> %rsrc, i32 0, i32 0); // call float @llvm.amdgcn.image.load.2dmsaa.f32.i32(i32 1, i32 %s, i32 %t, i32 1, <8 x i32> %rsrc, i32 0, i32 0); // call float @llvm.amdgcn.image.load.2dmsaa.f32.i32(i32 1, i32 %s, i32 %t, i32 2, <8 x i32> %rsrc, i32 0, i32 0); // call float @llvm.amdgcn.image.load.2dmsaa.f32.i32(i32 1, i32 %s, i32 %t, i32 3, <8 x i32> %rsrc, i32 0, i32 0); // ==>; // call <4 x float> @llvm.amdgcn.image.msaa.load.2dmsaa.v4f32.i32(i32 1, i32 %s, i32 %t, i32 0, <8 x i32> %rsrc, i32 0, i32 0); //; // clang-format on; //; // Future improvements:; //; // - We may occasionally not want to do the combine if it increases the maximum; // register pressure.; //; // - Ensure clausing when multiple MSAA_LOAD are generated.; //; // Note: Even though the image_msaa_load intrinsic already exists on gfx10, this; // combine only applies to gfx11, due to a limitation in gfx10: the gfx10; // IMAGE_MSAA_LOAD only works correctly with single-channel texture formats, and; // we don't know the format at compile time.; //===-----------------------------------",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp:2431,Performance,load,load,2431,"+-------+---------+------------+---------+----------+; // | 2 | 0 | 0 | 4 | 12 / 8 | 2 | 6 / 8 | yes |; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | 2 | 0 | 0 | 2 | 6 / 4 | 2 | 6 / 8 | no |; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | 1 | 0 | 1 | 2 | 6 / 2 | 1 | 3 / 2 | yes |; // +----------+-----+-----+-------+---------+------------+---------+----------+; //; // Some cases are of questionable benefit, like the one marked with ""yes?""; // above: fewer intrinsics and fewer vaddr and fewer total transfers between SP; // and TX, but higher vdata. We start by erring on the side of converting these; // to MSAA_LOAD.; //; // clang-format off; //; // This pass will combine intrinsics such as (not neccessarily consecutive):; // call float @llvm.amdgcn.image.load.2dmsaa.f32.i32(i32 1, i32 %s, i32 %t, i32 0, <8 x i32> %rsrc, i32 0, i32 0); // call float @llvm.amdgcn.image.load.2dmsaa.f32.i32(i32 1, i32 %s, i32 %t, i32 1, <8 x i32> %rsrc, i32 0, i32 0); // call float @llvm.amdgcn.image.load.2dmsaa.f32.i32(i32 1, i32 %s, i32 %t, i32 2, <8 x i32> %rsrc, i32 0, i32 0); // call float @llvm.amdgcn.image.load.2dmsaa.f32.i32(i32 1, i32 %s, i32 %t, i32 3, <8 x i32> %rsrc, i32 0, i32 0); // ==>; // call <4 x float> @llvm.amdgcn.image.msaa.load.2dmsaa.v4f32.i32(i32 1, i32 %s, i32 %t, i32 0, <8 x i32> %rsrc, i32 0, i32 0); //; // clang-format on; //; // Future improvements:; //; // - We may occasionally not want to do the combine if it increases the maximum; // register pressure.; //; // - Ensure clausing when multiple MSAA_LOAD are generated.; //; // Note: Even though the image_msaa_load intrinsic already exists on gfx10, this; // combine only applies to gfx11, due to a limitation in gfx10: the gfx10; // IMAGE_MSAA_LOAD only works correctly with single-channel texture formats, and; // we don't know the format at compile time.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp:2546,Performance,load,load,2546,"+-------+---------+------------+---------+----------+; // | 2 | 0 | 0 | 4 | 12 / 8 | 2 | 6 / 8 | yes |; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | 2 | 0 | 0 | 2 | 6 / 4 | 2 | 6 / 8 | no |; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | 1 | 0 | 1 | 2 | 6 / 2 | 1 | 3 / 2 | yes |; // +----------+-----+-----+-------+---------+------------+---------+----------+; //; // Some cases are of questionable benefit, like the one marked with ""yes?""; // above: fewer intrinsics and fewer vaddr and fewer total transfers between SP; // and TX, but higher vdata. We start by erring on the side of converting these; // to MSAA_LOAD.; //; // clang-format off; //; // This pass will combine intrinsics such as (not neccessarily consecutive):; // call float @llvm.amdgcn.image.load.2dmsaa.f32.i32(i32 1, i32 %s, i32 %t, i32 0, <8 x i32> %rsrc, i32 0, i32 0); // call float @llvm.amdgcn.image.load.2dmsaa.f32.i32(i32 1, i32 %s, i32 %t, i32 1, <8 x i32> %rsrc, i32 0, i32 0); // call float @llvm.amdgcn.image.load.2dmsaa.f32.i32(i32 1, i32 %s, i32 %t, i32 2, <8 x i32> %rsrc, i32 0, i32 0); // call float @llvm.amdgcn.image.load.2dmsaa.f32.i32(i32 1, i32 %s, i32 %t, i32 3, <8 x i32> %rsrc, i32 0, i32 0); // ==>; // call <4 x float> @llvm.amdgcn.image.msaa.load.2dmsaa.v4f32.i32(i32 1, i32 %s, i32 %t, i32 0, <8 x i32> %rsrc, i32 0, i32 0); //; // clang-format on; //; // Future improvements:; //; // - We may occasionally not want to do the combine if it increases the maximum; // register pressure.; //; // - Ensure clausing when multiple MSAA_LOAD are generated.; //; // Note: Even though the image_msaa_load intrinsic already exists on gfx10, this; // combine only applies to gfx11, due to a limitation in gfx10: the gfx10; // IMAGE_MSAA_LOAD only works correctly with single-channel texture formats, and; // we don't know the format at compile time.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp:2680,Performance,load,load,2680,"+-------+---------+------------+---------+----------+; // | 2 | 0 | 0 | 4 | 12 / 8 | 2 | 6 / 8 | yes |; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | 2 | 0 | 0 | 2 | 6 / 4 | 2 | 6 / 8 | no |; // +----------+-----+-----+-------+---------+------------+---------+----------+; // | 1 | 0 | 1 | 2 | 6 / 2 | 1 | 3 / 2 | yes |; // +----------+-----+-----+-------+---------+------------+---------+----------+; //; // Some cases are of questionable benefit, like the one marked with ""yes?""; // above: fewer intrinsics and fewer vaddr and fewer total transfers between SP; // and TX, but higher vdata. We start by erring on the side of converting these; // to MSAA_LOAD.; //; // clang-format off; //; // This pass will combine intrinsics such as (not neccessarily consecutive):; // call float @llvm.amdgcn.image.load.2dmsaa.f32.i32(i32 1, i32 %s, i32 %t, i32 0, <8 x i32> %rsrc, i32 0, i32 0); // call float @llvm.amdgcn.image.load.2dmsaa.f32.i32(i32 1, i32 %s, i32 %t, i32 1, <8 x i32> %rsrc, i32 0, i32 0); // call float @llvm.amdgcn.image.load.2dmsaa.f32.i32(i32 1, i32 %s, i32 %t, i32 2, <8 x i32> %rsrc, i32 0, i32 0); // call float @llvm.amdgcn.image.load.2dmsaa.f32.i32(i32 1, i32 %s, i32 %t, i32 3, <8 x i32> %rsrc, i32 0, i32 0); // ==>; // call <4 x float> @llvm.amdgcn.image.msaa.load.2dmsaa.v4f32.i32(i32 1, i32 %s, i32 %t, i32 0, <8 x i32> %rsrc, i32 0, i32 0); //; // clang-format on; //; // Future improvements:; //; // - We may occasionally not want to do the combine if it increases the maximum; // register pressure.; //; // - Ensure clausing when multiple MSAA_LOAD are generated.; //; // Note: Even though the image_msaa_load intrinsic already exists on gfx10, this; // combine only applies to gfx11, due to a limitation in gfx10: the gfx10; // IMAGE_MSAA_LOAD only works correctly with single-channel texture formats, and; // we don't know the format at compile time.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp:3,Security,Validat,Validate,3,"// Validate function argument and return types, extracting overloaded; // types along the way.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp:86,Energy Efficiency,reduce,reduced,86,// Number of instructions and the number of vaddr/vdata dword transfers; // should be reduced.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp:8,Performance,optimiz,optimization,8,// This optimization only applies to GFX11 and beyond.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp:9,Testability,test,test,9,// Early test to determine if the intrinsics are used.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUImageIntrinsicOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp:430,Safety,avoid,avoid,430,"//===- AMDGPUInsertDelayAlu.cpp - Insert s_delay_alu instructions ---------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Insert s_delay_alu instructions to avoid stalls on GFX11+.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp:107,Integrability,depend,depends,107,"// Also remember how many other (non-TRANS) VALU we have seen since it was; // issued. When an instruction depends on both a prior TRANS and a prior; // non-TRANS VALU, this is used to decide whether to encode a wait for just; // one or both of them.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp:100,Performance,latency,latency,100,// Guard against pseudo-instructions like SI_CALL which are marked as; // SALU but with a very high latency.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp:3,Deployability,Update,Update,3,"// Update this DelayInfo after issuing an instruction. IsVALU should be 1; // when issuing a (non-TRANS) VALU, else 0. IsTRANS should be 1 when issuing; // a TRANS, else 0. Cycles is the number of cycles it takes to issue the; // instruction. Return true if there is no longer any useful delay info.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp:102,Availability,redundant,redundant,102,"// One of the operands of the writelane is also the output operand.; // This creates the insertion of redundant delays. Hence, we have to; // ignore this operand.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp:102,Safety,redund,redundant,102,"// One of the operands of the writelane is also the output operand.; // This creates the insertion of redundant delays. Hence, we have to; // ignore this operand.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp:193,Deployability,pipeline,pipeline,193,// Advance by the number of cycles it takes to issue this instruction.; // TODO: Use a more advanced model that accounts for instructions that; // take multiple cycles to issue on a particular pipeline.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertSingleUseVDST.cpp:82,Safety,avoid,avoid,82,// TODO: MCRegUnits; // Handle boundaries at the end of basic block separately to avoid; // false positives. If they are live at the end of a basic block then; // assume it has more uses later on.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertSingleUseVDST.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertSingleUseVDST.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertSingleUseVDST.cpp:42,Availability,mask,mask,42,// Do not attempt to optimise across exec mask changes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertSingleUseVDST.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertSingleUseVDST.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertSingleUseVDST.cpp:32,Testability,log,logging,32,// TODO: Replace with candidate logging for instruction grouping; // later.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertSingleUseVDST.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertSingleUseVDST.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:46,Availability,down,down,46,"// We need to check that if we cast the index down to a half, we do not; // lose precision.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:46,Availability,down,down,46,"// We need to check that if we cast the index down to an i16, we do not; // lose precision.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:3,Performance,Optimiz,Optimize,3,// Optimize _L to _LZ when _L is zero,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:3,Performance,Optimiz,Optimize,3,"// Optimize _mip away, when 'lod' is zero",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:3,Performance,Optimiz,Optimize,3,// Optimize _bias away when 'bias' is zero,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:3,Performance,Optimiz,Optimize,3,// Optimize _offset away when 'offset' is zero,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:63,Performance,optimiz,optimize,63,"// If the values are not derived from 16-bit values, we cannot optimize.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:3,Safety,Detect,Detect,3,"// Detect identical elements in the shufflevector result, even though; // findScalarElement cannot tell us what that element is.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:33,Integrability,contract,contract,33,/// Return true if it's legal to contract llvm.amdgcn.rcp(llvm.sqrt),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:65,Integrability,contract,contractable,65,// llvm.amdgcn.rcp(llvm.amdgcn.sqrt(x)) -> llvm.amdgcn.rsq(x) if contractable; //; // llvm.amdgcn.rcp(llvm.sqrt(x)) -> llvm.amdgcn.rsq(x) if contractable and; // relaxed.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:141,Integrability,contract,contractable,141,// llvm.amdgcn.rcp(llvm.amdgcn.sqrt(x)) -> llvm.amdgcn.rsq(x) if contractable; //; // llvm.amdgcn.rcp(llvm.sqrt(x)) -> llvm.amdgcn.rsq(x) if contractable and; // relaxed.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:28,Availability,mask,mask,28,"// llvm.amdgcn.class(undef, mask) -> mask != 0",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:37,Availability,mask,mask,37,"// llvm.amdgcn.class(undef, mask) -> mask != 0",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:13,Usability,simpl,simple,13,// Decompose simple cases into standard shifts.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:80,Safety,safe,safe,80,"// The case of Width == 0 is handled above, which makes this transformation; // safe. If Width == 0, then the ashr and lshr instructions become poison; // value since the shift amount would be equal to the bit size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:111,Availability,mask,masked,111,"// The result of V_ICMP/V_FCMP assembly instructions (which this; // intrinsic exposes) is one bit per thread, masked with the EXEC; // register (which contains the bitmask of live threads). So a; // comparison that always returns true is the same as a read of the; // EXEC register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:79,Security,expose,exposes,79,"// The result of V_ICMP/V_FCMP assembly instructions (which this; // intrinsic exposes) is one bit per thread, masked with the EXEC; // register (which contains the bitmask of live threads). So a; // comparison that always returns true is the same as a read of the; // EXEC register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:228,Availability,redundant,redundant,228,"// Fold compare eq/ne with 0 from a compare result as the predicate to the; // intrinsic. The typical use is a wave vote function in the library, which; // will be fed from a user code condition compared with 0. Fold in the; // redundant compare.; // llvm.amdgcn.icmp([sz]ext ([if]cmp pred a, b), 0, ne); // -> llvm.amdgcn.[if]cmp(a, b, pred); //; // llvm.amdgcn.icmp([sz]ext ([if]cmp pred a, b), 0, eq); // -> llvm.amdgcn.[if]cmp(a, b, inv pred)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:228,Safety,redund,redundant,228,"// Fold compare eq/ne with 0 from a compare result as the predicate to the; // intrinsic. The typical use is a wave vote function in the library, which; // will be fed from a user code condition compared with 0. Fold in the; // redundant compare.; // llvm.amdgcn.icmp([sz]ext ([if]cmp pred a, b), 0, ne); // -> llvm.amdgcn.[if]cmp(a, b, pred); //; // llvm.amdgcn.icmp([sz]ext ([if]cmp pred a, b), 0, eq); // -> llvm.amdgcn.[if]cmp(a, b, inv pred)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:26,Availability,mask,mask,26,"// If bound_ctrl = 1, row mask = bank mask = 0xf we can omit old value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:38,Availability,mask,mask,38,"// If bound_ctrl = 1, row mask = bank mask = 0xf we can omit old value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:32,Safety,safe,safe,32,// The rest of these may not be safe if the exec may not be the same between; // the def and use.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:272,Performance,load,loads,272,"/// Implement SimplifyDemandedVectorElts for amdgcn buffer and image intrinsics.; ///; /// The result of simplifying amdgcn image and buffer store intrinsics is updating; /// definitions of the intrinsics vector argument, not Uses of the result like; /// image and buffer loads.; /// Note: This only supports non-TFE/LWE image intrinsic calls; those have; /// struct returns.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:14,Usability,Simpl,SimplifyDemandedVectorElts,14,"/// Implement SimplifyDemandedVectorElts for amdgcn buffer and image intrinsics.; ///; /// The result of simplifying amdgcn image and buffer store intrinsics is updating; /// definitions of the intrinsics vector argument, not Uses of the result like; /// image and buffer loads.; /// Note: This only supports non-TFE/LWE image intrinsic calls; those have; /// struct returns.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:105,Usability,simpl,simplifying,105,"/// Implement SimplifyDemandedVectorElts for amdgcn buffer and image intrinsics.; ///; /// The result of simplifying amdgcn image and buffer store intrinsics is updating; /// definitions of the intrinsics vector argument, not Uses of the result like; /// image and buffer loads.; /// Note: This only supports non-TFE/LWE image intrinsic calls; those have; /// struct returns.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:158,Deployability,update,update,158,"// Start assuming the prefix of elements is demanded, but possibly clear; // some other bits if there are trailing zeros (unused components at front); // and update offset.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:67,Usability,clear,clear,67,"// Start assuming the prefix of elements is demanded, but possibly clear; // some other bits if there are trailing zeros (unused components at front); // and update offset.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:78,Deployability,update,updated,78,"// If resulting type is vec3, there is no point in trimming the; // load with updated offset, as the vec3 would most likely be widened to; // vec4 anyway during lowering.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:68,Performance,load,load,68,"// If resulting type is vec3, there is no point in trimming the; // load with updated offset, as the vec3 would most likely be widened to; // vec4 anyway during lowering.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:27,Deployability,update,update,27,// Clear demanded bits and update the offset.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:3,Usability,Clear,Clear,3,// Clear demanded bits and update the offset.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:41,Usability,simpl,simplify,41,"// dmask 0 has special semantics, do not simplify.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:3,Availability,Mask,Mask,3,// Mask off values that are undefined because the dmask doesn't cover them,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:3,Security,Validat,Validate,3,"// Validate function argument and return types, extracting overloaded types; // along the way.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstrInfo.cpp:30,Performance,load,load,30,"// UndefValue means this is a load of a kernel input. These are uniform.; // Sometimes LDS instructions have constant pointers.; // If Ptr is null, then that means this mem operand contains a; // PseudoSourceValue like GOT.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:77,Availability,mask,masking,77,"// We can't trust the high bits at this point, so clear them.; // TODO: Skip masking high bits if def is known boolean.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:50,Usability,clear,clear,50,"// We can't trust the high bits at this point, so clear them.; // TODO: Skip masking high bits if def is known boolean.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:13,Testability,log,logic,13,"// Selection logic below is for V2S16 only.; // For G_BUILD_VECTOR_TRUNC, additionally check that the operands are s32.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:159,Usability,simpl,simple,159,"// First, before trying TableGen patterns, check if both sources are; // constants. In those cases, we can trivially compute the final constant; // and emit a simple move.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:10,Integrability,Interface,Interface,10,// FIXME: Interface for getConstrainedRegClassForOperand needs work. The; // regbank check here is to know why getConstrainedRegClassForOperand failed.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:94,Safety,hazard,hazard,94,"// If the lane selector was originally in a VGPR and copied with; // readfirstlane, there's a hazard to read the same SGPR from the; // VALU. Constrain to a different SGPR to help avoid needing a nop later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:180,Safety,avoid,avoid,180,"// If the lane selector was originally in a VGPR and copied with; // readfirstlane, there's a hazard to read the same SGPR from the; // VALU. Constrain to a different SGPR to help avoid needing a nop later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:32,Safety,avoid,avoid,32,// FIXME: Manually selecting to avoid dealing with the SReg_1 trick; // SelectionDAG uses for wave32 vs wave64.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:32,Safety,avoid,avoid,32,// FIXME: Manually selecting to avoid dealing with the SReg_1 trick; // SelectionDAG uses for wave32 vs wave64.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:77,Modifiability,variab,variable,77,"// We have the constant offset now, so put the readfirstlane back on the; // variable component.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:131,Usability,guid,guide,131,"// The resource id offset is computed as (<isa opaque base> + M0[21:16] +; // offset field) % 64. Some versions of the programming guide omit the m0; // part, or claim it's from offset 0.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:18,Performance,optimiz,optimization,18,// TODO no-return optimization,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:12,Performance,load,load,12,// An image load instruction with TFE/LWE only conditionally writes to its; // result registers. Initialize them to zero so that we always get well; // defined result values.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:82,Availability,mask,mask,82,// 64-bit should have been split up in RegBankSelect; // Try to use an and with a mask if it will save code size.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:10,Deployability,toggle,toggle,10,// Set or toggle sign bit.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:3,Usability,Clear,Clear,3,// Clear sign bit.; // TODO: Should this used S_BITSET0_*?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:38,Modifiability,variab,variable,38,"// TODO: Could handle constant base + variable offset, but a combine; // probably should have commuted it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:30,Performance,load,load,30,"// UndefValue means this is a load of a kernel input. These are uniform.; // Sometimes LDS instructions have constant pointers.; // If Ptr is null, then that means this mem operand contains a; // PseudoSourceValue like GOT.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:10,Safety,avoid,avoid,10,// Try to avoid emitting a bit operation when we only need to touch half of; // the 64-bit pointer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:15,Availability,mask,mask,15,// Extract the mask subregister and apply the and.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:103,Security,access,accessed,103,"/// Return the register to use for the index value, and the subregister to use; /// for the indirectly accessed register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:17,Modifiability,extend,extend,17,/// Match a zero extend from a 32-bit value to 64-bits.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:55,Integrability,depend,depending,55,"// Fold fsub [+-]0 into fneg. This may not have folded depending on the; // denormal mode, but we're implicitly canonicalizing in a source operand.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:120,Safety,avoid,avoid,120,"// If we looked through copies to find source modifiers on an SGPR operand,; // we now have an SGPR register source. To avoid potentially violating the; // constant bus restriction, we need to insert a copy to a VGPR.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:118,Modifiability,extend,extended,118,"// Literal i1 value set in intrinsic, represents SrcMods for the next operand.; // Value is in Imm operand as i1 sign extended to int64_t.; // 1(-1) promotes packed values to signed, 0 treats them as unsigned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:36,Performance,load,load,36,"// If we make it this far we have a load with an 32-bit immediate offset.; // It is OK to select this using a sgpr offset, because we have already; // failed trying to select this load into one of the _IMM variants since; // the _IMM Patterns are considered before the _SGPR patterns.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:180,Performance,load,load,180,"// If we make it this far we have a load with an 32-bit immediate offset.; // It is OK to select this using a sgpr offset, because we have already; // failed trying to select this load into one of the _IMM variants since; // the _IMM Patterns are considered before the _SGPR patterns.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:95,Performance,perform,perform,95,// We are adding a 64 bit SGPR and a constant. If constant bus limit; // is 1 we would need to perform 1 or 2 extra moves for each half of; // the constant and it is better to do a scalar add and then issue a; // single VALU instruction to materialize zero. Otherwise it is less; // instructions to perform VALU adds with immediates or inline literals.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:299,Performance,perform,perform,299,// We are adding a 64 bit SGPR and a constant. If constant bus limit; // is 1 we would need to perform 1 or 2 extra moves for each half of; // the constant and it is better to do a scalar add and then issue a; // single VALU instruction to materialize zero. Otherwise it is less; // instructions to perform VALU adds with immediates or inline literals.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:13,Modifiability,variab,variable,13,// Match the variable offset.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:63,Security,access,access,63,// Check whether the flat scratch SVS swizzle bug affects this access.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:40,Security,access,accesses,40,// The bug affects the swizzling of SVS accesses if there is any carry out; // from the two low order bits (i.e. from bit 1 into bit 2) when adding; // voffset to (soffset + inst_offset).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:47,Performance,load,load,47,// Check that the base address of flat scratch load/store in the form of `base +; // offset` is legal to be put in SGPR/VGPR (i.e. unsigned per hardware; // requirement). We always treat the first operand as the base address here.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:249,Security,access,access,249,"// If the immediate offset is negative and within certain range, the base; // address cannot also be negative. If the base is also negative, the sum; // would be either negative or much larger than the valid range of scratch; // memory a thread can access.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:249,Security,access,access,249,"// If the immediate offset is negative and within certain range, the base; // address cannot also be negative. If the base is also negative, the sum; // would be either negative or much larger than the valid range of scratch; // memory a thread can access.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:158,Performance,optimiz,optimizations,158,// FIXME: Need to fix extra SGPR->VGPRcopies inserted; // FIXME: Don't know this was defined by operand 0; //; // TODO: Remove this when we have copy folding optimizations after; // RegBankSelect.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:43,Integrability,depend,depending,43,/// Split an immediate offset \p ImmOffset depending on whether it fits in the; /// immediate field. Modifies \p ImmOffset and sets \p SOffset to the variable; /// component.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:150,Modifiability,variab,variable,150,/// Split an immediate offset \p ImmOffset depending on whether it fits in the; /// immediate field. Modifies \p ImmOffset and sets \p SOffset to the variable; /// component.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:63,Modifiability,extend,extended,63,"/// Get an immediate that must be 32-bits, and treated as zero extended.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:46,Integrability,depend,depending,46,"// If not inlinable, get reference to barrier depending on the instruction",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:104,Performance,Perform,Perform,104,// If reference to barrier id is not an inlinable constant then it must be; // referenced with M0[4:0]. Perform an OR with the member count to include; // it in M0 for S_BARRIER_INIT.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.h:59,Availability,mask,masking,59,// Returns true if TargetOpcode::G_AND MachineInstr `MI`'s masking of the; // shift amount operand's `ShAmtBits` bits is unneeded.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:93,Energy Efficiency,schedul,scheduling,93,"/// This pass converts a legalized DAG into a AMDGPU-specific; // DAG, ready for instruction scheduling.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:66,Usability,clear,clear,66,"// Fabs is lowered to a bit operation, but it's an and which will clear the; // high bits anyway.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:234,Integrability,depend,dependencies,234,"// build_vector lo, (load ptr) -> load_d16_hi ptr, lo; // build_vector lo, (zextload ptr from i8) -> load_d16_hi_u8 ptr, lo; // build_vector lo, (sextload ptr from i8) -> load_d16_hi_i8 ptr, lo; // Need to check for possible indirect dependencies on the other half of the; // vector to avoid introducing a cycle.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:21,Performance,load,load,21,"// build_vector lo, (load ptr) -> load_d16_hi ptr, lo; // build_vector lo, (zextload ptr from i8) -> load_d16_hi_u8 ptr, lo; // build_vector lo, (sextload ptr from i8) -> load_d16_hi_i8 ptr, lo; // Need to check for possible indirect dependencies on the other half of the; // vector to avoid introducing a cycle.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:286,Safety,avoid,avoid,286,"// build_vector lo, (load ptr) -> load_d16_hi ptr, lo; // build_vector lo, (zextload ptr from i8) -> load_d16_hi_u8 ptr, lo; // build_vector lo, (sextload ptr from i8) -> load_d16_hi_i8 ptr, lo; // Need to check for possible indirect dependencies on the other half of the; // vector to avoid introducing a cycle.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:17,Performance,load,load,17,"// build_vector (load ptr), hi -> load_d16_lo ptr, hi; // build_vector (zextload ptr from i8), hi -> load_d16_lo_u8 ptr, hi; // build_vector (sextload ptr from i8), hi -> load_d16_lo_i8 ptr, hi",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:15,Performance,load,load,15,"// TODO: Match load d16 from shl (extload:i16), 16",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:160,Performance,LOAD,LOAD,160,"// We are selecting i64 ADD here instead of custom lower it during; // DAG legalization, so we can fold some i64 ADDs used for address; // calculation into the LOAD and STORE instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:29,Availability,avail,available,29,"// There is a scalar version available, but unlike the vector version which; // has a separate operand for the offset and width, the scalar version packs; // the width and offset into a single operand. Try to move to the scalar; // version if the offsets are constant, so that we can try to keep extended; // loads of kernel arguments in SGPRs.; // TODO: Technically we could try to pattern match scalar bitshifts of; // dynamic values, but it's probably not useful.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:296,Modifiability,extend,extended,296,"// There is a scalar version available, but unlike the vector version which; // has a separate operand for the offset and width, the scalar version packs; // the width and offset into a single operand. Try to move to the scalar; // version if the offsets are constant, so that we can try to keep extended; // loads of kernel arguments in SGPRs.; // TODO: Technically we could try to pattern match scalar bitshifts of; // dynamic values, but it's probably not useful.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:309,Performance,load,loads,309,"// There is a scalar version available, but unlike the vector version which; // has a separate operand for the offset and width, the scalar version packs; // the width and offset into a single operand. Try to move to the scalar; // version if the offsets are constant, so that we can try to keep extended; // loads of kernel arguments in SGPRs.; // TODO: Technically we could try to pattern match scalar bitshifts of; // dynamic values, but it's probably not useful.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:105,Performance,load,load,105,"// If we have a constant address, prefer to put the constant into the; // offset. This can save moves to load the constant address since multiple; // operations can share the zero base address register, and enables merging; // into read2 / write2 instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp
