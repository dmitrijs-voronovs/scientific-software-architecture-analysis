id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/broadinstitute/gatk/issues/4922:741,Integrability,integrat,integration,741,"## Feature request. ### Tool(s) or class(es) involved; _FindBreakpointEvidenceSpark_, _StructuralVariationDiscoveryPipelineSpark_, when using _XGBoostEvidenceFilter.java_. ### Description; The SV pipeline filters BreakpointEvidence based on BreakpointDensityFilter, or optionally XGBoostEvidenceFilter. The XGBoostEvidenceFilter uses a saved classifier model trained with Python code external to the GATK. This poses two main problems:; 1) The external Python code was designed for proof-of-principle and method development, not maintainability or ease of use. Additionally, GATK users and developers are assumed to be familiar with Java, not necessarily Python.; 2) The external Python code must share heterogeneous data with Java for unit/integration tests (supplying test BreakpointEvidence, expected classifier features, and expected classifier probabilities). Currently this is done via JSON files organized to (invertibly) store Pandas or Numpy objects. The resulting code to load these JSON files in on the Java side is complex.; These problems can be resolved by; 1) Replacing external python code by porting to an **experimental** tool in the GATK.; 2) Replacing JSON files with a serialization strategy currently supported by the GATK (e.g. Kryo). Additional benefits can be obtained by ensuring that the classifier-training subroutines are sufficiently general to speed development for other projects that may want to use boosted decision trees for classification.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4922
https://github.com/broadinstitute/gatk/issues/4922:529,Modifiability,maintainab,maintainability,529,"## Feature request. ### Tool(s) or class(es) involved; _FindBreakpointEvidenceSpark_, _StructuralVariationDiscoveryPipelineSpark_, when using _XGBoostEvidenceFilter.java_. ### Description; The SV pipeline filters BreakpointEvidence based on BreakpointDensityFilter, or optionally XGBoostEvidenceFilter. The XGBoostEvidenceFilter uses a saved classifier model trained with Python code external to the GATK. This poses two main problems:; 1) The external Python code was designed for proof-of-principle and method development, not maintainability or ease of use. Additionally, GATK users and developers are assumed to be familiar with Java, not necessarily Python.; 2) The external Python code must share heterogeneous data with Java for unit/integration tests (supplying test BreakpointEvidence, expected classifier features, and expected classifier probabilities). Currently this is done via JSON files organized to (invertibly) store Pandas or Numpy objects. The resulting code to load these JSON files in on the Java side is complex.; These problems can be resolved by; 1) Replacing external python code by porting to an **experimental** tool in the GATK.; 2) Replacing JSON files with a serialization strategy currently supported by the GATK (e.g. Kryo). Additional benefits can be obtained by ensuring that the classifier-training subroutines are sufficiently general to speed development for other projects that may want to use boosted decision trees for classification.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4922
https://github.com/broadinstitute/gatk/issues/4922:982,Performance,load,load,982,"## Feature request. ### Tool(s) or class(es) involved; _FindBreakpointEvidenceSpark_, _StructuralVariationDiscoveryPipelineSpark_, when using _XGBoostEvidenceFilter.java_. ### Description; The SV pipeline filters BreakpointEvidence based on BreakpointDensityFilter, or optionally XGBoostEvidenceFilter. The XGBoostEvidenceFilter uses a saved classifier model trained with Python code external to the GATK. This poses two main problems:; 1) The external Python code was designed for proof-of-principle and method development, not maintainability or ease of use. Additionally, GATK users and developers are assumed to be familiar with Java, not necessarily Python.; 2) The external Python code must share heterogeneous data with Java for unit/integration tests (supplying test BreakpointEvidence, expected classifier features, and expected classifier probabilities). Currently this is done via JSON files organized to (invertibly) store Pandas or Numpy objects. The resulting code to load these JSON files in on the Java side is complex.; These problems can be resolved by; 1) Replacing external python code by porting to an **experimental** tool in the GATK.; 2) Replacing JSON files with a serialization strategy currently supported by the GATK (e.g. Kryo). Additional benefits can be obtained by ensuring that the classifier-training subroutines are sufficiently general to speed development for other projects that may want to use boosted decision trees for classification.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4922
https://github.com/broadinstitute/gatk/issues/4922:753,Testability,test,tests,753,"## Feature request. ### Tool(s) or class(es) involved; _FindBreakpointEvidenceSpark_, _StructuralVariationDiscoveryPipelineSpark_, when using _XGBoostEvidenceFilter.java_. ### Description; The SV pipeline filters BreakpointEvidence based on BreakpointDensityFilter, or optionally XGBoostEvidenceFilter. The XGBoostEvidenceFilter uses a saved classifier model trained with Python code external to the GATK. This poses two main problems:; 1) The external Python code was designed for proof-of-principle and method development, not maintainability or ease of use. Additionally, GATK users and developers are assumed to be familiar with Java, not necessarily Python.; 2) The external Python code must share heterogeneous data with Java for unit/integration tests (supplying test BreakpointEvidence, expected classifier features, and expected classifier probabilities). Currently this is done via JSON files organized to (invertibly) store Pandas or Numpy objects. The resulting code to load these JSON files in on the Java side is complex.; These problems can be resolved by; 1) Replacing external python code by porting to an **experimental** tool in the GATK.; 2) Replacing JSON files with a serialization strategy currently supported by the GATK (e.g. Kryo). Additional benefits can be obtained by ensuring that the classifier-training subroutines are sufficiently general to speed development for other projects that may want to use boosted decision trees for classification.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4922
https://github.com/broadinstitute/gatk/issues/4922:770,Testability,test,test,770,"## Feature request. ### Tool(s) or class(es) involved; _FindBreakpointEvidenceSpark_, _StructuralVariationDiscoveryPipelineSpark_, when using _XGBoostEvidenceFilter.java_. ### Description; The SV pipeline filters BreakpointEvidence based on BreakpointDensityFilter, or optionally XGBoostEvidenceFilter. The XGBoostEvidenceFilter uses a saved classifier model trained with Python code external to the GATK. This poses two main problems:; 1) The external Python code was designed for proof-of-principle and method development, not maintainability or ease of use. Additionally, GATK users and developers are assumed to be familiar with Java, not necessarily Python.; 2) The external Python code must share heterogeneous data with Java for unit/integration tests (supplying test BreakpointEvidence, expected classifier features, and expected classifier probabilities). Currently this is done via JSON files organized to (invertibly) store Pandas or Numpy objects. The resulting code to load these JSON files in on the Java side is complex.; These problems can be resolved by; 1) Replacing external python code by porting to an **experimental** tool in the GATK.; 2) Replacing JSON files with a serialization strategy currently supported by the GATK (e.g. Kryo). Additional benefits can be obtained by ensuring that the classifier-training subroutines are sufficiently general to speed development for other projects that may want to use boosted decision trees for classification.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4922
https://github.com/broadinstitute/gatk/pull/4927:13,Deployability,Release,Released,13,Fixes #4586. Released new version of datasources to go with this release (1.4.20180615).; This was necessary because the data sources needed to be made; consistent with hg19 (before they were a mix of hg19 and b37; contig names). Now Funcotator assumes all data sources for the hg19 reference are; compliant with hg19 contig names. Updated the minimum data source version to the new release (1.4.20180615). Simplified `Funcotator::enqueueAndHandleVariant`. Not clear that the `--allow-hg19-gencode-b37-contig-matching-override`; flag does anything anymore. Updated the `getDbSNP.sh` and `createSqliteCosmicDb.sh` data source; scripts to preprocess those data sources to be have hg19-compliant; contigs names. New speeds are ~20k variants/minute for hg19 and ~200k variants/minute for hg38,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4927
https://github.com/broadinstitute/gatk/pull/4927:65,Deployability,release,release,65,Fixes #4586. Released new version of datasources to go with this release (1.4.20180615).; This was necessary because the data sources needed to be made; consistent with hg19 (before they were a mix of hg19 and b37; contig names). Now Funcotator assumes all data sources for the hg19 reference are; compliant with hg19 contig names. Updated the minimum data source version to the new release (1.4.20180615). Simplified `Funcotator::enqueueAndHandleVariant`. Not clear that the `--allow-hg19-gencode-b37-contig-matching-override`; flag does anything anymore. Updated the `getDbSNP.sh` and `createSqliteCosmicDb.sh` data source; scripts to preprocess those data sources to be have hg19-compliant; contigs names. New speeds are ~20k variants/minute for hg19 and ~200k variants/minute for hg38,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4927
https://github.com/broadinstitute/gatk/pull/4927:332,Deployability,Update,Updated,332,Fixes #4586. Released new version of datasources to go with this release (1.4.20180615).; This was necessary because the data sources needed to be made; consistent with hg19 (before they were a mix of hg19 and b37; contig names). Now Funcotator assumes all data sources for the hg19 reference are; compliant with hg19 contig names. Updated the minimum data source version to the new release (1.4.20180615). Simplified `Funcotator::enqueueAndHandleVariant`. Not clear that the `--allow-hg19-gencode-b37-contig-matching-override`; flag does anything anymore. Updated the `getDbSNP.sh` and `createSqliteCosmicDb.sh` data source; scripts to preprocess those data sources to be have hg19-compliant; contigs names. New speeds are ~20k variants/minute for hg19 and ~200k variants/minute for hg38,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4927
https://github.com/broadinstitute/gatk/pull/4927:383,Deployability,release,release,383,Fixes #4586. Released new version of datasources to go with this release (1.4.20180615).; This was necessary because the data sources needed to be made; consistent with hg19 (before they were a mix of hg19 and b37; contig names). Now Funcotator assumes all data sources for the hg19 reference are; compliant with hg19 contig names. Updated the minimum data source version to the new release (1.4.20180615). Simplified `Funcotator::enqueueAndHandleVariant`. Not clear that the `--allow-hg19-gencode-b37-contig-matching-override`; flag does anything anymore. Updated the `getDbSNP.sh` and `createSqliteCosmicDb.sh` data source; scripts to preprocess those data sources to be have hg19-compliant; contigs names. New speeds are ~20k variants/minute for hg19 and ~200k variants/minute for hg38,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4927
https://github.com/broadinstitute/gatk/pull/4927:557,Deployability,Update,Updated,557,Fixes #4586. Released new version of datasources to go with this release (1.4.20180615).; This was necessary because the data sources needed to be made; consistent with hg19 (before they were a mix of hg19 and b37; contig names). Now Funcotator assumes all data sources for the hg19 reference are; compliant with hg19 contig names. Updated the minimum data source version to the new release (1.4.20180615). Simplified `Funcotator::enqueueAndHandleVariant`. Not clear that the `--allow-hg19-gencode-b37-contig-matching-override`; flag does anything anymore. Updated the `getDbSNP.sh` and `createSqliteCosmicDb.sh` data source; scripts to preprocess those data sources to be have hg19-compliant; contigs names. New speeds are ~20k variants/minute for hg19 and ~200k variants/minute for hg38,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4927
https://github.com/broadinstitute/gatk/pull/4927:407,Usability,Simpl,Simplified,407,Fixes #4586. Released new version of datasources to go with this release (1.4.20180615).; This was necessary because the data sources needed to be made; consistent with hg19 (before they were a mix of hg19 and b37; contig names). Now Funcotator assumes all data sources for the hg19 reference are; compliant with hg19 contig names. Updated the minimum data source version to the new release (1.4.20180615). Simplified `Funcotator::enqueueAndHandleVariant`. Not clear that the `--allow-hg19-gencode-b37-contig-matching-override`; flag does anything anymore. Updated the `getDbSNP.sh` and `createSqliteCosmicDb.sh` data source; scripts to preprocess those data sources to be have hg19-compliant; contigs names. New speeds are ~20k variants/minute for hg19 and ~200k variants/minute for hg38,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4927
https://github.com/broadinstitute/gatk/pull/4927:461,Usability,clear,clear,461,Fixes #4586. Released new version of datasources to go with this release (1.4.20180615).; This was necessary because the data sources needed to be made; consistent with hg19 (before they were a mix of hg19 and b37; contig names). Now Funcotator assumes all data sources for the hg19 reference are; compliant with hg19 contig names. Updated the minimum data source version to the new release (1.4.20180615). Simplified `Funcotator::enqueueAndHandleVariant`. Not clear that the `--allow-hg19-gencode-b37-contig-matching-override`; flag does anything anymore. Updated the `getDbSNP.sh` and `createSqliteCosmicDb.sh` data source; scripts to preprocess those data sources to be have hg19-compliant; contigs names. New speeds are ~20k variants/minute for hg19 and ~200k variants/minute for hg38,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4927
https://github.com/broadinstitute/gatk/issues/4929:164,Testability,test,test,164,## Bug Report. ### Affected tool(s) or class(es); _Funcotator_ _LocatableXsvFuncotationFactory_. ### Affected version(s); - [x] Latest master branch as of [date of test?]. ### Description ; The datasources (clinvar_hgmd in particular) contain multiple/overlapping entries for the same location. This causes too many funcotations to be added to a single variant for a single LocatableXsv data source. #### Expected behavior; For now Funcotator should just pick the first entry. Will need to discuss for what the correct behavior is.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4929
https://github.com/broadinstitute/gatk/pull/4931:224,Deployability,Update,Updated,224,"- User defined transcripts were being used as a filter rather than a priority order. The filtering step has been eliminated. Closes #4918 ; - Fixed previously unidentified issue where locus level ranking was being reversed. Updated tests. This was identified thanks to the thousands of tests in Funcotator (only one failed, but that was all it took).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4931
https://github.com/broadinstitute/gatk/pull/4931:232,Testability,test,tests,232,"- User defined transcripts were being used as a filter rather than a priority order. The filtering step has been eliminated. Closes #4918 ; - Fixed previously unidentified issue where locus level ranking was being reversed. Updated tests. This was identified thanks to the thousands of tests in Funcotator (only one failed, but that was all it took).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4931
https://github.com/broadinstitute/gatk/pull/4931:286,Testability,test,tests,286,"- User defined transcripts were being used as a filter rather than a priority order. The filtering step has been eliminated. Closes #4918 ; - Fixed previously unidentified issue where locus level ranking was being reversed. Updated tests. This was identified thanks to the thousands of tests in Funcotator (only one failed, but that was all it took).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4931
https://github.com/broadinstitute/gatk/issues/4937:306,Availability,down,downstream,306,"## Bug Report. ### Affected tool(s) or class(es); Funcotator. ### Affected version(s); - [ ] Latest master branch as of [June 22, 2018]. ### Description ; If there are no variants in the input VCF that are rendered in the MAF, the MAF file is blank. It should contain a header. This will cause problems in downstream tools expecting a valid MAF. #### Expected behavior; A MAF that is empty except for a header. #### Actual behavior; A zero-byte file. #### Possible solution; MafOutputRenderer can use the metadata to create a header.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4937
https://github.com/broadinstitute/gatk/pull/4940:235,Availability,toler,tolerances,235,"This is a port of the GATK3 version I actually ran. Some differences in the genotyping engines and the fixed median calculation in MathUtils between GATK3 and GATK4 make the results _slightly_ different in some cases, but still within tolerances.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4940
https://github.com/broadinstitute/gatk/pull/4941:62,Deployability,Update,Updated,62,"- MAF is the output of Funcotator in M2 WDL. Closes #4935 ; - Updated mutect2.wdl manually tested locally and manually tested in FireCloud.; - Updated mutect2_nio.wdl manually tested in FireCloud.; - Updated automatic Cromwell WDL tests. Closes #4807 ; - Empty MAFs will be devoid of variants, not a file of 0 bytes. Closes #4937 ; - Fixed issue where multiple transcripts could be selected in edge cases, even when CANONICAL or BEST_EFFECT was selected. Closes #4952",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4941
https://github.com/broadinstitute/gatk/pull/4941:143,Deployability,Update,Updated,143,"- MAF is the output of Funcotator in M2 WDL. Closes #4935 ; - Updated mutect2.wdl manually tested locally and manually tested in FireCloud.; - Updated mutect2_nio.wdl manually tested in FireCloud.; - Updated automatic Cromwell WDL tests. Closes #4807 ; - Empty MAFs will be devoid of variants, not a file of 0 bytes. Closes #4937 ; - Fixed issue where multiple transcripts could be selected in edge cases, even when CANONICAL or BEST_EFFECT was selected. Closes #4952",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4941
https://github.com/broadinstitute/gatk/pull/4941:200,Deployability,Update,Updated,200,"- MAF is the output of Funcotator in M2 WDL. Closes #4935 ; - Updated mutect2.wdl manually tested locally and manually tested in FireCloud.; - Updated mutect2_nio.wdl manually tested in FireCloud.; - Updated automatic Cromwell WDL tests. Closes #4807 ; - Empty MAFs will be devoid of variants, not a file of 0 bytes. Closes #4937 ; - Fixed issue where multiple transcripts could be selected in edge cases, even when CANONICAL or BEST_EFFECT was selected. Closes #4952",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4941
https://github.com/broadinstitute/gatk/pull/4941:91,Testability,test,tested,91,"- MAF is the output of Funcotator in M2 WDL. Closes #4935 ; - Updated mutect2.wdl manually tested locally and manually tested in FireCloud.; - Updated mutect2_nio.wdl manually tested in FireCloud.; - Updated automatic Cromwell WDL tests. Closes #4807 ; - Empty MAFs will be devoid of variants, not a file of 0 bytes. Closes #4937 ; - Fixed issue where multiple transcripts could be selected in edge cases, even when CANONICAL or BEST_EFFECT was selected. Closes #4952",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4941
https://github.com/broadinstitute/gatk/pull/4941:119,Testability,test,tested,119,"- MAF is the output of Funcotator in M2 WDL. Closes #4935 ; - Updated mutect2.wdl manually tested locally and manually tested in FireCloud.; - Updated mutect2_nio.wdl manually tested in FireCloud.; - Updated automatic Cromwell WDL tests. Closes #4807 ; - Empty MAFs will be devoid of variants, not a file of 0 bytes. Closes #4937 ; - Fixed issue where multiple transcripts could be selected in edge cases, even when CANONICAL or BEST_EFFECT was selected. Closes #4952",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4941
https://github.com/broadinstitute/gatk/pull/4941:176,Testability,test,tested,176,"- MAF is the output of Funcotator in M2 WDL. Closes #4935 ; - Updated mutect2.wdl manually tested locally and manually tested in FireCloud.; - Updated mutect2_nio.wdl manually tested in FireCloud.; - Updated automatic Cromwell WDL tests. Closes #4807 ; - Empty MAFs will be devoid of variants, not a file of 0 bytes. Closes #4937 ; - Fixed issue where multiple transcripts could be selected in edge cases, even when CANONICAL or BEST_EFFECT was selected. Closes #4952",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4941
https://github.com/broadinstitute/gatk/pull/4941:231,Testability,test,tests,231,"- MAF is the output of Funcotator in M2 WDL. Closes #4935 ; - Updated mutect2.wdl manually tested locally and manually tested in FireCloud.; - Updated mutect2_nio.wdl manually tested in FireCloud.; - Updated automatic Cromwell WDL tests. Closes #4807 ; - Empty MAFs will be devoid of variants, not a file of 0 bytes. Closes #4937 ; - Fixed issue where multiple transcripts could be selected in edge cases, even when CANONICAL or BEST_EFFECT was selected. Closes #4952",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4941
https://github.com/broadinstitute/gatk/issues/4942:467,Availability,recover,recover,467,"Just splitting off a chunk of @vruano's ideas in #264 here:. We start threading at the first unique kmer of each read (sequence). There are at least two problems with this. First, since we track unique kmers as we go the resulting graph may depend on the order in which reads were threaded. Second, we are throwing away information at the beginning of the read before the first unique (and existing) k-mer in each sequence is found. This is only partly fixed when we recover dangling heads.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4942
https://github.com/broadinstitute/gatk/issues/4942:241,Integrability,depend,depend,241,"Just splitting off a chunk of @vruano's ideas in #264 here:. We start threading at the first unique kmer of each read (sequence). There are at least two problems with this. First, since we track unique kmers as we go the resulting graph may depend on the order in which reads were threaded. Second, we are throwing away information at the beginning of the read before the first unique (and existing) k-mer in each sequence is found. This is only partly fixed when we recover dangling heads.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4942
https://github.com/broadinstitute/gatk/issues/4942:467,Safety,recover,recover,467,"Just splitting off a chunk of @vruano's ideas in #264 here:. We start threading at the first unique kmer of each read (sequence). There are at least two problems with this. First, since we track unique kmers as we go the resulting graph may depend on the order in which reads were threaded. Second, we are throwing away information at the beginning of the read before the first unique (and existing) k-mer in each sequence is found. This is only partly fixed when we recover dangling heads.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4942
https://github.com/broadinstitute/gatk/issues/4943:164,Testability,log,log,164,"~The AssemblyResultSet in AssemblyBasedCallerUtils.assembleReads is sup posed to output some stats about the active region (debugDump), but it never appears in the log.~. Also debug state is not properly passed to the EventMap.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4943
https://github.com/broadinstitute/gatk/pull/4947:105,Deployability,Pipeline,Pipeline,105,"This along with the GVCF reblocking branch constitute the new code I'm using for gnomAD v3 on the Gnarly Pipeline. Some of the GDB hacks are gross, but I can't clean it up until after the protobuf update.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947
https://github.com/broadinstitute/gatk/pull/4947:197,Deployability,update,update,197,"This along with the GVCF reblocking branch constitute the new code I'm using for gnomAD v3 on the Gnarly Pipeline. Some of the GDB hacks are gross, but I can't clean it up until after the protobuf update.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947
https://github.com/broadinstitute/gatk/issues/4948:275,Deployability,release,releases,275,"## Feature request. ### Tool(s) or class(es) involved; M2, at least. ### Description; With Cromwell v33, we should be able to merge the mutect2.wdl and mutect_nio.wdl into one WDL. There will need to be WDL modifications, for sure. https://github.com/broadinstitute/cromwell/releases",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4948
https://github.com/broadinstitute/gatk/pull/4950:20,Integrability,depend,dependency,20,removing the unused dependency 'com.github.wendykierp:JTransforms:3.1',MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4950
https://github.com/broadinstitute/gatk/issues/4951:68,Deployability,pipeline,pipeline,68,"## Bug Report. ### Affected tool(s) or class(es); SV type inference pipeline. ### Affected version(s); - [x] Latest public release version; - [x] Latest master branch as of 2018-06-26. ### Description ; Though we've never seen this case with `bwa mem`, it is theoretically possible&mdash;and happened in an experimental (mis-) run of another aligner&mdash; that a query sequence generates two alignment records, where the two alignments's overlap on the read and on the reference are of the same length. See illustration below.; ```; --------------------------sssssssssssssss READ ALN1; ssssssssssssssss------------------------- READ ALN2; | |; | |; | |; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ REF; ```. This scenario will trigger our code to resolve a complicated tandem duplication structure. ; But the code should check for such case, or better yet two alignments should really be stitched together. #### Steps to reproduce; Run the code with some funny alignments. #### Expected behavior; Two alignments should be merged into one, by the type inference pipeline. #### Actual behavior; Exception would be thrown currently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4951
https://github.com/broadinstitute/gatk/issues/4951:123,Deployability,release,release,123,"## Bug Report. ### Affected tool(s) or class(es); SV type inference pipeline. ### Affected version(s); - [x] Latest public release version; - [x] Latest master branch as of 2018-06-26. ### Description ; Though we've never seen this case with `bwa mem`, it is theoretically possible&mdash;and happened in an experimental (mis-) run of another aligner&mdash; that a query sequence generates two alignment records, where the two alignments's overlap on the read and on the reference are of the same length. See illustration below.; ```; --------------------------sssssssssssssss READ ALN1; ssssssssssssssss------------------------- READ ALN2; | |; | |; | |; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ REF; ```. This scenario will trigger our code to resolve a complicated tandem duplication structure. ; But the code should check for such case, or better yet two alignments should really be stitched together. #### Steps to reproduce; Run the code with some funny alignments. #### Expected behavior; Two alignments should be merged into one, by the type inference pipeline. #### Actual behavior; Exception would be thrown currently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4951
https://github.com/broadinstitute/gatk/issues/4951:1091,Deployability,pipeline,pipeline,1091,"## Bug Report. ### Affected tool(s) or class(es); SV type inference pipeline. ### Affected version(s); - [x] Latest public release version; - [x] Latest master branch as of 2018-06-26. ### Description ; Though we've never seen this case with `bwa mem`, it is theoretically possible&mdash;and happened in an experimental (mis-) run of another aligner&mdash; that a query sequence generates two alignment records, where the two alignments's overlap on the read and on the reference are of the same length. See illustration below.; ```; --------------------------sssssssssssssss READ ALN1; ssssssssssssssss------------------------- READ ALN2; | |; | |; | |; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ REF; ```. This scenario will trigger our code to resolve a complicated tandem duplication structure. ; But the code should check for such case, or better yet two alignments should really be stitched together. #### Steps to reproduce; Run the code with some funny alignments. #### Expected behavior; Two alignments should be merged into one, by the type inference pipeline. #### Actual behavior; Exception would be thrown currently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4951
https://github.com/broadinstitute/gatk/issues/4952:245,Availability,Error,Error,245,"## Bug Report. ### Affected tool(s) or class(es); Funcotator. ### Affected version(s); - [ ] Latest master branch as of [June 26, 2018]. ### Description ; This happens when there are transcripts with different gene names that overlap a variant. Error is in `createFuncotationsOnVariant` ... . #### Expected behavior; One transcript is selected. #### Actual behavior; One transcript per overlapping gene name is selected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4952
https://github.com/broadinstitute/gatk/issues/4958:562,Availability,error,errors,562,"Every overlapping read pair in HaplotypeCaller and Mutect2 ; goes through `FragmentUtils.adjustQualsOfOverlappingPairedFragments`, which has the following *hard-coded* logic:. ```; if ( firstReadBase == secondReadBase ) {; firstReadQuals[firstReadIndex] = Math.min(firstReadQuals[firstReadIndex], HALF_OF_DEFAULT_PCR_ERROR_QUAL);; secondReadQuals[i] = Math.min(secondReadQuals[i], HALF_OF_DEFAULT_PCR_ERROR_QUAL);; } else {; firstReadQuals[firstReadIndex] = 0;; secondReadQuals[i] = 0;; }; ```. This makes sense -- we modify the quals to reflect that sequencing errors are independent but PCR errors are not. However, `HALF_OF_DEFAULT_PCR_ERROR_QUAL` is 20, so if we start out with high-quality UMI reads we basically kill the quality and with it our SNV sensitivity. Note that in the very important application of cfDNA basically every base is an overlap. What's worse is that Mutect2 later throws out one of the reads, so that we could start out with two BQ = 60 reads and end up with one BQ = 20 read!. @fleharty @yfarjoun This is a problem. I could just make the PCR error quality an adjustable parameter, but can you think of anything else?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958
https://github.com/broadinstitute/gatk/issues/4958:593,Availability,error,errors,593,"Every overlapping read pair in HaplotypeCaller and Mutect2 ; goes through `FragmentUtils.adjustQualsOfOverlappingPairedFragments`, which has the following *hard-coded* logic:. ```; if ( firstReadBase == secondReadBase ) {; firstReadQuals[firstReadIndex] = Math.min(firstReadQuals[firstReadIndex], HALF_OF_DEFAULT_PCR_ERROR_QUAL);; secondReadQuals[i] = Math.min(secondReadQuals[i], HALF_OF_DEFAULT_PCR_ERROR_QUAL);; } else {; firstReadQuals[firstReadIndex] = 0;; secondReadQuals[i] = 0;; }; ```. This makes sense -- we modify the quals to reflect that sequencing errors are independent but PCR errors are not. However, `HALF_OF_DEFAULT_PCR_ERROR_QUAL` is 20, so if we start out with high-quality UMI reads we basically kill the quality and with it our SNV sensitivity. Note that in the very important application of cfDNA basically every base is an overlap. What's worse is that Mutect2 later throws out one of the reads, so that we could start out with two BQ = 60 reads and end up with one BQ = 20 read!. @fleharty @yfarjoun This is a problem. I could just make the PCR error quality an adjustable parameter, but can you think of anything else?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958
https://github.com/broadinstitute/gatk/issues/4958:1071,Availability,error,error,1071,"Every overlapping read pair in HaplotypeCaller and Mutect2 ; goes through `FragmentUtils.adjustQualsOfOverlappingPairedFragments`, which has the following *hard-coded* logic:. ```; if ( firstReadBase == secondReadBase ) {; firstReadQuals[firstReadIndex] = Math.min(firstReadQuals[firstReadIndex], HALF_OF_DEFAULT_PCR_ERROR_QUAL);; secondReadQuals[i] = Math.min(secondReadQuals[i], HALF_OF_DEFAULT_PCR_ERROR_QUAL);; } else {; firstReadQuals[firstReadIndex] = 0;; secondReadQuals[i] = 0;; }; ```. This makes sense -- we modify the quals to reflect that sequencing errors are independent but PCR errors are not. However, `HALF_OF_DEFAULT_PCR_ERROR_QUAL` is 20, so if we start out with high-quality UMI reads we basically kill the quality and with it our SNV sensitivity. Note that in the very important application of cfDNA basically every base is an overlap. What's worse is that Mutect2 later throws out one of the reads, so that we could start out with two BQ = 60 reads and end up with one BQ = 20 read!. @fleharty @yfarjoun This is a problem. I could just make the PCR error quality an adjustable parameter, but can you think of anything else?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958
https://github.com/broadinstitute/gatk/issues/4958:168,Testability,log,logic,168,"Every overlapping read pair in HaplotypeCaller and Mutect2 ; goes through `FragmentUtils.adjustQualsOfOverlappingPairedFragments`, which has the following *hard-coded* logic:. ```; if ( firstReadBase == secondReadBase ) {; firstReadQuals[firstReadIndex] = Math.min(firstReadQuals[firstReadIndex], HALF_OF_DEFAULT_PCR_ERROR_QUAL);; secondReadQuals[i] = Math.min(secondReadQuals[i], HALF_OF_DEFAULT_PCR_ERROR_QUAL);; } else {; firstReadQuals[firstReadIndex] = 0;; secondReadQuals[i] = 0;; }; ```. This makes sense -- we modify the quals to reflect that sequencing errors are independent but PCR errors are not. However, `HALF_OF_DEFAULT_PCR_ERROR_QUAL` is 20, so if we start out with high-quality UMI reads we basically kill the quality and with it our SNV sensitivity. Note that in the very important application of cfDNA basically every base is an overlap. What's worse is that Mutect2 later throws out one of the reads, so that we could start out with two BQ = 60 reads and end up with one BQ = 20 read!. @fleharty @yfarjoun This is a problem. I could just make the PCR error quality an adjustable parameter, but can you think of anything else?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958
https://github.com/broadinstitute/gatk/pull/4960:29,Deployability,configurat,configuration,29,Fixes #4581. Had to change a configuration method to allow Funcotator tests to work. Currently funcotator will not use the config cache - it will always; create a new configuration on startup. This should be fine for now.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4960
https://github.com/broadinstitute/gatk/pull/4960:167,Deployability,configurat,configuration,167,Fixes #4581. Had to change a configuration method to allow Funcotator tests to work. Currently funcotator will not use the config cache - it will always; create a new configuration on startup. This should be fine for now.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4960
https://github.com/broadinstitute/gatk/pull/4960:29,Modifiability,config,configuration,29,Fixes #4581. Had to change a configuration method to allow Funcotator tests to work. Currently funcotator will not use the config cache - it will always; create a new configuration on startup. This should be fine for now.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4960
https://github.com/broadinstitute/gatk/pull/4960:123,Modifiability,config,config,123,Fixes #4581. Had to change a configuration method to allow Funcotator tests to work. Currently funcotator will not use the config cache - it will always; create a new configuration on startup. This should be fine for now.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4960
https://github.com/broadinstitute/gatk/pull/4960:167,Modifiability,config,configuration,167,Fixes #4581. Had to change a configuration method to allow Funcotator tests to work. Currently funcotator will not use the config cache - it will always; create a new configuration on startup. This should be fine for now.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4960
https://github.com/broadinstitute/gatk/pull/4960:130,Performance,cache,cache,130,Fixes #4581. Had to change a configuration method to allow Funcotator tests to work. Currently funcotator will not use the config cache - it will always; create a new configuration on startup. This should be fine for now.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4960
https://github.com/broadinstitute/gatk/pull/4960:70,Testability,test,tests,70,Fixes #4581. Had to change a configuration method to allow Funcotator tests to work. Currently funcotator will not use the config cache - it will always; create a new configuration on startup. This should be fine for now.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4960
https://github.com/broadinstitute/gatk/pull/4962:490,Availability,down,downstream,490,"The problem is that when alignments are de-overlapped (i.e. they overlap on the read) in the CPX logic, some resulting alignment could be only 1 base long, leading to problems. This was not caught before because in practice a heuristic alignment filtering step is in place in `AssemblyContigAlignmentsConfigPicker` to filter out, in a post-hoc way, such small alignments.; When I experimented with switching orders of the filtering steps, this behavior was observed. The aim is to make the downstream logic agnostic to upstream filtering detail.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962
https://github.com/broadinstitute/gatk/pull/4962:97,Testability,log,logic,97,"The problem is that when alignments are de-overlapped (i.e. they overlap on the read) in the CPX logic, some resulting alignment could be only 1 base long, leading to problems. This was not caught before because in practice a heuristic alignment filtering step is in place in `AssemblyContigAlignmentsConfigPicker` to filter out, in a post-hoc way, such small alignments.; When I experimented with switching orders of the filtering steps, this behavior was observed. The aim is to make the downstream logic agnostic to upstream filtering detail.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962
https://github.com/broadinstitute/gatk/pull/4962:501,Testability,log,logic,501,"The problem is that when alignments are de-overlapped (i.e. they overlap on the read) in the CPX logic, some resulting alignment could be only 1 base long, leading to problems. This was not caught before because in practice a heuristic alignment filtering step is in place in `AssemblyContigAlignmentsConfigPicker` to filter out, in a post-hoc way, such small alignments.; When I experimented with switching orders of the filtering steps, this behavior was observed. The aim is to make the downstream logic agnostic to upstream filtering detail.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962
https://github.com/broadinstitute/gatk/pull/4963:980,Integrability,message,message,980,"hat it can output and genotype spanning deletion alleles represented by the `*` allele. . Currently, the output of `HaplotypeCaller` will not include spanning deletion alleles when run in single sample VCF mode or in genotype given alleles mode, even when that genotype would be more appropriate. In the joint calling workflow `GenotypeGVCFs` adds genotypes for spanning deletions, although the input likelihoods will not be broken out to specifically account for spanning deletion alleles. Some implementation notes:. - I also fixed some behavior specific to GGA mode that I encountered while testing this bug. In particular, when GGA mode was run with multiple variants with the same start position or with spanning events, `HaplotypeCaller` used to emit the warning `""Multiple valid VCF records detected in the alleles input file at site "" + loc + "", only considering the first record""` for each such site. This was a bit of a misleading message, since the other variants were in fact taken into account UNLESS HC decided to emit an empty variant context, for example due to zero coverage.; - I rewrote the `createAlleleMapper` method in `AssemblyBasedCallerGenotypingEngine`. The old version had a very brittle mapping scheme that depended heavily on the ordering of alleles in the variant context created by `AssemblyBasedCallerUtils.makeMergedVariantContext` and `getEventsAtThisLoc`. This proved to be difficult to ensure when spanning deletions were added in, and there was an ominous TODO in the old method saying that the logic was not good enough, so I ended up re-writing it from scratch. The new version is longer but I hope it is easier to read and less fragile, but let me know if I've missed anything. Test currently fail on this branch and therefore it should not be merged. To make them pass we need a fix to https://github.com/broadinstitute/gatk/issues/4716 which is currently being worked on in https://github.com/broadinstitute/gatk/pull/4645. However, since that PR is taking a ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963
https://github.com/broadinstitute/gatk/pull/4963:1274,Integrability,depend,depended,1274,"of `HaplotypeCaller` will not include spanning deletion alleles when run in single sample VCF mode or in genotype given alleles mode, even when that genotype would be more appropriate. In the joint calling workflow `GenotypeGVCFs` adds genotypes for spanning deletions, although the input likelihoods will not be broken out to specifically account for spanning deletion alleles. Some implementation notes:. - I also fixed some behavior specific to GGA mode that I encountered while testing this bug. In particular, when GGA mode was run with multiple variants with the same start position or with spanning events, `HaplotypeCaller` used to emit the warning `""Multiple valid VCF records detected in the alleles input file at site "" + loc + "", only considering the first record""` for each such site. This was a bit of a misleading message, since the other variants were in fact taken into account UNLESS HC decided to emit an empty variant context, for example due to zero coverage.; - I rewrote the `createAlleleMapper` method in `AssemblyBasedCallerGenotypingEngine`. The old version had a very brittle mapping scheme that depended heavily on the ordering of alleles in the variant context created by `AssemblyBasedCallerUtils.makeMergedVariantContext` and `getEventsAtThisLoc`. This proved to be difficult to ensure when spanning deletions were added in, and there was an ominous TODO in the old method saying that the logic was not good enough, so I ended up re-writing it from scratch. The new version is longer but I hope it is easier to read and less fragile, but let me know if I've missed anything. Test currently fail on this branch and therefore it should not be merged. To make them pass we need a fix to https://github.com/broadinstitute/gatk/issues/4716 which is currently being worked on in https://github.com/broadinstitute/gatk/pull/4645. However, since that PR is taking a while to make it through code review, I thought it might be good to start the review process for these changes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963
https://github.com/broadinstitute/gatk/pull/4963:837,Safety,detect,detected,837,"This PR modifies `HaplotypeCaller` so that it can output and genotype spanning deletion alleles represented by the `*` allele. . Currently, the output of `HaplotypeCaller` will not include spanning deletion alleles when run in single sample VCF mode or in genotype given alleles mode, even when that genotype would be more appropriate. In the joint calling workflow `GenotypeGVCFs` adds genotypes for spanning deletions, although the input likelihoods will not be broken out to specifically account for spanning deletion alleles. Some implementation notes:. - I also fixed some behavior specific to GGA mode that I encountered while testing this bug. In particular, when GGA mode was run with multiple variants with the same start position or with spanning events, `HaplotypeCaller` used to emit the warning `""Multiple valid VCF records detected in the alleles input file at site "" + loc + "", only considering the first record""` for each such site. This was a bit of a misleading message, since the other variants were in fact taken into account UNLESS HC decided to emit an empty variant context, for example due to zero coverage.; - I rewrote the `createAlleleMapper` method in `AssemblyBasedCallerGenotypingEngine`. The old version had a very brittle mapping scheme that depended heavily on the ordering of alleles in the variant context created by `AssemblyBasedCallerUtils.makeMergedVariantContext` and `getEventsAtThisLoc`. This proved to be difficult to ensure when spanning deletions were added in, and there was an ominous TODO in the old method saying that the logic was not good enough, so I ended up re-writing it from scratch. The new version is longer but I hope it is easier to read and less fragile, but let me know if I've missed anything. Test currently fail on this branch and therefore it should not be merged. To make them pass we need a fix to https://github.com/broadinstitute/gatk/issues/4716 which is currently being worked on in https://github.com/broadinstitute/gatk/pull/46",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963
https://github.com/broadinstitute/gatk/pull/4963:633,Testability,test,testing,633,"This PR modifies `HaplotypeCaller` so that it can output and genotype spanning deletion alleles represented by the `*` allele. . Currently, the output of `HaplotypeCaller` will not include spanning deletion alleles when run in single sample VCF mode or in genotype given alleles mode, even when that genotype would be more appropriate. In the joint calling workflow `GenotypeGVCFs` adds genotypes for spanning deletions, although the input likelihoods will not be broken out to specifically account for spanning deletion alleles. Some implementation notes:. - I also fixed some behavior specific to GGA mode that I encountered while testing this bug. In particular, when GGA mode was run with multiple variants with the same start position or with spanning events, `HaplotypeCaller` used to emit the warning `""Multiple valid VCF records detected in the alleles input file at site "" + loc + "", only considering the first record""` for each such site. This was a bit of a misleading message, since the other variants were in fact taken into account UNLESS HC decided to emit an empty variant context, for example due to zero coverage.; - I rewrote the `createAlleleMapper` method in `AssemblyBasedCallerGenotypingEngine`. The old version had a very brittle mapping scheme that depended heavily on the ordering of alleles in the variant context created by `AssemblyBasedCallerUtils.makeMergedVariantContext` and `getEventsAtThisLoc`. This proved to be difficult to ensure when spanning deletions were added in, and there was an ominous TODO in the old method saying that the logic was not good enough, so I ended up re-writing it from scratch. The new version is longer but I hope it is easier to read and less fragile, but let me know if I've missed anything. Test currently fail on this branch and therefore it should not be merged. To make them pass we need a fix to https://github.com/broadinstitute/gatk/issues/4716 which is currently being worked on in https://github.com/broadinstitute/gatk/pull/46",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963
https://github.com/broadinstitute/gatk/pull/4963:1571,Testability,log,logic,1571,"of `HaplotypeCaller` will not include spanning deletion alleles when run in single sample VCF mode or in genotype given alleles mode, even when that genotype would be more appropriate. In the joint calling workflow `GenotypeGVCFs` adds genotypes for spanning deletions, although the input likelihoods will not be broken out to specifically account for spanning deletion alleles. Some implementation notes:. - I also fixed some behavior specific to GGA mode that I encountered while testing this bug. In particular, when GGA mode was run with multiple variants with the same start position or with spanning events, `HaplotypeCaller` used to emit the warning `""Multiple valid VCF records detected in the alleles input file at site "" + loc + "", only considering the first record""` for each such site. This was a bit of a misleading message, since the other variants were in fact taken into account UNLESS HC decided to emit an empty variant context, for example due to zero coverage.; - I rewrote the `createAlleleMapper` method in `AssemblyBasedCallerGenotypingEngine`. The old version had a very brittle mapping scheme that depended heavily on the ordering of alleles in the variant context created by `AssemblyBasedCallerUtils.makeMergedVariantContext` and `getEventsAtThisLoc`. This proved to be difficult to ensure when spanning deletions were added in, and there was an ominous TODO in the old method saying that the logic was not good enough, so I ended up re-writing it from scratch. The new version is longer but I hope it is easier to read and less fragile, but let me know if I've missed anything. Test currently fail on this branch and therefore it should not be merged. To make them pass we need a fix to https://github.com/broadinstitute/gatk/issues/4716 which is currently being worked on in https://github.com/broadinstitute/gatk/pull/4645. However, since that PR is taking a while to make it through code review, I thought it might be good to start the review process for these changes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963
https://github.com/broadinstitute/gatk/pull/4963:1757,Testability,Test,Test,1757,"of `HaplotypeCaller` will not include spanning deletion alleles when run in single sample VCF mode or in genotype given alleles mode, even when that genotype would be more appropriate. In the joint calling workflow `GenotypeGVCFs` adds genotypes for spanning deletions, although the input likelihoods will not be broken out to specifically account for spanning deletion alleles. Some implementation notes:. - I also fixed some behavior specific to GGA mode that I encountered while testing this bug. In particular, when GGA mode was run with multiple variants with the same start position or with spanning events, `HaplotypeCaller` used to emit the warning `""Multiple valid VCF records detected in the alleles input file at site "" + loc + "", only considering the first record""` for each such site. This was a bit of a misleading message, since the other variants were in fact taken into account UNLESS HC decided to emit an empty variant context, for example due to zero coverage.; - I rewrote the `createAlleleMapper` method in `AssemblyBasedCallerGenotypingEngine`. The old version had a very brittle mapping scheme that depended heavily on the ordering of alleles in the variant context created by `AssemblyBasedCallerUtils.makeMergedVariantContext` and `getEventsAtThisLoc`. This proved to be difficult to ensure when spanning deletions were added in, and there was an ominous TODO in the old method saying that the logic was not good enough, so I ended up re-writing it from scratch. The new version is longer but I hope it is easier to read and less fragile, but let me know if I've missed anything. Test currently fail on this branch and therefore it should not be merged. To make them pass we need a fix to https://github.com/broadinstitute/gatk/issues/4716 which is currently being worked on in https://github.com/broadinstitute/gatk/pull/4645. However, since that PR is taking a while to make it through code review, I thought it might be good to start the review process for these changes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963
https://github.com/broadinstitute/gatk/pull/4964:134,Modifiability,inherit,inherits,134,Added the following methods to `GATKTool`:. - `getReferenceDataSource()`; - `getReadsDataSource()`; - `getFeatureManager()`. `Walker` inherits directly from `GATKTool` and overrides these methods to throw an exception if they are called. No walker should need to directly access the data.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4964
https://github.com/broadinstitute/gatk/pull/4964:272,Security,access,access,272,Added the following methods to `GATKTool`:. - `getReferenceDataSource()`; - `getReadsDataSource()`; - `getFeatureManager()`. `Walker` inherits directly from `GATKTool` and overrides these methods to throw an exception if they are called. No walker should need to directly access the data.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4964
https://github.com/broadinstitute/gatk/issues/4973:244,Availability,down,down,244,"----; User Report; ----. We are also experiencing a problem wherein GATK 4.0.5.1 GenotypeGVCFs processes hang for many hours. . The last thing our processes logged was the same as reported here:; ```; 08:48:23.075 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),0.044110324,Cpu time(s),0.026897973000000006; ```. This particular job ran for about 3m before outputting this line, then stayed running (but apparently doing nothing) for 8 hours before we killed it. . It is one of 1996 jobs that all did pretty much exactly the same thing in a similar time frame - in all cases these were the last two lines logged but GATK failed to terminate afterwards. At the same time, we did have about 8k jobs finish successfully and exit 0, so it appears that the rate at which this happens is (at least for our workload) is around 20%. Don't know yet whether or not this behaviour is deterministic. More on that later. . This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/50019#Comment_50019",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4973
https://github.com/broadinstitute/gatk/issues/4973:157,Testability,log,logged,157,"----; User Report; ----. We are also experiencing a problem wherein GATK 4.0.5.1 GenotypeGVCFs processes hang for many hours. . The last thing our processes logged was the same as reported here:; ```; 08:48:23.075 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),0.044110324,Cpu time(s),0.026897973000000006; ```. This particular job ran for about 3m before outputting this line, then stayed running (but apparently doing nothing) for 8 hours before we killed it. . It is one of 1996 jobs that all did pretty much exactly the same thing in a similar time frame - in all cases these were the last two lines logged but GATK failed to terminate afterwards. At the same time, we did have about 8k jobs finish successfully and exit 0, so it appears that the rate at which this happens is (at least for our workload) is around 20%. Don't know yet whether or not this behaviour is deterministic. More on that later. . This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/50019#Comment_50019",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4973
https://github.com/broadinstitute/gatk/issues/4973:669,Testability,log,logged,669,"----; User Report; ----. We are also experiencing a problem wherein GATK 4.0.5.1 GenotypeGVCFs processes hang for many hours. . The last thing our processes logged was the same as reported here:; ```; 08:48:23.075 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),0.044110324,Cpu time(s),0.026897973000000006; ```. This particular job ran for about 3m before outputting this line, then stayed running (but apparently doing nothing) for 8 hours before we killed it. . It is one of 1996 jobs that all did pretty much exactly the same thing in a similar time frame - in all cases these were the last two lines logged but GATK failed to terminate afterwards. At the same time, we did have about 8k jobs finish successfully and exit 0, so it appears that the rate at which this happens is (at least for our workload) is around 20%. Don't know yet whether or not this behaviour is deterministic. More on that later. . This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/50019#Comment_50019",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4973
https://github.com/broadinstitute/gatk/issues/4974:164,Modifiability,refactor,refactored,164,"## Feature request. ### Tool(s) or class(es) involved; Funcotator. ### Description. Currently, there is a Caching mechanism in VcfFuncotationFactory. This could be refactored into a separate component and shared with other funcotation factories. Not all funcotation factories would benefit from this cache, but certainly LocatableXsvFuncotationFactory would. If the refactoring is to put the cache into DataSourceFuncotationFactory, then there needs to be a mechanism for subclasses to disable it. Additionally, the cache should be made into a separate class that can be used from DataSourceFuncotationFactory. Just to encapsulate the cache functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4974
https://github.com/broadinstitute/gatk/issues/4974:366,Modifiability,refactor,refactoring,366,"## Feature request. ### Tool(s) or class(es) involved; Funcotator. ### Description. Currently, there is a Caching mechanism in VcfFuncotationFactory. This could be refactored into a separate component and shared with other funcotation factories. Not all funcotation factories would benefit from this cache, but certainly LocatableXsvFuncotationFactory would. If the refactoring is to put the cache into DataSourceFuncotationFactory, then there needs to be a mechanism for subclasses to disable it. Additionally, the cache should be made into a separate class that can be used from DataSourceFuncotationFactory. Just to encapsulate the cache functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4974
https://github.com/broadinstitute/gatk/issues/4974:300,Performance,cache,cache,300,"## Feature request. ### Tool(s) or class(es) involved; Funcotator. ### Description. Currently, there is a Caching mechanism in VcfFuncotationFactory. This could be refactored into a separate component and shared with other funcotation factories. Not all funcotation factories would benefit from this cache, but certainly LocatableXsvFuncotationFactory would. If the refactoring is to put the cache into DataSourceFuncotationFactory, then there needs to be a mechanism for subclasses to disable it. Additionally, the cache should be made into a separate class that can be used from DataSourceFuncotationFactory. Just to encapsulate the cache functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4974
https://github.com/broadinstitute/gatk/issues/4974:392,Performance,cache,cache,392,"## Feature request. ### Tool(s) or class(es) involved; Funcotator. ### Description. Currently, there is a Caching mechanism in VcfFuncotationFactory. This could be refactored into a separate component and shared with other funcotation factories. Not all funcotation factories would benefit from this cache, but certainly LocatableXsvFuncotationFactory would. If the refactoring is to put the cache into DataSourceFuncotationFactory, then there needs to be a mechanism for subclasses to disable it. Additionally, the cache should be made into a separate class that can be used from DataSourceFuncotationFactory. Just to encapsulate the cache functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4974
https://github.com/broadinstitute/gatk/issues/4974:516,Performance,cache,cache,516,"## Feature request. ### Tool(s) or class(es) involved; Funcotator. ### Description. Currently, there is a Caching mechanism in VcfFuncotationFactory. This could be refactored into a separate component and shared with other funcotation factories. Not all funcotation factories would benefit from this cache, but certainly LocatableXsvFuncotationFactory would. If the refactoring is to put the cache into DataSourceFuncotationFactory, then there needs to be a mechanism for subclasses to disable it. Additionally, the cache should be made into a separate class that can be used from DataSourceFuncotationFactory. Just to encapsulate the cache functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4974
https://github.com/broadinstitute/gatk/issues/4974:635,Performance,cache,cache,635,"## Feature request. ### Tool(s) or class(es) involved; Funcotator. ### Description. Currently, there is a Caching mechanism in VcfFuncotationFactory. This could be refactored into a separate component and shared with other funcotation factories. Not all funcotation factories would benefit from this cache, but certainly LocatableXsvFuncotationFactory would. If the refactoring is to put the cache into DataSourceFuncotationFactory, then there needs to be a mechanism for subclasses to disable it. Additionally, the cache should be made into a separate class that can be used from DataSourceFuncotationFactory. Just to encapsulate the cache functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4974
https://github.com/broadinstitute/gatk/issues/4975:19,Availability,error,error,19,"Researcher reports error in GenotypeGVCFs that uses a GenomicsDB database using v4.0.5.0. Removing `new qual` param OR using v4.0.4.0 allows the command to run without error. I saw a similar error with v4.0.5.1 when I tried to add `new qual` to a workshop hands-on tutorial GenotypeGVCFs step using a GenomicsDB database. ---; Hi,. I am trying to process locally 260 WES gvcf through joint discovery wdl pipeline. I encountered an error at GenotypeGVCFs below which I am not sure how to proceed. I have used all the default reference libraries and only modified the merge_count in the script to be 8144 so that my server resources won't be maxout fully in the ImportGVCFs step. . [https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl ""https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl""). ```; 23:17:43.992 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 23:17:44.064 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 23:17:46.334 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),30.197597194999727,Cpu time(s),28.791204838999864; [June 25, 2018 11:17:46 PM UTC] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 4.01 minutes.; Runtime.totalMemory()=5354029056; java.lang.IllegalArgumentException: log10LikelihoodsOfAC are bad 2.559797571100845E-21,NaN; 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AFCalculationResult.<init>(AFCalculationResult.java:72); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator.getLog10PNonRef(AlleleFrequencyCalculator.java:143); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotype",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975
https://github.com/broadinstitute/gatk/issues/4975:168,Availability,error,error,168,"Researcher reports error in GenotypeGVCFs that uses a GenomicsDB database using v4.0.5.0. Removing `new qual` param OR using v4.0.4.0 allows the command to run without error. I saw a similar error with v4.0.5.1 when I tried to add `new qual` to a workshop hands-on tutorial GenotypeGVCFs step using a GenomicsDB database. ---; Hi,. I am trying to process locally 260 WES gvcf through joint discovery wdl pipeline. I encountered an error at GenotypeGVCFs below which I am not sure how to proceed. I have used all the default reference libraries and only modified the merge_count in the script to be 8144 so that my server resources won't be maxout fully in the ImportGVCFs step. . [https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl ""https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl""). ```; 23:17:43.992 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 23:17:44.064 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 23:17:46.334 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),30.197597194999727,Cpu time(s),28.791204838999864; [June 25, 2018 11:17:46 PM UTC] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 4.01 minutes.; Runtime.totalMemory()=5354029056; java.lang.IllegalArgumentException: log10LikelihoodsOfAC are bad 2.559797571100845E-21,NaN; 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AFCalculationResult.<init>(AFCalculationResult.java:72); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator.getLog10PNonRef(AlleleFrequencyCalculator.java:143); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotype",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975
https://github.com/broadinstitute/gatk/issues/4975:191,Availability,error,error,191,"Researcher reports error in GenotypeGVCFs that uses a GenomicsDB database using v4.0.5.0. Removing `new qual` param OR using v4.0.4.0 allows the command to run without error. I saw a similar error with v4.0.5.1 when I tried to add `new qual` to a workshop hands-on tutorial GenotypeGVCFs step using a GenomicsDB database. ---; Hi,. I am trying to process locally 260 WES gvcf through joint discovery wdl pipeline. I encountered an error at GenotypeGVCFs below which I am not sure how to proceed. I have used all the default reference libraries and only modified the merge_count in the script to be 8144 so that my server resources won't be maxout fully in the ImportGVCFs step. . [https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl ""https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl""). ```; 23:17:43.992 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 23:17:44.064 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 23:17:46.334 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),30.197597194999727,Cpu time(s),28.791204838999864; [June 25, 2018 11:17:46 PM UTC] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 4.01 minutes.; Runtime.totalMemory()=5354029056; java.lang.IllegalArgumentException: log10LikelihoodsOfAC are bad 2.559797571100845E-21,NaN; 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AFCalculationResult.<init>(AFCalculationResult.java:72); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator.getLog10PNonRef(AlleleFrequencyCalculator.java:143); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotype",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975
https://github.com/broadinstitute/gatk/issues/4975:431,Availability,error,error,431,"Researcher reports error in GenotypeGVCFs that uses a GenomicsDB database using v4.0.5.0. Removing `new qual` param OR using v4.0.4.0 allows the command to run without error. I saw a similar error with v4.0.5.1 when I tried to add `new qual` to a workshop hands-on tutorial GenotypeGVCFs step using a GenomicsDB database. ---; Hi,. I am trying to process locally 260 WES gvcf through joint discovery wdl pipeline. I encountered an error at GenotypeGVCFs below which I am not sure how to proceed. I have used all the default reference libraries and only modified the merge_count in the script to be 8144 so that my server resources won't be maxout fully in the ImportGVCFs step. . [https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl ""https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl""). ```; 23:17:43.992 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 23:17:44.064 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 23:17:46.334 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),30.197597194999727,Cpu time(s),28.791204838999864; [June 25, 2018 11:17:46 PM UTC] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 4.01 minutes.; Runtime.totalMemory()=5354029056; java.lang.IllegalArgumentException: log10LikelihoodsOfAC are bad 2.559797571100845E-21,NaN; 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AFCalculationResult.<init>(AFCalculationResult.java:72); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator.getLog10PNonRef(AlleleFrequencyCalculator.java:143); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotype",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975
https://github.com/broadinstitute/gatk/issues/4975:1259,Availability,down,down,1259,"CFs step using a GenomicsDB database. ---; Hi,. I am trying to process locally 260 WES gvcf through joint discovery wdl pipeline. I encountered an error at GenotypeGVCFs below which I am not sure how to proceed. I have used all the default reference libraries and only modified the merge_count in the script to be 8144 so that my server resources won't be maxout fully in the ImportGVCFs step. . [https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl ""https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl""). ```; 23:17:43.992 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 23:17:44.064 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 23:17:46.334 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),30.197597194999727,Cpu time(s),28.791204838999864; [June 25, 2018 11:17:46 PM UTC] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 4.01 minutes.; Runtime.totalMemory()=5354029056; java.lang.IllegalArgumentException: log10LikelihoodsOfAC are bad 2.559797571100845E-21,NaN; 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AFCalculationResult.<init>(AFCalculationResult.java:72); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator.getLog10PNonRef(AlleleFrequencyCalculator.java:143); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:255); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:210); 	at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.calculateGenotypes(GenotypeGVCFs.java:266); 	at org.broadinstitute.hel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975
https://github.com/broadinstitute/gatk/issues/4975:404,Deployability,pipeline,pipeline,404,"Researcher reports error in GenotypeGVCFs that uses a GenomicsDB database using v4.0.5.0. Removing `new qual` param OR using v4.0.4.0 allows the command to run without error. I saw a similar error with v4.0.5.1 when I tried to add `new qual` to a workshop hands-on tutorial GenotypeGVCFs step using a GenomicsDB database. ---; Hi,. I am trying to process locally 260 WES gvcf through joint discovery wdl pipeline. I encountered an error at GenotypeGVCFs below which I am not sure how to proceed. I have used all the default reference libraries and only modified the merge_count in the script to be 8144 so that my server resources won't be maxout fully in the ImportGVCFs step. . [https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl ""https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl""). ```; 23:17:43.992 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 23:17:44.064 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 23:17:46.334 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),30.197597194999727,Cpu time(s),28.791204838999864; [June 25, 2018 11:17:46 PM UTC] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 4.01 minutes.; Runtime.totalMemory()=5354029056; java.lang.IllegalArgumentException: log10LikelihoodsOfAC are bad 2.559797571100845E-21,NaN; 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AFCalculationResult.<init>(AFCalculationResult.java:72); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator.getLog10PNonRef(AlleleFrequencyCalculator.java:143); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotype",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975
https://github.com/broadinstitute/gatk/issues/4975:4477,Deployability,pipeline,pipeline,4477,ps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:107); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:980); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /gatk/build/libs/gatk-package-4.0.5.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx5g -Xms5g -jar /gatk/build/libs/gatk-package-4.0.5.0-local.jar GenotypeGVCFs -R /cromwell-executions/JointGenotyping/3a733624-60a0-4e57-b7f5-7b7e99795738/call-GenotypeGVCFs/shard-25/inputs/data1/bill/pipeline/cromwell/hg38/Homo_sapiens_assembly38.fasta -O output.vcf.gz -D /cromwell-executions/JointGenotyping/3a733624-60a0-4e57-b7f5-7b7e99795738/call-GenotypeGVCFs/shard-25/inputs/data1/bill/pipeline/cromwell/hg38/Homo_sapiens_assembly38.dbsnp138.vcf -G StandardAnnotation --only-output-calls-starting-in-intervals --use-new-qual-calculator -V gendb://genomicsdb -L chrY:10821870-11717714; ```; This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/12283/running-260-wes-samples-through-joint-discovery-and-vqsr/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975
https://github.com/broadinstitute/gatk/issues/4975:4670,Deployability,pipeline,pipeline,4670,ps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:107); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:980); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /gatk/build/libs/gatk-package-4.0.5.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx5g -Xms5g -jar /gatk/build/libs/gatk-package-4.0.5.0-local.jar GenotypeGVCFs -R /cromwell-executions/JointGenotyping/3a733624-60a0-4e57-b7f5-7b7e99795738/call-GenotypeGVCFs/shard-25/inputs/data1/bill/pipeline/cromwell/hg38/Homo_sapiens_assembly38.fasta -O output.vcf.gz -D /cromwell-executions/JointGenotyping/3a733624-60a0-4e57-b7f5-7b7e99795738/call-GenotypeGVCFs/shard-25/inputs/data1/bill/pipeline/cromwell/hg38/Homo_sapiens_assembly38.dbsnp138.vcf -G StandardAnnotation --only-output-calls-starting-in-intervals --use-new-qual-calculator -V gendb://genomicsdb -L chrY:10821870-11717714; ```; This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/12283/running-260-wes-samples-through-joint-discovery-and-vqsr/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975
https://github.com/broadinstitute/gatk/issues/4975:2976,Integrability,wrap,wrapAndCopyInto,2976,eGenotypes(GenotypingEngine.java:255); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:210); 	at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.calculateGenotypes(GenotypeGVCFs.java:266); 	at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.regenotypeVC(GenotypeGVCFs.java:222); 	at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.apply(GenotypeGVCFs.java:201); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:109); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:107); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:980); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975
https://github.com/broadinstitute/gatk/pull/4977:409,Performance,bottleneck,bottleneck,409,"- When querying VCFs, the VcfFuncotationFactory will consider the allele (i.e. Number=""R"" and ""A"") in the output. This allows single-allele queries hitting datasource multiallelic variant contexts to be rendered properly. Closes #4957 ; - Added very simple caching to VCF FuncotationFactory; - VCF Funcotation factory can recognize when alleles are not exactly the same, but equivalent. ; - Fixed small speed bottleneck where Set.equals(...) could be used instead of more complex method. This was happening in validation of funcotation metadata.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4977
https://github.com/broadinstitute/gatk/pull/4977:510,Security,validat,validation,510,"- When querying VCFs, the VcfFuncotationFactory will consider the allele (i.e. Number=""R"" and ""A"") in the output. This allows single-allele queries hitting datasource multiallelic variant contexts to be rendered properly. Closes #4957 ; - Added very simple caching to VCF FuncotationFactory; - VCF Funcotation factory can recognize when alleles are not exactly the same, but equivalent. ; - Fixed small speed bottleneck where Set.equals(...) could be used instead of more complex method. This was happening in validation of funcotation metadata.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4977
https://github.com/broadinstitute/gatk/pull/4977:250,Usability,simpl,simple,250,"- When querying VCFs, the VcfFuncotationFactory will consider the allele (i.e. Number=""R"" and ""A"") in the output. This allows single-allele queries hitting datasource multiallelic variant contexts to be rendered properly. Closes #4957 ; - Added very simple caching to VCF FuncotationFactory; - VCF Funcotation factory can recognize when alleles are not exactly the same, but equivalent. ; - Fixed small speed bottleneck where Set.equals(...) could be used instead of more complex method. This was happening in validation of funcotation metadata.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4977
https://github.com/broadinstitute/gatk/issues/4978:283,Safety,detect,detect,283,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_. ### Description. If a user creates a data source for the `hg19` reference and erroneously uses `b37` contig names, Funcotator will not produce annotations. This is correct behavior. In this case, Funcotator should detect that the data source is not `hg19-compliant` and issue a warning to the user. This can be done by checking the first record in every data source and then making sure that the contig name appears in the hg19 contig name list.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4978
https://github.com/broadinstitute/gatk/issues/4979:85,Performance,perform,perform,85,"`BaseQualityRankSumTest` gets the base qualities of read at the variant position and perform a rank sum test. Reads that are uninformative are skipped. For SNPs, this means that the annotation uses the base quality at the SNP position. So far, so good. Indels are surprising. Lets say we have a deletion ATTTCCCGGG -> A, where the position of A is 10. If we have a read that starts with ATTTCCCGGG, the base quality used is at the A. If the read starts with CCCGGG (note that this is informative because the deletion allele doesn't have CCCGGG and thus the read supports the ref even though it doesn't span it) it is assigned the base quality at the first C, etc. Insertions are similar. Suppose we have A -> ATTTCCCGGG. Then a read starting inside the TTTCCCGGG is informative but not spanning and gets annotated with the base quality at its first base. This behavior should go in the javadoc. If you're ambitious, you could ask if this behavior is desirable and potentially change it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4979
https://github.com/broadinstitute/gatk/issues/4979:104,Testability,test,test,104,"`BaseQualityRankSumTest` gets the base qualities of read at the variant position and perform a rank sum test. Reads that are uninformative are skipped. For SNPs, this means that the annotation uses the base quality at the SNP position. So far, so good. Indels are surprising. Lets say we have a deletion ATTTCCCGGG -> A, where the position of A is 10. If we have a read that starts with ATTTCCCGGG, the base quality used is at the A. If the read starts with CCCGGG (note that this is informative because the deletion allele doesn't have CCCGGG and thus the read supports the ref even though it doesn't span it) it is assigned the base quality at the first C, etc. Insertions are similar. Suppose we have A -> ATTTCCCGGG. Then a read starting inside the TTTCCCGGG is informative but not spanning and gets annotated with the base quality at its first base. This behavior should go in the javadoc. If you're ambitious, you could ask if this behavior is desirable and potentially change it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4979
https://github.com/broadinstitute/gatk/pull/4980:34,Testability,test,tests,34,@droazen Can you review this once tests pass?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4980
https://github.com/broadinstitute/gatk/pull/4981:153,Modifiability,refactor,refactor,153,"There's such feature for GATKTool, but not yet for GATKSparkTool. Much of the code is copied from the GATKTool version; Engine team, please comment if a refactor is needed and how. Thanks!; (Tagging @droazen @lbergelson and @cmnbroad )",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4981
https://github.com/broadinstitute/gatk/pull/4982:55,Security,validat,validation,55,"With this addition, summarizing the results of the MC3 validation will use more GATK, less scripting.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4982
https://github.com/broadinstitute/gatk/issues/4983:24,Deployability,integrat,integration,24,"Currently the travis CI integration test task is the longest running task, which currently clocks in uncomfortably close to 70 minute hard limit on task runtime. One solution to this would be to split off a second job encompassing some of the longest running integration tests, namely a `VariantCallingIntegrationTests` tag. If we wanted to split the integration tests further then we could also consider splitting off the spark integration tests, which also take some time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4983
https://github.com/broadinstitute/gatk/issues/4983:259,Deployability,integrat,integration,259,"Currently the travis CI integration test task is the longest running task, which currently clocks in uncomfortably close to 70 minute hard limit on task runtime. One solution to this would be to split off a second job encompassing some of the longest running integration tests, namely a `VariantCallingIntegrationTests` tag. If we wanted to split the integration tests further then we could also consider splitting off the spark integration tests, which also take some time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4983
https://github.com/broadinstitute/gatk/issues/4983:351,Deployability,integrat,integration,351,"Currently the travis CI integration test task is the longest running task, which currently clocks in uncomfortably close to 70 minute hard limit on task runtime. One solution to this would be to split off a second job encompassing some of the longest running integration tests, namely a `VariantCallingIntegrationTests` tag. If we wanted to split the integration tests further then we could also consider splitting off the spark integration tests, which also take some time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4983
https://github.com/broadinstitute/gatk/issues/4983:429,Deployability,integrat,integration,429,"Currently the travis CI integration test task is the longest running task, which currently clocks in uncomfortably close to 70 minute hard limit on task runtime. One solution to this would be to split off a second job encompassing some of the longest running integration tests, namely a `VariantCallingIntegrationTests` tag. If we wanted to split the integration tests further then we could also consider splitting off the spark integration tests, which also take some time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4983
https://github.com/broadinstitute/gatk/issues/4983:24,Integrability,integrat,integration,24,"Currently the travis CI integration test task is the longest running task, which currently clocks in uncomfortably close to 70 minute hard limit on task runtime. One solution to this would be to split off a second job encompassing some of the longest running integration tests, namely a `VariantCallingIntegrationTests` tag. If we wanted to split the integration tests further then we could also consider splitting off the spark integration tests, which also take some time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4983
https://github.com/broadinstitute/gatk/issues/4983:259,Integrability,integrat,integration,259,"Currently the travis CI integration test task is the longest running task, which currently clocks in uncomfortably close to 70 minute hard limit on task runtime. One solution to this would be to split off a second job encompassing some of the longest running integration tests, namely a `VariantCallingIntegrationTests` tag. If we wanted to split the integration tests further then we could also consider splitting off the spark integration tests, which also take some time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4983
https://github.com/broadinstitute/gatk/issues/4983:351,Integrability,integrat,integration,351,"Currently the travis CI integration test task is the longest running task, which currently clocks in uncomfortably close to 70 minute hard limit on task runtime. One solution to this would be to split off a second job encompassing some of the longest running integration tests, namely a `VariantCallingIntegrationTests` tag. If we wanted to split the integration tests further then we could also consider splitting off the spark integration tests, which also take some time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4983
https://github.com/broadinstitute/gatk/issues/4983:429,Integrability,integrat,integration,429,"Currently the travis CI integration test task is the longest running task, which currently clocks in uncomfortably close to 70 minute hard limit on task runtime. One solution to this would be to split off a second job encompassing some of the longest running integration tests, namely a `VariantCallingIntegrationTests` tag. If we wanted to split the integration tests further then we could also consider splitting off the spark integration tests, which also take some time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4983
https://github.com/broadinstitute/gatk/issues/4983:36,Testability,test,test,36,"Currently the travis CI integration test task is the longest running task, which currently clocks in uncomfortably close to 70 minute hard limit on task runtime. One solution to this would be to split off a second job encompassing some of the longest running integration tests, namely a `VariantCallingIntegrationTests` tag. If we wanted to split the integration tests further then we could also consider splitting off the spark integration tests, which also take some time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4983
https://github.com/broadinstitute/gatk/issues/4983:271,Testability,test,tests,271,"Currently the travis CI integration test task is the longest running task, which currently clocks in uncomfortably close to 70 minute hard limit on task runtime. One solution to this would be to split off a second job encompassing some of the longest running integration tests, namely a `VariantCallingIntegrationTests` tag. If we wanted to split the integration tests further then we could also consider splitting off the spark integration tests, which also take some time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4983
https://github.com/broadinstitute/gatk/issues/4983:363,Testability,test,tests,363,"Currently the travis CI integration test task is the longest running task, which currently clocks in uncomfortably close to 70 minute hard limit on task runtime. One solution to this would be to split off a second job encompassing some of the longest running integration tests, namely a `VariantCallingIntegrationTests` tag. If we wanted to split the integration tests further then we could also consider splitting off the spark integration tests, which also take some time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4983
https://github.com/broadinstitute/gatk/issues/4983:441,Testability,test,tests,441,"Currently the travis CI integration test task is the longest running task, which currently clocks in uncomfortably close to 70 minute hard limit on task runtime. One solution to this would be to split off a second job encompassing some of the longest running integration tests, namely a `VariantCallingIntegrationTests` tag. If we wanted to split the integration tests further then we could also consider splitting off the spark integration tests, which also take some time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4983
https://github.com/broadinstitute/gatk/issues/4987:47,Availability,error,errors,47,"Researcher reports that GATK4 VariantAnnotator errors with a PED file that works fine with CalculateGenotypePosteriors. The same VariantAnnotator command and PED file work fine in GATK3. ---; Hi @shlee ,. We have ped files. And it worked fine for CalculateGenotypePosteriors too. But not with VariantAnnotator. So we downraded to GATK3 with same files on same parameters in order to finish that part - we hope calculations are same. Thank You!; Sergey. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/50327#Comment_50327",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4987
https://github.com/broadinstitute/gatk/issues/4987:317,Availability,down,downraded,317,"Researcher reports that GATK4 VariantAnnotator errors with a PED file that works fine with CalculateGenotypePosteriors. The same VariantAnnotator command and PED file work fine in GATK3. ---; Hi @shlee ,. We have ped files. And it worked fine for CalculateGenotypePosteriors too. But not with VariantAnnotator. So we downraded to GATK3 with same files on same parameters in order to finish that part - we hope calculations are same. Thank You!; Sergey. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/50327#Comment_50327",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4987
https://github.com/broadinstitute/gatk/issues/4989:370,Deployability,configurat,configuration,370,When we refactored the docker image to no longer contain the source directory we found that the changes had the side effect of causing the tests inside of the docker image to execute more slowly (~10 minutes per docker test). We solved this problem by further splitting the tests up but this is only a temporary solution. It is worth figuring out what about this gradle configuration is slow to save everyone time.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4989
https://github.com/broadinstitute/gatk/issues/4989:8,Modifiability,refactor,refactored,8,When we refactored the docker image to no longer contain the source directory we found that the changes had the side effect of causing the tests inside of the docker image to execute more slowly (~10 minutes per docker test). We solved this problem by further splitting the tests up but this is only a temporary solution. It is worth figuring out what about this gradle configuration is slow to save everyone time.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4989
https://github.com/broadinstitute/gatk/issues/4989:370,Modifiability,config,configuration,370,When we refactored the docker image to no longer contain the source directory we found that the changes had the side effect of causing the tests inside of the docker image to execute more slowly (~10 minutes per docker test). We solved this problem by further splitting the tests up but this is only a temporary solution. It is worth figuring out what about this gradle configuration is slow to save everyone time.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4989
https://github.com/broadinstitute/gatk/issues/4989:139,Testability,test,tests,139,When we refactored the docker image to no longer contain the source directory we found that the changes had the side effect of causing the tests inside of the docker image to execute more slowly (~10 minutes per docker test). We solved this problem by further splitting the tests up but this is only a temporary solution. It is worth figuring out what about this gradle configuration is slow to save everyone time.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4989
https://github.com/broadinstitute/gatk/issues/4989:219,Testability,test,test,219,When we refactored the docker image to no longer contain the source directory we found that the changes had the side effect of causing the tests inside of the docker image to execute more slowly (~10 minutes per docker test). We solved this problem by further splitting the tests up but this is only a temporary solution. It is worth figuring out what about this gradle configuration is slow to save everyone time.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4989
https://github.com/broadinstitute/gatk/issues/4989:274,Testability,test,tests,274,When we refactored the docker image to no longer contain the source directory we found that the changes had the side effect of causing the tests inside of the docker image to execute more slowly (~10 minutes per docker test). We solved this problem by further splitting the tests up but this is only a temporary solution. It is worth figuring out what about this gradle configuration is slow to save everyone time.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4989
https://github.com/broadinstitute/gatk/issues/4990:103,Deployability,integrat,integration,103,"We recently created the ""variantcalling"" task in the travis CI test suite to reduce the runtime of our integration tests. Once we refactored the docker image we found that the the integration tests are still taking an uncomfortable amount of time to run (upwards of 57 minutes). Short of resolving the issue more permanently (#4989) we can temporarily fix the solution by just splitting off more of the integration tests to other jobs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4990
https://github.com/broadinstitute/gatk/issues/4990:180,Deployability,integrat,integration,180,"We recently created the ""variantcalling"" task in the travis CI test suite to reduce the runtime of our integration tests. Once we refactored the docker image we found that the the integration tests are still taking an uncomfortable amount of time to run (upwards of 57 minutes). Short of resolving the issue more permanently (#4989) we can temporarily fix the solution by just splitting off more of the integration tests to other jobs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4990
https://github.com/broadinstitute/gatk/issues/4990:403,Deployability,integrat,integration,403,"We recently created the ""variantcalling"" task in the travis CI test suite to reduce the runtime of our integration tests. Once we refactored the docker image we found that the the integration tests are still taking an uncomfortable amount of time to run (upwards of 57 minutes). Short of resolving the issue more permanently (#4989) we can temporarily fix the solution by just splitting off more of the integration tests to other jobs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4990
https://github.com/broadinstitute/gatk/issues/4990:77,Energy Efficiency,reduce,reduce,77,"We recently created the ""variantcalling"" task in the travis CI test suite to reduce the runtime of our integration tests. Once we refactored the docker image we found that the the integration tests are still taking an uncomfortable amount of time to run (upwards of 57 minutes). Short of resolving the issue more permanently (#4989) we can temporarily fix the solution by just splitting off more of the integration tests to other jobs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4990
https://github.com/broadinstitute/gatk/issues/4990:103,Integrability,integrat,integration,103,"We recently created the ""variantcalling"" task in the travis CI test suite to reduce the runtime of our integration tests. Once we refactored the docker image we found that the the integration tests are still taking an uncomfortable amount of time to run (upwards of 57 minutes). Short of resolving the issue more permanently (#4989) we can temporarily fix the solution by just splitting off more of the integration tests to other jobs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4990
https://github.com/broadinstitute/gatk/issues/4990:180,Integrability,integrat,integration,180,"We recently created the ""variantcalling"" task in the travis CI test suite to reduce the runtime of our integration tests. Once we refactored the docker image we found that the the integration tests are still taking an uncomfortable amount of time to run (upwards of 57 minutes). Short of resolving the issue more permanently (#4989) we can temporarily fix the solution by just splitting off more of the integration tests to other jobs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4990
https://github.com/broadinstitute/gatk/issues/4990:403,Integrability,integrat,integration,403,"We recently created the ""variantcalling"" task in the travis CI test suite to reduce the runtime of our integration tests. Once we refactored the docker image we found that the the integration tests are still taking an uncomfortable amount of time to run (upwards of 57 minutes). Short of resolving the issue more permanently (#4989) we can temporarily fix the solution by just splitting off more of the integration tests to other jobs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4990
https://github.com/broadinstitute/gatk/issues/4990:130,Modifiability,refactor,refactored,130,"We recently created the ""variantcalling"" task in the travis CI test suite to reduce the runtime of our integration tests. Once we refactored the docker image we found that the the integration tests are still taking an uncomfortable amount of time to run (upwards of 57 minutes). Short of resolving the issue more permanently (#4989) we can temporarily fix the solution by just splitting off more of the integration tests to other jobs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4990
https://github.com/broadinstitute/gatk/issues/4990:63,Testability,test,test,63,"We recently created the ""variantcalling"" task in the travis CI test suite to reduce the runtime of our integration tests. Once we refactored the docker image we found that the the integration tests are still taking an uncomfortable amount of time to run (upwards of 57 minutes). Short of resolving the issue more permanently (#4989) we can temporarily fix the solution by just splitting off more of the integration tests to other jobs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4990
https://github.com/broadinstitute/gatk/issues/4990:115,Testability,test,tests,115,"We recently created the ""variantcalling"" task in the travis CI test suite to reduce the runtime of our integration tests. Once we refactored the docker image we found that the the integration tests are still taking an uncomfortable amount of time to run (upwards of 57 minutes). Short of resolving the issue more permanently (#4989) we can temporarily fix the solution by just splitting off more of the integration tests to other jobs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4990
https://github.com/broadinstitute/gatk/issues/4990:192,Testability,test,tests,192,"We recently created the ""variantcalling"" task in the travis CI test suite to reduce the runtime of our integration tests. Once we refactored the docker image we found that the the integration tests are still taking an uncomfortable amount of time to run (upwards of 57 minutes). Short of resolving the issue more permanently (#4989) we can temporarily fix the solution by just splitting off more of the integration tests to other jobs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4990
https://github.com/broadinstitute/gatk/issues/4990:415,Testability,test,tests,415,"We recently created the ""variantcalling"" task in the travis CI test suite to reduce the runtime of our integration tests. Once we refactored the docker image we found that the the integration tests are still taking an uncomfortable amount of time to run (upwards of 57 minutes). Short of resolving the issue more permanently (#4989) we can temporarily fix the solution by just splitting off more of the integration tests to other jobs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4990
https://github.com/broadinstitute/gatk/pull/4991:113,Usability,feedback,feedback,113,"The tool is currently hard-coded to work with funcotations from v3 of the AoU data source, but opening for early feedback / discussion of the best way to generalize things.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4991
https://github.com/broadinstitute/gatk/pull/4993:121,Availability,ping,ping,121,Sample code to show how to modify INFO field combine operation while querying GenomicsDB using the Protobuf API. #4541 . ping @ldgauthier @droazen @lbergelson @jamesemery,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4993
https://github.com/broadinstitute/gatk/issues/4994:284,Availability,error,error,284,"@kgururaj We got this issue report in the forum, could you please look into it? Thanks!. https://gatkforums.broadinstitute.org/gatk/discussion/12388/how-to-use-multi-interval-in-genomicsdbimport-with-gatk-4-0-6-0. ----. I used the GenomicsDBImport with a interval list file and got a error like below.; So what is the correct way to use Multi-interval in GenomicsDBImport?. gatk version: 4.0.6.0. Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -Xms4g -jar /mnt/gatk/gatk-4.0.6.0/gatk-package-4.0.6.0-local.jar GenomicsDBImport -L test.intervals --genomicsdb-workspace-path ../RAW_VCF/my_database -V file1 -V file2 -V file3. 02:57:15.591 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/workshop/xinchen.pan/test/gatk/gatk-4.0.6.0/gatk-package-4.0.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 02:57:15.772 INFO GenomicsDBImport - ------------------------------------------------------------; 02:57:15.772 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.6.0; 02:57:15.772 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 02:57:15.772 INFO GenomicsDBImport - Executing as on Linux v3.10.0-514.6.1.el7.x86_64 amd64; 02:57:15.772 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b13; 02:57:15.773 INFO GenomicsDBImport - Start Date/Time: July 10, 2018 2:57:15 AM EDT; 02:57:15.773 INFO GenomicsDBImport - ------------------------------------------------------------; 02:57:15.773 INFO GenomicsDBImport - ------------------------------------------------------------; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Version: 2.16.0; 02:57:15.773 INFO GenomicsDBImport - Picard Version: 2.18.7; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_S",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4994
https://github.com/broadinstitute/gatk/issues/4994:3460,Availability,down,down,3460,"bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 02:57:15.774 INFO GenomicsDBImport - Initializing engine; 02:57:18.389 INFO IntervalArgumentCollection - Processing 11228744 bp from intervals; 02:57:18.437 INFO GenomicsDBImport - Done initializing engine; Created workspace ../RAW_VCF/my_database; 02:57:18.583 INFO GenomicsDBImport - Vid Map JSON file will be written to ../RAW_VCF/my_database/vidmap.json; 02:57:18.583 INFO GenomicsDBImport - Callset Map JSON file will be written to ../RAW_VCF/my_database/callset.json; 02:57:18.583 INFO GenomicsDBImport - Complete VCF Header will be written to ../RAW_VCF/my_database/vcfheader.vcf; 02:57:18.583 INFO GenomicsDBImport - Importing to array - ../RAW_VCF/my_database/genomicsdb_array; 02:57:18.583 INFO ProgressMeter - Starting traversal; 02:57:18.583 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 02:57:31.082 INFO GenomicsDBImport - Shutting down engine; [July 10, 2018 2:57:31 AM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.26 minutes.; Runtime.totalMemory()=4116185088; Exception in thread ""main"" java.lang.StackOverflowError; 	at com.intel.genomicsdb.model.ImportConfig.isThereChromosomeIntervalIntersection(ImportConfig.java:95); 	at com.intel.genomicsdb.model.ImportConfig.isThereChromosomeIntervalIntersection(ImportConfig.java:104); 	at com.intel.genomicsdb.model.ImportConfig.isThereChromosomeIntervalIntersection(ImportConfig.java:104); 	at com.intel.genomicsdb.model.ImportConfig.isThereChromosomeIntervalIntersection(ImportConfig.java:104); 	at com.intel.genomicsdb.model.ImportConfig.isThereChromosomeIntervalIntersection(ImportConfig.java:104); 	at com.intel.genomicsdb.model.ImportConfig.isThereChromosomeIntervalIntersection(ImportConfig.java:104); 	at com.intel.genomicsdb.model.ImportConfig.isThereChromosomeIntervalIntersection(ImportConfig.java:104). This Issue was generated from y",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4994
https://github.com/broadinstitute/gatk/issues/4994:2455,Deployability,patch,patch,2455," GenomicsDBImport - Start Date/Time: July 10, 2018 2:57:15 AM EDT; 02:57:15.773 INFO GenomicsDBImport - ------------------------------------------------------------; 02:57:15.773 INFO GenomicsDBImport - ------------------------------------------------------------; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Version: 2.16.0; 02:57:15.773 INFO GenomicsDBImport - Picard Version: 2.18.7; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 02:57:15.774 INFO GenomicsDBImport - Deflater: IntelDeflater; 02:57:15.774 INFO GenomicsDBImport - Inflater: IntelInflater; 02:57:15.774 INFO GenomicsDBImport - GCS max retries/reopens: 20; 02:57:15.774 INFO GenomicsDBImport - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 02:57:15.774 INFO GenomicsDBImport - Initializing engine; 02:57:18.389 INFO IntervalArgumentCollection - Processing 11228744 bp from intervals; 02:57:18.437 INFO GenomicsDBImport - Done initializing engine; Created workspace ../RAW_VCF/my_database; 02:57:18.583 INFO GenomicsDBImport - Vid Map JSON file will be written to ../RAW_VCF/my_database/vidmap.json; 02:57:18.583 INFO GenomicsDBImport - Callset Map JSON file will be written to ../RAW_VCF/my_database/callset.json; 02:57:18.583 INFO GenomicsDBImport - Complete VCF Header will be written to ../RAW_VCF/my_database/vcfheader.vcf; 02:57:18.583 INFO GenomicsDBImport - Importing to array - ../RAW_VCF/my_database/genomicsdb_array; 02:57:18.583 INFO ProgressMeter - Starting traversal; 02:57:18.583 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 02:57:31.082 INFO GenomicsDBImport - Shutting dow",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4994
https://github.com/broadinstitute/gatk/issues/4994:794,Performance,Load,Loading,794,"@kgururaj We got this issue report in the forum, could you please look into it? Thanks!. https://gatkforums.broadinstitute.org/gatk/discussion/12388/how-to-use-multi-interval-in-genomicsdbimport-with-gatk-4-0-6-0. ----. I used the GenomicsDBImport with a interval list file and got a error like below.; So what is the correct way to use Multi-interval in GenomicsDBImport?. gatk version: 4.0.6.0. Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -Xms4g -jar /mnt/gatk/gatk-4.0.6.0/gatk-package-4.0.6.0-local.jar GenomicsDBImport -L test.intervals --genomicsdb-workspace-path ../RAW_VCF/my_database -V file1 -V file2 -V file3. 02:57:15.591 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/workshop/xinchen.pan/test/gatk/gatk-4.0.6.0/gatk-package-4.0.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 02:57:15.772 INFO GenomicsDBImport - ------------------------------------------------------------; 02:57:15.772 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.6.0; 02:57:15.772 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 02:57:15.772 INFO GenomicsDBImport - Executing as on Linux v3.10.0-514.6.1.el7.x86_64 amd64; 02:57:15.772 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b13; 02:57:15.773 INFO GenomicsDBImport - Start Date/Time: July 10, 2018 2:57:15 AM EDT; 02:57:15.773 INFO GenomicsDBImport - ------------------------------------------------------------; 02:57:15.773 INFO GenomicsDBImport - ------------------------------------------------------------; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Version: 2.16.0; 02:57:15.773 INFO GenomicsDBImport - Picard Version: 2.18.7; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_S",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4994
https://github.com/broadinstitute/gatk/issues/4994:660,Testability,test,test,660,"@kgururaj We got this issue report in the forum, could you please look into it? Thanks!. https://gatkforums.broadinstitute.org/gatk/discussion/12388/how-to-use-multi-interval-in-genomicsdbimport-with-gatk-4-0-6-0. ----. I used the GenomicsDBImport with a interval list file and got a error like below.; So what is the correct way to use Multi-interval in GenomicsDBImport?. gatk version: 4.0.6.0. Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -Xms4g -jar /mnt/gatk/gatk-4.0.6.0/gatk-package-4.0.6.0-local.jar GenomicsDBImport -L test.intervals --genomicsdb-workspace-path ../RAW_VCF/my_database -V file1 -V file2 -V file3. 02:57:15.591 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/workshop/xinchen.pan/test/gatk/gatk-4.0.6.0/gatk-package-4.0.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 02:57:15.772 INFO GenomicsDBImport - ------------------------------------------------------------; 02:57:15.772 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.6.0; 02:57:15.772 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 02:57:15.772 INFO GenomicsDBImport - Executing as on Linux v3.10.0-514.6.1.el7.x86_64 amd64; 02:57:15.772 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b13; 02:57:15.773 INFO GenomicsDBImport - Start Date/Time: July 10, 2018 2:57:15 AM EDT; 02:57:15.773 INFO GenomicsDBImport - ------------------------------------------------------------; 02:57:15.773 INFO GenomicsDBImport - ------------------------------------------------------------; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Version: 2.16.0; 02:57:15.773 INFO GenomicsDBImport - Picard Version: 2.18.7; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_S",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4994
https://github.com/broadinstitute/gatk/issues/4994:864,Testability,test,test,864,"@kgururaj We got this issue report in the forum, could you please look into it? Thanks!. https://gatkforums.broadinstitute.org/gatk/discussion/12388/how-to-use-multi-interval-in-genomicsdbimport-with-gatk-4-0-6-0. ----. I used the GenomicsDBImport with a interval list file and got a error like below.; So what is the correct way to use Multi-interval in GenomicsDBImport?. gatk version: 4.0.6.0. Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -Xms4g -jar /mnt/gatk/gatk-4.0.6.0/gatk-package-4.0.6.0-local.jar GenomicsDBImport -L test.intervals --genomicsdb-workspace-path ../RAW_VCF/my_database -V file1 -V file2 -V file3. 02:57:15.591 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/workshop/xinchen.pan/test/gatk/gatk-4.0.6.0/gatk-package-4.0.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 02:57:15.772 INFO GenomicsDBImport - ------------------------------------------------------------; 02:57:15.772 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.6.0; 02:57:15.772 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 02:57:15.772 INFO GenomicsDBImport - Executing as on Linux v3.10.0-514.6.1.el7.x86_64 amd64; 02:57:15.772 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b13; 02:57:15.773 INFO GenomicsDBImport - Start Date/Time: July 10, 2018 2:57:15 AM EDT; 02:57:15.773 INFO GenomicsDBImport - ------------------------------------------------------------; 02:57:15.773 INFO GenomicsDBImport - ------------------------------------------------------------; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Version: 2.16.0; 02:57:15.773 INFO GenomicsDBImport - Picard Version: 2.18.7; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_S",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4994
https://github.com/broadinstitute/gatk/issues/4995:178,Availability,failure,failure,178,"We should add a runtime check, probably at the ScriptExecutor level, to verify that the version of the python package we're running matches the version of GATK. Otherwise subtle failure modes could ensue when a user upgrades GATK but does not re-establish the conda env.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4995
https://github.com/broadinstitute/gatk/issues/4995:216,Deployability,upgrade,upgrades,216,"We should add a runtime check, probably at the ScriptExecutor level, to verify that the version of the python package we're running matches the version of GATK. Otherwise subtle failure modes could ensue when a user upgrades GATK but does not re-establish the conda env.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4995
https://github.com/broadinstitute/gatk/pull/4996:408,Availability,down,downstream,408,"One more step towards using this new tool. Does:; * output a single VCF containing `<INS>`, `<DEL>`, `<DUP>`, `<INV>` calls (there will be more `<INV>` calls, but that cannot happen until someone takes a look at PR #4789 and check if the proposed algorithm makes sense); * since this new tool applies more permissive filters on MQ and alignment length of the assembly contigs' mappings, I've introduced some downstream filtering parameters allowing to filter VCF records based on annotations `MAPPING_QUALITIES` and `MAX_ALIGN_LENGTH`; the default value is chosen after some experimentation using the CHM PacBio as truth and the branch ; sh-sv-interlvatree-eval.; * cleans up VCF headers and related tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4996
https://github.com/broadinstitute/gatk/pull/4996:700,Testability,test,tests,700,"One more step towards using this new tool. Does:; * output a single VCF containing `<INS>`, `<DEL>`, `<DUP>`, `<INV>` calls (there will be more `<INV>` calls, but that cannot happen until someone takes a look at PR #4789 and check if the proposed algorithm makes sense); * since this new tool applies more permissive filters on MQ and alignment length of the assembly contigs' mappings, I've introduced some downstream filtering parameters allowing to filter VCF records based on annotations `MAPPING_QUALITIES` and `MAX_ALIGN_LENGTH`; the default value is chosen after some experimentation using the CHM PacBio as truth and the branch ; sh-sv-interlvatree-eval.; * cleans up VCF headers and related tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4996
https://github.com/broadinstitute/gatk/pull/4998:28,Availability,failure,failure,28,@takutosato Here it is. The failure has nothing to do with this branch -- every branch is failing in the same way currently. I have tested and debugged this thoroughly and I am deliberately not writing unit tests for now because we're too busy with MC3 and the M2 paper. I intend to come back to these later.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4998
https://github.com/broadinstitute/gatk/pull/4998:132,Testability,test,tested,132,@takutosato Here it is. The failure has nothing to do with this branch -- every branch is failing in the same way currently. I have tested and debugged this thoroughly and I am deliberately not writing unit tests for now because we're too busy with MC3 and the M2 paper. I intend to come back to these later.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4998
https://github.com/broadinstitute/gatk/pull/4998:207,Testability,test,tests,207,@takutosato Here it is. The failure has nothing to do with this branch -- every branch is failing in the same way currently. I have tested and debugged this thoroughly and I am deliberately not writing unit tests for now because we're too busy with MC3 and the M2 paper. I intend to come back to these later.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4998
https://github.com/broadinstitute/gatk/pull/4999:23,Security,Validat,ValidateBasicSomaticShortMutations,23,@takutosato This lets `ValidateBasicSomaticShortMutations` optionally annotate the `eval` vcf with validation `INFO` fields in addition to the standard output of a tsv. Having a vcf is more convenient for some things in MC3.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4999
https://github.com/broadinstitute/gatk/pull/4999:99,Security,validat,validation,99,@takutosato This lets `ValidateBasicSomaticShortMutations` optionally annotate the `eval` vcf with validation `INFO` fields in addition to the standard output of a tsv. Having a vcf is more convenient for some things in MC3.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4999
https://github.com/broadinstitute/gatk/issues/5000:25,Availability,error,error,25,So how has the -new-qual error modified? I have used gatk 4.0.4.0 for all my analysis. How badly should I have to re-analyse with the new version of GATK? Is it really important?. __(edited to remove distracting boilerplate text - lbergelson)__,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5000
https://github.com/broadinstitute/gatk/issues/5001:158,Testability,test,tests,158,"Somehow our code coverage has dropped by 20% since the changes to shrink the docker image. I'm not sure if the issue is that we're not running a chunk of our tests or if we're not reporting coverage for some of them, but either way there is a problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5001
https://github.com/broadinstitute/gatk/issues/5003:302,Availability,error,error,302,"There is an issue in with gcloud 208.0.0 that prevents the `gcloud dataproc` commands from functioning. We should temporarily pin our version to 207.0.0 instead until the problem can be addressed by google. . ```; Customers affected by this issue are using gcloud version 208.0.0 and may experience an error like ""Problem loading gcloud.dataproc.clusters.create: No module named jsonschema."" when interacting with Google Cloud Dataproc.; Workaround; The workaround is to use gcloud version 207.0.0, a downgrade from 208.0.0 can be done by issuing the command: ""gcloud components update --version 207.0.0"". If installed via apt: sudo apt-get update && sudo apt-get install google-cloud-sdk=207.0.0-0. If installed via yum: sudo yum downgrade google-cloud-sdk-207.0.0; ```. We have to remember to unpin it afterward the problem is fixed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5003
https://github.com/broadinstitute/gatk/issues/5003:501,Availability,down,downgrade,501,"There is an issue in with gcloud 208.0.0 that prevents the `gcloud dataproc` commands from functioning. We should temporarily pin our version to 207.0.0 instead until the problem can be addressed by google. . ```; Customers affected by this issue are using gcloud version 208.0.0 and may experience an error like ""Problem loading gcloud.dataproc.clusters.create: No module named jsonschema."" when interacting with Google Cloud Dataproc.; Workaround; The workaround is to use gcloud version 207.0.0, a downgrade from 208.0.0 can be done by issuing the command: ""gcloud components update --version 207.0.0"". If installed via apt: sudo apt-get update && sudo apt-get install google-cloud-sdk=207.0.0-0. If installed via yum: sudo yum downgrade google-cloud-sdk-207.0.0; ```. We have to remember to unpin it afterward the problem is fixed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5003
https://github.com/broadinstitute/gatk/issues/5003:731,Availability,down,downgrade,731,"There is an issue in with gcloud 208.0.0 that prevents the `gcloud dataproc` commands from functioning. We should temporarily pin our version to 207.0.0 instead until the problem can be addressed by google. . ```; Customers affected by this issue are using gcloud version 208.0.0 and may experience an error like ""Problem loading gcloud.dataproc.clusters.create: No module named jsonschema."" when interacting with Google Cloud Dataproc.; Workaround; The workaround is to use gcloud version 207.0.0, a downgrade from 208.0.0 can be done by issuing the command: ""gcloud components update --version 207.0.0"". If installed via apt: sudo apt-get update && sudo apt-get install google-cloud-sdk=207.0.0-0. If installed via yum: sudo yum downgrade google-cloud-sdk-207.0.0; ```. We have to remember to unpin it afterward the problem is fixed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5003
https://github.com/broadinstitute/gatk/issues/5003:579,Deployability,update,update,579,"There is an issue in with gcloud 208.0.0 that prevents the `gcloud dataproc` commands from functioning. We should temporarily pin our version to 207.0.0 instead until the problem can be addressed by google. . ```; Customers affected by this issue are using gcloud version 208.0.0 and may experience an error like ""Problem loading gcloud.dataproc.clusters.create: No module named jsonschema."" when interacting with Google Cloud Dataproc.; Workaround; The workaround is to use gcloud version 207.0.0, a downgrade from 208.0.0 can be done by issuing the command: ""gcloud components update --version 207.0.0"". If installed via apt: sudo apt-get update && sudo apt-get install google-cloud-sdk=207.0.0-0. If installed via yum: sudo yum downgrade google-cloud-sdk-207.0.0; ```. We have to remember to unpin it afterward the problem is fixed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5003
https://github.com/broadinstitute/gatk/issues/5003:609,Deployability,install,installed,609,"There is an issue in with gcloud 208.0.0 that prevents the `gcloud dataproc` commands from functioning. We should temporarily pin our version to 207.0.0 instead until the problem can be addressed by google. . ```; Customers affected by this issue are using gcloud version 208.0.0 and may experience an error like ""Problem loading gcloud.dataproc.clusters.create: No module named jsonschema."" when interacting with Google Cloud Dataproc.; Workaround; The workaround is to use gcloud version 207.0.0, a downgrade from 208.0.0 can be done by issuing the command: ""gcloud components update --version 207.0.0"". If installed via apt: sudo apt-get update && sudo apt-get install google-cloud-sdk=207.0.0-0. If installed via yum: sudo yum downgrade google-cloud-sdk-207.0.0; ```. We have to remember to unpin it afterward the problem is fixed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5003
https://github.com/broadinstitute/gatk/issues/5003:641,Deployability,update,update,641,"There is an issue in with gcloud 208.0.0 that prevents the `gcloud dataproc` commands from functioning. We should temporarily pin our version to 207.0.0 instead until the problem can be addressed by google. . ```; Customers affected by this issue are using gcloud version 208.0.0 and may experience an error like ""Problem loading gcloud.dataproc.clusters.create: No module named jsonschema."" when interacting with Google Cloud Dataproc.; Workaround; The workaround is to use gcloud version 207.0.0, a downgrade from 208.0.0 can be done by issuing the command: ""gcloud components update --version 207.0.0"". If installed via apt: sudo apt-get update && sudo apt-get install google-cloud-sdk=207.0.0-0. If installed via yum: sudo yum downgrade google-cloud-sdk-207.0.0; ```. We have to remember to unpin it afterward the problem is fixed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5003
https://github.com/broadinstitute/gatk/issues/5003:664,Deployability,install,install,664,"There is an issue in with gcloud 208.0.0 that prevents the `gcloud dataproc` commands from functioning. We should temporarily pin our version to 207.0.0 instead until the problem can be addressed by google. . ```; Customers affected by this issue are using gcloud version 208.0.0 and may experience an error like ""Problem loading gcloud.dataproc.clusters.create: No module named jsonschema."" when interacting with Google Cloud Dataproc.; Workaround; The workaround is to use gcloud version 207.0.0, a downgrade from 208.0.0 can be done by issuing the command: ""gcloud components update --version 207.0.0"". If installed via apt: sudo apt-get update && sudo apt-get install google-cloud-sdk=207.0.0-0. If installed via yum: sudo yum downgrade google-cloud-sdk-207.0.0; ```. We have to remember to unpin it afterward the problem is fixed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5003
https://github.com/broadinstitute/gatk/issues/5003:703,Deployability,install,installed,703,"There is an issue in with gcloud 208.0.0 that prevents the `gcloud dataproc` commands from functioning. We should temporarily pin our version to 207.0.0 instead until the problem can be addressed by google. . ```; Customers affected by this issue are using gcloud version 208.0.0 and may experience an error like ""Problem loading gcloud.dataproc.clusters.create: No module named jsonschema."" when interacting with Google Cloud Dataproc.; Workaround; The workaround is to use gcloud version 207.0.0, a downgrade from 208.0.0 can be done by issuing the command: ""gcloud components update --version 207.0.0"". If installed via apt: sudo apt-get update && sudo apt-get install google-cloud-sdk=207.0.0-0. If installed via yum: sudo yum downgrade google-cloud-sdk-207.0.0; ```. We have to remember to unpin it afterward the problem is fixed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5003
https://github.com/broadinstitute/gatk/issues/5003:322,Performance,load,loading,322,"There is an issue in with gcloud 208.0.0 that prevents the `gcloud dataproc` commands from functioning. We should temporarily pin our version to 207.0.0 instead until the problem can be addressed by google. . ```; Customers affected by this issue are using gcloud version 208.0.0 and may experience an error like ""Problem loading gcloud.dataproc.clusters.create: No module named jsonschema."" when interacting with Google Cloud Dataproc.; Workaround; The workaround is to use gcloud version 207.0.0, a downgrade from 208.0.0 can be done by issuing the command: ""gcloud components update --version 207.0.0"". If installed via apt: sudo apt-get update && sudo apt-get install google-cloud-sdk=207.0.0-0. If installed via yum: sudo yum downgrade google-cloud-sdk-207.0.0; ```. We have to remember to unpin it afterward the problem is fixed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5003
https://github.com/broadinstitute/gatk/issues/5006:66,Deployability,integrat,integration,66,"The following idiom occurs about 25 times in this repo, mainly in integration tests:; ```; StreamSupport.stream(new FeatureDataSource<VariantContext>(vcf).spliterator(), false). . .; ```; We should extract a method, perhaps `Utils.streamVcf(final File vcf)`, to replace this unwieldy construct.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5006
https://github.com/broadinstitute/gatk/issues/5006:66,Integrability,integrat,integration,66,"The following idiom occurs about 25 times in this repo, mainly in integration tests:; ```; StreamSupport.stream(new FeatureDataSource<VariantContext>(vcf).spliterator(), false). . .; ```; We should extract a method, perhaps `Utils.streamVcf(final File vcf)`, to replace this unwieldy construct.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5006
https://github.com/broadinstitute/gatk/issues/5006:78,Testability,test,tests,78,"The following idiom occurs about 25 times in this repo, mainly in integration tests:; ```; StreamSupport.stream(new FeatureDataSource<VariantContext>(vcf).spliterator(), false). . .; ```; We should extract a method, perhaps `Utils.streamVcf(final File vcf)`, to replace this unwieldy construct.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5006
https://github.com/broadinstitute/gatk/pull/5007:170,Security,validat,validate,170,"@LeeTL1220 This will make it easy to basically re-do the MC3 analysis as if M2 had been there from the beginning. The idea is:. * run M2; * merge M2 variants into MC3; * validate all variants (M2 *and* MC3); * compare all callers on equal footing. It would be nice if there were a barclay annotation for an unsupported feature, but I'm not aware of one. . .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5007
https://github.com/broadinstitute/gatk/issues/5009:371,Availability,error,error,371,"Hi, I am working with WES data with 130 samples. I've been following GATK4 best practices and also using the GRCh38 reference from the GATK bundle. I've been able to pre-process all the samples and to use Haplotypecaller for the 130 samples, then I proceed to merge all into a single gVCF file to then perform a join-call of SNPs and INdels. However, I got the following error message when using ""GenotypeGVCF"" ; Thank you; Cristian. ### Affected tool(s) or class(es); GenotypeGVCFs. ### Affected version(s); GATK v4.0.5.2; ### Description ; 12:37:00.202 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 12:37:00.306 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home-1/cvalenc1@jhu.edu/apps/GATK4/gatk-4.0.5.2/gatk-package-4.0.5.2-local.jar!/com/intel/gkl/native/libg; kl_compression.so; 12:37:00.524 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.524 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.5.2; 12:37:00.524 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:37:00.529 INFO GenotypeGVCFs - Executing as cvalenc1@jhu.edu@compute0207 on Linux v2.6.32-696.28.1.el6.x86_64 amd64; 12:37:00.530 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_45-b14; 12:37:00.530 INFO GenotypeGVCFs - Start Date/Time: July 12, 2018 12:37:00 PM EDT; 12:37:00.530 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.530 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.530 INFO GenotypeGVCFs - HTSJDK Version: 2.16.0; 12:37:00.530 INFO GenotypeGVCFs - Picard Version: 2.18.7; 12:37:00.530 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:37:00.531 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:37:00.531 INFO GenotypeGVCFs - HTSJDK Defau",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5009
https://github.com/broadinstitute/gatk/issues/5009:593,Availability,Redundant,Redundant,593,"Hi, I am working with WES data with 130 samples. I've been following GATK4 best practices and also using the GRCh38 reference from the GATK bundle. I've been able to pre-process all the samples and to use Haplotypecaller for the 130 samples, then I proceed to merge all into a single gVCF file to then perform a join-call of SNPs and INdels. However, I got the following error message when using ""GenotypeGVCF"" ; Thank you; Cristian. ### Affected tool(s) or class(es); GenotypeGVCFs. ### Affected version(s); GATK v4.0.5.2; ### Description ; 12:37:00.202 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 12:37:00.306 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home-1/cvalenc1@jhu.edu/apps/GATK4/gatk-4.0.5.2/gatk-package-4.0.5.2-local.jar!/com/intel/gkl/native/libg; kl_compression.so; 12:37:00.524 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.524 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.5.2; 12:37:00.524 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:37:00.529 INFO GenotypeGVCFs - Executing as cvalenc1@jhu.edu@compute0207 on Linux v2.6.32-696.28.1.el6.x86_64 amd64; 12:37:00.530 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_45-b14; 12:37:00.530 INFO GenotypeGVCFs - Start Date/Time: July 12, 2018 12:37:00 PM EDT; 12:37:00.530 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.530 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.530 INFO GenotypeGVCFs - HTSJDK Version: 2.16.0; 12:37:00.530 INFO GenotypeGVCFs - Picard Version: 2.18.7; 12:37:00.530 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:37:00.531 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:37:00.531 INFO GenotypeGVCFs - HTSJDK Defau",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5009
https://github.com/broadinstitute/gatk/issues/5009:3238,Availability,down,down,3238,"flater; 12:37:00.531 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 12:37:00.531 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 12:37:00.531 INFO GenotypeGVCFs - Initializing engine; 12:37:02.095 INFO FeatureManager - Using codec VCFCodec to read file file:///home-1/cvalenc1@jhu.edu/work/cvalenc1/Paralysis/NEW_ALIGNEMENT/Sample_VCF/Multi.g.vcf; 12:37:03.426 INFO GenotypeGVCFs - Done initializing engine; 12:37:03.683 INFO ProgressMeter - Starting traversal; 12:37:03.683 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 12:37:13.942 INFO ProgressMeter - chr1:1034498 0.2 14000 81887.3; 12:37:24.351 INFO ProgressMeter - chr1:1322991 0.3 41000 119030.3; 12:37:34.716 INFO ProgressMeter - chr1:1926324 0.5 83000 160474.3; 12:37:44.726 INFO ProgressMeter - chr1:3786982 0.7 124000 181273.3; 12:37:45.921 INFO GenotypeGVCFs - Shutting down engine; [July 12, 2018 12:37:45 PM EDT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.76 minutes.; Runtime.totalMemory()=5942280192; java.lang.IllegalArgumentException: log10LikelihoodsOfAC are bad 8.038983630564185E-17,NaN; at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AFCalculationResult.<init>(AFCalculationResult.java:72); at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator.getLog10PNonRef(AlleleFrequencyCalculator.java:143); at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:255); at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:210); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.calculateGenotypes(GenotypeGVCFs.java:266); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.regenotypeVC(GenotypeGVCFs.java:222); at org.broadinstitute.hellbender.t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5009
https://github.com/broadinstitute/gatk/issues/5009:2374,Deployability,patch,patch,2374," VM v1.8.0_45-b14; 12:37:00.530 INFO GenotypeGVCFs - Start Date/Time: July 12, 2018 12:37:00 PM EDT; 12:37:00.530 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.530 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.530 INFO GenotypeGVCFs - HTSJDK Version: 2.16.0; 12:37:00.530 INFO GenotypeGVCFs - Picard Version: 2.18.7; 12:37:00.530 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:37:00.531 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:37:00.531 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:37:00.531 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:37:00.531 INFO GenotypeGVCFs - Deflater: IntelDeflater; 12:37:00.531 INFO GenotypeGVCFs - Inflater: IntelInflater; 12:37:00.531 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 12:37:00.531 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 12:37:00.531 INFO GenotypeGVCFs - Initializing engine; 12:37:02.095 INFO FeatureManager - Using codec VCFCodec to read file file:///home-1/cvalenc1@jhu.edu/work/cvalenc1/Paralysis/NEW_ALIGNEMENT/Sample_VCF/Multi.g.vcf; 12:37:03.426 INFO GenotypeGVCFs - Done initializing engine; 12:37:03.683 INFO ProgressMeter - Starting traversal; 12:37:03.683 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 12:37:13.942 INFO ProgressMeter - chr1:1034498 0.2 14000 81887.3; 12:37:24.351 INFO ProgressMeter - chr1:1322991 0.3 41000 119030.3; 12:37:34.716 INFO ProgressMeter - chr1:1926324 0.5 83000 160474.3; 12:37:44.726 INFO ProgressMeter - chr1:3786982 0.7 124000 181273.3; 12:37:45.921 INFO GenotypeGVCFs - Shutting down engine; [July 12, 2018 12:37:45 PM EDT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.76 minutes.; Runtime",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5009
https://github.com/broadinstitute/gatk/issues/5009:377,Integrability,message,message,377,"Hi, I am working with WES data with 130 samples. I've been following GATK4 best practices and also using the GRCh38 reference from the GATK bundle. I've been able to pre-process all the samples and to use Haplotypecaller for the 130 samples, then I proceed to merge all into a single gVCF file to then perform a join-call of SNPs and INdels. However, I got the following error message when using ""GenotypeGVCF"" ; Thank you; Cristian. ### Affected tool(s) or class(es); GenotypeGVCFs. ### Affected version(s); GATK v4.0.5.2; ### Description ; 12:37:00.202 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 12:37:00.306 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home-1/cvalenc1@jhu.edu/apps/GATK4/gatk-4.0.5.2/gatk-package-4.0.5.2-local.jar!/com/intel/gkl/native/libg; kl_compression.so; 12:37:00.524 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.524 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.5.2; 12:37:00.524 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:37:00.529 INFO GenotypeGVCFs - Executing as cvalenc1@jhu.edu@compute0207 on Linux v2.6.32-696.28.1.el6.x86_64 amd64; 12:37:00.530 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_45-b14; 12:37:00.530 INFO GenotypeGVCFs - Start Date/Time: July 12, 2018 12:37:00 PM EDT; 12:37:00.530 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.530 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.530 INFO GenotypeGVCFs - HTSJDK Version: 2.16.0; 12:37:00.530 INFO GenotypeGVCFs - Picard Version: 2.18.7; 12:37:00.530 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:37:00.531 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:37:00.531 INFO GenotypeGVCFs - HTSJDK Defau",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5009
https://github.com/broadinstitute/gatk/issues/5009:5081,Integrability,wrap,wrapAndCopyInto,5081,VCFs.java:266); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.regenotypeVC(GenotypeGVCFs.java:222); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.apply(GenotypeGVCFs.java:201); at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:151); at org.broadinstitute.hellbender.engine.VariantWalkerBase$$Lambda$96/1188871851.accept(Unknown Source); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:149); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:984); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hel,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5009
https://github.com/broadinstitute/gatk/issues/5009:302,Performance,perform,perform,302,"Hi, I am working with WES data with 130 samples. I've been following GATK4 best practices and also using the GRCh38 reference from the GATK bundle. I've been able to pre-process all the samples and to use Haplotypecaller for the 130 samples, then I proceed to merge all into a single gVCF file to then perform a join-call of SNPs and INdels. However, I got the following error message when using ""GenotypeGVCF"" ; Thank you; Cristian. ### Affected tool(s) or class(es); GenotypeGVCFs. ### Affected version(s); GATK v4.0.5.2; ### Description ; 12:37:00.202 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 12:37:00.306 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home-1/cvalenc1@jhu.edu/apps/GATK4/gatk-4.0.5.2/gatk-package-4.0.5.2-local.jar!/com/intel/gkl/native/libg; kl_compression.so; 12:37:00.524 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.524 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.5.2; 12:37:00.524 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:37:00.529 INFO GenotypeGVCFs - Executing as cvalenc1@jhu.edu@compute0207 on Linux v2.6.32-696.28.1.el6.x86_64 amd64; 12:37:00.530 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_45-b14; 12:37:00.530 INFO GenotypeGVCFs - Start Date/Time: July 12, 2018 12:37:00 PM EDT; 12:37:00.530 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.530 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.530 INFO GenotypeGVCFs - HTSJDK Version: 2.16.0; 12:37:00.530 INFO GenotypeGVCFs - Picard Version: 2.18.7; 12:37:00.530 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:37:00.531 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:37:00.531 INFO GenotypeGVCFs - HTSJDK Defau",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5009
https://github.com/broadinstitute/gatk/issues/5009:726,Performance,Load,Loading,726,"Hi, I am working with WES data with 130 samples. I've been following GATK4 best practices and also using the GRCh38 reference from the GATK bundle. I've been able to pre-process all the samples and to use Haplotypecaller for the 130 samples, then I proceed to merge all into a single gVCF file to then perform a join-call of SNPs and INdels. However, I got the following error message when using ""GenotypeGVCF"" ; Thank you; Cristian. ### Affected tool(s) or class(es); GenotypeGVCFs. ### Affected version(s); GATK v4.0.5.2; ### Description ; 12:37:00.202 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 12:37:00.306 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home-1/cvalenc1@jhu.edu/apps/GATK4/gatk-4.0.5.2/gatk-package-4.0.5.2-local.jar!/com/intel/gkl/native/libg; kl_compression.so; 12:37:00.524 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.524 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.5.2; 12:37:00.524 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:37:00.529 INFO GenotypeGVCFs - Executing as cvalenc1@jhu.edu@compute0207 on Linux v2.6.32-696.28.1.el6.x86_64 amd64; 12:37:00.530 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_45-b14; 12:37:00.530 INFO GenotypeGVCFs - Start Date/Time: July 12, 2018 12:37:00 PM EDT; 12:37:00.530 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.530 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.530 INFO GenotypeGVCFs - HTSJDK Version: 2.16.0; 12:37:00.530 INFO GenotypeGVCFs - Picard Version: 2.18.7; 12:37:00.530 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:37:00.531 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:37:00.531 INFO GenotypeGVCFs - HTSJDK Defau",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5009
https://github.com/broadinstitute/gatk/issues/5009:6876,Performance,load,load,6876,"ineProgram.instanceMain(CommandLineProgram.java:199); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /home-1/cvalenc1@jhu.edu/apps/GATK4/gatk-4.0.5.2/gatk-package-4.0.5.2-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_E; XCEPTION=true -jar /home-1/cvalenc1@jhu.edu/apps/GATK4/gatk-4.0.5.2/gatk-package-4.0.5.2-local.jar GenotypeGVCFs -R /home-1/cvalenc1@jhu.edu/work/cvalenc1/hg38_broad/Homo_sapiens_assembly38.f; asta -V /home-1/cvalenc1@jhu.edu/work/cvalenc1/Paralysis/NEW_ALIGNEMENT/Sample_VCF/Multi.g.vcf -G StandardAnnotation -new-qual -O /home-1/cvalenc1@jhu.edu/work/cvalenc1/Paralysis/NEW_ALIGNEME; NT/VCF/Cohort_call.vcf. #### Steps to reproduce; My script:; # load modules for GATK4; module load java/JDK_1.8.0_45; module load python/3.6.5; PICARD=/home-1/cvalenc1@jhu.edu/apps/picard/2/build/libs/picard.jar; # setting reference; REF=~/work/cvalenc1/hg38_broad/Homo_sapiens_assembly38.fasta; VCF=~/work/cvalenc1/Paralysis/NEW_ALIGNEMENT/Sample_VCF/Multi.g.vcf; NEWVCF=~/work/cvalenc1/Paralysis/NEW_ALIGNEMENT/VCF/Cohort_call.vcf. # run GATK ; ~/apps/GATK4/gatk-4.0.5.2/gatk --java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true' GenotypeGVCFs -R $REF -V $VCF -G StandardAnnotation -new-qual -O $NEWVCF. # done. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) in",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5009
https://github.com/broadinstitute/gatk/issues/5009:6907,Performance,load,load,6907,"ineProgram.instanceMain(CommandLineProgram.java:199); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /home-1/cvalenc1@jhu.edu/apps/GATK4/gatk-4.0.5.2/gatk-package-4.0.5.2-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_E; XCEPTION=true -jar /home-1/cvalenc1@jhu.edu/apps/GATK4/gatk-4.0.5.2/gatk-package-4.0.5.2-local.jar GenotypeGVCFs -R /home-1/cvalenc1@jhu.edu/work/cvalenc1/hg38_broad/Homo_sapiens_assembly38.f; asta -V /home-1/cvalenc1@jhu.edu/work/cvalenc1/Paralysis/NEW_ALIGNEMENT/Sample_VCF/Multi.g.vcf -G StandardAnnotation -new-qual -O /home-1/cvalenc1@jhu.edu/work/cvalenc1/Paralysis/NEW_ALIGNEME; NT/VCF/Cohort_call.vcf. #### Steps to reproduce; My script:; # load modules for GATK4; module load java/JDK_1.8.0_45; module load python/3.6.5; PICARD=/home-1/cvalenc1@jhu.edu/apps/picard/2/build/libs/picard.jar; # setting reference; REF=~/work/cvalenc1/hg38_broad/Homo_sapiens_assembly38.fasta; VCF=~/work/cvalenc1/Paralysis/NEW_ALIGNEMENT/Sample_VCF/Multi.g.vcf; NEWVCF=~/work/cvalenc1/Paralysis/NEW_ALIGNEMENT/VCF/Cohort_call.vcf. # run GATK ; ~/apps/GATK4/gatk-4.0.5.2/gatk --java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true' GenotypeGVCFs -R $REF -V $VCF -G StandardAnnotation -new-qual -O $NEWVCF. # done. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) in",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5009
https://github.com/broadinstitute/gatk/issues/5009:6938,Performance,load,load,6938,"g.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /home-1/cvalenc1@jhu.edu/apps/GATK4/gatk-4.0.5.2/gatk-package-4.0.5.2-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_E; XCEPTION=true -jar /home-1/cvalenc1@jhu.edu/apps/GATK4/gatk-4.0.5.2/gatk-package-4.0.5.2-local.jar GenotypeGVCFs -R /home-1/cvalenc1@jhu.edu/work/cvalenc1/hg38_broad/Homo_sapiens_assembly38.f; asta -V /home-1/cvalenc1@jhu.edu/work/cvalenc1/Paralysis/NEW_ALIGNEMENT/Sample_VCF/Multi.g.vcf -G StandardAnnotation -new-qual -O /home-1/cvalenc1@jhu.edu/work/cvalenc1/Paralysis/NEW_ALIGNEME; NT/VCF/Cohort_call.vcf. #### Steps to reproduce; My script:; # load modules for GATK4; module load java/JDK_1.8.0_45; module load python/3.6.5; PICARD=/home-1/cvalenc1@jhu.edu/apps/picard/2/build/libs/picard.jar; # setting reference; REF=~/work/cvalenc1/hg38_broad/Homo_sapiens_assembly38.fasta; VCF=~/work/cvalenc1/Paralysis/NEW_ALIGNEMENT/Sample_VCF/Multi.g.vcf; NEWVCF=~/work/cvalenc1/Paralysis/NEW_ALIGNEMENT/VCF/Cohort_call.vcf. # run GATK ; ~/apps/GATK4/gatk-4.0.5.2/gatk --java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true' GenotypeGVCFs -R $REF -V $VCF -G StandardAnnotation -new-qual -O $NEWVCF. # done. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5009
https://github.com/broadinstitute/gatk/issues/5009:593,Safety,Redund,Redundant,593,"Hi, I am working with WES data with 130 samples. I've been following GATK4 best practices and also using the GRCh38 reference from the GATK bundle. I've been able to pre-process all the samples and to use Haplotypecaller for the 130 samples, then I proceed to merge all into a single gVCF file to then perform a join-call of SNPs and INdels. However, I got the following error message when using ""GenotypeGVCF"" ; Thank you; Cristian. ### Affected tool(s) or class(es); GenotypeGVCFs. ### Affected version(s); GATK v4.0.5.2; ### Description ; 12:37:00.202 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 12:37:00.306 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home-1/cvalenc1@jhu.edu/apps/GATK4/gatk-4.0.5.2/gatk-package-4.0.5.2-local.jar!/com/intel/gkl/native/libg; kl_compression.so; 12:37:00.524 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.524 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.5.2; 12:37:00.524 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:37:00.529 INFO GenotypeGVCFs - Executing as cvalenc1@jhu.edu@compute0207 on Linux v2.6.32-696.28.1.el6.x86_64 amd64; 12:37:00.530 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_45-b14; 12:37:00.530 INFO GenotypeGVCFs - Start Date/Time: July 12, 2018 12:37:00 PM EDT; 12:37:00.530 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.530 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.530 INFO GenotypeGVCFs - HTSJDK Version: 2.16.0; 12:37:00.530 INFO GenotypeGVCFs - Picard Version: 2.18.7; 12:37:00.530 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:37:00.531 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:37:00.531 INFO GenotypeGVCFs - HTSJDK Defau",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5009
https://github.com/broadinstitute/gatk/pull/5011:0,Testability,Test,Tests,0,Tests ported from picard: https://github.com/broadinstitute/picard/pull/1195. Fixes #4707,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5011
https://github.com/broadinstitute/gatk/issues/5012:54,Availability,down,downstream,54,"### Affected class(es); All test classes in GATK (and downstream projects) extending `BaseTest`. ### Affected version(s); - [x] Latest public release version; - [x] Latest master branch. ### Description ; The GATK toolkit assumes `US` locale (set in a `Main` static method), which in turn produces all the test files using the `US` locale; if the test suite is run in a different locale, it might fail unexpectedly. For example, if the locale has a comma-separated decimals instead of dot-separated, comparing the expected file output with `US` locale against the generated by the tests fail. . #### Expected behavior; `BaseTest` should set the locale in a `@BeforeSuite` method (or static method) to set the assumptions of the toolkit to all tests (also for downstream toolkits). #### Actual behavior; `BaseTest` picks default locale.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5012
https://github.com/broadinstitute/gatk/issues/5012:759,Availability,down,downstream,759,"### Affected class(es); All test classes in GATK (and downstream projects) extending `BaseTest`. ### Affected version(s); - [x] Latest public release version; - [x] Latest master branch. ### Description ; The GATK toolkit assumes `US` locale (set in a `Main` static method), which in turn produces all the test files using the `US` locale; if the test suite is run in a different locale, it might fail unexpectedly. For example, if the locale has a comma-separated decimals instead of dot-separated, comparing the expected file output with `US` locale against the generated by the tests fail. . #### Expected behavior; `BaseTest` should set the locale in a `@BeforeSuite` method (or static method) to set the assumptions of the toolkit to all tests (also for downstream toolkits). #### Actual behavior; `BaseTest` picks default locale.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5012
https://github.com/broadinstitute/gatk/issues/5012:142,Deployability,release,release,142,"### Affected class(es); All test classes in GATK (and downstream projects) extending `BaseTest`. ### Affected version(s); - [x] Latest public release version; - [x] Latest master branch. ### Description ; The GATK toolkit assumes `US` locale (set in a `Main` static method), which in turn produces all the test files using the `US` locale; if the test suite is run in a different locale, it might fail unexpectedly. For example, if the locale has a comma-separated decimals instead of dot-separated, comparing the expected file output with `US` locale against the generated by the tests fail. . #### Expected behavior; `BaseTest` should set the locale in a `@BeforeSuite` method (or static method) to set the assumptions of the toolkit to all tests (also for downstream toolkits). #### Actual behavior; `BaseTest` picks default locale.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5012
https://github.com/broadinstitute/gatk/issues/5012:75,Modifiability,extend,extending,75,"### Affected class(es); All test classes in GATK (and downstream projects) extending `BaseTest`. ### Affected version(s); - [x] Latest public release version; - [x] Latest master branch. ### Description ; The GATK toolkit assumes `US` locale (set in a `Main` static method), which in turn produces all the test files using the `US` locale; if the test suite is run in a different locale, it might fail unexpectedly. For example, if the locale has a comma-separated decimals instead of dot-separated, comparing the expected file output with `US` locale against the generated by the tests fail. . #### Expected behavior; `BaseTest` should set the locale in a `@BeforeSuite` method (or static method) to set the assumptions of the toolkit to all tests (also for downstream toolkits). #### Actual behavior; `BaseTest` picks default locale.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5012
https://github.com/broadinstitute/gatk/issues/5012:28,Testability,test,test,28,"### Affected class(es); All test classes in GATK (and downstream projects) extending `BaseTest`. ### Affected version(s); - [x] Latest public release version; - [x] Latest master branch. ### Description ; The GATK toolkit assumes `US` locale (set in a `Main` static method), which in turn produces all the test files using the `US` locale; if the test suite is run in a different locale, it might fail unexpectedly. For example, if the locale has a comma-separated decimals instead of dot-separated, comparing the expected file output with `US` locale against the generated by the tests fail. . #### Expected behavior; `BaseTest` should set the locale in a `@BeforeSuite` method (or static method) to set the assumptions of the toolkit to all tests (also for downstream toolkits). #### Actual behavior; `BaseTest` picks default locale.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5012
https://github.com/broadinstitute/gatk/issues/5012:306,Testability,test,test,306,"### Affected class(es); All test classes in GATK (and downstream projects) extending `BaseTest`. ### Affected version(s); - [x] Latest public release version; - [x] Latest master branch. ### Description ; The GATK toolkit assumes `US` locale (set in a `Main` static method), which in turn produces all the test files using the `US` locale; if the test suite is run in a different locale, it might fail unexpectedly. For example, if the locale has a comma-separated decimals instead of dot-separated, comparing the expected file output with `US` locale against the generated by the tests fail. . #### Expected behavior; `BaseTest` should set the locale in a `@BeforeSuite` method (or static method) to set the assumptions of the toolkit to all tests (also for downstream toolkits). #### Actual behavior; `BaseTest` picks default locale.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5012
https://github.com/broadinstitute/gatk/issues/5012:347,Testability,test,test,347,"### Affected class(es); All test classes in GATK (and downstream projects) extending `BaseTest`. ### Affected version(s); - [x] Latest public release version; - [x] Latest master branch. ### Description ; The GATK toolkit assumes `US` locale (set in a `Main` static method), which in turn produces all the test files using the `US` locale; if the test suite is run in a different locale, it might fail unexpectedly. For example, if the locale has a comma-separated decimals instead of dot-separated, comparing the expected file output with `US` locale against the generated by the tests fail. . #### Expected behavior; `BaseTest` should set the locale in a `@BeforeSuite` method (or static method) to set the assumptions of the toolkit to all tests (also for downstream toolkits). #### Actual behavior; `BaseTest` picks default locale.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5012
https://github.com/broadinstitute/gatk/issues/5012:581,Testability,test,tests,581,"### Affected class(es); All test classes in GATK (and downstream projects) extending `BaseTest`. ### Affected version(s); - [x] Latest public release version; - [x] Latest master branch. ### Description ; The GATK toolkit assumes `US` locale (set in a `Main` static method), which in turn produces all the test files using the `US` locale; if the test suite is run in a different locale, it might fail unexpectedly. For example, if the locale has a comma-separated decimals instead of dot-separated, comparing the expected file output with `US` locale against the generated by the tests fail. . #### Expected behavior; `BaseTest` should set the locale in a `@BeforeSuite` method (or static method) to set the assumptions of the toolkit to all tests (also for downstream toolkits). #### Actual behavior; `BaseTest` picks default locale.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5012
https://github.com/broadinstitute/gatk/issues/5012:743,Testability,test,tests,743,"### Affected class(es); All test classes in GATK (and downstream projects) extending `BaseTest`. ### Affected version(s); - [x] Latest public release version; - [x] Latest master branch. ### Description ; The GATK toolkit assumes `US` locale (set in a `Main` static method), which in turn produces all the test files using the `US` locale; if the test suite is run in a different locale, it might fail unexpectedly. For example, if the locale has a comma-separated decimals instead of dot-separated, comparing the expected file output with `US` locale against the generated by the tests fail. . #### Expected behavior; `BaseTest` should set the locale in a `@BeforeSuite` method (or static method) to set the assumptions of the toolkit to all tests (also for downstream toolkits). #### Actual behavior; `BaseTest` picks default locale.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5012
https://github.com/broadinstitute/gatk/issues/5013:275,Availability,Down,Downstream,275,"Could it be possible to set all these properties and configurations in a method annotated with `@BeforeSuite`?. https://github.com/broadinstitute/gatk/blob/3c960c9d7174785a82d272fe9cd33076ae7ed271/src/main/java/org/broadinstitute/hellbender/utils/test/BaseTest.java#L35-L42. Downstream toolkits using the testing framework provided by GATK (and thus, extending `BaseTest`) might benefit for that change - currently they are kept unset if not explicitly defined by the implementation. If it is not possible because it is not correctly handled by TestNG, feel free to close the issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5013
https://github.com/broadinstitute/gatk/issues/5013:53,Deployability,configurat,configurations,53,"Could it be possible to set all these properties and configurations in a method annotated with `@BeforeSuite`?. https://github.com/broadinstitute/gatk/blob/3c960c9d7174785a82d272fe9cd33076ae7ed271/src/main/java/org/broadinstitute/hellbender/utils/test/BaseTest.java#L35-L42. Downstream toolkits using the testing framework provided by GATK (and thus, extending `BaseTest`) might benefit for that change - currently they are kept unset if not explicitly defined by the implementation. If it is not possible because it is not correctly handled by TestNG, feel free to close the issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5013
https://github.com/broadinstitute/gatk/issues/5013:53,Modifiability,config,configurations,53,"Could it be possible to set all these properties and configurations in a method annotated with `@BeforeSuite`?. https://github.com/broadinstitute/gatk/blob/3c960c9d7174785a82d272fe9cd33076ae7ed271/src/main/java/org/broadinstitute/hellbender/utils/test/BaseTest.java#L35-L42. Downstream toolkits using the testing framework provided by GATK (and thus, extending `BaseTest`) might benefit for that change - currently they are kept unset if not explicitly defined by the implementation. If it is not possible because it is not correctly handled by TestNG, feel free to close the issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5013
https://github.com/broadinstitute/gatk/issues/5013:351,Modifiability,extend,extending,351,"Could it be possible to set all these properties and configurations in a method annotated with `@BeforeSuite`?. https://github.com/broadinstitute/gatk/blob/3c960c9d7174785a82d272fe9cd33076ae7ed271/src/main/java/org/broadinstitute/hellbender/utils/test/BaseTest.java#L35-L42. Downstream toolkits using the testing framework provided by GATK (and thus, extending `BaseTest`) might benefit for that change - currently they are kept unset if not explicitly defined by the implementation. If it is not possible because it is not correctly handled by TestNG, feel free to close the issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5013
https://github.com/broadinstitute/gatk/issues/5013:247,Testability,test,test,247,"Could it be possible to set all these properties and configurations in a method annotated with `@BeforeSuite`?. https://github.com/broadinstitute/gatk/blob/3c960c9d7174785a82d272fe9cd33076ae7ed271/src/main/java/org/broadinstitute/hellbender/utils/test/BaseTest.java#L35-L42. Downstream toolkits using the testing framework provided by GATK (and thus, extending `BaseTest`) might benefit for that change - currently they are kept unset if not explicitly defined by the implementation. If it is not possible because it is not correctly handled by TestNG, feel free to close the issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5013
https://github.com/broadinstitute/gatk/issues/5013:305,Testability,test,testing,305,"Could it be possible to set all these properties and configurations in a method annotated with `@BeforeSuite`?. https://github.com/broadinstitute/gatk/blob/3c960c9d7174785a82d272fe9cd33076ae7ed271/src/main/java/org/broadinstitute/hellbender/utils/test/BaseTest.java#L35-L42. Downstream toolkits using the testing framework provided by GATK (and thus, extending `BaseTest`) might benefit for that change - currently they are kept unset if not explicitly defined by the implementation. If it is not possible because it is not correctly handled by TestNG, feel free to close the issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5013
https://github.com/broadinstitute/gatk/issues/5013:545,Testability,Test,TestNG,545,"Could it be possible to set all these properties and configurations in a method annotated with `@BeforeSuite`?. https://github.com/broadinstitute/gatk/blob/3c960c9d7174785a82d272fe9cd33076ae7ed271/src/main/java/org/broadinstitute/hellbender/utils/test/BaseTest.java#L35-L42. Downstream toolkits using the testing framework provided by GATK (and thus, extending `BaseTest`) might benefit for that change - currently they are kept unset if not explicitly defined by the implementation. If it is not possible because it is not correctly handled by TestNG, feel free to close the issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5013
https://github.com/broadinstitute/gatk/issues/5014:173,Availability,down,downstream,173,Could it be possible to move the locale setup and picard-parser config to the first method in `mainEntry` (or another place that can be overrided and/or picked by the tests/downstream toolkits)?. https://github.com/broadinstitute/gatk/blob/51273676b20d25cacf238d1c0429ebc79b321a85/src/main/java/org/broadinstitute/hellbender/Main.java#L38-L50,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5014
https://github.com/broadinstitute/gatk/issues/5014:64,Modifiability,config,config,64,Could it be possible to move the locale setup and picard-parser config to the first method in `mainEntry` (or another place that can be overrided and/or picked by the tests/downstream toolkits)?. https://github.com/broadinstitute/gatk/blob/51273676b20d25cacf238d1c0429ebc79b321a85/src/main/java/org/broadinstitute/hellbender/Main.java#L38-L50,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5014
https://github.com/broadinstitute/gatk/issues/5014:167,Testability,test,tests,167,Could it be possible to move the locale setup and picard-parser config to the first method in `mainEntry` (or another place that can be overrided and/or picked by the tests/downstream toolkits)?. https://github.com/broadinstitute/gatk/blob/51273676b20d25cacf238d1c0429ebc79b321a85/src/main/java/org/broadinstitute/hellbender/Main.java#L38-L50,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5014
https://github.com/broadinstitute/gatk/pull/5015:86,Deployability,pipeline,pipeline,86,"Going ahead and making this change so TAG team can start trying out the ModelSegments pipeline. It's possible that we could use NIO for other files (reference, interval lists), but this will not have as much impact as using it for BAMs (and in some cases, decreases performance, see comments in #4806). Closes #4806.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5015
https://github.com/broadinstitute/gatk/pull/5015:266,Performance,perform,performance,266,"Going ahead and making this change so TAG team can start trying out the ModelSegments pipeline. It's possible that we could use NIO for other files (reference, interval lists), but this will not have as much impact as using it for BAMs (and in some cases, decreases performance, see comments in #4806). Closes #4806.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5015
https://github.com/broadinstitute/gatk/pull/5017:785,Modifiability,variab,variable,785,"Currently, only Posix filesystem paths can be passed as workspaces and arrays to GenomicsDB via GenomicsDBImport and SelectVariants. This PR will allow for hdfs and gcs (and emrfs/s3) URIs to be supported as well. ; Examples; ```; ./gatk GenomicsDBImport -V /vcfs/sample.vcf.gz --genomicsdb-workspace-path hdfs://master:9000/gdb_ws -L 1:500-10000; ./gatk GenomicsDBImport -V /vcfs/sample.vcf.gz --genomicsdb-workspace-path gs://my_bucket/gdb_ws -L 1:500-10000; ```; ```; ./gatk SelectVariants -V gendb.hdfs://master:9000/gdb_ws -R hs37d5.fa -O out.vcf; ./gatk SelectVariants -V gendb.gs://my_bucket/gdb_ws -R hs37d5.fa -O out.vcf; ```; GenomicsDB supports GCS via the [Cloud Storage Connector](https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage). Set environment variable GOOGLE_APPLICATION_CREDENTIALS to point to the GCS Service Account json file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5017
https://github.com/broadinstitute/gatk/pull/5019:172,Availability,failure,failure,172,- M2 WDL will now default to having the orientation bias filter (`FilterByOrientationBias`) turned on. Closes #5016 ; - Empty artifact modes parameter will no longer cause failure in `FilterByOrientationBias`. Instead the task `FilterByOrientationBias` will not run. Closes #5025,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5019
https://github.com/broadinstitute/gatk/issues/5020:46,Performance,load,load,46,It would be useful to many people if we could load FASTQ directly in BwaSpark instead of requiring uBam. . Potential options for doing so are:. 1. Use existing ADAM support for FASTQ. 2. Add support to disq,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5020
https://github.com/broadinstitute/gatk/issues/5022:94,Integrability,depend,dependancies,94,See https://gatkforums.broadinstitute.org/gatk/discussion/12078/gatk-4-4-docker-image-missing-dependancies. Looks like I inadvertently removed this in the great purge of #3935. We should add a test to AnalyzeCovariates to cover plotting as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5022
https://github.com/broadinstitute/gatk/issues/5022:193,Testability,test,test,193,See https://gatkforums.broadinstitute.org/gatk/discussion/12078/gatk-4-4-docker-image-missing-dependancies. Looks like I inadvertently removed this in the great purge of #3935. We should add a test to AnalyzeCovariates to cover plotting as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5022
https://github.com/broadinstitute/gatk/issues/5024:2461,Availability,failure,failure,2461,".0.4.0/bin/gatk"" --java-options ""-Xmx8g -Xms8g"" \; GenotypeGVCFs \; -R ""/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e64f393e-2ac6-43e6-9b20-cbfa905e7c33/call-GenotypeGVCFs/shard-17/inputs/1017648146/Homo_sapiens_assembly38.fasta"" \; -O ""$tmp_vcf"" \; -D ""/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e64f393e-2ac6-43e6-9b20-cbfa905e7c33/call-GenotypeGVCFs/shard-17/inputs/1017648146/Homo_sapiens_assembly38.dbsnp138.vcf"" \; -G StandardAnnotation \; --only-output-calls-starting-in-intervals \; --use-new-qual-calculator \; -V gendb://""$genomicsdb"" \; -L ""chr18:1-80373285"". ""/share/ClusterShare/software/contrib/evaben/gatk/prebuilt/4.0.4.0/bin/gatk"" --java-options ""-Xmx8g -Xms8g"" \; VariantFiltration \; --filter-expression ""ExcessHet > 54.69"" \; --filter-name ExcessHet \; -O ""output.vcf.gz"" \; -V ""$tmp_vcf""; ```. And a SGE hard memory limit of 40G (GenotypeGVCFs has -Xmx8g).; On gatk 4.0.4.0 I see peak memory usage of 15.7G, while with gatk 4.0.6.0 I get:. ```; ...; 19:06:23.757 INFO GenotypeGVCFs - Initializing engine; 19:06:24.785 INFO FeatureManager - Using codec VCFCodec to read file file:///share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e9bf8c5e-3e70-476a-99a2-833f9d38cb2f/call-GenotypeGVCFs/shard-0/inputs/1017648146/Homo_sapiens_assembly38.dbsnp138.vcf; terminate called after throwing an instance of 'std::length_error'; what(): vector::_M_default_append; ```. It seems unlikely to be just a performance regression, maybe something is wrong with my commandline/inputs that only the new version is revealing. This may be in the genomicsdb part of the codebase, as that is the input file I am reading. . [stderr of failure (4.0.6.0) ](https://github.com/broadinstitute/gatk/files/2204252/gengvcferr.txt); [stderr of success (4.0.4.0) ](https://github.com/broadinstitute/gatk/files/2204253/gengvcfgood.txt); [script of failure](https://github.com/broadinstitute/gatk/files/2204254/gengvcfscript.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024
https://github.com/broadinstitute/gatk/issues/5024:2664,Availability,failure,failure,2664,".0.4.0/bin/gatk"" --java-options ""-Xmx8g -Xms8g"" \; GenotypeGVCFs \; -R ""/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e64f393e-2ac6-43e6-9b20-cbfa905e7c33/call-GenotypeGVCFs/shard-17/inputs/1017648146/Homo_sapiens_assembly38.fasta"" \; -O ""$tmp_vcf"" \; -D ""/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e64f393e-2ac6-43e6-9b20-cbfa905e7c33/call-GenotypeGVCFs/shard-17/inputs/1017648146/Homo_sapiens_assembly38.dbsnp138.vcf"" \; -G StandardAnnotation \; --only-output-calls-starting-in-intervals \; --use-new-qual-calculator \; -V gendb://""$genomicsdb"" \; -L ""chr18:1-80373285"". ""/share/ClusterShare/software/contrib/evaben/gatk/prebuilt/4.0.4.0/bin/gatk"" --java-options ""-Xmx8g -Xms8g"" \; VariantFiltration \; --filter-expression ""ExcessHet > 54.69"" \; --filter-name ExcessHet \; -O ""output.vcf.gz"" \; -V ""$tmp_vcf""; ```. And a SGE hard memory limit of 40G (GenotypeGVCFs has -Xmx8g).; On gatk 4.0.4.0 I see peak memory usage of 15.7G, while with gatk 4.0.6.0 I get:. ```; ...; 19:06:23.757 INFO GenotypeGVCFs - Initializing engine; 19:06:24.785 INFO FeatureManager - Using codec VCFCodec to read file file:///share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e9bf8c5e-3e70-476a-99a2-833f9d38cb2f/call-GenotypeGVCFs/shard-0/inputs/1017648146/Homo_sapiens_assembly38.dbsnp138.vcf; terminate called after throwing an instance of 'std::length_error'; what(): vector::_M_default_append; ```. It seems unlikely to be just a performance regression, maybe something is wrong with my commandline/inputs that only the new version is revealing. This may be in the genomicsdb part of the codebase, as that is the input file I am reading. . [stderr of failure (4.0.6.0) ](https://github.com/broadinstitute/gatk/files/2204252/gengvcferr.txt); [stderr of success (4.0.4.0) ](https://github.com/broadinstitute/gatk/files/2204253/gengvcfgood.txt); [script of failure](https://github.com/broadinstitute/gatk/files/2204254/gengvcfscript.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024
https://github.com/broadinstitute/gatk/issues/5024:2,Deployability,update,updated,2,"I updated from 4.0.4.0 to 4.0.6.0 and noticed either a memory bug or spike in GenotypeGVCF. . Based on https://github.com/EvanTheB/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.simple.wdl:. ```; ""/share/ClusterShare/software/contrib/evaben/gatk/prebuilt/4.0.4.0/bin/gatk"" --java-options ""-Xmx8g -Xms8g"" \; GenomicsDBImport \; --genomicsdb-workspace-path ""$genomicsdb"" \; --batch-size ""50"" \; -L ""chr18:1-80373285"" \; --sample-name-map ""/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e64f393e-2ac6-43e6-9b20-cbfa905e7c33/call-GenotypeGVCFs/shard-17/inputs/-321562876/sample_map"" \; --reader-threads 5 \; -ip 500. tmp_vcf=""$TMPDIR""/tmp.vcf.gz. ""/share/ClusterShare/software/contrib/evaben/gatk/prebuilt/4.0.4.0/bin/gatk"" --java-options ""-Xmx8g -Xms8g"" \; GenotypeGVCFs \; -R ""/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e64f393e-2ac6-43e6-9b20-cbfa905e7c33/call-GenotypeGVCFs/shard-17/inputs/1017648146/Homo_sapiens_assembly38.fasta"" \; -O ""$tmp_vcf"" \; -D ""/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e64f393e-2ac6-43e6-9b20-cbfa905e7c33/call-GenotypeGVCFs/shard-17/inputs/1017648146/Homo_sapiens_assembly38.dbsnp138.vcf"" \; -G StandardAnnotation \; --only-output-calls-starting-in-intervals \; --use-new-qual-calculator \; -V gendb://""$genomicsdb"" \; -L ""chr18:1-80373285"". ""/share/ClusterShare/software/contrib/evaben/gatk/prebuilt/4.0.4.0/bin/gatk"" --java-options ""-Xmx8g -Xms8g"" \; VariantFiltration \; --filter-expression ""ExcessHet > 54.69"" \; --filter-name ExcessHet \; -O ""output.vcf.gz"" \; -V ""$tmp_vcf""; ```. And a SGE hard memory limit of 40G (GenotypeGVCFs has -Xmx8g).; On gatk 4.0.4.0 I see peak memory usage of 15.7G, while with gatk 4.0.6.0 I get:. ```; ...; 19:06:23.757 INFO GenotypeGVCFs - Initializing engine; 19:06:24.785 INFO FeatureManager - Using codec VCFCodec to read file file:///share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e9bf8c5e-3e70-476a-9",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024
https://github.com/broadinstitute/gatk/issues/5024:2240,Performance,perform,performance,2240,".0.4.0/bin/gatk"" --java-options ""-Xmx8g -Xms8g"" \; GenotypeGVCFs \; -R ""/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e64f393e-2ac6-43e6-9b20-cbfa905e7c33/call-GenotypeGVCFs/shard-17/inputs/1017648146/Homo_sapiens_assembly38.fasta"" \; -O ""$tmp_vcf"" \; -D ""/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e64f393e-2ac6-43e6-9b20-cbfa905e7c33/call-GenotypeGVCFs/shard-17/inputs/1017648146/Homo_sapiens_assembly38.dbsnp138.vcf"" \; -G StandardAnnotation \; --only-output-calls-starting-in-intervals \; --use-new-qual-calculator \; -V gendb://""$genomicsdb"" \; -L ""chr18:1-80373285"". ""/share/ClusterShare/software/contrib/evaben/gatk/prebuilt/4.0.4.0/bin/gatk"" --java-options ""-Xmx8g -Xms8g"" \; VariantFiltration \; --filter-expression ""ExcessHet > 54.69"" \; --filter-name ExcessHet \; -O ""output.vcf.gz"" \; -V ""$tmp_vcf""; ```. And a SGE hard memory limit of 40G (GenotypeGVCFs has -Xmx8g).; On gatk 4.0.4.0 I see peak memory usage of 15.7G, while with gatk 4.0.6.0 I get:. ```; ...; 19:06:23.757 INFO GenotypeGVCFs - Initializing engine; 19:06:24.785 INFO FeatureManager - Using codec VCFCodec to read file file:///share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e9bf8c5e-3e70-476a-99a2-833f9d38cb2f/call-GenotypeGVCFs/shard-0/inputs/1017648146/Homo_sapiens_assembly38.dbsnp138.vcf; terminate called after throwing an instance of 'std::length_error'; what(): vector::_M_default_append; ```. It seems unlikely to be just a performance regression, maybe something is wrong with my commandline/inputs that only the new version is revealing. This may be in the genomicsdb part of the codebase, as that is the input file I am reading. . [stderr of failure (4.0.6.0) ](https://github.com/broadinstitute/gatk/files/2204252/gengvcferr.txt); [stderr of success (4.0.4.0) ](https://github.com/broadinstitute/gatk/files/2204253/gengvcfgood.txt); [script of failure](https://github.com/broadinstitute/gatk/files/2204254/gengvcfscript.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024
https://github.com/broadinstitute/gatk/issues/5024:192,Usability,simpl,simple,192,"I updated from 4.0.4.0 to 4.0.6.0 and noticed either a memory bug or spike in GenotypeGVCF. . Based on https://github.com/EvanTheB/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.simple.wdl:. ```; ""/share/ClusterShare/software/contrib/evaben/gatk/prebuilt/4.0.4.0/bin/gatk"" --java-options ""-Xmx8g -Xms8g"" \; GenomicsDBImport \; --genomicsdb-workspace-path ""$genomicsdb"" \; --batch-size ""50"" \; -L ""chr18:1-80373285"" \; --sample-name-map ""/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e64f393e-2ac6-43e6-9b20-cbfa905e7c33/call-GenotypeGVCFs/shard-17/inputs/-321562876/sample_map"" \; --reader-threads 5 \; -ip 500. tmp_vcf=""$TMPDIR""/tmp.vcf.gz. ""/share/ClusterShare/software/contrib/evaben/gatk/prebuilt/4.0.4.0/bin/gatk"" --java-options ""-Xmx8g -Xms8g"" \; GenotypeGVCFs \; -R ""/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e64f393e-2ac6-43e6-9b20-cbfa905e7c33/call-GenotypeGVCFs/shard-17/inputs/1017648146/Homo_sapiens_assembly38.fasta"" \; -O ""$tmp_vcf"" \; -D ""/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e64f393e-2ac6-43e6-9b20-cbfa905e7c33/call-GenotypeGVCFs/shard-17/inputs/1017648146/Homo_sapiens_assembly38.dbsnp138.vcf"" \; -G StandardAnnotation \; --only-output-calls-starting-in-intervals \; --use-new-qual-calculator \; -V gendb://""$genomicsdb"" \; -L ""chr18:1-80373285"". ""/share/ClusterShare/software/contrib/evaben/gatk/prebuilt/4.0.4.0/bin/gatk"" --java-options ""-Xmx8g -Xms8g"" \; VariantFiltration \; --filter-expression ""ExcessHet > 54.69"" \; --filter-name ExcessHet \; -O ""output.vcf.gz"" \; -V ""$tmp_vcf""; ```. And a SGE hard memory limit of 40G (GenotypeGVCFs has -Xmx8g).; On gatk 4.0.4.0 I see peak memory usage of 15.7G, while with gatk 4.0.6.0 I get:. ```; ...; 19:06:23.757 INFO GenotypeGVCFs - Initializing engine; 19:06:24.785 INFO FeatureManager - Using codec VCFCodec to read file file:///share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e9bf8c5e-3e70-476a-9",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024
https://github.com/broadinstitute/gatk/issues/5025:505,Availability,error,error,505,"## Bug Report. ### Affected tool(s) or class(es); M2 WDL. ### Description ; If you pass an empty array for artifact_modes (i.e. artifact_modes = []) to the `FilterByOrientationBias` task when `run_orientation_bias_filter` is true, it will create an erroneous command line. #### Expected behavior; The orientation bias filter should not run, or should not do anything. #### Actual behavior; UNCONFIRMED: The tool will crash due to erroneous command line invocation (`... -AM -P ...`) which should yield an error that `""-P""` is not a valid artifact mode.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5025
https://github.com/broadinstitute/gatk/pull/5027:53,Testability,test,tested,53,"This is a relatively simple change, but has not been tested. The reviewer should make sure I haven't screwed it up!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5027
https://github.com/broadinstitute/gatk/pull/5027:21,Usability,simpl,simple,21,"This is a relatively simple change, but has not been tested. The reviewer should make sure I haven't screwed it up!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5027
https://github.com/broadinstitute/gatk/issues/5029:19,Testability,test,tests,19,These get put into tests/testOnPackagedReleaseJar rather than tests/test.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5029
https://github.com/broadinstitute/gatk/issues/5029:25,Testability,test,testOnPackagedReleaseJar,25,These get put into tests/testOnPackagedReleaseJar rather than tests/test.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5029
https://github.com/broadinstitute/gatk/issues/5029:62,Testability,test,tests,62,These get put into tests/testOnPackagedReleaseJar rather than tests/test.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5029
https://github.com/broadinstitute/gatk/issues/5029:68,Testability,test,test,68,These get put into tests/testOnPackagedReleaseJar rather than tests/test.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5029
https://github.com/broadinstitute/gatk/pull/5033:75,Deployability,update,updated,75,"@anowlcalledjosh and @bbimber Thanks for your contributions to GATK - I've updated the AUTHORs list with your names - can you verify that it looks correct ? (@bbimber not sure I got your name right, and let me know if you have an email address to include). Thx.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5033
https://github.com/broadinstitute/gatk/issues/5034:325,Modifiability,refactor,refactored,325,"The MendelianViolation class, as ported from GATK3, is used both to get mendelian violation state for a single variant/set of samples, as well as an accumulator for counting violations for multiple variants. It contains two isViolation methods, one of which clobbers the cumulative state without warning. The class should be refactored to make the two usage patterns distinct and less prone to accidental misuse.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5034
https://github.com/broadinstitute/gatk/issues/5036:264,Availability,error,error,264,## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of 7/18/18. ### Description ; When running Mutect yesterday on Mitochondrial data I got the following error:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:chrM start:-4 end:65. 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:728); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:86); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:48); 	at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); 	at org.broadinstitute.hellbender.transformers.ReadTransformer$$Lambda$107/1786040872.apply(Unknown Source); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.fillDownsampledReadsCache(ReadsDownsamplingIterator.java:69); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.advanceToNextRead(ReadsDownsamplingIterator.java:55); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.<init>(ReadsDownsamplingIterator.java:34); 	at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:149); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:109); 	at org.broadinstitute.hellbend,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:1392,Availability,down,downsampling,1392,s.Utils.validateArg(Utils.java:728); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:86); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:48); 	at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); 	at org.broadinstitute.hellbender.transformers.ReadTransformer$$Lambda$107/1786040872.apply(Unknown Source); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.fillDownsampledReadsCache(ReadsDownsamplingIterator.java:69); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.advanceToNextRead(ReadsDownsamplingIterator.java:55); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.<init>(ReadsDownsamplingIterator.java:34); 	at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:149); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:109); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:282); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:993); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdli,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:1533,Availability,down,downsampling,1533,g.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:86); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:48); 	at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); 	at org.broadinstitute.hellbender.transformers.ReadTransformer$$Lambda$107/1786040872.apply(Unknown Source); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.fillDownsampledReadsCache(ReadsDownsamplingIterator.java:69); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.advanceToNextRead(ReadsDownsamplingIterator.java:55); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.<init>(ReadsDownsamplingIterator.java:34); 	at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:149); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:109); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:282); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:993); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.in,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:1666,Availability,down,downsampling,1666,dromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:86); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:48); 	at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); 	at org.broadinstitute.hellbender.transformers.ReadTransformer$$Lambda$107/1786040872.apply(Unknown Source); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.fillDownsampledReadsCache(ReadsDownsamplingIterator.java:69); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.advanceToNextRead(ReadsDownsamplingIterator.java:55); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.<init>(ReadsDownsamplingIterator.java:34); 	at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:149); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:109); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:282); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:993); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:5499,Availability,error,error,5499,"r.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; ""-"" + M2ArgumentCollection.TUMOR_SAMPLE_SHORT_NAME, ""198489"",; ""-R"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/MitochondriaOnlyFastas/Homo_sapiens_assembly38.mt_only.fasta"",; ""-O"", outputVcf.getAbsolutePath(),; ""--max-reads-per-alignment-start"", ""0"",; ""-default-af"", ""0"",; ""--initial-tumor-lod"", ""0"",; ""--tumor-lod-to-emit"", ""0"",; ""--min-pruning"", ""10"",; ""--annotation"", ""StrandBiasBySample"",; //""--ignore-itr-artifacts"", ""true"",; };; ```. I tried it with and without `--ignore-itr-artifacts` but got the same error both times. @davidbenjamin Any idea if this is an easy fix?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:104,Deployability,release,release,104,## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of 7/18/18. ### Description ; When running Mutect yesterday on Mitochondrial data I got the following error:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:chrM start:-4 end:65. 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:728); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:86); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:48); 	at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); 	at org.broadinstitute.hellbender.transformers.ReadTransformer$$Lambda$107/1786040872.apply(Unknown Source); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.fillDownsampledReadsCache(ReadsDownsamplingIterator.java:69); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.advanceToNextRead(ReadsDownsamplingIterator.java:55); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.<init>(ReadsDownsamplingIterator.java:34); 	at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:149); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:109); 	at org.broadinstitute.hellbend,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:406,Security,validat,validateArg,406,## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of 7/18/18. ### Description ; When running Mutect yesterday on Mitochondrial data I got the following error:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:chrM start:-4 end:65. 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:728); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:86); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:48); 	at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); 	at org.broadinstitute.hellbender.transformers.ReadTransformer$$Lambda$107/1786040872.apply(Unknown Source); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.fillDownsampledReadsCache(ReadsDownsamplingIterator.java:69); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.advanceToNextRead(ReadsDownsamplingIterator.java:55); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.<init>(ReadsDownsamplingIterator.java:34); 	at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:149); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:109); 	at org.broadinstitute.hellbend,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:490,Security,validat,validatePositions,490,## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of 7/18/18. ### Description ; When running Mutect yesterday on Mitochondrial data I got the following error:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:chrM start:-4 end:65. 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:728); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:86); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:48); 	at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); 	at org.broadinstitute.hellbender.transformers.ReadTransformer$$Lambda$107/1786040872.apply(Unknown Source); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.fillDownsampledReadsCache(ReadsDownsamplingIterator.java:69); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.advanceToNextRead(ReadsDownsamplingIterator.java:55); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.<init>(ReadsDownsamplingIterator.java:34); 	at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:149); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:109); 	at org.broadinstitute.hellbend,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:2939,Testability,test,test,2939,blyRegionIterator.java:109); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:282); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:993); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:146); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:187); 	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:32); 	at org.broadinstitute.hellbender.utils.test.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:97); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2IntegrationTest.testTumorNormal(Mutect2IntegrationTest.java:237); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunne,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:3097,Testability,test,testTumorNormal,3097,ngine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:993); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:146); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:187); 	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:32); 	at org.broadinstitute.hellbender.utils.test.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:97); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2IntegrationTest.testTumorNormal(Mutect2IntegrationTest.java:237); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunn,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:3448,Testability,test,testng,3448,andLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:146); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:187); 	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:32); 	at org.broadinstitute.hellbender.utils.test.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:97); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2IntegrationTest.testTumorNormal(Mutect2IntegrationTest.java:237); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:3542,Testability,test,testng,3542,eMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:146); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:187); 	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:32); 	at org.broadinstitute.hellbender.utils.test.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:97); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2IntegrationTest.testTumorNormal(Mutect2IntegrationTest.java:237); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:3606,Testability,test,testng,3606,ender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:146); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:187); 	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:32); 	at org.broadinstitute.hellbender.utils.test.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:97); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2IntegrationTest.testTumorNormal(Mutect2IntegrationTest.java:237); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:3674,Testability,test,testng,3674,ute.hellbender.Main.instanceMain(Main.java:146); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:187); 	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:32); 	at org.broadinstitute.hellbender.utils.test.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:97); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2IntegrationTest.testTumorNormal(Mutect2IntegrationTest.java:237); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.Re,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:3744,Testability,test,testng,3744,e.hellbender.Main.instanceMain(Main.java:187); 	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:32); 	at org.broadinstitute.hellbender.utils.test.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:97); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2IntegrationTest.testTumorNormal(Mutect2IntegrationTest.java:237); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:3760,Testability,Test,TestMethodWorker,3760,tanceMain(Main.java:187); 	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:32); 	at org.broadinstitute.hellbender.utils.test.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:97); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2IntegrationTest.testTumorNormal(Mutect2IntegrationTest.java:237); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:3795,Testability,Test,TestMethodWorker,3795,	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:32); 	at org.broadinstitute.hellbender.utils.test.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:97); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2IntegrationTest.testTumorNormal(Mutect2IntegrationTest.java:237); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (t,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:3831,Testability,test,testng,3831,dLineProgramTest.runCommandLine(CommandLineProgramTest.java:32); 	at org.broadinstitute.hellbender.utils.test.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:97); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2IntegrationTest.testTumorNormal(Mutect2IntegrationTest.java:237); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ``,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:3847,Testability,Test,TestMethodWorker,3847,ommandLine(CommandLineProgramTest.java:32); 	at org.broadinstitute.hellbender.utils.test.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:97); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2IntegrationTest.testTumorNormal(Mutect2IntegrationTest.java:237); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] arg,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:3868,Testability,Test,TestMethodWorker,3868,"ineProgramTest.java:32); 	at org.broadinstitute.hellbender.utils.test.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:97); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2IntegrationTest.testTumorNormal(Mutect2IntegrationTest.java:237); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humg",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:3904,Testability,test,testng,3904,"broadinstitute.hellbender.utils.test.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:97); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2IntegrationTest.testTumorNormal(Mutect2IntegrationTest.java:237); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/Spec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:3911,Testability,Test,TestRunner,3911,"itute.hellbender.utils.test.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:97); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2IntegrationTest.testTumorNormal(Mutect2IntegrationTest.java:237); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitoc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:3933,Testability,Test,TestRunner,3933,"utils.test.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:97); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2IntegrationTest.testTumorNormal(Mutect2IntegrationTest.java:237); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filterin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:3963,Testability,test,testng,3963,"er.runCommandLine(CommandLineProgramTester.java:97); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2IntegrationTest.testTumorNormal(Mutect2IntegrationTest.java:237); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:3970,Testability,Test,TestRunner,3970,"mandLine(CommandLineProgramTester.java:97); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2IntegrationTest.testTumorNormal(Mutect2IntegrationTest.java:237); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; """,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:3985,Testability,Test,TestRunner,3985,"andLineProgramTester.java:97); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2IntegrationTest.testTumorNormal(Mutect2IntegrationTest.java:237); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; ""-"" + M2Argume",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:4015,Testability,test,testng,4015," 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2IntegrationTest.testTumorNormal(Mutect2IntegrationTest.java:237); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; ""-"" + M2ArgumentCollection.TUMOR_SAMPLE_SHOR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:4073,Testability,test,testng,4073,"tect2IntegrationTest.testTumorNormal(Mutect2IntegrationTest.java:237); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; ""-"" + M2ArgumentCollection.TUMOR_SAMPLE_SHORT_NAME, ""198489"",; ""-R"", ""/humgen/gsa-hpprojects/dev/mshan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:4139,Testability,test,testng,4139,"37); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; ""-"" + M2ArgumentCollection.TUMOR_SAMPLE_SHORT_NAME, ""198489"",; ""-R"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/MitochondriaOnlyFastas/Homo_sapiens_assembl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:4200,Testability,test,testng,4200,"Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; ""-"" + M2ArgumentCollection.TUMOR_SAMPLE_SHORT_NAME, ""198489"",; ""-R"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/MitochondriaOnlyFastas/Homo_sapiens_assembly38.mt_only.fasta"",; ""-O"", outputVcf.getAbsolutePath(),; ""--m",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:4254,Testability,test,testng,4254,"ke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; ""-"" + M2ArgumentCollection.TUMOR_SAMPLE_SHORT_NAME, ""198489"",; ""-R"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/MitochondriaOnlyFastas/Homo_sapiens_assembly38.mt_only.fasta"",; ""-O"", outputVcf.getAbsolutePath(),; ""--max-reads-per-alignment-start"", ""0"",; ""-default-af"", ""0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:4324,Testability,test,testng,4324,"AccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; ""-"" + M2ArgumentCollection.TUMOR_SAMPLE_SHORT_NAME, ""198489"",; ""-R"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/MitochondriaOnlyFastas/Homo_sapiens_assembly38.mt_only.fasta"",; ""-O"", outputVcf.getAbsolutePath(),; ""--max-reads-per-alignment-start"", ""0"",; ""-default-af"", ""0"",; ""--initial-tumor-lod"", ""0"",; ""--tumor-lod-to-emit"", ""0"",; ""--min-p",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:4389,Testability,test,testng,4389,"va.lang.reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; ""-"" + M2ArgumentCollection.TUMOR_SAMPLE_SHORT_NAME, ""198489"",; ""-R"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/MitochondriaOnlyFastas/Homo_sapiens_assembly38.mt_only.fasta"",; ""-O"", outputVcf.getAbsolutePath(),; ""--max-reads-per-alignment-start"", ""0"",; ""-default-af"", ""0"",; ""--initial-tumor-lod"", ""0"",; ""--tumor-lod-to-emit"", ""0"",; ""--min-pruning"", ""10"",; ""--annotation"", ""StrandBiasBySample"",; //""--ignor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:4396,Testability,Test,TestNG,4396,".reflect.Method.invoke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; ""-"" + M2ArgumentCollection.TUMOR_SAMPLE_SHORT_NAME, ""198489"",; ""-R"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/MitochondriaOnlyFastas/Homo_sapiens_assembly38.mt_only.fasta"",; ""-O"", outputVcf.getAbsolutePath(),; ""--max-reads-per-alignment-start"", ""0"",; ""-default-af"", ""0"",; ""--initial-tumor-lod"", ""0"",; ""--tumor-lod-to-emit"", ""0"",; ""--min-pruning"", ""10"",; ""--annotation"", ""StrandBiasBySample"",; //""--ignore-itr-a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:4425,Testability,Test,TestNG,4425,"voke(Method.java:497); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; ""-"" + M2ArgumentCollection.TUMOR_SAMPLE_SHORT_NAME, ""198489"",; ""-R"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/MitochondriaOnlyFastas/Homo_sapiens_assembly38.mt_only.fasta"",; ""-O"", outputVcf.getAbsolutePath(),; ""--max-reads-per-alignment-start"", ""0"",; ""-default-af"", ""0"",; ""--initial-tumor-lod"", ""0"",; ""--tumor-lod-to-emit"", ""0"",; ""--min-pruning"", ""10"",; ""--annotation"", ""StrandBiasBySample"",; //""--ignore-itr-artifacts"", ""true"",",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:4452,Testability,test,testng,4452,"internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; ""-"" + M2ArgumentCollection.TUMOR_SAMPLE_SHORT_NAME, ""198489"",; ""-R"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/MitochondriaOnlyFastas/Homo_sapiens_assembly38.mt_only.fasta"",; ""-O"", outputVcf.getAbsolutePath(),; ""--max-reads-per-alignment-start"", ""0"",; ""-default-af"", ""0"",; ""--initial-tumor-lod"", ""0"",; ""--tumor-lod-to-emit"", ""0"",; ""--min-pruning"", ""10"",; ""--annotation"", ""StrandBiasBySample"",; //""--ignore-itr-artifacts"", ""true"",; };; ```. I tried it with and without",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:4459,Testability,Test,TestNG,4459,"l.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; ""-"" + M2ArgumentCollection.TUMOR_SAMPLE_SHORT_NAME, ""198489"",; ""-R"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/MitochondriaOnlyFastas/Homo_sapiens_assembly38.mt_only.fasta"",; ""-O"", outputVcf.getAbsolutePath(),; ""--max-reads-per-alignment-start"", ""0"",; ""-default-af"", ""0"",; ""--initial-tumor-lod"", ""0"",; ""--tumor-lod-to-emit"", ""0"",; ""--min-pruning"", ""10"",; ""--annotation"", ""StrandBiasBySample"",; //""--ignore-itr-artifacts"", ""true"",; };; ```. I tried it with and without `--ign",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:4483,Testability,Test,TestNG,4483,"onHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; ""-"" + M2ArgumentCollection.TUMOR_SAMPLE_SHORT_NAME, ""198489"",; ""-R"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/MitochondriaOnlyFastas/Homo_sapiens_assembly38.mt_only.fasta"",; ""-O"", outputVcf.getAbsolutePath(),; ""--max-reads-per-alignment-start"", ""0"",; ""-default-af"", ""0"",; ""--initial-tumor-lod"", ""0"",; ""--tumor-lod-to-emit"", ""0"",; ""--min-pruning"", ""10"",; ""--annotation"", ""StrandBiasBySample"",; //""--ignore-itr-artifacts"", ""true"",; };; ```. I tried it with and without `--ignore-itr-artifac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:4510,Testability,test,testng,4510,"ionHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; ""-"" + M2ArgumentCollection.TUMOR_SAMPLE_SHORT_NAME, ""198489"",; ""-R"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/MitochondriaOnlyFastas/Homo_sapiens_assembly38.mt_only.fasta"",; ""-O"", outputVcf.getAbsolutePath(),; ""--max-reads-per-alignment-start"", ""0"",; ""-default-af"", ""0"",; ""--initial-tumor-lod"", ""0"",; ""--tumor-lod-to-emit"", ""0"",; ""--min-pruning"", ""10"",; ""--annotation"", ""StrandBiasBySample"",; //""--ignore-itr-artifacts"", ""true"",; };; ```. I tried it with and without `--ignore-itr-artifacts` but got the same error both time",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:4517,Testability,Test,TestNG,4517,"er.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; ""-"" + M2ArgumentCollection.TUMOR_SAMPLE_SHORT_NAME, ""198489"",; ""-R"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/MitochondriaOnlyFastas/Homo_sapiens_assembly38.mt_only.fasta"",; ""-O"", outputVcf.getAbsolutePath(),; ""--max-reads-per-alignment-start"", ""0"",; ""-default-af"", ""0"",; ""--initial-tumor-lod"", ""0"",; ""--tumor-lod-to-emit"", ""0"",; ""--min-pruning"", ""10"",; ""--annotation"", ""StrandBiasBySample"",; //""--ignore-itr-artifacts"", ""true"",; };; ```. I tried it with and without `--ignore-itr-artifacts` but got the same error both times. @dav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:4534,Testability,Test,TestNG,4534,"; 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; ""-"" + M2ArgumentCollection.TUMOR_SAMPLE_SHORT_NAME, ""198489"",; ""-R"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/MitochondriaOnlyFastas/Homo_sapiens_assembly38.mt_only.fasta"",; ""-O"", outputVcf.getAbsolutePath(),; ""--max-reads-per-alignment-start"", ""0"",; ""-default-af"", ""0"",; ""--initial-tumor-lod"", ""0"",; ""--tumor-lod-to-emit"", ""0"",; ""--min-pruning"", ""10"",; ""--annotation"", ""StrandBiasBySample"",; //""--ignore-itr-artifacts"", ""true"",; };; ```. I tried it with and without `--ignore-itr-artifacts` but got the same error both times. @davidbenjamin A",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:4561,Testability,test,testng,4561,"r.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; ""-"" + M2ArgumentCollection.TUMOR_SAMPLE_SHORT_NAME, ""198489"",; ""-R"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/MitochondriaOnlyFastas/Homo_sapiens_assembly38.mt_only.fasta"",; ""-O"", outputVcf.getAbsolutePath(),; ""--max-reads-per-alignment-start"", ""0"",; ""-default-af"", ""0"",; ""--initial-tumor-lod"", ""0"",; ""--tumor-lod-to-emit"", ""0"",; ""--min-pruning"", ""10"",; ""--annotation"", ""StrandBiasBySample"",; //""--ignore-itr-artifacts"", ""true"",; };; ```. I tried it with and without `--ignore-itr-artifacts` but got the same error both times. @davidbenjamin Any idea if this is an easy fix?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:4568,Testability,Test,TestNG,4568,"r.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; ""-"" + M2ArgumentCollection.TUMOR_SAMPLE_SHORT_NAME, ""198489"",; ""-R"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/MitochondriaOnlyFastas/Homo_sapiens_assembly38.mt_only.fasta"",; ""-O"", outputVcf.getAbsolutePath(),; ""--max-reads-per-alignment-start"", ""0"",; ""-default-af"", ""0"",; ""--initial-tumor-lod"", ""0"",; ""--tumor-lod-to-emit"", ""0"",; ""--min-pruning"", ""10"",; ""--annotation"", ""StrandBiasBySample"",; //""--ignore-itr-artifacts"", ""true"",; };; ```. I tried it with and without `--ignore-itr-artifacts` but got the same error both times. @davidbenjamin Any idea if this is an easy fix?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:4579,Testability,Test,TestNG,4579,"r.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; ""-"" + M2ArgumentCollection.TUMOR_SAMPLE_SHORT_NAME, ""198489"",; ""-R"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/MitochondriaOnlyFastas/Homo_sapiens_assembly38.mt_only.fasta"",; ""-O"", outputVcf.getAbsolutePath(),; ""--max-reads-per-alignment-start"", ""0"",; ""-default-af"", ""0"",; ""--initial-tumor-lod"", ""0"",; ""--tumor-lod-to-emit"", ""0"",; ""--min-pruning"", ""10"",; ""--annotation"", ""StrandBiasBySample"",; //""--ignore-itr-artifacts"", ""true"",; };; ```. I tried it with and without `--ignore-itr-artifacts` but got the same error both times. @davidbenjamin Any idea if this is an easy fix?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:4606,Testability,test,testng,4606,"r.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; ""-"" + M2ArgumentCollection.TUMOR_SAMPLE_SHORT_NAME, ""198489"",; ""-R"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/MitochondriaOnlyFastas/Homo_sapiens_assembly38.mt_only.fasta"",; ""-O"", outputVcf.getAbsolutePath(),; ""--max-reads-per-alignment-start"", ""0"",; ""-default-af"", ""0"",; ""--initial-tumor-lod"", ""0"",; ""--tumor-lod-to-emit"", ""0"",; ""--min-pruning"", ""10"",; ""--annotation"", ""StrandBiasBySample"",; //""--ignore-itr-artifacts"", ""true"",; };; ```. I tried it with and without `--ignore-itr-artifacts` but got the same error both times. @davidbenjamin Any idea if this is an easy fix?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:4669,Testability,test,testng,4669,"r.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; ""-"" + M2ArgumentCollection.TUMOR_SAMPLE_SHORT_NAME, ""198489"",; ""-R"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/MitochondriaOnlyFastas/Homo_sapiens_assembly38.mt_only.fasta"",; ""-O"", outputVcf.getAbsolutePath(),; ""--max-reads-per-alignment-start"", ""0"",; ""-default-af"", ""0"",; ""--initial-tumor-lod"", ""0"",; ""--tumor-lod-to-emit"", ""0"",; ""--min-pruning"", ""10"",; ""--annotation"", ""StrandBiasBySample"",; //""--ignore-itr-artifacts"", ""true"",; };; ```. I tried it with and without `--ignore-itr-artifacts` but got the same error both times. @davidbenjamin Any idea if this is an easy fix?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:475,Usability,Simpl,SimpleInterval,475,## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of 7/18/18. ### Description ; When running Mutect yesterday on Mitochondrial data I got the following error:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:chrM start:-4 end:65. 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:728); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:86); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:48); 	at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); 	at org.broadinstitute.hellbender.transformers.ReadTransformer$$Lambda$107/1786040872.apply(Unknown Source); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.fillDownsampledReadsCache(ReadsDownsamplingIterator.java:69); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.advanceToNextRead(ReadsDownsamplingIterator.java:55); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.<init>(ReadsDownsamplingIterator.java:34); 	at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:149); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:109); 	at org.broadinstitute.hellbend,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:508,Usability,Simpl,SimpleInterval,508,## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of 7/18/18. ### Description ; When running Mutect yesterday on Mitochondrial data I got the following error:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:chrM start:-4 end:65. 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:728); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:86); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:48); 	at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); 	at org.broadinstitute.hellbender.transformers.ReadTransformer$$Lambda$107/1786040872.apply(Unknown Source); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.fillDownsampledReadsCache(ReadsDownsamplingIterator.java:69); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.advanceToNextRead(ReadsDownsamplingIterator.java:55); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.<init>(ReadsDownsamplingIterator.java:34); 	at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:149); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:109); 	at org.broadinstitute.hellbend,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:573,Usability,Simpl,SimpleInterval,573,## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of 7/18/18. ### Description ; When running Mutect yesterday on Mitochondrial data I got the following error:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:chrM start:-4 end:65. 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:728); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:86); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:48); 	at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); 	at org.broadinstitute.hellbender.transformers.ReadTransformer$$Lambda$107/1786040872.apply(Unknown Source); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.fillDownsampledReadsCache(ReadsDownsamplingIterator.java:69); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.advanceToNextRead(ReadsDownsamplingIterator.java:55); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.<init>(ReadsDownsamplingIterator.java:34); 	at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:149); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:109); 	at org.broadinstitute.hellbend,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5036:595,Usability,Simpl,SimpleInterval,595,## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of 7/18/18. ### Description ; When running Mutect yesterday on Mitochondrial data I got the following error:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:chrM start:-4 end:65. 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:728); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:86); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:48); 	at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); 	at org.broadinstitute.hellbender.transformers.ReadTransformer$$Lambda$107/1786040872.apply(Unknown Source); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.fillDownsampledReadsCache(ReadsDownsamplingIterator.java:69); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.advanceToNextRead(ReadsDownsamplingIterator.java:55); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.<init>(ReadsDownsamplingIterator.java:34); 	at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:149); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:109); 	at org.broadinstitute.hellbend,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036
https://github.com/broadinstitute/gatk/issues/5039:92,Performance,Load,Load,92,"I'm working to migrate GATK3's VariantEval to GATK4. This includes this code ~line 319:. // Load the sample list, using an intermediate tree set to sort the samples; final Set<String> allSampleNames = SampleUtils.getSamplesFromCommandLineInput(vcfSamples);; sampleNamesForEvaluation.addAll(new TreeSet<String>(SampleUtils.getSamplesFromCommandLineInput(vcfSamples, SAMPLE_EXPRESSIONS)));; isSubsettingSamples = ! sampleNamesForEvaluation.containsAll(allSampleNames);. Is there a GATK4 equivalent to SampleUtils.getSamplesFromCommandLineInput()? The basic features this has are: 1) Given a list of string or filepaths, it parses everything into a set, 2) Filters the list based on SAMPLE_EXPRESSIONS (which is less hard to do). Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5039
https://github.com/broadinstitute/gatk/pull/5040:33,Deployability,install,installed,33,"The ggplot2 R dependency was not installed correctly in the gatkbase-2.0.0 Docker image. It appears that this resulted from a recent ggplot2 update that has broken dependencies (perhaps for the version of R that we use). This missing ggplot2 dependency was the root cause of #5022. I updated the install_R_packages.R script, which should now fail if any package fails to install, and pushed an updated gatkbase-2.0.1 image. The second commit addresses #5022. This should be considered a temporary fix until #5026 is in. Closes #5022.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5040
https://github.com/broadinstitute/gatk/pull/5040:141,Deployability,update,update,141,"The ggplot2 R dependency was not installed correctly in the gatkbase-2.0.0 Docker image. It appears that this resulted from a recent ggplot2 update that has broken dependencies (perhaps for the version of R that we use). This missing ggplot2 dependency was the root cause of #5022. I updated the install_R_packages.R script, which should now fail if any package fails to install, and pushed an updated gatkbase-2.0.1 image. The second commit addresses #5022. This should be considered a temporary fix until #5026 is in. Closes #5022.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5040
https://github.com/broadinstitute/gatk/pull/5040:284,Deployability,update,updated,284,"The ggplot2 R dependency was not installed correctly in the gatkbase-2.0.0 Docker image. It appears that this resulted from a recent ggplot2 update that has broken dependencies (perhaps for the version of R that we use). This missing ggplot2 dependency was the root cause of #5022. I updated the install_R_packages.R script, which should now fail if any package fails to install, and pushed an updated gatkbase-2.0.1 image. The second commit addresses #5022. This should be considered a temporary fix until #5026 is in. Closes #5022.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5040
https://github.com/broadinstitute/gatk/pull/5040:371,Deployability,install,install,371,"The ggplot2 R dependency was not installed correctly in the gatkbase-2.0.0 Docker image. It appears that this resulted from a recent ggplot2 update that has broken dependencies (perhaps for the version of R that we use). This missing ggplot2 dependency was the root cause of #5022. I updated the install_R_packages.R script, which should now fail if any package fails to install, and pushed an updated gatkbase-2.0.1 image. The second commit addresses #5022. This should be considered a temporary fix until #5026 is in. Closes #5022.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5040
https://github.com/broadinstitute/gatk/pull/5040:394,Deployability,update,updated,394,"The ggplot2 R dependency was not installed correctly in the gatkbase-2.0.0 Docker image. It appears that this resulted from a recent ggplot2 update that has broken dependencies (perhaps for the version of R that we use). This missing ggplot2 dependency was the root cause of #5022. I updated the install_R_packages.R script, which should now fail if any package fails to install, and pushed an updated gatkbase-2.0.1 image. The second commit addresses #5022. This should be considered a temporary fix until #5026 is in. Closes #5022.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5040
https://github.com/broadinstitute/gatk/pull/5040:14,Integrability,depend,dependency,14,"The ggplot2 R dependency was not installed correctly in the gatkbase-2.0.0 Docker image. It appears that this resulted from a recent ggplot2 update that has broken dependencies (perhaps for the version of R that we use). This missing ggplot2 dependency was the root cause of #5022. I updated the install_R_packages.R script, which should now fail if any package fails to install, and pushed an updated gatkbase-2.0.1 image. The second commit addresses #5022. This should be considered a temporary fix until #5026 is in. Closes #5022.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5040
https://github.com/broadinstitute/gatk/pull/5040:164,Integrability,depend,dependencies,164,"The ggplot2 R dependency was not installed correctly in the gatkbase-2.0.0 Docker image. It appears that this resulted from a recent ggplot2 update that has broken dependencies (perhaps for the version of R that we use). This missing ggplot2 dependency was the root cause of #5022. I updated the install_R_packages.R script, which should now fail if any package fails to install, and pushed an updated gatkbase-2.0.1 image. The second commit addresses #5022. This should be considered a temporary fix until #5026 is in. Closes #5022.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5040
https://github.com/broadinstitute/gatk/pull/5040:242,Integrability,depend,dependency,242,"The ggplot2 R dependency was not installed correctly in the gatkbase-2.0.0 Docker image. It appears that this resulted from a recent ggplot2 update that has broken dependencies (perhaps for the version of R that we use). This missing ggplot2 dependency was the root cause of #5022. I updated the install_R_packages.R script, which should now fail if any package fails to install, and pushed an updated gatkbase-2.0.1 image. The second commit addresses #5022. This should be considered a temporary fix until #5026 is in. Closes #5022.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5040
https://github.com/broadinstitute/gatk/issues/5041:838,Performance,perform,performing,838,"## Feature request. ### Tool(s) or class(es) involved; _StructuralVariationDiscoveryPipelineSpark, XGBoostEvidenceFilter.java_. ### Description; Currently there are two unresolved large structural decisions about features for the XGBoostEvidenceFilter classifier. At the moment these decisions are switched by static member booleans, however that results in bad software engineering with one active code path and one inactive code path. The decisions to be made are:; 1. Whether to merge templateSize and readCount; * Yes: avoid NaN properties and decrease the number of columns by 1; * No: properties are easier to understand; 2. Whether to merge overlapping mappability k-mers for the mappability score; * Yes: the property is much easier to understand and explain. This seems like a no-brainer.; * No: unfortunately the currently best-performing classifier was trained on unmerged k-mers. Resolving this issue requires training new classifiers (altering feature design and training approach) in order to come to a definitive decision on these decisions (with the strong hope that decision 2 is to merge overlapping k-mers). Then inactive code paths and boolean switches can be removed. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5041
https://github.com/broadinstitute/gatk/issues/5041:523,Safety,avoid,avoid,523,"## Feature request. ### Tool(s) or class(es) involved; _StructuralVariationDiscoveryPipelineSpark, XGBoostEvidenceFilter.java_. ### Description; Currently there are two unresolved large structural decisions about features for the XGBoostEvidenceFilter classifier. At the moment these decisions are switched by static member booleans, however that results in bad software engineering with one active code path and one inactive code path. The decisions to be made are:; 1. Whether to merge templateSize and readCount; * Yes: avoid NaN properties and decrease the number of columns by 1; * No: properties are easier to understand; 2. Whether to merge overlapping mappability k-mers for the mappability score; * Yes: the property is much easier to understand and explain. This seems like a no-brainer.; * No: unfortunately the currently best-performing classifier was trained on unmerged k-mers. Resolving this issue requires training new classifiers (altering feature design and training approach) in order to come to a definitive decision on these decisions (with the strong hope that decision 2 is to merge overlapping k-mers). Then inactive code paths and boolean switches can be removed. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5041
https://github.com/broadinstitute/gatk/pull/5043:271,Testability,test,tests,271,"This is not ready for review, but because this is almost certainly going to take some iteration, I wanted to make sure we're on the same page with what needs to occur. I tried to touch the GATK3 code as little as possible. I changed what was needed, and ported the GATK3 tests. I have these passing locally using the GATK3 test data. I'd like to preserve that as we work through this review (even though it will not get checked in), as a way to measure GATK3/4 parity. There are probably going to be some places where GATK3 behavior could get questioned too. Because I tried to leave old code alone, that means there are plenty of existing issues, ranging from style/code-convention (i.e. arguments not 'final') to pieces of code that existed in GATK3, but might get questioned if put under the microscope. with that in mind, @droazen , @cmnbroad : do you have any general thoughts on the bar for porting this to GATK4? Can I leave the GATK3 code alone, or is every piece of code getting the level of review we just did for the core changes for this?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043
https://github.com/broadinstitute/gatk/pull/5043:323,Testability,test,test,323,"This is not ready for review, but because this is almost certainly going to take some iteration, I wanted to make sure we're on the same page with what needs to occur. I tried to touch the GATK3 code as little as possible. I changed what was needed, and ported the GATK3 tests. I have these passing locally using the GATK3 test data. I'd like to preserve that as we work through this review (even though it will not get checked in), as a way to measure GATK3/4 parity. There are probably going to be some places where GATK3 behavior could get questioned too. Because I tried to leave old code alone, that means there are plenty of existing issues, ranging from style/code-convention (i.e. arguments not 'final') to pieces of code that existed in GATK3, but might get questioned if put under the microscope. with that in mind, @droazen , @cmnbroad : do you have any general thoughts on the bar for porting this to GATK4? Can I leave the GATK3 code alone, or is every piece of code getting the level of review we just did for the core changes for this?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043
https://github.com/broadinstitute/gatk/issues/5045:406,Availability,error,error,406,"Hi,. I am using GATK `version gatk4-4.0.6.0-0` as part of the bcbio-nextgen pipeline for RNA-seq variant calling. There is one step in the pipeline i.e. `gatk GenomicsDBImport` that's been failing consistently no matter how less or many resources in terms of memory and cores I provide. I have tried to run the command as part of the pipeline and in stand-alone mode (like below) and both produce the same error:. ```; [rathik@reslnrefo01 log]$ gatk --java-options '-Xms454m -Xmx3181m -XX:+UseSerialGC' GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path CDL-164-04P-1_0_249250621_genomicsdb -L 1:1-249250621 --variant /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; Using GATK jar /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms454m -Xmx3181m -XX:+UseSerialGC -jar /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path CDL-164-04P-1_0_249250621_genomicsdb -L 1:1-249250621 --variant /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/mnt/isilon/cbmi/variome/tmp/rathik; 11:49:24.784 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:49:25.130 INFO GenomicsDBImport - ------------------------------------------------------------; 11:49",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045
https://github.com/broadinstitute/gatk/issues/5045:4901,Availability,error,error,4901,"aseq/gatk_output/CDL-164-04P/log/CDL-164-04P-1_0_249250621_genomicsdb; 11:49:26.339 INFO GenomicsDBImport - Vid Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/vidmap.json; 11:49:26.339 INFO GenomicsDBImport - Callset Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/callset.json; 11:49:26.339 INFO GenomicsDBImport - Complete VCF Header will be written to CDL-164-04P-1_0_249250621_genomicsdb/vcfheader.vcf; 11:49:26.339 INFO GenomicsDBImport - Importing to array - CDL-164-04P-1_0_249250621_genomicsdb/genomicsdb_array; 11:49:26.339 INFO ProgressMeter - Starting traversal; 11:49:26.340 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 11:49:26.697 INFO GenomicsDBImport - Importing batch 1 with 1 samples; Buffer resized from 22726bytes to 32529; Buffer resized from 32529bytes to 32693; Buffer resized from 32693bytes to 32738; Buffer resized from 32738bytes to 32741; Buffer resized from 32741bytes to 32756; Buffer resized from 32756bytes to 32768; Buffer resized from 32768bytes to 32769; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f9935bac359, pid=8320, tid=0x00007f99881bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_171-b10) (build 1.8.0_171-b10); # Java VM: OpenJDK 64-Bit Server VM (25.171-b10 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libtiledbgenomicsdb1661720680664773125.so+0x155359] BufferVariantCell::set_cell(void const*)+0x99; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/log/hs_err_pid8320.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045
https://github.com/broadinstitute/gatk/issues/5045:5485,Availability,error,error,5485,"atches/Minute; 11:49:26.697 INFO GenomicsDBImport - Importing batch 1 with 1 samples; Buffer resized from 22726bytes to 32529; Buffer resized from 32529bytes to 32693; Buffer resized from 32693bytes to 32738; Buffer resized from 32738bytes to 32741; Buffer resized from 32741bytes to 32756; Buffer resized from 32756bytes to 32768; Buffer resized from 32768bytes to 32769; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f9935bac359, pid=8320, tid=0x00007f99881bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_171-b10) (build 1.8.0_171-b10); # Java VM: OpenJDK 64-Bit Server VM (25.171-b10 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libtiledbgenomicsdb1661720680664773125.so+0x155359] BufferVariantCell::set_cell(void const*)+0x99; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/log/hs_err_pid8320.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. Then, I tried to enable core dumping:. ```; [rathik@reslnrefo01 CDL-164-04P]$ ulimit -c unlimited. [rathik@reslnrefo01 CDL-164-04P]$ gatk --java-options '-Xms454m -Xmx3181m -XX:+UseSerialGC -Djava.io.tmpdir=/mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/joint/gatk-haplotype-joint/CDL-164-04P/1/bcbiotx/tmp1SAfjS' GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path CDL-164-04P-1_0_249250621_genomicsdb -L 1:1-249250621 --variant /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annota",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045
https://github.com/broadinstitute/gatk/issues/5045:10769,Availability,error,error,10769,"n_rnaseq/gatk_output/CDL-164-04P/CDL-164-04P-1_0_249250621_genomicsdb; 10:24:57.553 INFO GenomicsDBImport - Vid Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/vidmap.json; 10:24:57.554 INFO GenomicsDBImport - Callset Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/callset.json; 10:24:57.554 INFO GenomicsDBImport - Complete VCF Header will be written to CDL-164-04P-1_0_249250621_genomicsdb/vcfheader.vcf; 10:24:57.554 INFO GenomicsDBImport - Importing to array - CDL-164-04P-1_0_249250621_genomicsdb/genomicsdb_array; 10:24:57.554 INFO ProgressMeter - Starting traversal; 10:24:57.554 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 10:24:57.971 INFO GenomicsDBImport - Importing batch 1 with 1 samples; Buffer resized from 22726bytes to 32529; Buffer resized from 32529bytes to 32693; Buffer resized from 32693bytes to 32738; Buffer resized from 32738bytes to 32741; Buffer resized from 32741bytes to 32756; Buffer resized from 32756bytes to 32768; Buffer resized from 32768bytes to 32769; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f7288295359, pid=68672, tid=0x00007f72dc187700; #; # JRE version: OpenJDK Runtime Environment (8.0_171-b10) (build 1.8.0_171-b10); # Java VM: OpenJDK 64-Bit Server VM (25.171-b10 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libtiledbgenomicsdb8064358042335455262.so+0x155359] BufferVariantCell::set_cell(void const*)+0x99; #; # Core dump written. Default location: /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/core or core.68672; #; # An error report file with more information is saved as:; # /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/hs_err_pid68672.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045
https://github.com/broadinstitute/gatk/issues/5045:11348,Availability,error,error,11348,"7.554 INFO GenomicsDBImport - Callset Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/callset.json; 10:24:57.554 INFO GenomicsDBImport - Complete VCF Header will be written to CDL-164-04P-1_0_249250621_genomicsdb/vcfheader.vcf; 10:24:57.554 INFO GenomicsDBImport - Importing to array - CDL-164-04P-1_0_249250621_genomicsdb/genomicsdb_array; 10:24:57.554 INFO ProgressMeter - Starting traversal; 10:24:57.554 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 10:24:57.971 INFO GenomicsDBImport - Importing batch 1 with 1 samples; Buffer resized from 22726bytes to 32529; Buffer resized from 32529bytes to 32693; Buffer resized from 32693bytes to 32738; Buffer resized from 32738bytes to 32741; Buffer resized from 32741bytes to 32756; Buffer resized from 32756bytes to 32768; Buffer resized from 32768bytes to 32769; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f7288295359, pid=68672, tid=0x00007f72dc187700; #; # JRE version: OpenJDK Runtime Environment (8.0_171-b10) (build 1.8.0_171-b10); # Java VM: OpenJDK 64-Bit Server VM (25.171-b10 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libtiledbgenomicsdb8064358042335455262.so+0x155359] BufferVariantCell::set_cell(void const*)+0x99; #; # Core dump written. Default location: /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/core or core.68672; #; # An error report file with more information is saved as:; # /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/hs_err_pid68672.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #. ```. And here is the `hs_err_pid68672.log` file: ; [hs_err_pid68672.log](https://github.com/broadinstitute/gatk/files/2219689/hs_err_pid68672.log)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045
https://github.com/broadinstitute/gatk/issues/5045:76,Deployability,pipeline,pipeline,76,"Hi,. I am using GATK `version gatk4-4.0.6.0-0` as part of the bcbio-nextgen pipeline for RNA-seq variant calling. There is one step in the pipeline i.e. `gatk GenomicsDBImport` that's been failing consistently no matter how less or many resources in terms of memory and cores I provide. I have tried to run the command as part of the pipeline and in stand-alone mode (like below) and both produce the same error:. ```; [rathik@reslnrefo01 log]$ gatk --java-options '-Xms454m -Xmx3181m -XX:+UseSerialGC' GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path CDL-164-04P-1_0_249250621_genomicsdb -L 1:1-249250621 --variant /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; Using GATK jar /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms454m -Xmx3181m -XX:+UseSerialGC -jar /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path CDL-164-04P-1_0_249250621_genomicsdb -L 1:1-249250621 --variant /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/mnt/isilon/cbmi/variome/tmp/rathik; 11:49:24.784 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:49:25.130 INFO GenomicsDBImport - ------------------------------------------------------------; 11:49",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045
https://github.com/broadinstitute/gatk/issues/5045:139,Deployability,pipeline,pipeline,139,"Hi,. I am using GATK `version gatk4-4.0.6.0-0` as part of the bcbio-nextgen pipeline for RNA-seq variant calling. There is one step in the pipeline i.e. `gatk GenomicsDBImport` that's been failing consistently no matter how less or many resources in terms of memory and cores I provide. I have tried to run the command as part of the pipeline and in stand-alone mode (like below) and both produce the same error:. ```; [rathik@reslnrefo01 log]$ gatk --java-options '-Xms454m -Xmx3181m -XX:+UseSerialGC' GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path CDL-164-04P-1_0_249250621_genomicsdb -L 1:1-249250621 --variant /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; Using GATK jar /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms454m -Xmx3181m -XX:+UseSerialGC -jar /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path CDL-164-04P-1_0_249250621_genomicsdb -L 1:1-249250621 --variant /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/mnt/isilon/cbmi/variome/tmp/rathik; 11:49:24.784 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:49:25.130 INFO GenomicsDBImport - ------------------------------------------------------------; 11:49",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045
https://github.com/broadinstitute/gatk/issues/5045:334,Deployability,pipeline,pipeline,334,"Hi,. I am using GATK `version gatk4-4.0.6.0-0` as part of the bcbio-nextgen pipeline for RNA-seq variant calling. There is one step in the pipeline i.e. `gatk GenomicsDBImport` that's been failing consistently no matter how less or many resources in terms of memory and cores I provide. I have tried to run the command as part of the pipeline and in stand-alone mode (like below) and both produce the same error:. ```; [rathik@reslnrefo01 log]$ gatk --java-options '-Xms454m -Xmx3181m -XX:+UseSerialGC' GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path CDL-164-04P-1_0_249250621_genomicsdb -L 1:1-249250621 --variant /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; Using GATK jar /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms454m -Xmx3181m -XX:+UseSerialGC -jar /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path CDL-164-04P-1_0_249250621_genomicsdb -L 1:1-249250621 --variant /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/mnt/isilon/cbmi/variome/tmp/rathik; 11:49:24.784 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:49:25.130 INFO GenomicsDBImport - ------------------------------------------------------------; 11:49",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045
https://github.com/broadinstitute/gatk/issues/5045:3423,Deployability,patch,patch,3423,"GenomicsDBImport - Start Date/Time: July 20, 2018 11:49:24 AM EDT; 11:49:25.134 INFO GenomicsDBImport - ------------------------------------------------------------; 11:49:25.134 INFO GenomicsDBImport - ------------------------------------------------------------; 11:49:25.135 INFO GenomicsDBImport - HTSJDK Version: 2.16.0; 11:49:25.135 INFO GenomicsDBImport - Picard Version: 2.18.7; 11:49:25.136 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:49:25.136 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:49:25.136 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:49:25.136 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:49:25.136 INFO GenomicsDBImport - Deflater: IntelDeflater; 11:49:25.136 INFO GenomicsDBImport - Inflater: IntelInflater; 11:49:25.136 INFO GenomicsDBImport - GCS max retries/reopens: 20; 11:49:25.136 INFO GenomicsDBImport - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 11:49:25.137 INFO GenomicsDBImport - Initializing engine; 11:49:25.926 INFO IntervalArgumentCollection - Processing 249250621 bp from intervals; 11:49:25.931 INFO GenomicsDBImport - Done initializing engine; Created workspace /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/log/CDL-164-04P-1_0_249250621_genomicsdb; 11:49:26.339 INFO GenomicsDBImport - Vid Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/vidmap.json; 11:49:26.339 INFO GenomicsDBImport - Callset Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/callset.json; 11:49:26.339 INFO GenomicsDBImport - Complete VCF Header will be written to CDL-164-04P-1_0_249250621_genomicsdb/vcfheader.vcf; 11:49:26.339 INFO GenomicsDBImport - Importing to array - CDL-164-04P-1_0_249250621_genomicsdb/genomicsdb_array; 11:49:26.339 INFO ProgressMeter - Starting traversal;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045
https://github.com/broadinstitute/gatk/issues/5045:9295,Deployability,patch,patch,9295,"GenomicsDBImport - Start Date/Time: July 23, 2018 10:24:55 AM EDT; 10:24:56.136 INFO GenomicsDBImport - ------------------------------------------------------------; 10:24:56.136 INFO GenomicsDBImport - ------------------------------------------------------------; 10:24:56.137 INFO GenomicsDBImport - HTSJDK Version: 2.16.0; 10:24:56.138 INFO GenomicsDBImport - Picard Version: 2.18.7; 10:24:56.138 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:24:56.138 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:24:56.138 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:24:56.138 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:24:56.138 INFO GenomicsDBImport - Deflater: IntelDeflater; 10:24:56.138 INFO GenomicsDBImport - Inflater: IntelInflater; 10:24:56.139 INFO GenomicsDBImport - GCS max retries/reopens: 20; 10:24:56.139 INFO GenomicsDBImport - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 10:24:56.139 INFO GenomicsDBImport - Initializing engine; 10:24:57.198 INFO IntervalArgumentCollection - Processing 249250621 bp from intervals; 10:24:57.205 INFO GenomicsDBImport - Done initializing engine; Created workspace /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/CDL-164-04P-1_0_249250621_genomicsdb; 10:24:57.553 INFO GenomicsDBImport - Vid Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/vidmap.json; 10:24:57.554 INFO GenomicsDBImport - Callset Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/callset.json; 10:24:57.554 INFO GenomicsDBImport - Complete VCF Header will be written to CDL-164-04P-1_0_249250621_genomicsdb/vcfheader.vcf; 10:24:57.554 INFO GenomicsDBImport - Importing to array - CDL-164-04P-1_0_249250621_genomicsdb/genomicsdb_array; 10:24:57.554 INFO ProgressMeter - Starting traversal; 10:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045
https://github.com/broadinstitute/gatk/issues/5045:1697,Performance,Load,Loading,1697,"-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; Using GATK jar /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms454m -Xmx3181m -XX:+UseSerialGC -jar /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path CDL-164-04P-1_0_249250621_genomicsdb -L 1:1-249250621 --variant /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/mnt/isilon/cbmi/variome/tmp/rathik; 11:49:24.784 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:49:25.130 INFO GenomicsDBImport - ------------------------------------------------------------; 11:49:25.130 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.6.0; 11:49:25.131 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:49:25.134 INFO GenomicsDBImport - Executing as rathik@reslnrefo01.research.chop.edu on Linux v3.10.0-862.el7.x86_64 amd64; 11:49:25.134 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-b10; 11:49:25.134 INFO GenomicsDBImport - Start Date/Time: July 20, 2018 11:49:24 AM EDT; 11:49:25.134 INFO GenomicsDBImport - ------------------------------------------------------------; 11:49:25.134 INFO GenomicsDBImport - ------------------------------------------------------------; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045
https://github.com/broadinstitute/gatk/issues/5045:7565,Performance,Load,Loading,7565,"bmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms454m -Xmx3181m -XX:+UseSerialGC -Djava.io.tmpdir=/mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/joint/gatk-haplotype-joint/CDL-164-04P/1/bcbiotx/tmp1SAfjS -jar /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path CDL-164-04P-1_0_249250621_genomicsdb -L 1:1-249250621 --variant /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/mnt/isilon/cbmi/variome/tmp/rathik; 10:24:55.378 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:24:56.133 INFO GenomicsDBImport - ------------------------------------------------------------; 10:24:56.133 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.6.0; 10:24:56.134 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:24:56.136 INFO GenomicsDBImport - Executing as rathik@reslnrefo01.research.chop.edu on Linux v3.10.0-862.9.1.el7.x86_64 amd64; 10:24:56.136 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-b10; 10:24:56.136 INFO GenomicsDBImport - Start Date/Time: July 23, 2018 10:24:55 AM EDT; 10:24:56.136 INFO GenomicsDBImport - ------------------------------------------------------------; 10:24:56.136 INFO GenomicsDBImport - ----------------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045
https://github.com/broadinstitute/gatk/issues/5045:4916,Safety,detect,detected,4916,"aseq/gatk_output/CDL-164-04P/log/CDL-164-04P-1_0_249250621_genomicsdb; 11:49:26.339 INFO GenomicsDBImport - Vid Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/vidmap.json; 11:49:26.339 INFO GenomicsDBImport - Callset Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/callset.json; 11:49:26.339 INFO GenomicsDBImport - Complete VCF Header will be written to CDL-164-04P-1_0_249250621_genomicsdb/vcfheader.vcf; 11:49:26.339 INFO GenomicsDBImport - Importing to array - CDL-164-04P-1_0_249250621_genomicsdb/genomicsdb_array; 11:49:26.339 INFO ProgressMeter - Starting traversal; 11:49:26.340 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 11:49:26.697 INFO GenomicsDBImport - Importing batch 1 with 1 samples; Buffer resized from 22726bytes to 32529; Buffer resized from 32529bytes to 32693; Buffer resized from 32693bytes to 32738; Buffer resized from 32738bytes to 32741; Buffer resized from 32741bytes to 32756; Buffer resized from 32756bytes to 32768; Buffer resized from 32768bytes to 32769; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f9935bac359, pid=8320, tid=0x00007f99881bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_171-b10) (build 1.8.0_171-b10); # Java VM: OpenJDK 64-Bit Server VM (25.171-b10 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libtiledbgenomicsdb1661720680664773125.so+0x155359] BufferVariantCell::set_cell(void const*)+0x99; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/log/hs_err_pid8320.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045
https://github.com/broadinstitute/gatk/issues/5045:10784,Safety,detect,detected,10784,"n_rnaseq/gatk_output/CDL-164-04P/CDL-164-04P-1_0_249250621_genomicsdb; 10:24:57.553 INFO GenomicsDBImport - Vid Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/vidmap.json; 10:24:57.554 INFO GenomicsDBImport - Callset Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/callset.json; 10:24:57.554 INFO GenomicsDBImport - Complete VCF Header will be written to CDL-164-04P-1_0_249250621_genomicsdb/vcfheader.vcf; 10:24:57.554 INFO GenomicsDBImport - Importing to array - CDL-164-04P-1_0_249250621_genomicsdb/genomicsdb_array; 10:24:57.554 INFO ProgressMeter - Starting traversal; 10:24:57.554 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 10:24:57.971 INFO GenomicsDBImport - Importing batch 1 with 1 samples; Buffer resized from 22726bytes to 32529; Buffer resized from 32529bytes to 32693; Buffer resized from 32693bytes to 32738; Buffer resized from 32738bytes to 32741; Buffer resized from 32741bytes to 32756; Buffer resized from 32756bytes to 32768; Buffer resized from 32768bytes to 32769; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f7288295359, pid=68672, tid=0x00007f72dc187700; #; # JRE version: OpenJDK Runtime Environment (8.0_171-b10) (build 1.8.0_171-b10); # Java VM: OpenJDK 64-Bit Server VM (25.171-b10 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libtiledbgenomicsdb8064358042335455262.so+0x155359] BufferVariantCell::set_cell(void const*)+0x99; #; # Core dump written. Default location: /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/core or core.68672; #; # An error report file with more information is saved as:; # /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/hs_err_pid68672.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045
https://github.com/broadinstitute/gatk/issues/5045:439,Testability,log,log,439,"Hi,. I am using GATK `version gatk4-4.0.6.0-0` as part of the bcbio-nextgen pipeline for RNA-seq variant calling. There is one step in the pipeline i.e. `gatk GenomicsDBImport` that's been failing consistently no matter how less or many resources in terms of memory and cores I provide. I have tried to run the command as part of the pipeline and in stand-alone mode (like below) and both produce the same error:. ```; [rathik@reslnrefo01 log]$ gatk --java-options '-Xms454m -Xmx3181m -XX:+UseSerialGC' GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path CDL-164-04P-1_0_249250621_genomicsdb -L 1:1-249250621 --variant /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; Using GATK jar /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms454m -Xmx3181m -XX:+UseSerialGC -jar /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path CDL-164-04P-1_0_249250621_genomicsdb -L 1:1-249250621 --variant /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/mnt/isilon/cbmi/variome/tmp/rathik; 11:49:24.784 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:49:25.130 INFO GenomicsDBImport - ------------------------------------------------------------; 11:49",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045
https://github.com/broadinstitute/gatk/issues/5045:3842,Testability,log,log,3842,card Version: 2.18.7; 11:49:25.136 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:49:25.136 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:49:25.136 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:49:25.136 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:49:25.136 INFO GenomicsDBImport - Deflater: IntelDeflater; 11:49:25.136 INFO GenomicsDBImport - Inflater: IntelInflater; 11:49:25.136 INFO GenomicsDBImport - GCS max retries/reopens: 20; 11:49:25.136 INFO GenomicsDBImport - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 11:49:25.137 INFO GenomicsDBImport - Initializing engine; 11:49:25.926 INFO IntervalArgumentCollection - Processing 249250621 bp from intervals; 11:49:25.931 INFO GenomicsDBImport - Done initializing engine; Created workspace /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/log/CDL-164-04P-1_0_249250621_genomicsdb; 11:49:26.339 INFO GenomicsDBImport - Vid Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/vidmap.json; 11:49:26.339 INFO GenomicsDBImport - Callset Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/callset.json; 11:49:26.339 INFO GenomicsDBImport - Complete VCF Header will be written to CDL-164-04P-1_0_249250621_genomicsdb/vcfheader.vcf; 11:49:26.339 INFO GenomicsDBImport - Importing to array - CDL-164-04P-1_0_249250621_genomicsdb/genomicsdb_array; 11:49:26.339 INFO ProgressMeter - Starting traversal; 11:49:26.340 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 11:49:26.697 INFO GenomicsDBImport - Importing batch 1 with 1 samples; Buffer resized from 22726bytes to 32529; Buffer resized from 32529bytes to 32693; Buffer resized from 32693bytes to 32738; Buffer resized from 32738bytes to 32741; Buffer resized from 32741bytes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045
https://github.com/broadinstitute/gatk/issues/5045:5614,Testability,log,log,5614,"atches/Minute; 11:49:26.697 INFO GenomicsDBImport - Importing batch 1 with 1 samples; Buffer resized from 22726bytes to 32529; Buffer resized from 32529bytes to 32693; Buffer resized from 32693bytes to 32738; Buffer resized from 32738bytes to 32741; Buffer resized from 32741bytes to 32756; Buffer resized from 32756bytes to 32768; Buffer resized from 32768bytes to 32769; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f9935bac359, pid=8320, tid=0x00007f99881bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_171-b10) (build 1.8.0_171-b10); # Java VM: OpenJDK 64-Bit Server VM (25.171-b10 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libtiledbgenomicsdb1661720680664773125.so+0x155359] BufferVariantCell::set_cell(void const*)+0x99; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/log/hs_err_pid8320.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. Then, I tried to enable core dumping:. ```; [rathik@reslnrefo01 CDL-164-04P]$ ulimit -c unlimited. [rathik@reslnrefo01 CDL-164-04P]$ gatk --java-options '-Xms454m -Xmx3181m -XX:+UseSerialGC -Djava.io.tmpdir=/mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/joint/gatk-haplotype-joint/CDL-164-04P/1/bcbiotx/tmp1SAfjS' GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path CDL-164-04P-1_0_249250621_genomicsdb -L 1:1-249250621 --variant /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annota",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045
https://github.com/broadinstitute/gatk/issues/5045:5633,Testability,log,log,5633,"32693; Buffer resized from 32693bytes to 32738; Buffer resized from 32738bytes to 32741; Buffer resized from 32741bytes to 32756; Buffer resized from 32756bytes to 32768; Buffer resized from 32768bytes to 32769; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f9935bac359, pid=8320, tid=0x00007f99881bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_171-b10) (build 1.8.0_171-b10); # Java VM: OpenJDK 64-Bit Server VM (25.171-b10 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libtiledbgenomicsdb1661720680664773125.so+0x155359] BufferVariantCell::set_cell(void const*)+0x99; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/log/hs_err_pid8320.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. Then, I tried to enable core dumping:. ```; [rathik@reslnrefo01 CDL-164-04P]$ ulimit -c unlimited. [rathik@reslnrefo01 CDL-164-04P]$ gatk --java-options '-Xms454m -Xmx3181m -XX:+UseSerialGC -Djava.io.tmpdir=/mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/joint/gatk-haplotype-joint/CDL-164-04P/1/bcbiotx/tmp1SAfjS' GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path CDL-164-04P-1_0_249250621_genomicsdb -L 1:1-249250621 --variant /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; Using GATK jar /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar; Running:; java",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045
https://github.com/broadinstitute/gatk/issues/5045:11493,Testability,log,log,11493,"7.554 INFO GenomicsDBImport - Callset Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/callset.json; 10:24:57.554 INFO GenomicsDBImport - Complete VCF Header will be written to CDL-164-04P-1_0_249250621_genomicsdb/vcfheader.vcf; 10:24:57.554 INFO GenomicsDBImport - Importing to array - CDL-164-04P-1_0_249250621_genomicsdb/genomicsdb_array; 10:24:57.554 INFO ProgressMeter - Starting traversal; 10:24:57.554 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 10:24:57.971 INFO GenomicsDBImport - Importing batch 1 with 1 samples; Buffer resized from 22726bytes to 32529; Buffer resized from 32529bytes to 32693; Buffer resized from 32693bytes to 32738; Buffer resized from 32738bytes to 32741; Buffer resized from 32741bytes to 32756; Buffer resized from 32756bytes to 32768; Buffer resized from 32768bytes to 32769; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f7288295359, pid=68672, tid=0x00007f72dc187700; #; # JRE version: OpenJDK Runtime Environment (8.0_171-b10) (build 1.8.0_171-b10); # Java VM: OpenJDK 64-Bit Server VM (25.171-b10 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libtiledbgenomicsdb8064358042335455262.so+0x155359] BufferVariantCell::set_cell(void const*)+0x99; #; # Core dump written. Default location: /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/core or core.68672; #; # An error report file with more information is saved as:; # /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/hs_err_pid68672.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #. ```. And here is the `hs_err_pid68672.log` file: ; [hs_err_pid68672.log](https://github.com/broadinstitute/gatk/files/2219689/hs_err_pid68672.log)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045
https://github.com/broadinstitute/gatk/issues/5045:11775,Testability,log,log,11775,"7.554 INFO GenomicsDBImport - Callset Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/callset.json; 10:24:57.554 INFO GenomicsDBImport - Complete VCF Header will be written to CDL-164-04P-1_0_249250621_genomicsdb/vcfheader.vcf; 10:24:57.554 INFO GenomicsDBImport - Importing to array - CDL-164-04P-1_0_249250621_genomicsdb/genomicsdb_array; 10:24:57.554 INFO ProgressMeter - Starting traversal; 10:24:57.554 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 10:24:57.971 INFO GenomicsDBImport - Importing batch 1 with 1 samples; Buffer resized from 22726bytes to 32529; Buffer resized from 32529bytes to 32693; Buffer resized from 32693bytes to 32738; Buffer resized from 32738bytes to 32741; Buffer resized from 32741bytes to 32756; Buffer resized from 32756bytes to 32768; Buffer resized from 32768bytes to 32769; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f7288295359, pid=68672, tid=0x00007f72dc187700; #; # JRE version: OpenJDK Runtime Environment (8.0_171-b10) (build 1.8.0_171-b10); # Java VM: OpenJDK 64-Bit Server VM (25.171-b10 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libtiledbgenomicsdb8064358042335455262.so+0x155359] BufferVariantCell::set_cell(void const*)+0x99; #; # Core dump written. Default location: /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/core or core.68672; #; # An error report file with more information is saved as:; # /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/hs_err_pid68672.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #. ```. And here is the `hs_err_pid68672.log` file: ; [hs_err_pid68672.log](https://github.com/broadinstitute/gatk/files/2219689/hs_err_pid68672.log)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045
https://github.com/broadinstitute/gatk/issues/5045:11805,Testability,log,log,11805,"7.554 INFO GenomicsDBImport - Callset Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/callset.json; 10:24:57.554 INFO GenomicsDBImport - Complete VCF Header will be written to CDL-164-04P-1_0_249250621_genomicsdb/vcfheader.vcf; 10:24:57.554 INFO GenomicsDBImport - Importing to array - CDL-164-04P-1_0_249250621_genomicsdb/genomicsdb_array; 10:24:57.554 INFO ProgressMeter - Starting traversal; 10:24:57.554 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 10:24:57.971 INFO GenomicsDBImport - Importing batch 1 with 1 samples; Buffer resized from 22726bytes to 32529; Buffer resized from 32529bytes to 32693; Buffer resized from 32693bytes to 32738; Buffer resized from 32738bytes to 32741; Buffer resized from 32741bytes to 32756; Buffer resized from 32756bytes to 32768; Buffer resized from 32768bytes to 32769; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f7288295359, pid=68672, tid=0x00007f72dc187700; #; # JRE version: OpenJDK Runtime Environment (8.0_171-b10) (build 1.8.0_171-b10); # Java VM: OpenJDK 64-Bit Server VM (25.171-b10 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libtiledbgenomicsdb8064358042335455262.so+0x155359] BufferVariantCell::set_cell(void const*)+0x99; #; # Core dump written. Default location: /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/core or core.68672; #; # An error report file with more information is saved as:; # /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/hs_err_pid68672.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #. ```. And here is the `hs_err_pid68672.log` file: ; [hs_err_pid68672.log](https://github.com/broadinstitute/gatk/files/2219689/hs_err_pid68672.log)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045
https://github.com/broadinstitute/gatk/issues/5045:11879,Testability,log,log,11879,"7.554 INFO GenomicsDBImport - Callset Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/callset.json; 10:24:57.554 INFO GenomicsDBImport - Complete VCF Header will be written to CDL-164-04P-1_0_249250621_genomicsdb/vcfheader.vcf; 10:24:57.554 INFO GenomicsDBImport - Importing to array - CDL-164-04P-1_0_249250621_genomicsdb/genomicsdb_array; 10:24:57.554 INFO ProgressMeter - Starting traversal; 10:24:57.554 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 10:24:57.971 INFO GenomicsDBImport - Importing batch 1 with 1 samples; Buffer resized from 22726bytes to 32529; Buffer resized from 32529bytes to 32693; Buffer resized from 32693bytes to 32738; Buffer resized from 32738bytes to 32741; Buffer resized from 32741bytes to 32756; Buffer resized from 32756bytes to 32768; Buffer resized from 32768bytes to 32769; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f7288295359, pid=68672, tid=0x00007f72dc187700; #; # JRE version: OpenJDK Runtime Environment (8.0_171-b10) (build 1.8.0_171-b10); # Java VM: OpenJDK 64-Bit Server VM (25.171-b10 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libtiledbgenomicsdb8064358042335455262.so+0x155359] BufferVariantCell::set_cell(void const*)+0x99; #; # Core dump written. Default location: /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/core or core.68672; #; # An error report file with more information is saved as:; # /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/hs_err_pid68672.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #. ```. And here is the `hs_err_pid68672.log` file: ; [hs_err_pid68672.log](https://github.com/broadinstitute/gatk/files/2219689/hs_err_pid68672.log)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045
https://github.com/broadinstitute/gatk/pull/5046:42,Security,Validat,Validation,42,"- Funcotator will populate the MAF DB SNP Validation status field with proper values (e.g. ""by1000genomes"") instead of boolean value (e.g. ""TRUE"") Closes #4985 ; - Funcotator now handles multiple records in a VCF funcotation factory that have the same pos, ref, and alt combination, even if equivalent and not exact matches. Closes #4972",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5046
https://github.com/broadinstitute/gatk/pull/5049:62,Availability,error,errors,62,"@takutosato This is really important for overcoming transient errors in Firecloud, especially as long as we don't have call-caching for NIO.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5049
https://github.com/broadinstitute/gatk/issues/5051:243,Availability,error,error,243,"## Bug Report. ### Affected tool(s) or class(es); `PrintReadsSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description . I first encountered this type of error in a prototype tool I'm writing, so to dig further about what's happening, I run our simplest Spark tool&mdash;`PrintReadsSpark`. `PrintReadsSpark` reports errors when intervals are specified in a BED file (see command given in the stack trace). * Scenario 1: run with a WGS bam and give intervals via `-L PATH_TO_BED_FILE`, error is reported; * Scenario 2: run with the WGS bam and give intervals via `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 3: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L PATH_TO_BED_FILE`, no error; * Scenario 4: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 5: download the shrunken bam to local machine and run `PrintReadsSpark` with `-L PATH_TO_BED_FILE`, no error. Stack trace from scenario 1:; ```; ./gatk PrintReadsSpark \; -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam \; -O hdfs://shuang-small-m:8020/results/temp.bam \; -L hdfs://shuang-small-m:8020/data/intervals.bed \; -- \; --spark-runner GCS \; --cluster shuang-small \; --project broad-dsde-methods. Using GATK jar /Users/shuang/GATK/gatk/build/libs/gatk-spark.jar; found cached jar: gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar. Replacing spark-submit style args with dataproc style args. --cluster shuang-small --project broad-dsde-methods -> --cluster shuang-small --project broad-dsde-methods --properties spark.kryoserializer.buffer.max=512m,spark.driver.maxResultSize=0,spark.driver.userClassPathFirst=false,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-DGATK_STA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:405,Availability,error,errors,405,"## Bug Report. ### Affected tool(s) or class(es); `PrintReadsSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description . I first encountered this type of error in a prototype tool I'm writing, so to dig further about what's happening, I run our simplest Spark tool&mdash;`PrintReadsSpark`. `PrintReadsSpark` reports errors when intervals are specified in a BED file (see command given in the stack trace). * Scenario 1: run with a WGS bam and give intervals via `-L PATH_TO_BED_FILE`, error is reported; * Scenario 2: run with the WGS bam and give intervals via `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 3: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L PATH_TO_BED_FILE`, no error; * Scenario 4: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 5: download the shrunken bam to local machine and run `PrintReadsSpark` with `-L PATH_TO_BED_FILE`, no error. Stack trace from scenario 1:; ```; ./gatk PrintReadsSpark \; -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam \; -O hdfs://shuang-small-m:8020/results/temp.bam \; -L hdfs://shuang-small-m:8020/data/intervals.bed \; -- \; --spark-runner GCS \; --cluster shuang-small \; --project broad-dsde-methods. Using GATK jar /Users/shuang/GATK/gatk/build/libs/gatk-spark.jar; found cached jar: gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar. Replacing spark-submit style args with dataproc style args. --cluster shuang-small --project broad-dsde-methods -> --cluster shuang-small --project broad-dsde-methods --properties spark.kryoserializer.buffer.max=512m,spark.driver.maxResultSize=0,spark.driver.userClassPathFirst=false,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-DGATK_STA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:574,Availability,error,error,574,"## Bug Report. ### Affected tool(s) or class(es); `PrintReadsSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description . I first encountered this type of error in a prototype tool I'm writing, so to dig further about what's happening, I run our simplest Spark tool&mdash;`PrintReadsSpark`. `PrintReadsSpark` reports errors when intervals are specified in a BED file (see command given in the stack trace). * Scenario 1: run with a WGS bam and give intervals via `-L PATH_TO_BED_FILE`, error is reported; * Scenario 2: run with the WGS bam and give intervals via `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 3: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L PATH_TO_BED_FILE`, no error; * Scenario 4: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 5: download the shrunken bam to local machine and run `PrintReadsSpark` with `-L PATH_TO_BED_FILE`, no error. Stack trace from scenario 1:; ```; ./gatk PrintReadsSpark \; -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam \; -O hdfs://shuang-small-m:8020/results/temp.bam \; -L hdfs://shuang-small-m:8020/data/intervals.bed \; -- \; --spark-runner GCS \; --cluster shuang-small \; --project broad-dsde-methods. Using GATK jar /Users/shuang/GATK/gatk/build/libs/gatk-spark.jar; found cached jar: gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar. Replacing spark-submit style args with dataproc style args. --cluster shuang-small --project broad-dsde-methods -> --cluster shuang-small --project broad-dsde-methods --properties spark.kryoserializer.buffer.max=512m,spark.driver.maxResultSize=0,spark.driver.userClassPathFirst=false,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-DGATK_STA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:679,Availability,error,error,679,"## Bug Report. ### Affected tool(s) or class(es); `PrintReadsSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description . I first encountered this type of error in a prototype tool I'm writing, so to dig further about what's happening, I run our simplest Spark tool&mdash;`PrintReadsSpark`. `PrintReadsSpark` reports errors when intervals are specified in a BED file (see command given in the stack trace). * Scenario 1: run with a WGS bam and give intervals via `-L PATH_TO_BED_FILE`, error is reported; * Scenario 2: run with the WGS bam and give intervals via `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 3: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L PATH_TO_BED_FILE`, no error; * Scenario 4: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 5: download the shrunken bam to local machine and run `PrintReadsSpark` with `-L PATH_TO_BED_FILE`, no error. Stack trace from scenario 1:; ```; ./gatk PrintReadsSpark \; -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam \; -O hdfs://shuang-small-m:8020/results/temp.bam \; -L hdfs://shuang-small-m:8020/data/intervals.bed \; -- \; --spark-runner GCS \; --cluster shuang-small \; --project broad-dsde-methods. Using GATK jar /Users/shuang/GATK/gatk/build/libs/gatk-spark.jar; found cached jar: gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar. Replacing spark-submit style args with dataproc style args. --cluster shuang-small --project broad-dsde-methods -> --cluster shuang-small --project broad-dsde-methods --properties spark.kryoserializer.buffer.max=512m,spark.driver.maxResultSize=0,spark.driver.userClassPathFirst=false,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-DGATK_STA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:832,Availability,error,error,832,"## Bug Report. ### Affected tool(s) or class(es); `PrintReadsSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description . I first encountered this type of error in a prototype tool I'm writing, so to dig further about what's happening, I run our simplest Spark tool&mdash;`PrintReadsSpark`. `PrintReadsSpark` reports errors when intervals are specified in a BED file (see command given in the stack trace). * Scenario 1: run with a WGS bam and give intervals via `-L PATH_TO_BED_FILE`, error is reported; * Scenario 2: run with the WGS bam and give intervals via `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 3: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L PATH_TO_BED_FILE`, no error; * Scenario 4: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 5: download the shrunken bam to local machine and run `PrintReadsSpark` with `-L PATH_TO_BED_FILE`, no error. Stack trace from scenario 1:; ```; ./gatk PrintReadsSpark \; -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam \; -O hdfs://shuang-small-m:8020/results/temp.bam \; -L hdfs://shuang-small-m:8020/data/intervals.bed \; -- \; --spark-runner GCS \; --cluster shuang-small \; --project broad-dsde-methods. Using GATK jar /Users/shuang/GATK/gatk/build/libs/gatk-spark.jar; found cached jar: gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar. Replacing spark-submit style args with dataproc style args. --cluster shuang-small --project broad-dsde-methods -> --cluster shuang-small --project broad-dsde-methods --properties spark.kryoserializer.buffer.max=512m,spark.driver.maxResultSize=0,spark.driver.userClassPathFirst=false,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-DGATK_STA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:987,Availability,error,error,987,"## Bug Report. ### Affected tool(s) or class(es); `PrintReadsSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description . I first encountered this type of error in a prototype tool I'm writing, so to dig further about what's happening, I run our simplest Spark tool&mdash;`PrintReadsSpark`. `PrintReadsSpark` reports errors when intervals are specified in a BED file (see command given in the stack trace). * Scenario 1: run with a WGS bam and give intervals via `-L PATH_TO_BED_FILE`, error is reported; * Scenario 2: run with the WGS bam and give intervals via `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 3: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L PATH_TO_BED_FILE`, no error; * Scenario 4: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 5: download the shrunken bam to local machine and run `PrintReadsSpark` with `-L PATH_TO_BED_FILE`, no error. Stack trace from scenario 1:; ```; ./gatk PrintReadsSpark \; -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam \; -O hdfs://shuang-small-m:8020/results/temp.bam \; -L hdfs://shuang-small-m:8020/data/intervals.bed \; -- \; --spark-runner GCS \; --cluster shuang-small \; --project broad-dsde-methods. Using GATK jar /Users/shuang/GATK/gatk/build/libs/gatk-spark.jar; found cached jar: gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar. Replacing spark-submit style args with dataproc style args. --cluster shuang-small --project broad-dsde-methods -> --cluster shuang-small --project broad-dsde-methods --properties spark.kryoserializer.buffer.max=512m,spark.driver.maxResultSize=0,spark.driver.userClassPathFirst=false,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-DGATK_STA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:1008,Availability,down,download,1008,"## Bug Report. ### Affected tool(s) or class(es); `PrintReadsSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description . I first encountered this type of error in a prototype tool I'm writing, so to dig further about what's happening, I run our simplest Spark tool&mdash;`PrintReadsSpark`. `PrintReadsSpark` reports errors when intervals are specified in a BED file (see command given in the stack trace). * Scenario 1: run with a WGS bam and give intervals via `-L PATH_TO_BED_FILE`, error is reported; * Scenario 2: run with the WGS bam and give intervals via `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 3: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L PATH_TO_BED_FILE`, no error; * Scenario 4: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 5: download the shrunken bam to local machine and run `PrintReadsSpark` with `-L PATH_TO_BED_FILE`, no error. Stack trace from scenario 1:; ```; ./gatk PrintReadsSpark \; -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam \; -O hdfs://shuang-small-m:8020/results/temp.bam \; -L hdfs://shuang-small-m:8020/data/intervals.bed \; -- \; --spark-runner GCS \; --cluster shuang-small \; --project broad-dsde-methods. Using GATK jar /Users/shuang/GATK/gatk/build/libs/gatk-spark.jar; found cached jar: gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar. Replacing spark-submit style args with dataproc style args. --cluster shuang-small --project broad-dsde-methods -> --cluster shuang-small --project broad-dsde-methods --properties spark.kryoserializer.buffer.max=512m,spark.driver.maxResultSize=0,spark.driver.userClassPathFirst=false,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-DGATK_STA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:1108,Availability,error,error,1108,"## Bug Report. ### Affected tool(s) or class(es); `PrintReadsSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description . I first encountered this type of error in a prototype tool I'm writing, so to dig further about what's happening, I run our simplest Spark tool&mdash;`PrintReadsSpark`. `PrintReadsSpark` reports errors when intervals are specified in a BED file (see command given in the stack trace). * Scenario 1: run with a WGS bam and give intervals via `-L PATH_TO_BED_FILE`, error is reported; * Scenario 2: run with the WGS bam and give intervals via `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 3: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L PATH_TO_BED_FILE`, no error; * Scenario 4: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 5: download the shrunken bam to local machine and run `PrintReadsSpark` with `-L PATH_TO_BED_FILE`, no error. Stack trace from scenario 1:; ```; ./gatk PrintReadsSpark \; -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam \; -O hdfs://shuang-small-m:8020/results/temp.bam \; -L hdfs://shuang-small-m:8020/data/intervals.bed \; -- \; --spark-runner GCS \; --cluster shuang-small \; --project broad-dsde-methods. Using GATK jar /Users/shuang/GATK/gatk/build/libs/gatk-spark.jar; found cached jar: gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar. Replacing spark-submit style args with dataproc style args. --cluster shuang-small --project broad-dsde-methods -> --cluster shuang-small --project broad-dsde-methods --properties spark.kryoserializer.buffer.max=512m,spark.driver.maxResultSize=0,spark.driver.userClassPathFirst=false,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-DGATK_STA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:11106,Availability,ERROR,ERROR,11106,"); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). 18/07/24 21:02:27 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 0.0 failed 4 times; aborting job; 18/07/24 21:02:27 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 21:02:27.703 INFO PrintReadsSpark - Shutting down engine; [July 24, 2018 9:02:27 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.32 minutes.; Runtime.totalMemory()=2463629312; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 7, shuang-small-m.c.broad-dsde-methods.internal, executor 2): htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsj",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:11382,Availability,down,down,11382,"rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). 18/07/24 21:02:27 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 0.0 failed 4 times; aborting job; 18/07/24 21:02:27 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 21:02:27.703 INFO PrintReadsSpark - Shutting down engine; [July 24, 2018 9:02:27 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.32 minutes.; Runtime.totalMemory()=2463629312; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 7, shuang-small-m.c.broad-dsde-methods.internal, executor 2): htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.seek(BlockCompressedInputStream.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:11621,Availability,failure,failure,11621,"); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). 18/07/24 21:02:27 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 0.0 failed 4 times; aborting job; 18/07/24 21:02:27 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 21:02:27.703 INFO PrintReadsSpark - Shutting down engine; [July 24, 2018 9:02:27 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.32 minutes.; Runtime.totalMemory()=2463629312; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 7, shuang-small-m.c.broad-dsde-methods.internal, executor 2): htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.seek(BlockCompressedInputStream.java:380); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:977); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFile",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:11678,Availability,failure,failure,11678,"t org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). 18/07/24 21:02:27 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 0.0 failed 4 times; aborting job; 18/07/24 21:02:27 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 21:02:27.703 INFO PrintReadsSpark - Shutting down engine; [July 24, 2018 9:02:27 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.32 minutes.; Runtime.totalMemory()=2463629312; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 7, shuang-small-m.c.broad-dsde-methods.internal, executor 2): htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.seek(BlockCompressedInputStream.java:380); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:977); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.samtools.B",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:22761,Availability,ERROR,ERROR,22761,1); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [5838bd7dec2d4533ad090ce03ecc7c0c] entered state [ERROR] while waiting for [DONE].; ```. #### Steps to reproduce. See command given in stack trace above.; WGS bam is available at ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam` ; and ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam.bai`. Interval list BED file content given below. ```; chrX	67113957	67114130; chrX	71903370	71903687; chrX	74330484	74330552; chrX	75379902	75379965; chrX	78441355	78441953; ```. #### Expected behavior; Pass. #### Actual behavior; Error! This could be related to ticket #2722,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:22858,Availability,ERROR,ERROR,22858,1); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [5838bd7dec2d4533ad090ce03ecc7c0c] entered state [ERROR] while waiting for [DONE].; ```. #### Steps to reproduce. See command given in stack trace above.; WGS bam is available at ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam` ; and ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam.bai`. Interval list BED file content given below. ```; chrX	67113957	67114130; chrX	71903370	71903687; chrX	74330484	74330552; chrX	75379902	75379965; chrX	78441355	78441953; ```. #### Expected behavior; Pass. #### Actual behavior; Error! This could be related to ticket #2722,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:22974,Availability,avail,available,22974,1); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [5838bd7dec2d4533ad090ce03ecc7c0c] entered state [ERROR] while waiting for [DONE].; ```. #### Steps to reproduce. See command given in stack trace above.; WGS bam is available at ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam` ; and ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam.bai`. Interval list BED file content given below. ```; chrX	67113957	67114130; chrX	71903370	71903687; chrX	74330484	74330552; chrX	75379902	75379965; chrX	78441355	78441953; ```. #### Expected behavior; Pass. #### Actual behavior; Error! This could be related to ticket #2722,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:23360,Availability,Error,Error,23360,1); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [5838bd7dec2d4533ad090ce03ecc7c0c] entered state [ERROR] while waiting for [DONE].; ```. #### Steps to reproduce. See command given in stack trace above.; WGS bam is available at ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam` ; and ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam.bai`. Interval list BED file content given below. ```; chrX	67113957	67114130; chrX	71903370	71903687; chrX	74330484	74330552; chrX	75379902	75379965; chrX	78441355	78441953; ```. #### Expected behavior; Pass. #### Actual behavior; Error! This could be related to ticket #2722,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:114,Deployability,release,release,114,"## Bug Report. ### Affected tool(s) or class(es); `PrintReadsSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description . I first encountered this type of error in a prototype tool I'm writing, so to dig further about what's happening, I run our simplest Spark tool&mdash;`PrintReadsSpark`. `PrintReadsSpark` reports errors when intervals are specified in a BED file (see command given in the stack trace). * Scenario 1: run with a WGS bam and give intervals via `-L PATH_TO_BED_FILE`, error is reported; * Scenario 2: run with the WGS bam and give intervals via `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 3: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L PATH_TO_BED_FILE`, no error; * Scenario 4: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 5: download the shrunken bam to local machine and run `PrintReadsSpark` with `-L PATH_TO_BED_FILE`, no error. Stack trace from scenario 1:; ```; ./gatk PrintReadsSpark \; -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam \; -O hdfs://shuang-small-m:8020/results/temp.bam \; -L hdfs://shuang-small-m:8020/data/intervals.bed \; -- \; --spark-runner GCS \; --cluster shuang-small \; --project broad-dsde-methods. Using GATK jar /Users/shuang/GATK/gatk/build/libs/gatk-spark.jar; found cached jar: gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar. Replacing spark-submit style args with dataproc style args. --cluster shuang-small --project broad-dsde-methods -> --cluster shuang-small --project broad-dsde-methods --properties spark.kryoserializer.buffer.max=512m,spark.driver.maxResultSize=0,spark.driver.userClassPathFirst=false,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-DGATK_STA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:3591,Deployability,configurat,configuration,3591,"ark.driver.userClassPathFirst=false,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 ,spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --jar gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar -- PrintReadsSpark -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam -O hdfs://shuang-small-m:8020/results/temp.bam -L hdfs://shuang-small-m:8020/data/intervals.bed --spark-master yarn; Job [5838bd7dec2d4533ad090ce03ecc7c0c] submitted.; Waiting for job output...; 18/07/24 21:02:03 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 21:02:08.430 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 21:02:08.594 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/5838bd7dec2d4533ad090ce03ecc7c0c/gatk-spark_5710525a8758807e46bbb660ac998e63.jar!/com/intel/gkl/native/libgkl_compression.so; 21:02:08.889 INFO PrintReadsSpark - ------------------------------------------------------------; 21:02:08.890 INFO PrintReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.6.0-26-g3979bdb-SNAPSHOT; 21:02:08.890 INFO PrintReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:02:08.890 INFO PrintReadsSpark - Executing as root@shuang-small-m on Linux v3.16.0-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:5719,Deployability,patch,patch,5719,"08.890 INFO PrintReadsSpark - Start Date/Time: July 24, 2018 9:02:08 PM UTC; 21:02:08.890 INFO PrintReadsSpark - ------------------------------------------------------------; 21:02:08.890 INFO PrintReadsSpark - ------------------------------------------------------------; 21:02:08.891 INFO PrintReadsSpark - HTSJDK Version: 2.16.0; 21:02:08.891 INFO PrintReadsSpark - Picard Version: 2.18.7; 21:02:08.891 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 21:02:08.892 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:02:08.892 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 21:02:08.892 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:02:08.892 INFO PrintReadsSpark - Deflater: IntelDeflater; 21:02:08.892 INFO PrintReadsSpark - Inflater: IntelInflater; 21:02:08.892 INFO PrintReadsSpark - GCS max retries/reopens: 20; 21:02:08.892 INFO PrintReadsSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:02:08.892 WARN PrintReadsSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: PrintReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:02:08.892 INFO PrintReadsSpark - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:6258,Deployability,configurat,configuration,6258,"D_FOR_SAMTOOLS : false; 21:02:08.892 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 21:02:08.892 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:02:08.892 INFO PrintReadsSpark - Deflater: IntelDeflater; 21:02:08.892 INFO PrintReadsSpark - Inflater: IntelInflater; 21:02:08.892 INFO PrintReadsSpark - GCS max retries/reopens: 20; 21:02:08.892 INFO PrintReadsSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:02:08.892 WARN PrintReadsSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: PrintReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:02:08.892 INFO PrintReadsSpark - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:6928,Deployability,configurat,configuration,6928,"ng: PrintReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:02:08.892 INFO PrintReadsSpark - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at shuang-small-m/10.128.5.217:8032; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at shuang-small-m/10.128.5.217:10200; 18/07/24 21:02:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1532457503538_0038; 21:02:16.702 INFO FeatureManager - Using codec BEDCodec to read file hdfs://shuang-small-m:8020/data/intervals.bed; 21:02:16.863 INFO IntervalArgumentCollection - Processing 1219 bp from intervals; 18/07/24 21:02:17 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:7126,Deployability,configurat,configuration,7126,"k - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at shuang-small-m/10.128.5.217:8032; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at shuang-small-m/10.128.5.217:10200; 18/07/24 21:02:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1532457503538_0038; 21:02:16.702 INFO FeatureManager - Using codec BEDCodec to read file hdfs://shuang-small-m:8020/data/intervals.bed; 21:02:16.863 INFO IntervalArgumentCollection - Processing 1219 bp from intervals; 18/07/24 21:02:17 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 1; 18/07/24 21:02:25 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, shuang-small-m.c.broad-dsde-methods.internal, exec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:11468,Deployability,pipeline,pipelines,11468," org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). 18/07/24 21:02:27 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 0.0 failed 4 times; aborting job; 18/07/24 21:02:27 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 21:02:27.703 INFO PrintReadsSpark - Shutting down engine; [July 24, 2018 9:02:27 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.32 minutes.; Runtime.totalMemory()=2463629312; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 7, shuang-small-m.c.broad-dsde-methods.internal, executor 2): htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.seek(BlockCompressedInputStream.java:380); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:18227,Deployability,pipeline,pipelines,18227,rg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:61); 	at org.apache.spark.api.java.JavaPairRDD.sortByKey(JavaPairRDD.scala:936); 	at org.broadinstitute.hellbender.utils.spark.SparkUtils.sortUsingElementsAsKeys(SparkUtils.java:164); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.sortSamRecordsToMatchHeader(ReadsSparkSink.java:382); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:289); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:206); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:307); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:295); 	at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:35); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:461); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMet,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:19333,Deployability,deploy,deploy,19333,TKSparkTool.runPipeline(GATKSparkTool.java:461); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtool,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:19415,Deployability,deploy,deploy,19415,.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.seek(BlockCompressedInputStream.java:380); 	at h,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:19452,Deployability,deploy,deploy,19452,SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.seek(BlockCompressedInputStream.java:380); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIter,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:19525,Deployability,deploy,deploy,19525,er.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.seek(BlockCompressedInputStream.java:380); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:977); 	at htsjdk.samtools.B,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:19602,Deployability,deploy,deploy,19602,roadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.seek(BlockCompressedInputStream.java:380); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:977); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samt,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:19674,Deployability,deploy,deploy,19674,eArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.seek(BlockCompressedInputStream.java:380); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:977); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at hts,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:19744,Deployability,deploy,deploy,19744,cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.seek(BlockCompressedInputStream.java:380); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:977); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:6879,Energy Efficiency,schedul,scheduler,6879,"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: PrintReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:02:08.892 INFO PrintReadsSpark - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at shuang-small-m/10.128.5.217:8032; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at shuang-small-m/10.128.5.217:10200; 18/07/24 21:02:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1532457503538_0038; 21:02:16.702 INFO FeatureManager - Using codec BEDCodec to read file hdfs://shuang-small-m:8020/data/intervals.bed; 21:02:16.863 INFO IntervalArgumentCollection - Processing 1219 bp from intervals; 18/07/24 21:02:17 INFO org.apa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:6918,Energy Efficiency,Schedul,Scheduler,6918,"ng: PrintReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:02:08.892 INFO PrintReadsSpark - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at shuang-small-m/10.128.5.217:8032; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at shuang-small-m/10.128.5.217:10200; 18/07/24 21:02:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1532457503538_0038; 21:02:16.702 INFO FeatureManager - Using codec BEDCodec to read file hdfs://shuang-small-m:8020/data/intervals.bed; 21:02:16.863 INFO IntervalArgumentCollection - Processing 1219 bp from intervals; 18/07/24 21:02:17 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:6973,Energy Efficiency,schedul,scheduled,6973,"ng: PrintReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:02:08.892 INFO PrintReadsSpark - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at shuang-small-m/10.128.5.217:8032; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at shuang-small-m/10.128.5.217:10200; 18/07/24 21:02:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1532457503538_0038; 21:02:16.702 INFO FeatureManager - Using codec BEDCodec to read file hdfs://shuang-small-m:8020/data/intervals.bed; 21:02:16.863 INFO IntervalArgumentCollection - Processing 1219 bp from intervals; 18/07/24 21:02:17 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:7010,Energy Efficiency,schedul,scheduling,7010,"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:02:08.892 INFO PrintReadsSpark - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at shuang-small-m/10.128.5.217:8032; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at shuang-small-m/10.128.5.217:10200; 18/07/24 21:02:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1532457503538_0038; 21:02:16.702 INFO FeatureManager - Using codec BEDCodec to read file hdfs://shuang-small-m:8020/data/intervals.bed; 21:02:16.863 INFO IntervalArgumentCollection - Processing 1219 bp from intervals; 18/07/24 21:02:17 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 1; 18/07/24 21:02:25 WARN org.apache.spark.scheduler.TaskSetMan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:7072,Energy Efficiency,schedul,scheduler,7072,"!!!!!. 21:02:08.892 INFO PrintReadsSpark - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at shuang-small-m/10.128.5.217:8032; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at shuang-small-m/10.128.5.217:10200; 18/07/24 21:02:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1532457503538_0038; 21:02:16.702 INFO FeatureManager - Using codec BEDCodec to read file hdfs://shuang-small-m:8020/data/intervals.bed; 21:02:16.863 INFO IntervalArgumentCollection - Processing 1219 bp from intervals; 18/07/24 21:02:17 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 1; 18/07/24 21:02:25 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, shuang-sma",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:8006,Energy Efficiency,schedul,scheduler,8006,"cheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at shuang-small-m/10.128.5.217:8032; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at shuang-small-m/10.128.5.217:10200; 18/07/24 21:02:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1532457503538_0038; 21:02:16.702 INFO FeatureManager - Using codec BEDCodec to read file hdfs://shuang-small-m:8020/data/intervals.bed; 21:02:16.863 INFO IntervalArgumentCollection - Processing 1219 bp from intervals; 18/07/24 21:02:17 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 1; 18/07/24 21:02:25 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, shuang-small-m.c.broad-dsde-methods.internal, executor 1): htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.seek(BlockCompressedInputStream.java:380); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:977); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.sa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:10692,Energy Efficiency,schedul,scheduler,10692,"$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). 18/07/24 21:02:27 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 0.0 failed 4 times; aborting job; 18/07/24 21:02:27 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 21:02:27.703 INFO PrintReadsSpark - Shutting down engine; [July 24, 2018 9:02:27 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.32 minutes.; Runtime.totalMemory()=2463629312; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:10764,Energy Efficiency,schedul,scheduler,10764,"n$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). 18/07/24 21:02:27 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 0.0 failed 4 times; aborting job; 18/07/24 21:02:27 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 21:02:27.703 INFO PrintReadsSpark - Shutting down engine; [July 24, 2018 9:02:27 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.32 minutes.; Runtime.totalMemory()=2463629312; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 7, shuang-small-m.c.broad-dsde-methods.internal, e",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:11129,Energy Efficiency,schedul,scheduler,11129,"itioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). 18/07/24 21:02:27 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 0.0 failed 4 times; aborting job; 18/07/24 21:02:27 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 21:02:27.703 INFO PrintReadsSpark - Shutting down engine; [July 24, 2018 9:02:27 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.32 minutes.; Runtime.totalMemory()=2463629312; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 7, shuang-small-m.c.broad-dsde-methods.internal, executor 2): htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressed",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:14347,Energy Efficiency,schedul,scheduler,14347,$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); 	at org.apache.sp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:14419,Energy Efficiency,schedul,scheduler,14419,n$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSched,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:14784,Energy Efficiency,schedul,scheduler,14784,itioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); 	at org.apa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:14824,Energy Efficiency,schedul,scheduler,14824,pache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:14923,Energy Efficiency,schedul,scheduler,14923,pache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:15021,Energy Efficiency,schedul,scheduler,15021,org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:15275,Energy Efficiency,schedul,scheduler,15275,apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:15356,Energy Efficiency,schedul,scheduler,15356,ltTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); 	at org.apache.spark.SparkContext.runJob(Spar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:15462,Energy Efficiency,schedul,scheduler,15462,he.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939); 	at org.apache,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:15612,Energy Efficiency,schedul,scheduler,15612,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:11,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:15701,Energy Efficiency,schedul,scheduler,15701,va.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:15799,Energy Efficiency,schedul,scheduler,15799,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:938); 	at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:306); 	at o,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:15895,Energy Efficiency,schedul,scheduler,15895,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:938); 	at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:306); 	at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:168); 	at org.apache.spark.RangePartit,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:16060,Energy Efficiency,schedul,scheduler,16060,.apply(DAGScheduler.scala:1586); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:938); 	at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:306); 	at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:168); 	at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:148); 	at org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:62); 	at org.apache.spark.rdd.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:22365,Energy Efficiency,schedul,scheduler,22365,$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [5838bd7dec2d4533ad090ce03ecc7c0c] entered state [ERROR] while waiting for [DONE].; ```. #### Steps to reproduce. See command given in stack trace above.; WGS bam is available at ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam` ; and ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam.bai`. Interval list BED file content given below. ```; chrX	67113957	67114130; chrX	71903370	71903687; chrX	74330484	74330552; chrX	75379902	75379965; chrX	78441355	78441953; ```. #### Expected behavior; Pass. #### Actual behavior; Error! Thi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:22437,Energy Efficiency,schedul,scheduler,22437,1); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [5838bd7dec2d4533ad090ce03ecc7c0c] entered state [ERROR] while waiting for [DONE].; ```. #### Steps to reproduce. See command given in stack trace above.; WGS bam is available at ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam` ; and ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam.bai`. Interval list BED file content given below. ```; chrX	67113957	67114130; chrX	71903370	71903687; chrX	74330484	74330552; chrX	75379902	75379965; chrX	78441355	78441953; ```. #### Expected behavior; Pass. #### Actual behavior; Error! This could be related to ticket #2722,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:3591,Modifiability,config,configuration,3591,"ark.driver.userClassPathFirst=false,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 ,spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --jar gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar -- PrintReadsSpark -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam -O hdfs://shuang-small-m:8020/results/temp.bam -L hdfs://shuang-small-m:8020/data/intervals.bed --spark-master yarn; Job [5838bd7dec2d4533ad090ce03ecc7c0c] submitted.; Waiting for job output...; 18/07/24 21:02:03 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 21:02:08.430 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 21:02:08.594 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/5838bd7dec2d4533ad090ce03ecc7c0c/gatk-spark_5710525a8758807e46bbb660ac998e63.jar!/com/intel/gkl/native/libgkl_compression.so; 21:02:08.889 INFO PrintReadsSpark - ------------------------------------------------------------; 21:02:08.890 INFO PrintReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.6.0-26-g3979bdb-SNAPSHOT; 21:02:08.890 INFO PrintReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:02:08.890 INFO PrintReadsSpark - Executing as root@shuang-small-m on Linux v3.16.0-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:3833,Modifiability,variab,variables,3833,"bble=false -Dsamjdk.compression_level=2 ,spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --jar gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar -- PrintReadsSpark -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam -O hdfs://shuang-small-m:8020/results/temp.bam -L hdfs://shuang-small-m:8020/data/intervals.bed --spark-master yarn; Job [5838bd7dec2d4533ad090ce03ecc7c0c] submitted.; Waiting for job output...; 18/07/24 21:02:03 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 21:02:08.430 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 21:02:08.594 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/5838bd7dec2d4533ad090ce03ecc7c0c/gatk-spark_5710525a8758807e46bbb660ac998e63.jar!/com/intel/gkl/native/libgkl_compression.so; 21:02:08.889 INFO PrintReadsSpark - ------------------------------------------------------------; 21:02:08.890 INFO PrintReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.6.0-26-g3979bdb-SNAPSHOT; 21:02:08.890 INFO PrintReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:02:08.890 INFO PrintReadsSpark - Executing as root@shuang-small-m on Linux v3.16.0-6-amd64 amd64; 21:02:08.890 INFO PrintReadsSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-8u171-b11-1~bpo8+1-b11; 21:02:08.890 INFO PrintReadsSpark - Start Date/Time: July 24, 2018 9:02:08 PM UTC; 21:02:08.890 INFO PrintReadsSpark - --------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:3959,Modifiability,config,configured,3959,"bble=false -Dsamjdk.compression_level=2 ,spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --jar gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar -- PrintReadsSpark -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam -O hdfs://shuang-small-m:8020/results/temp.bam -L hdfs://shuang-small-m:8020/data/intervals.bed --spark-master yarn; Job [5838bd7dec2d4533ad090ce03ecc7c0c] submitted.; Waiting for job output...; 18/07/24 21:02:03 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 21:02:08.430 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 21:02:08.594 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/5838bd7dec2d4533ad090ce03ecc7c0c/gatk-spark_5710525a8758807e46bbb660ac998e63.jar!/com/intel/gkl/native/libgkl_compression.so; 21:02:08.889 INFO PrintReadsSpark - ------------------------------------------------------------; 21:02:08.890 INFO PrintReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.6.0-26-g3979bdb-SNAPSHOT; 21:02:08.890 INFO PrintReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:02:08.890 INFO PrintReadsSpark - Executing as root@shuang-small-m on Linux v3.16.0-6-amd64 amd64; 21:02:08.890 INFO PrintReadsSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-8u171-b11-1~bpo8+1-b11; 21:02:08.890 INFO PrintReadsSpark - Start Date/Time: July 24, 2018 9:02:08 PM UTC; 21:02:08.890 INFO PrintReadsSpark - --------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:6258,Modifiability,config,configuration,6258,"D_FOR_SAMTOOLS : false; 21:02:08.892 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 21:02:08.892 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:02:08.892 INFO PrintReadsSpark - Deflater: IntelDeflater; 21:02:08.892 INFO PrintReadsSpark - Inflater: IntelInflater; 21:02:08.892 INFO PrintReadsSpark - GCS max retries/reopens: 20; 21:02:08.892 INFO PrintReadsSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:02:08.892 WARN PrintReadsSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: PrintReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:02:08.892 INFO PrintReadsSpark - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:6928,Modifiability,config,configuration,6928,"ng: PrintReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:02:08.892 INFO PrintReadsSpark - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at shuang-small-m/10.128.5.217:8032; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at shuang-small-m/10.128.5.217:10200; 18/07/24 21:02:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1532457503538_0038; 21:02:16.702 INFO FeatureManager - Using codec BEDCodec to read file hdfs://shuang-small-m:8020/data/intervals.bed; 21:02:16.863 INFO IntervalArgumentCollection - Processing 1219 bp from intervals; 18/07/24 21:02:17 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:7022,Modifiability,config,configure,7022,"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:02:08.892 INFO PrintReadsSpark - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at shuang-small-m/10.128.5.217:8032; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at shuang-small-m/10.128.5.217:10200; 18/07/24 21:02:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1532457503538_0038; 21:02:16.702 INFO FeatureManager - Using codec BEDCodec to read file hdfs://shuang-small-m:8020/data/intervals.bed; 21:02:16.863 INFO IntervalArgumentCollection - Processing 1219 bp from intervals; 18/07/24 21:02:17 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 1; 18/07/24 21:02:25 WARN org.apache.spark.scheduler.TaskSetMan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:7126,Modifiability,config,configuration,7126,"k - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at shuang-small-m/10.128.5.217:8032; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at shuang-small-m/10.128.5.217:10200; 18/07/24 21:02:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1532457503538_0038; 21:02:16.702 INFO FeatureManager - Using codec BEDCodec to read file hdfs://shuang-small-m:8020/data/intervals.bed; 21:02:16.863 INFO IntervalArgumentCollection - Processing 1219 bp from intervals; 18/07/24 21:02:17 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 1; 18/07/24 21:02:25 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, shuang-small-m.c.broad-dsde-methods.internal, exec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:1501,Performance,cache,cached,1501," via `-L PATH_TO_BED_FILE`, error is reported; * Scenario 2: run with the WGS bam and give intervals via `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 3: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L PATH_TO_BED_FILE`, no error; * Scenario 4: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 5: download the shrunken bam to local machine and run `PrintReadsSpark` with `-L PATH_TO_BED_FILE`, no error. Stack trace from scenario 1:; ```; ./gatk PrintReadsSpark \; -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam \; -O hdfs://shuang-small-m:8020/results/temp.bam \; -L hdfs://shuang-small-m:8020/data/intervals.bed \; -- \; --spark-runner GCS \; --cluster shuang-small \; --project broad-dsde-methods. Using GATK jar /Users/shuang/GATK/gatk/build/libs/gatk-spark.jar; found cached jar: gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar. Replacing spark-submit style args with dataproc style args. --cluster shuang-small --project broad-dsde-methods -> --cluster shuang-small --project broad-dsde-methods --properties spark.kryoserializer.buffer.max=512m,spark.driver.maxResultSize=0,spark.driver.userClassPathFirst=false,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 ,spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2. Running:; gcloud dataproc jobs submit spark --cluster shuang-small --project broad-dsde-methods --properties spark.kryoserializ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:4020,Performance,Load,Loading,4020,"ync_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --jar gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar -- PrintReadsSpark -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam -O hdfs://shuang-small-m:8020/results/temp.bam -L hdfs://shuang-small-m:8020/data/intervals.bed --spark-master yarn; Job [5838bd7dec2d4533ad090ce03ecc7c0c] submitted.; Waiting for job output...; 18/07/24 21:02:03 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 21:02:08.430 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 21:02:08.594 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/5838bd7dec2d4533ad090ce03ecc7c0c/gatk-spark_5710525a8758807e46bbb660ac998e63.jar!/com/intel/gkl/native/libgkl_compression.so; 21:02:08.889 INFO PrintReadsSpark - ------------------------------------------------------------; 21:02:08.890 INFO PrintReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.6.0-26-g3979bdb-SNAPSHOT; 21:02:08.890 INFO PrintReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:02:08.890 INFO PrintReadsSpark - Executing as root@shuang-small-m on Linux v3.16.0-6-amd64 amd64; 21:02:08.890 INFO PrintReadsSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-8u171-b11-1~bpo8+1-b11; 21:02:08.890 INFO PrintReadsSpark - Start Date/Time: July 24, 2018 9:02:08 PM UTC; 21:02:08.890 INFO PrintReadsSpark - ------------------------------------------------------------; 21:02:08.890 INFO PrintReadsSpark - ------------------------------------------------------------; 21:02:08.891 INFO ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:10889,Performance,concurren,concurrent,10889,"erator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). 18/07/24 21:02:27 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 0.0 failed 4 times; aborting job; 18/07/24 21:02:27 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 21:02:27.703 INFO PrintReadsSpark - Shutting down engine; [July 24, 2018 9:02:27 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.32 minutes.; Runtime.totalMemory()=2463629312; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 7, shuang-small-m.c.broad-dsde-methods.internal, executor 2): htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockG",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:10974,Performance,concurren,concurrent,10974,"asNext(Iterator.scala:408); 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). 18/07/24 21:02:27 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 0.0 failed 4 times; aborting job; 18/07/24 21:02:27 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 21:02:27.703 INFO PrintReadsSpark - Shutting down engine; [July 24, 2018 9:02:27 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.32 minutes.; Runtime.totalMemory()=2463629312; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 7, shuang-small-m.c.broad-dsde-methods.internal, executor 2): htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:14544,Performance,concurren,concurrent,14544,erator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:14629,Performance,concurren,concurrent,14629,asNext(Iterator.scala:408); 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:22562,Performance,concurren,concurrent,22562,1); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [5838bd7dec2d4533ad090ce03ecc7c0c] entered state [ERROR] while waiting for [DONE].; ```. #### Steps to reproduce. See command given in stack trace above.; WGS bam is available at ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam` ; and ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam.bai`. Interval list BED file content given below. ```; chrX	67113957	67114130; chrX	71903370	71903687; chrX	74330484	74330552; chrX	75379902	75379965; chrX	78441355	78441953; ```. #### Expected behavior; Pass. #### Actual behavior; Error! This could be related to ticket #2722,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:22647,Performance,concurren,concurrent,22647,1); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [5838bd7dec2d4533ad090ce03ecc7c0c] entered state [ERROR] while waiting for [DONE].; ```. #### Steps to reproduce. See command given in stack trace above.; WGS bam is available at ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam` ; and ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam.bai`. Interval list BED file content given below. ```; chrX	67113957	67114130; chrX	71903370	71903687; chrX	74330484	74330552; chrX	75379902	75379965; chrX	78441355	78441953; ```. #### Expected behavior; Pass. #### Actual behavior; Error! This could be related to ticket #2722,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:11191,Safety,abort,aborting,11191,"rk.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). 18/07/24 21:02:27 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 0.0 failed 4 times; aborting job; 18/07/24 21:02:27 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 21:02:27.703 INFO PrintReadsSpark - Shutting down engine; [July 24, 2018 9:02:27 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.32 minutes.; Runtime.totalMemory()=2463629312; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 7, shuang-small-m.c.broad-dsde-methods.internal, executor 2): htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:11600,Safety,abort,aborted,11600,"); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). 18/07/24 21:02:27 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 0.0 failed 4 times; aborting job; 18/07/24 21:02:27 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 21:02:27.703 INFO PrintReadsSpark - Shutting down engine; [July 24, 2018 9:02:27 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.32 minutes.; Runtime.totalMemory()=2463629312; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 7, shuang-small-m.c.broad-dsde-methods.internal, executor 2): htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.seek(BlockCompressedInputStream.java:380); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:977); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFile",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:14955,Safety,abort,abortStage,14955,nonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:15053,Safety,abort,abortStage,15053,DD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:15298,Safety,abort,abortStage,15298,RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:184,Testability,test,test,184,"## Bug Report. ### Affected tool(s) or class(es); `PrintReadsSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description . I first encountered this type of error in a prototype tool I'm writing, so to dig further about what's happening, I run our simplest Spark tool&mdash;`PrintReadsSpark`. `PrintReadsSpark` reports errors when intervals are specified in a BED file (see command given in the stack trace). * Scenario 1: run with a WGS bam and give intervals via `-L PATH_TO_BED_FILE`, error is reported; * Scenario 2: run with the WGS bam and give intervals via `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 3: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L PATH_TO_BED_FILE`, no error; * Scenario 4: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 5: download the shrunken bam to local machine and run `PrintReadsSpark` with `-L PATH_TO_BED_FILE`, no error. Stack trace from scenario 1:; ```; ./gatk PrintReadsSpark \; -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam \; -O hdfs://shuang-small-m:8020/results/temp.bam \; -L hdfs://shuang-small-m:8020/data/intervals.bed \; -- \; --spark-runner GCS \; --cluster shuang-small \; --project broad-dsde-methods. Using GATK jar /Users/shuang/GATK/gatk/build/libs/gatk-spark.jar; found cached jar: gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar. Replacing spark-submit style args with dataproc style args. --cluster shuang-small --project broad-dsde-methods -> --cluster shuang-small --project broad-dsde-methods --properties spark.kryoserializer.buffer.max=512m,spark.driver.maxResultSize=0,spark.driver.userClassPathFirst=false,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-DGATK_STA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:6500,Testability,log,log,6500,"r; 21:02:08.892 INFO PrintReadsSpark - Inflater: IntelInflater; 21:02:08.892 INFO PrintReadsSpark - GCS max retries/reopens: 20; 21:02:08.892 INFO PrintReadsSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:02:08.892 WARN PrintReadsSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: PrintReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:02:08.892 INFO PrintReadsSpark - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at shuang-small-m/10.128.5.217:8032; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at shuang-small-m/10.128.5.217:10200; 18/07/2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:6505,Testability,Log,Logging,6505,"r; 21:02:08.892 INFO PrintReadsSpark - Inflater: IntelInflater; 21:02:08.892 INFO PrintReadsSpark - GCS max retries/reopens: 20; 21:02:08.892 INFO PrintReadsSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:02:08.892 WARN PrintReadsSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: PrintReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:02:08.892 INFO PrintReadsSpark - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at shuang-small-m/10.128.5.217:8032; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at shuang-small-m/10.128.5.217:10200; 18/07/2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5051:334,Usability,simpl,simplest,334,"## Bug Report. ### Affected tool(s) or class(es); `PrintReadsSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description . I first encountered this type of error in a prototype tool I'm writing, so to dig further about what's happening, I run our simplest Spark tool&mdash;`PrintReadsSpark`. `PrintReadsSpark` reports errors when intervals are specified in a BED file (see command given in the stack trace). * Scenario 1: run with a WGS bam and give intervals via `-L PATH_TO_BED_FILE`, error is reported; * Scenario 2: run with the WGS bam and give intervals via `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 3: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L PATH_TO_BED_FILE`, no error; * Scenario 4: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 5: download the shrunken bam to local machine and run `PrintReadsSpark` with `-L PATH_TO_BED_FILE`, no error. Stack trace from scenario 1:; ```; ./gatk PrintReadsSpark \; -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam \; -O hdfs://shuang-small-m:8020/results/temp.bam \; -L hdfs://shuang-small-m:8020/data/intervals.bed \; -- \; --spark-runner GCS \; --cluster shuang-small \; --project broad-dsde-methods. Using GATK jar /Users/shuang/GATK/gatk/build/libs/gatk-spark.jar; found cached jar: gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar. Replacing spark-submit style args with dataproc style args. --cluster shuang-small --project broad-dsde-methods -> --cluster shuang-small --project broad-dsde-methods --properties spark.kryoserializer.buffer.max=512m,spark.driver.maxResultSize=0,spark.driver.userClassPathFirst=false,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-DGATK_STA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051
https://github.com/broadinstitute/gatk/issues/5053:374,Availability,error,error,374,"## Bug Report. ### Affected tool(s) or class(es); GermlineCNVCaller. ### Affected version(s); - GATK4 4.0.5.1. ### Description ; I'm trying to do a germline CNV calling with 387 exomes samples (I know it's a lot). The CollectReadCounts and DetermineGermlineContigPloidy were successfull. But for the GermlineCNVCaller I got what I think is a Python ""cannot allocate memory"" error. I tried to specify to the JVM a max memory to allocate ``` --java-options ""-Xmx192G"" ``` , but no improvements. The machine I'm working on got 32 threads and 192 Gb RAM. #### Steps to reproduce; I guess try to do a CNV calling with a large cohort. #### Output; ```10:56:25.124 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/tintest/miniconda2/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:56:25.342 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.343 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.0.5.1; 10:56:25.344 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:56:25.345 INFO GermlineCNVCaller - Executing as tintest@dahu39 on Linux v4.9.0-6-amd64 amd64; 10:56:25.346 INFO GermlineCNVCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b15; 10:56:25.347 INFO GermlineCNVCaller - Start Date/Time: July 25, 2018 10:56:24 AM CEST; 10:56:25.348 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.349 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.350 INFO GermlineCNVCaller - HTSJDK Version: 2.15.1; 10:56:25.351 INFO GermlineCNVCaller - Picard Version: 2.18.2; 10:56:25.352 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:56:25.353 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:56:25.354 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:42550,Availability,down,down,42550,"le hdf5/grexome0280.hdf5 (378 / 387); 10:58:46.452 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0078.hdf5 (379 / 387); 10:58:46.743 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0186.hdf5 (380 / 387); 10:58:47.081 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0284.hdf5 (381 / 387); 10:58:47.421 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0179.hdf5 (382 / 387); 10:58:47.659 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0221.hdf5 (383 / 387); 10:58:47.889 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0358.hdf5 (384 / 387); 10:58:48.140 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0144.hdf5 (385 / 387); 10:58:48.423 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0231.hdf5 (386 / 387); 10:58:48.650 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0263.hdf5 (387 / 387); 11:04:58.313 INFO GermlineCNVCaller - Shutting down engine; [July 25, 2018 11:04:58 AM CEST] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 8.56 minutes.; Runtime.totalMemory()=10962337792; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 1; Command Line: python /tmp/tintest/cohort_denoising_calling.4390748645603329412.py --ploidy_calls_path=DetermineGermlineContigPloidy/DetermineGermlineContigPloidy-calls --output_calls_path=GermlineCNVCaller/GermlineCNVCaller-calls --output_tracking_path=GermlineCNVCaller/GermlineCNVCaller-tracking --modeling_interval_list=/tmp/tintest/intervals7440933759308039041.tsv --output_model_path=GermlineCNVCaller/GermlineCNVCaller-model --enable_explicit_gc_bias_modeling=False --read_count_tsv_files /tmp/tintest/sample-04881922240505697119.tsv /tmp/tintest/sample-17677568501630201512.tsv /tmp/tintest/sample-26804327235005483714.tsv /tmp/tintest/sample-37617330639944470775.tsv /tmp/tintest/sample-46355851108368390478.tsv /tmp/tin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:61691,Availability,ERROR,ERROR,61691,4068.tsv /tmp/tintest/sample-3734484951576950052743.tsv /tmp/tintest/sample-3745007638909244994571.tsv /tmp/tintest/sample-3758817480300622528681.tsv /tmp/tintest/sample-3765561422653477541111.tsv /tmp/tintest/sample-377681127346074691924.tsv /tmp/tintest/sample-3788006936711929575536.tsv /tmp/tintest/sample-3794598448303416401276.tsv /tmp/tintest/sample-380910670101098136635.tsv /tmp/tintest/sample-3815864583095389374312.tsv /tmp/tintest/sample-3821063008346821202582.tsv /tmp/tintest/sample-3836550848258521825191.tsv /tmp/tintest/sample-3842488752532231097400.tsv /tmp/tintest/sample-3855124216409092357090.tsv /tmp/tintest/sample-3866989755460133829309.tsv ; Stdout: 10:58:52.820 INFO cohort_denoising_calling - Loading 387 read counts file(s)...; 11:01:01.618 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 11:02:11.422 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 11:04:53.672 ERROR theano.gof.cmodule - [Errno 12] Cannot allocate memory. Stderr: Problem occurred during compilation with the command line below:; /usr/bin/g++ -shared -g -O3 -fno-math-errno -Wno-unused-label -Wno-unused-variable -Wno-write-strings -fopenmp -march=knl -mmmx -mno-3dnow -msse -msse2 -msse3 -mssse3 -mno-sse4a -mcx16 -msahf -mmovbe -maes -mno-sha -mpclmul -mpopcnt -mabm -mno-lwp -mfma -mno-fma4 -mno-xop -mbmi -mbmi2 -mno-tbm -mavx -mavx2 -msse4.2 -msse4.1 -mlzcnt -mrtm -mhle -mrdrnd -mf16c -mfsgsbase -mrdseed -mprfchw -madx -mfxsr -mxsave -mxsaveopt -mavx512f -mno-avx512er -mavx512cd -mno-avx512pf -mno-prefetchwt1 -mclflushopt -mxsavec -mxsaves -mavx512dq -mavx512bw -mno-avx512vl -mno-avx512ifma -mno-avx512vbmi -mclwb -mno-mwaitx -mno-clzero -mpku --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cache-size=22528 -mtune=generic -DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -m64 -fPIC -I/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/numpy/core/include -I/h,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:63149,Availability,ERROR,ERROR,63149,"mlzcnt -mrtm -mhle -mrdrnd -mf16c -mfsgsbase -mrdseed -mprfchw -madx -mfxsr -mxsave -mxsaveopt -mavx512f -mno-avx512er -mavx512cd -mno-avx512pf -mno-prefetchwt1 -mclflushopt -mxsavec -mxsaves -mavx512dq -mavx512bw -mno-avx512vl -mno-avx512ifma -mno-avx512vbmi -mclwb -mno-mwaitx -mno-clzero -mpku --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cache-size=22528 -mtune=generic -DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -m64 -fPIC -I/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/numpy/core/include -I/home/tintest/miniconda2/envs/aurexome/include/python3.6m -I/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof -L/home/tintest/miniconda2/envs/aurexome/lib -fvisibility=hidden -o /home/tintest/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.4--3.6.2-64/tmpueark7lw/m421cdb2b133a2578e9a2670dfbb5d33e.so /home/tintest/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.4--3.6.2-64/tmpueark7lw/mod.cpp -lpython3.6m; ERROR (theano.gof.cmodule): [Errno 12] Cannot allocate memory; Traceback (most recent call last):; File ""/tmp/tintest/cohort_denoising_calling.4390748645603329412.py"", line 143, in <module>; shared_workspace, initial_params_supplier); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/gcnvkernel/tasks/task_cohort_denoising_calling.py"", line 140, in __init__; denoising_model = DenoisingModel(denoising_config, shared_workspace, initial_param_supplier); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/model.py"", line 197, in __call__; instance.__init__(*args, **kwargs); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 851, in __init__; observed=shared_workspace.n_st); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/distributions/distribution.py"", line 39, in __new__; return model.Var(name, dist, data, total_size); File ""/home/tintest/min",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:2367,Deployability,patch,patch,2367,"ler - Start Date/Time: July 25, 2018 10:56:24 AM CEST; 10:56:25.348 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.349 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.350 INFO GermlineCNVCaller - HTSJDK Version: 2.15.1; 10:56:25.351 INFO GermlineCNVCaller - Picard Version: 2.18.2; 10:56:25.352 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:56:25.353 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:56:25.354 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:56:25.355 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:56:25.356 INFO GermlineCNVCaller - Deflater: IntelDeflater; 10:56:25.357 INFO GermlineCNVCaller - Inflater: IntelInflater; 10:56:25.358 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 10:56:25.358 INFO GermlineCNVCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 10:56:25.360 WARN GermlineCNVCaller -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: GermlineCNVCaller is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 10:56:25.361 INFO GermlineCNVCaller - Initializing engine; 10:56:54.347 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 10:56:55.287 INFO GermlineCNVCaller - Retrieving intervals from first read-count file (hdf5/grexome0426.hdf5)...; 10:56:55.384 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 10:56:55.482 INFO GermlineCNVC",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:357,Energy Efficiency,allocate,allocate,357,"## Bug Report. ### Affected tool(s) or class(es); GermlineCNVCaller. ### Affected version(s); - GATK4 4.0.5.1. ### Description ; I'm trying to do a germline CNV calling with 387 exomes samples (I know it's a lot). The CollectReadCounts and DetermineGermlineContigPloidy were successfull. But for the GermlineCNVCaller I got what I think is a Python ""cannot allocate memory"" error. I tried to specify to the JVM a max memory to allocate ``` --java-options ""-Xmx192G"" ``` , but no improvements. The machine I'm working on got 32 threads and 192 Gb RAM. #### Steps to reproduce; I guess try to do a CNV calling with a large cohort. #### Output; ```10:56:25.124 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/tintest/miniconda2/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:56:25.342 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.343 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.0.5.1; 10:56:25.344 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:56:25.345 INFO GermlineCNVCaller - Executing as tintest@dahu39 on Linux v4.9.0-6-amd64 amd64; 10:56:25.346 INFO GermlineCNVCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b15; 10:56:25.347 INFO GermlineCNVCaller - Start Date/Time: July 25, 2018 10:56:24 AM CEST; 10:56:25.348 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.349 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.350 INFO GermlineCNVCaller - HTSJDK Version: 2.15.1; 10:56:25.351 INFO GermlineCNVCaller - Picard Version: 2.18.2; 10:56:25.352 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:56:25.353 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:56:25.354 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:427,Energy Efficiency,allocate,allocate,427,"## Bug Report. ### Affected tool(s) or class(es); GermlineCNVCaller. ### Affected version(s); - GATK4 4.0.5.1. ### Description ; I'm trying to do a germline CNV calling with 387 exomes samples (I know it's a lot). The CollectReadCounts and DetermineGermlineContigPloidy were successfull. But for the GermlineCNVCaller I got what I think is a Python ""cannot allocate memory"" error. I tried to specify to the JVM a max memory to allocate ``` --java-options ""-Xmx192G"" ``` , but no improvements. The machine I'm working on got 32 threads and 192 Gb RAM. #### Steps to reproduce; I guess try to do a CNV calling with a large cohort. #### Output; ```10:56:25.124 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/tintest/miniconda2/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:56:25.342 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.343 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.0.5.1; 10:56:25.344 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:56:25.345 INFO GermlineCNVCaller - Executing as tintest@dahu39 on Linux v4.9.0-6-amd64 amd64; 10:56:25.346 INFO GermlineCNVCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b15; 10:56:25.347 INFO GermlineCNVCaller - Start Date/Time: July 25, 2018 10:56:24 AM CEST; 10:56:25.348 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.349 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.350 INFO GermlineCNVCaller - HTSJDK Version: 2.15.1; 10:56:25.351 INFO GermlineCNVCaller - Picard Version: 2.18.2; 10:56:25.352 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:56:25.353 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:56:25.354 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:61736,Energy Efficiency,allocate,allocate,61736,4951576950052743.tsv /tmp/tintest/sample-3745007638909244994571.tsv /tmp/tintest/sample-3758817480300622528681.tsv /tmp/tintest/sample-3765561422653477541111.tsv /tmp/tintest/sample-377681127346074691924.tsv /tmp/tintest/sample-3788006936711929575536.tsv /tmp/tintest/sample-3794598448303416401276.tsv /tmp/tintest/sample-380910670101098136635.tsv /tmp/tintest/sample-3815864583095389374312.tsv /tmp/tintest/sample-3821063008346821202582.tsv /tmp/tintest/sample-3836550848258521825191.tsv /tmp/tintest/sample-3842488752532231097400.tsv /tmp/tintest/sample-3855124216409092357090.tsv /tmp/tintest/sample-3866989755460133829309.tsv ; Stdout: 10:58:52.820 INFO cohort_denoising_calling - Loading 387 read counts file(s)...; 11:01:01.618 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 11:02:11.422 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 11:04:53.672 ERROR theano.gof.cmodule - [Errno 12] Cannot allocate memory. Stderr: Problem occurred during compilation with the command line below:; /usr/bin/g++ -shared -g -O3 -fno-math-errno -Wno-unused-label -Wno-unused-variable -Wno-write-strings -fopenmp -march=knl -mmmx -mno-3dnow -msse -msse2 -msse3 -mssse3 -mno-sse4a -mcx16 -msahf -mmovbe -maes -mno-sha -mpclmul -mpopcnt -mabm -mno-lwp -mfma -mno-fma4 -mno-xop -mbmi -mbmi2 -mno-tbm -mavx -mavx2 -msse4.2 -msse4.1 -mlzcnt -mrtm -mhle -mrdrnd -mf16c -mfsgsbase -mrdseed -mprfchw -madx -mfxsr -mxsave -mxsaveopt -mavx512f -mno-avx512er -mavx512cd -mno-avx512pf -mno-prefetchwt1 -mclflushopt -mxsavec -mxsaves -mavx512dq -mavx512bw -mno-avx512vl -mno-avx512ifma -mno-avx512vbmi -mclwb -mno-mwaitx -mno-clzero -mpku --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cache-size=22528 -mtune=generic -DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -m64 -fPIC -I/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/numpy/core/include -I/home/tintest/miniconda2/envs/aurexo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:63195,Energy Efficiency,allocate,allocate,63195," -mxsave -mxsaveopt -mavx512f -mno-avx512er -mavx512cd -mno-avx512pf -mno-prefetchwt1 -mclflushopt -mxsavec -mxsaves -mavx512dq -mavx512bw -mno-avx512vl -mno-avx512ifma -mno-avx512vbmi -mclwb -mno-mwaitx -mno-clzero -mpku --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cache-size=22528 -mtune=generic -DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -m64 -fPIC -I/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/numpy/core/include -I/home/tintest/miniconda2/envs/aurexome/include/python3.6m -I/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof -L/home/tintest/miniconda2/envs/aurexome/lib -fvisibility=hidden -o /home/tintest/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.4--3.6.2-64/tmpueark7lw/m421cdb2b133a2578e9a2670dfbb5d33e.so /home/tintest/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.4--3.6.2-64/tmpueark7lw/mod.cpp -lpython3.6m; ERROR (theano.gof.cmodule): [Errno 12] Cannot allocate memory; Traceback (most recent call last):; File ""/tmp/tintest/cohort_denoising_calling.4390748645603329412.py"", line 143, in <module>; shared_workspace, initial_params_supplier); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/gcnvkernel/tasks/task_cohort_denoising_calling.py"", line 140, in __init__; denoising_model = DenoisingModel(denoising_config, shared_workspace, initial_param_supplier); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/model.py"", line 197, in __call__; instance.__init__(*args, **kwargs); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 851, in __init__; observed=shared_workspace.n_st); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/distributions/distribution.py"", line 39, in __new__; return model.Var(name, dist, data, total_size); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/model.py"", line 545,",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:66905,Energy Efficiency,allocate,allocate,66905,"odule_from_key; module = lnk.compile_cmodule(location); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof/cc.py"", line 1489, in compile_cmodule; preargs=preargs); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2294, in compile_str; p_out = output_subprocess_Popen(cmd); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/misc/windows.py"", line 77, in output_subprocess_Popen; p = subprocess_Popen(command, **params); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/misc/windows.py"", line 43, in subprocess_Popen; proc = subprocess.Popen(command, startupinfo=startupinfo, **params); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/subprocess.py"", line 707, in __init__; restore_signals, start_new_session); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/subprocess.py"", line 1267, in _execute_child; restore_signals, start_new_session, preexec_fn); OSError: [Errno 12] Cannot allocate memory. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.executeGermlineCNVCallerPythonScript(GermlineCNVCaller.java:454); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:291); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); at org.broadinstitute.hellbender.cmdline.Co",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:68416,Energy Efficiency,allocate,allocate,68416,"hon3.6/site-packages/theano/misc/windows.py"", line 43, in subprocess_Popen; proc = subprocess.Popen(command, startupinfo=startupinfo, **params); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/subprocess.py"", line 707, in __init__; restore_signals, start_new_session); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/subprocess.py"", line 1267, in _execute_child; restore_signals, start_new_session, preexec_fn); OSError: [Errno 12] Cannot allocate memory. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.executeGermlineCNVCallerPythonScript(GermlineCNVCaller.java:454); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:291); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. ----. ## Solutions. Any ideas ?. Maybe my command need to be modified to handle such a great number of samples ?. Maybe you should add some option to expand the python memory to allocate ? . ----. Thank you",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:61901,Modifiability,variab,variable,61901,est/sample-3788006936711929575536.tsv /tmp/tintest/sample-3794598448303416401276.tsv /tmp/tintest/sample-380910670101098136635.tsv /tmp/tintest/sample-3815864583095389374312.tsv /tmp/tintest/sample-3821063008346821202582.tsv /tmp/tintest/sample-3836550848258521825191.tsv /tmp/tintest/sample-3842488752532231097400.tsv /tmp/tintest/sample-3855124216409092357090.tsv /tmp/tintest/sample-3866989755460133829309.tsv ; Stdout: 10:58:52.820 INFO cohort_denoising_calling - Loading 387 read counts file(s)...; 11:01:01.618 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 11:02:11.422 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 11:04:53.672 ERROR theano.gof.cmodule - [Errno 12] Cannot allocate memory. Stderr: Problem occurred during compilation with the command line below:; /usr/bin/g++ -shared -g -O3 -fno-math-errno -Wno-unused-label -Wno-unused-variable -Wno-write-strings -fopenmp -march=knl -mmmx -mno-3dnow -msse -msse2 -msse3 -mssse3 -mno-sse4a -mcx16 -msahf -mmovbe -maes -mno-sha -mpclmul -mpopcnt -mabm -mno-lwp -mfma -mno-fma4 -mno-xop -mbmi -mbmi2 -mno-tbm -mavx -mavx2 -msse4.2 -msse4.1 -mlzcnt -mrtm -mhle -mrdrnd -mf16c -mfsgsbase -mrdseed -mprfchw -madx -mfxsr -mxsave -mxsaveopt -mavx512f -mno-avx512er -mavx512cd -mno-avx512pf -mno-prefetchwt1 -mclflushopt -mxsavec -mxsaves -mavx512dq -mavx512bw -mno-avx512vl -mno-avx512ifma -mno-avx512vbmi -mclwb -mno-mwaitx -mno-clzero -mpku --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cache-size=22528 -mtune=generic -DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -m64 -fPIC -I/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/numpy/core/include -I/home/tintest/miniconda2/envs/aurexome/include/python3.6m -I/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof -L/home/tintest/miniconda2/envs/aurexome/lib -fvisibility=hidden -o /home/tintest/.theano/compiledir_Linux-4.9--amd,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:685,Performance,Load,Loading,685,"## Bug Report. ### Affected tool(s) or class(es); GermlineCNVCaller. ### Affected version(s); - GATK4 4.0.5.1. ### Description ; I'm trying to do a germline CNV calling with 387 exomes samples (I know it's a lot). The CollectReadCounts and DetermineGermlineContigPloidy were successfull. But for the GermlineCNVCaller I got what I think is a Python ""cannot allocate memory"" error. I tried to specify to the JVM a max memory to allocate ``` --java-options ""-Xmx192G"" ``` , but no improvements. The machine I'm working on got 32 threads and 192 Gb RAM. #### Steps to reproduce; I guess try to do a CNV calling with a large cohort. #### Output; ```10:56:25.124 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/tintest/miniconda2/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:56:25.342 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.343 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.0.5.1; 10:56:25.344 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:56:25.345 INFO GermlineCNVCaller - Executing as tintest@dahu39 on Linux v4.9.0-6-amd64 amd64; 10:56:25.346 INFO GermlineCNVCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b15; 10:56:25.347 INFO GermlineCNVCaller - Start Date/Time: July 25, 2018 10:56:24 AM CEST; 10:56:25.348 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.349 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.350 INFO GermlineCNVCaller - HTSJDK Version: 2.15.1; 10:56:25.351 INFO GermlineCNVCaller - Picard Version: 2.18.2; 10:56:25.352 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:56:25.353 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:56:25.354 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:3330,Performance,perform,performed,3330,CS max retries/reopens: 20; 10:56:25.358 INFO GermlineCNVCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 10:56:25.360 WARN GermlineCNVCaller -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: GermlineCNVCaller is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 10:56:25.361 INFO GermlineCNVCaller - Initializing engine; 10:56:54.347 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 10:56:55.287 INFO GermlineCNVCaller - Retrieving intervals from first read-count file (hdf5/grexome0426.hdf5)...; 10:56:55.384 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 10:56:55.482 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 10:56:55.485 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 10:56:55.511 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0426.hdf5 (1 / 387); 10:56:55.812 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0342.hdf5 (2 / 387); 10:56:56.274 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0316.hdf5 (3 / 387); 10:56:56.635 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0338.hdf5 (4 / 387); 10:56:57.092 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0360.hdf5 (5 / 387); 10:56:57.728 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0384.hdf5 (6 / 387); 10:56:58.144 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0213.hdf5 (7 / 387); 10:56:58.681 INFO GermlineCNVCaller - Aggregating read-count,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:61415,Performance,Load,Loading,61415,0418.tsv /tmp/tintest/sample-3678073345547351139852.tsv /tmp/tintest/sample-3687670170611066239891.tsv /tmp/tintest/sample-3697896582170286914413.tsv /tmp/tintest/sample-3704126767513966037718.tsv /tmp/tintest/sample-3718480935718374256974.tsv /tmp/tintest/sample-3729118447183914284068.tsv /tmp/tintest/sample-3734484951576950052743.tsv /tmp/tintest/sample-3745007638909244994571.tsv /tmp/tintest/sample-3758817480300622528681.tsv /tmp/tintest/sample-3765561422653477541111.tsv /tmp/tintest/sample-377681127346074691924.tsv /tmp/tintest/sample-3788006936711929575536.tsv /tmp/tintest/sample-3794598448303416401276.tsv /tmp/tintest/sample-380910670101098136635.tsv /tmp/tintest/sample-3815864583095389374312.tsv /tmp/tintest/sample-3821063008346821202582.tsv /tmp/tintest/sample-3836550848258521825191.tsv /tmp/tintest/sample-3842488752532231097400.tsv /tmp/tintest/sample-3855124216409092357090.tsv /tmp/tintest/sample-3866989755460133829309.tsv ; Stdout: 10:58:52.820 INFO cohort_denoising_calling - Loading 387 read counts file(s)...; 11:01:01.618 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 11:02:11.422 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 11:04:53.672 ERROR theano.gof.cmodule - [Errno 12] Cannot allocate memory. Stderr: Problem occurred during compilation with the command line below:; /usr/bin/g++ -shared -g -O3 -fno-math-errno -Wno-unused-label -Wno-unused-variable -Wno-write-strings -fopenmp -march=knl -mmmx -mno-3dnow -msse -msse2 -msse3 -mssse3 -mno-sse4a -mcx16 -msahf -mmovbe -maes -mno-sha -mpclmul -mpopcnt -mabm -mno-lwp -mfma -mno-fma4 -mno-xop -mbmi -mbmi2 -mno-tbm -mavx -mavx2 -msse4.2 -msse4.1 -mlzcnt -mrtm -mhle -mrdrnd -mf16c -mfsgsbase -mrdseed -mprfchw -madx -mfxsr -mxsave -mxsaveopt -mavx512f -mno-avx512er -mavx512cd -mno-avx512pf -mno-prefetchwt1 -mclflushopt -mxsavec -mxsaves -mavx512dq -mavx512bw -mno-avx512vl -mno-avx512ifma -mno-avx512vbmi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:61497,Performance,Load,Loading,61497,tintest/sample-3697896582170286914413.tsv /tmp/tintest/sample-3704126767513966037718.tsv /tmp/tintest/sample-3718480935718374256974.tsv /tmp/tintest/sample-3729118447183914284068.tsv /tmp/tintest/sample-3734484951576950052743.tsv /tmp/tintest/sample-3745007638909244994571.tsv /tmp/tintest/sample-3758817480300622528681.tsv /tmp/tintest/sample-3765561422653477541111.tsv /tmp/tintest/sample-377681127346074691924.tsv /tmp/tintest/sample-3788006936711929575536.tsv /tmp/tintest/sample-3794598448303416401276.tsv /tmp/tintest/sample-380910670101098136635.tsv /tmp/tintest/sample-3815864583095389374312.tsv /tmp/tintest/sample-3821063008346821202582.tsv /tmp/tintest/sample-3836550848258521825191.tsv /tmp/tintest/sample-3842488752532231097400.tsv /tmp/tintest/sample-3855124216409092357090.tsv /tmp/tintest/sample-3866989755460133829309.tsv ; Stdout: 10:58:52.820 INFO cohort_denoising_calling - Loading 387 read counts file(s)...; 11:01:01.618 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 11:02:11.422 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 11:04:53.672 ERROR theano.gof.cmodule - [Errno 12] Cannot allocate memory. Stderr: Problem occurred during compilation with the command line below:; /usr/bin/g++ -shared -g -O3 -fno-math-errno -Wno-unused-label -Wno-unused-variable -Wno-write-strings -fopenmp -march=knl -mmmx -mno-3dnow -msse -msse2 -msse3 -mssse3 -mno-sse4a -mcx16 -msahf -mmovbe -maes -mno-sha -mpclmul -mpopcnt -mabm -mno-lwp -mfma -mno-fma4 -mno-xop -mbmi -mbmi2 -mno-tbm -mavx -mavx2 -msse4.2 -msse4.1 -mlzcnt -mrtm -mhle -mrdrnd -mf16c -mfsgsbase -mrdseed -mprfchw -madx -mfxsr -mxsave -mxsaveopt -mavx512f -mno-avx512er -mavx512cd -mno-avx512pf -mno-prefetchwt1 -mclflushopt -mxsavec -mxsaves -mavx512dq -mavx512bw -mno-avx512vl -mno-avx512ifma -mno-avx512vbmi -mclwb -mno-mwaitx -mno-clzero -mpku --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cach,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:62462,Performance,cache,cache-size,62462,"_calling - Loading 387 read counts file(s)...; 11:01:01.618 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 11:02:11.422 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 11:04:53.672 ERROR theano.gof.cmodule - [Errno 12] Cannot allocate memory. Stderr: Problem occurred during compilation with the command line below:; /usr/bin/g++ -shared -g -O3 -fno-math-errno -Wno-unused-label -Wno-unused-variable -Wno-write-strings -fopenmp -march=knl -mmmx -mno-3dnow -msse -msse2 -msse3 -mssse3 -mno-sse4a -mcx16 -msahf -mmovbe -maes -mno-sha -mpclmul -mpopcnt -mabm -mno-lwp -mfma -mno-fma4 -mno-xop -mbmi -mbmi2 -mno-tbm -mavx -mavx2 -msse4.2 -msse4.1 -mlzcnt -mrtm -mhle -mrdrnd -mf16c -mfsgsbase -mrdseed -mprfchw -madx -mfxsr -mxsave -mxsaveopt -mavx512f -mno-avx512er -mavx512cd -mno-avx512pf -mno-prefetchwt1 -mclflushopt -mxsavec -mxsaves -mavx512dq -mavx512bw -mno-avx512vl -mno-avx512ifma -mno-avx512vbmi -mclwb -mno-mwaitx -mno-clzero -mpku --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cache-size=22528 -mtune=generic -DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -m64 -fPIC -I/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/numpy/core/include -I/home/tintest/miniconda2/envs/aurexome/include/python3.6m -I/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof -L/home/tintest/miniconda2/envs/aurexome/lib -fvisibility=hidden -o /home/tintest/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.4--3.6.2-64/tmpueark7lw/m421cdb2b133a2578e9a2670dfbb5d33e.so /home/tintest/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.4--3.6.2-64/tmpueark7lw/mod.cpp -lpython3.6m; ERROR (theano.gof.cmodule): [Errno 12] Cannot allocate memory; Traceback (most recent call last):; File ""/tmp/tintest/cohort_denoising_calling.4390748645603329412.py"", line 143, in <module>; shared_workspace, initial_params_supplier); File ""/home/tintest/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:62487,Performance,cache,cache-line-size,62487,"_calling - Loading 387 read counts file(s)...; 11:01:01.618 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 11:02:11.422 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 11:04:53.672 ERROR theano.gof.cmodule - [Errno 12] Cannot allocate memory. Stderr: Problem occurred during compilation with the command line below:; /usr/bin/g++ -shared -g -O3 -fno-math-errno -Wno-unused-label -Wno-unused-variable -Wno-write-strings -fopenmp -march=knl -mmmx -mno-3dnow -msse -msse2 -msse3 -mssse3 -mno-sse4a -mcx16 -msahf -mmovbe -maes -mno-sha -mpclmul -mpopcnt -mabm -mno-lwp -mfma -mno-fma4 -mno-xop -mbmi -mbmi2 -mno-tbm -mavx -mavx2 -msse4.2 -msse4.1 -mlzcnt -mrtm -mhle -mrdrnd -mf16c -mfsgsbase -mrdseed -mprfchw -madx -mfxsr -mxsave -mxsaveopt -mavx512f -mno-avx512er -mavx512cd -mno-avx512pf -mno-prefetchwt1 -mclflushopt -mxsavec -mxsaves -mavx512dq -mavx512bw -mno-avx512vl -mno-avx512ifma -mno-avx512vbmi -mclwb -mno-mwaitx -mno-clzero -mpku --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cache-size=22528 -mtune=generic -DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -m64 -fPIC -I/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/numpy/core/include -I/home/tintest/miniconda2/envs/aurexome/include/python3.6m -I/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof -L/home/tintest/miniconda2/envs/aurexome/lib -fvisibility=hidden -o /home/tintest/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.4--3.6.2-64/tmpueark7lw/m421cdb2b133a2578e9a2670dfbb5d33e.so /home/tintest/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.4--3.6.2-64/tmpueark7lw/mod.cpp -lpython3.6m; ERROR (theano.gof.cmodule): [Errno 12] Cannot allocate memory; Traceback (most recent call last):; File ""/tmp/tintest/cohort_denoising_calling.4390748645603329412.py"", line 143, in <module>; shared_workspace, initial_params_supplier); File ""/home/tintest/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:62517,Performance,cache,cache-size,62517,"_calling - Loading 387 read counts file(s)...; 11:01:01.618 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 11:02:11.422 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 11:04:53.672 ERROR theano.gof.cmodule - [Errno 12] Cannot allocate memory. Stderr: Problem occurred during compilation with the command line below:; /usr/bin/g++ -shared -g -O3 -fno-math-errno -Wno-unused-label -Wno-unused-variable -Wno-write-strings -fopenmp -march=knl -mmmx -mno-3dnow -msse -msse2 -msse3 -mssse3 -mno-sse4a -mcx16 -msahf -mmovbe -maes -mno-sha -mpclmul -mpopcnt -mabm -mno-lwp -mfma -mno-fma4 -mno-xop -mbmi -mbmi2 -mno-tbm -mavx -mavx2 -msse4.2 -msse4.1 -mlzcnt -mrtm -mhle -mrdrnd -mf16c -mfsgsbase -mrdseed -mprfchw -madx -mfxsr -mxsave -mxsaveopt -mavx512f -mno-avx512er -mavx512cd -mno-avx512pf -mno-prefetchwt1 -mclflushopt -mxsavec -mxsaves -mavx512dq -mavx512bw -mno-avx512vl -mno-avx512ifma -mno-avx512vbmi -mclwb -mno-mwaitx -mno-clzero -mpku --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cache-size=22528 -mtune=generic -DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -m64 -fPIC -I/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/numpy/core/include -I/home/tintest/miniconda2/envs/aurexome/include/python3.6m -I/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof -L/home/tintest/miniconda2/envs/aurexome/lib -fvisibility=hidden -o /home/tintest/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.4--3.6.2-64/tmpueark7lw/m421cdb2b133a2578e9a2670dfbb5d33e.so /home/tintest/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.4--3.6.2-64/tmpueark7lw/mod.cpp -lpython3.6m; ERROR (theano.gof.cmodule): [Errno 12] Cannot allocate memory; Traceback (most recent call last):; File ""/tmp/tintest/cohort_denoising_calling.4390748645603329412.py"", line 143, in <module>; shared_workspace, initial_params_supplier); File ""/home/tintest/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:3460,Security,Validat,Validating,3460,_nio_fixes; 10:56:25.360 WARN GermlineCNVCaller -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: GermlineCNVCaller is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 10:56:25.361 INFO GermlineCNVCaller - Initializing engine; 10:56:54.347 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 10:56:55.287 INFO GermlineCNVCaller - Retrieving intervals from first read-count file (hdf5/grexome0426.hdf5)...; 10:56:55.384 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 10:56:55.482 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 10:56:55.485 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 10:56:55.511 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0426.hdf5 (1 / 387); 10:56:55.812 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0342.hdf5 (2 / 387); 10:56:56.274 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0316.hdf5 (3 / 387); 10:56:56.635 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0338.hdf5 (4 / 387); 10:56:57.092 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0360.hdf5 (5 / 387); 10:56:57.728 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0384.hdf5 (6 / 387); 10:56:58.144 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0213.hdf5 (7 / 387); 10:56:58.681 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0347.hdf5 (8 / 387); 10:56:59.192 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0125.hdf5 (9 / 387); 10:56:59.643 INFO GermlineCNVCaller - Aggregating read-count ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:2902,Testability,log,logger,2902,Caller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:56:25.354 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:56:25.355 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:56:25.356 INFO GermlineCNVCaller - Deflater: IntelDeflater; 10:56:25.357 INFO GermlineCNVCaller - Inflater: IntelInflater; 10:56:25.358 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 10:56:25.358 INFO GermlineCNVCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 10:56:25.360 WARN GermlineCNVCaller -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: GermlineCNVCaller is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 10:56:25.361 INFO GermlineCNVCaller - Initializing engine; 10:56:54.347 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 10:56:55.287 INFO GermlineCNVCaller - Retrieving intervals from first read-count file (hdf5/grexome0426.hdf5)...; 10:56:55.384 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 10:56:55.482 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 10:56:55.485 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 10:56:55.511 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0426.hdf5 (1 / 387); 10:56:55.812 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0342.hdf5 (2 / 387); 10:56:56.274 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0316.hdf5 (3 / 387); 10:56:56.635 INFO GermlineCNVCaller - ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:3028,Testability,log,logging,3028,:56:25.355 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:56:25.356 INFO GermlineCNVCaller - Deflater: IntelDeflater; 10:56:25.357 INFO GermlineCNVCaller - Inflater: IntelInflater; 10:56:25.358 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 10:56:25.358 INFO GermlineCNVCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 10:56:25.360 WARN GermlineCNVCaller -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: GermlineCNVCaller is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 10:56:25.361 INFO GermlineCNVCaller - Initializing engine; 10:56:54.347 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 10:56:55.287 INFO GermlineCNVCaller - Retrieving intervals from first read-count file (hdf5/grexome0426.hdf5)...; 10:56:55.384 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 10:56:55.482 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 10:56:55.485 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 10:56:55.511 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0426.hdf5 (1 / 387); 10:56:55.812 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0342.hdf5 (2 / 387); 10:56:56.274 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0316.hdf5 (3 / 387); 10:56:56.635 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0338.hdf5 (4 / 387); 10:56:57.092 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0360.hdf5 (5 / 387); 1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:64423,Testability,log,logp,64423,"gcnvkernel/tasks/task_cohort_denoising_calling.py"", line 140, in __init__; denoising_model = DenoisingModel(denoising_config, shared_workspace, initial_param_supplier); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/model.py"", line 197, in __call__; instance.__init__(*args, **kwargs); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 851, in __init__; observed=shared_workspace.n_st); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/distributions/distribution.py"", line 39, in __new__; return model.Var(name, dist, data, total_size); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/model.py"", line 545, in Var; total_size=total_size, model=self); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/model.py"", line 970, in __init__; self.logp_elemwiset = distribution.logp(data); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 844, in _copy_number_emission_logp; return pm.math.logsumexp(shared_workspace.log_q_c_stc + _log_copy_number_emission_stc, axis=2); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/math.py"", line 32, in logsumexp; return tt.log(tt.sum(tt.exp(x - x_max), axis=axis, keepdims=True)) + x_max; File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof/op.py"", line 670, in __call__; no_recycling=[]); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof/op.py"", line 935, in make_thunk; no_recycling); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof/op.py"", line 839, in make_c_thunk; output_storage=node_output_storage); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof/cc.py"", line 1190, in make_thunk; keep_lock=keep_lock); File ""/home/tinte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:64611,Testability,log,logsumexp,64611,"; File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/model.py"", line 197, in __call__; instance.__init__(*args, **kwargs); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 851, in __init__; observed=shared_workspace.n_st); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/distributions/distribution.py"", line 39, in __new__; return model.Var(name, dist, data, total_size); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/model.py"", line 545, in Var; total_size=total_size, model=self); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/model.py"", line 970, in __init__; self.logp_elemwiset = distribution.logp(data); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 844, in _copy_number_emission_logp; return pm.math.logsumexp(shared_workspace.log_q_c_stc + _log_copy_number_emission_stc, axis=2); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/math.py"", line 32, in logsumexp; return tt.log(tt.sum(tt.exp(x - x_max), axis=axis, keepdims=True)) + x_max; File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof/op.py"", line 670, in __call__; no_recycling=[]); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof/op.py"", line 935, in make_thunk; no_recycling); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof/op.py"", line 839, in make_c_thunk; output_storage=node_output_storage); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof/cc.py"", line 1190, in make_thunk; keep_lock=keep_lock); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof/cc.py"", line 1131, in __compile__; keep_lock=keep_lock); File ""/home/tintest/miniconda2/envs/aurexom",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:64793,Testability,log,logsumexp,64793,"miniconda2/envs/aurexome/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 851, in __init__; observed=shared_workspace.n_st); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/distributions/distribution.py"", line 39, in __new__; return model.Var(name, dist, data, total_size); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/model.py"", line 545, in Var; total_size=total_size, model=self); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/model.py"", line 970, in __init__; self.logp_elemwiset = distribution.logp(data); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 844, in _copy_number_emission_logp; return pm.math.logsumexp(shared_workspace.log_q_c_stc + _log_copy_number_emission_stc, axis=2); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/math.py"", line 32, in logsumexp; return tt.log(tt.sum(tt.exp(x - x_max), axis=axis, keepdims=True)) + x_max; File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof/op.py"", line 670, in __call__; no_recycling=[]); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof/op.py"", line 935, in make_thunk; no_recycling); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof/op.py"", line 839, in make_c_thunk; output_storage=node_output_storage); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof/cc.py"", line 1190, in make_thunk; keep_lock=keep_lock); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof/cc.py"", line 1131, in __compile__; keep_lock=keep_lock); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof/cc.py"", line 1586, in cthunk_factory; key=key, lnk=self, keep_lock=keep_lock); File ""/home/tintest/miniconda2/envs/aurexome/lib/p",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/issues/5053:64814,Testability,log,log,64814,"me/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 851, in __init__; observed=shared_workspace.n_st); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/distributions/distribution.py"", line 39, in __new__; return model.Var(name, dist, data, total_size); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/model.py"", line 545, in Var; total_size=total_size, model=self); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/model.py"", line 970, in __init__; self.logp_elemwiset = distribution.logp(data); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 844, in _copy_number_emission_logp; return pm.math.logsumexp(shared_workspace.log_q_c_stc + _log_copy_number_emission_stc, axis=2); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/math.py"", line 32, in logsumexp; return tt.log(tt.sum(tt.exp(x - x_max), axis=axis, keepdims=True)) + x_max; File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof/op.py"", line 670, in __call__; no_recycling=[]); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof/op.py"", line 935, in make_thunk; no_recycling); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof/op.py"", line 839, in make_c_thunk; output_storage=node_output_storage); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof/cc.py"", line 1190, in make_thunk; keep_lock=keep_lock); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof/cc.py"", line 1131, in __compile__; keep_lock=keep_lock); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof/cc.py"", line 1586, in cthunk_factory; key=key, lnk=self, keep_lock=keep_lock); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053
https://github.com/broadinstitute/gatk/pull/5057:90,Deployability,release,release,90,Closes #5060. @meganshand This fixes your bug. Do you have time to review before Monday's release?. @takutosato @LeeTL1220 It improves sensitivity and specificity. @ldgauthier This probably affects HaplotypeCaller as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5057
https://github.com/broadinstitute/gatk/issues/5059:51,Security,Validat,ValidateBasicSomaticShortMutations,51,"## Bug Report. ### Tool(s) or class(es) involved; `ValidateBasicSomaticShortMutations`. ### Description; The first fix should simply count the number of supporting alt reads in the normal. For now, if the variant has more than 2 reads supporting the alt in the normal, we will flag as not validating. We may change the strategy later to account for possible tumor-in-normal contamination.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5059
https://github.com/broadinstitute/gatk/issues/5059:289,Security,validat,validating,289,"## Bug Report. ### Tool(s) or class(es) involved; `ValidateBasicSomaticShortMutations`. ### Description; The first fix should simply count the number of supporting alt reads in the normal. For now, if the variant has more than 2 reads supporting the alt in the normal, we will flag as not validating. We may change the strategy later to account for possible tumor-in-normal contamination.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5059
https://github.com/broadinstitute/gatk/issues/5059:126,Usability,simpl,simply,126,"## Bug Report. ### Tool(s) or class(es) involved; `ValidateBasicSomaticShortMutations`. ### Description; The first fix should simply count the number of supporting alt reads in the normal. For now, if the variant has more than 2 reads supporting the alt in the normal, we will flag as not validating. We may change the strategy later to account for possible tumor-in-normal contamination.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5059
https://github.com/broadinstitute/gatk/issues/5060:1082,Security,validat,validation,1082,"Suppose the reference haplotype is; TAAGC. . . . TAAGG. . . . and an alt haplotype (SNV at the last G shown) is; TAAGC. . . TAAG**C**. . . Suppose further that we have a read ending in the first TAAGC that has been hard-clipped (to fit the assembly region) to just a 5-base TAAGC stub. Pair-HMM is fully Bayesian and computes the total likelihood of *all* possible alignments of a read to each haplotype. This gives the alt haplotype a factor of 2 advantage because TAAGC matches it in two locations, so there are two perfectly matching alignments instead of one. In log 10 space this is log_10(2) = 0.301, which is greater than our 0.2 threshold for a likelihood to be considered informative. Therefore, by clipping the read and losing the information of its first 96 bases, we end up considering it informative for the wrong haplotype. This can lead to false positives. It *also* causes false negatives because sometimes a read stub from the normal sample get misaligned to the alt haplotype, triggering the normal artifact filter. It also causes problems in our bamout-based MC3 validation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5060
https://github.com/broadinstitute/gatk/issues/5060:280,Testability,stub,stub,280,"Suppose the reference haplotype is; TAAGC. . . . TAAGG. . . . and an alt haplotype (SNV at the last G shown) is; TAAGC. . . TAAG**C**. . . Suppose further that we have a read ending in the first TAAGC that has been hard-clipped (to fit the assembly region) to just a 5-base TAAGC stub. Pair-HMM is fully Bayesian and computes the total likelihood of *all* possible alignments of a read to each haplotype. This gives the alt haplotype a factor of 2 advantage because TAAGC matches it in two locations, so there are two perfectly matching alignments instead of one. In log 10 space this is log_10(2) = 0.301, which is greater than our 0.2 threshold for a likelihood to be considered informative. Therefore, by clipping the read and losing the information of its first 96 bases, we end up considering it informative for the wrong haplotype. This can lead to false positives. It *also* causes false negatives because sometimes a read stub from the normal sample get misaligned to the alt haplotype, triggering the normal artifact filter. It also causes problems in our bamout-based MC3 validation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5060
https://github.com/broadinstitute/gatk/issues/5060:567,Testability,log,log,567,"Suppose the reference haplotype is; TAAGC. . . . TAAGG. . . . and an alt haplotype (SNV at the last G shown) is; TAAGC. . . TAAG**C**. . . Suppose further that we have a read ending in the first TAAGC that has been hard-clipped (to fit the assembly region) to just a 5-base TAAGC stub. Pair-HMM is fully Bayesian and computes the total likelihood of *all* possible alignments of a read to each haplotype. This gives the alt haplotype a factor of 2 advantage because TAAGC matches it in two locations, so there are two perfectly matching alignments instead of one. In log 10 space this is log_10(2) = 0.301, which is greater than our 0.2 threshold for a likelihood to be considered informative. Therefore, by clipping the read and losing the information of its first 96 bases, we end up considering it informative for the wrong haplotype. This can lead to false positives. It *also* causes false negatives because sometimes a read stub from the normal sample get misaligned to the alt haplotype, triggering the normal artifact filter. It also causes problems in our bamout-based MC3 validation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5060
https://github.com/broadinstitute/gatk/issues/5060:930,Testability,stub,stub,930,"Suppose the reference haplotype is; TAAGC. . . . TAAGG. . . . and an alt haplotype (SNV at the last G shown) is; TAAGC. . . TAAG**C**. . . Suppose further that we have a read ending in the first TAAGC that has been hard-clipped (to fit the assembly region) to just a 5-base TAAGC stub. Pair-HMM is fully Bayesian and computes the total likelihood of *all* possible alignments of a read to each haplotype. This gives the alt haplotype a factor of 2 advantage because TAAGC matches it in two locations, so there are two perfectly matching alignments instead of one. In log 10 space this is log_10(2) = 0.301, which is greater than our 0.2 threshold for a likelihood to be considered informative. Therefore, by clipping the read and losing the information of its first 96 bases, we end up considering it informative for the wrong haplotype. This can lead to false positives. It *also* causes false negatives because sometimes a read stub from the normal sample get misaligned to the alt haplotype, triggering the normal artifact filter. It also causes problems in our bamout-based MC3 validation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5060
https://github.com/broadinstitute/gatk/issues/5061:558,Security,validat,validation,558,"## Bug Report. ### Affected tool(s) or class(es); GATKProtectedVariantContextUtils.chooseAlleleForRead(...). ### Affected version(s); Latest master branch as of [July 27, 2018]. ### Description ; This method has no facility for recognizing equivalent alleles in a read. For example:. Let's say we have a pileup with a single base insertion (of 'T') after an 'A' in a poly-T.; In IGV, the reads will all show the insertion right after the A.; However, if the caller said ref alt was AT --> ATT, then `chooseAlleleForRead` will miss the supporting alts in the validation normal, since it needed A --> AT. However, AT-->ATT and A --> AT are equivalent. See ValidateBasicSomaticShortMutationsIntegrationTest line ~151 for the corresponding TODO and a test case. #### Steps to reproduce; Leverage the existing test (`ValidateBasicSomaticShortMutationsIntegrationTest`) and change the gtNumAltReadsInValidationNormal to 1, instead of zero. #### Expected behavior; The test should pass. #### Actual behavior; The test will fail expecting zero.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5061
https://github.com/broadinstitute/gatk/issues/5061:654,Security,Validat,ValidateBasicSomaticShortMutationsIntegrationTest,654,"## Bug Report. ### Affected tool(s) or class(es); GATKProtectedVariantContextUtils.chooseAlleleForRead(...). ### Affected version(s); Latest master branch as of [July 27, 2018]. ### Description ; This method has no facility for recognizing equivalent alleles in a read. For example:. Let's say we have a pileup with a single base insertion (of 'T') after an 'A' in a poly-T.; In IGV, the reads will all show the insertion right after the A.; However, if the caller said ref alt was AT --> ATT, then `chooseAlleleForRead` will miss the supporting alts in the validation normal, since it needed A --> AT. However, AT-->ATT and A --> AT are equivalent. See ValidateBasicSomaticShortMutationsIntegrationTest line ~151 for the corresponding TODO and a test case. #### Steps to reproduce; Leverage the existing test (`ValidateBasicSomaticShortMutationsIntegrationTest`) and change the gtNumAltReadsInValidationNormal to 1, instead of zero. #### Expected behavior; The test should pass. #### Actual behavior; The test will fail expecting zero.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5061
https://github.com/broadinstitute/gatk/issues/5061:812,Security,Validat,ValidateBasicSomaticShortMutationsIntegrationTest,812,"## Bug Report. ### Affected tool(s) or class(es); GATKProtectedVariantContextUtils.chooseAlleleForRead(...). ### Affected version(s); Latest master branch as of [July 27, 2018]. ### Description ; This method has no facility for recognizing equivalent alleles in a read. For example:. Let's say we have a pileup with a single base insertion (of 'T') after an 'A' in a poly-T.; In IGV, the reads will all show the insertion right after the A.; However, if the caller said ref alt was AT --> ATT, then `chooseAlleleForRead` will miss the supporting alts in the validation normal, since it needed A --> AT. However, AT-->ATT and A --> AT are equivalent. See ValidateBasicSomaticShortMutationsIntegrationTest line ~151 for the corresponding TODO and a test case. #### Steps to reproduce; Leverage the existing test (`ValidateBasicSomaticShortMutationsIntegrationTest`) and change the gtNumAltReadsInValidationNormal to 1, instead of zero. #### Expected behavior; The test should pass. #### Actual behavior; The test will fail expecting zero.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5061
https://github.com/broadinstitute/gatk/issues/5061:747,Testability,test,test,747,"## Bug Report. ### Affected tool(s) or class(es); GATKProtectedVariantContextUtils.chooseAlleleForRead(...). ### Affected version(s); Latest master branch as of [July 27, 2018]. ### Description ; This method has no facility for recognizing equivalent alleles in a read. For example:. Let's say we have a pileup with a single base insertion (of 'T') after an 'A' in a poly-T.; In IGV, the reads will all show the insertion right after the A.; However, if the caller said ref alt was AT --> ATT, then `chooseAlleleForRead` will miss the supporting alts in the validation normal, since it needed A --> AT. However, AT-->ATT and A --> AT are equivalent. See ValidateBasicSomaticShortMutationsIntegrationTest line ~151 for the corresponding TODO and a test case. #### Steps to reproduce; Leverage the existing test (`ValidateBasicSomaticShortMutationsIntegrationTest`) and change the gtNumAltReadsInValidationNormal to 1, instead of zero. #### Expected behavior; The test should pass. #### Actual behavior; The test will fail expecting zero.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5061
https://github.com/broadinstitute/gatk/issues/5061:805,Testability,test,test,805,"## Bug Report. ### Affected tool(s) or class(es); GATKProtectedVariantContextUtils.chooseAlleleForRead(...). ### Affected version(s); Latest master branch as of [July 27, 2018]. ### Description ; This method has no facility for recognizing equivalent alleles in a read. For example:. Let's say we have a pileup with a single base insertion (of 'T') after an 'A' in a poly-T.; In IGV, the reads will all show the insertion right after the A.; However, if the caller said ref alt was AT --> ATT, then `chooseAlleleForRead` will miss the supporting alts in the validation normal, since it needed A --> AT. However, AT-->ATT and A --> AT are equivalent. See ValidateBasicSomaticShortMutationsIntegrationTest line ~151 for the corresponding TODO and a test case. #### Steps to reproduce; Leverage the existing test (`ValidateBasicSomaticShortMutationsIntegrationTest`) and change the gtNumAltReadsInValidationNormal to 1, instead of zero. #### Expected behavior; The test should pass. #### Actual behavior; The test will fail expecting zero.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5061
https://github.com/broadinstitute/gatk/issues/5061:962,Testability,test,test,962,"## Bug Report. ### Affected tool(s) or class(es); GATKProtectedVariantContextUtils.chooseAlleleForRead(...). ### Affected version(s); Latest master branch as of [July 27, 2018]. ### Description ; This method has no facility for recognizing equivalent alleles in a read. For example:. Let's say we have a pileup with a single base insertion (of 'T') after an 'A' in a poly-T.; In IGV, the reads will all show the insertion right after the A.; However, if the caller said ref alt was AT --> ATT, then `chooseAlleleForRead` will miss the supporting alts in the validation normal, since it needed A --> AT. However, AT-->ATT and A --> AT are equivalent. See ValidateBasicSomaticShortMutationsIntegrationTest line ~151 for the corresponding TODO and a test case. #### Steps to reproduce; Leverage the existing test (`ValidateBasicSomaticShortMutationsIntegrationTest`) and change the gtNumAltReadsInValidationNormal to 1, instead of zero. #### Expected behavior; The test should pass. #### Actual behavior; The test will fail expecting zero.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5061
https://github.com/broadinstitute/gatk/issues/5061:1006,Testability,test,test,1006,"## Bug Report. ### Affected tool(s) or class(es); GATKProtectedVariantContextUtils.chooseAlleleForRead(...). ### Affected version(s); Latest master branch as of [July 27, 2018]. ### Description ; This method has no facility for recognizing equivalent alleles in a read. For example:. Let's say we have a pileup with a single base insertion (of 'T') after an 'A' in a poly-T.; In IGV, the reads will all show the insertion right after the A.; However, if the caller said ref alt was AT --> ATT, then `chooseAlleleForRead` will miss the supporting alts in the validation normal, since it needed A --> AT. However, AT-->ATT and A --> AT are equivalent. See ValidateBasicSomaticShortMutationsIntegrationTest line ~151 for the corresponding TODO and a test case. #### Steps to reproduce; Leverage the existing test (`ValidateBasicSomaticShortMutationsIntegrationTest`) and change the gtNumAltReadsInValidationNormal to 1, instead of zero. #### Expected behavior; The test should pass. #### Actual behavior; The test will fail expecting zero.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5061
https://github.com/broadinstitute/gatk/pull/5062:3,Security,Validat,ValidateBasicSomaticShortMutations,3,- `ValidateBasicSomaticShortMutations` will now provide a count of supporting alt alleles in the validation normals. This value does not affect the validation status. Closes #5059 ; - Note that there are indels that will not be caught by this functionality. See #5061,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5062
https://github.com/broadinstitute/gatk/pull/5062:97,Security,validat,validation,97,- `ValidateBasicSomaticShortMutations` will now provide a count of supporting alt alleles in the validation normals. This value does not affect the validation status. Closes #5059 ; - Note that there are indels that will not be caught by this functionality. See #5061,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5062
https://github.com/broadinstitute/gatk/pull/5062:148,Security,validat,validation,148,- `ValidateBasicSomaticShortMutations` will now provide a count of supporting alt alleles in the validation normals. This value does not affect the validation status. Closes #5059 ; - Note that there are indels that will not be caught by this functionality. See #5061,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5062
https://github.com/broadinstitute/gatk/issues/5064:53,Availability,error,error,53,Using 4.0.4.0 and got the following rather unhelpful error from a ImportGenomicsDB. I suspect it might be a variant in one of my VCFs being imported causing an issue but the error message doesn't even hint at which file it was working on. ```; ~/gatk-4.0.4.0/gatk GenomicsDBImport -R hs37d5.fa --sample-name-map gvcfs.samplemap --genomicsdb-workspace-path outputsb.workspace -L 6:29691241-33054015 --batch-size 50 --consolidate true; Using GATK jar /home/cloud-user/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/cloud-user/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar GenomicsDBImport -R hs37d5.fa --sample-name-map gvcfs.samplemap --genomicsdb-workspace-path outputsb.workspace -L 6:29691241-33054015 --batch-size 50 --consolidate true; 17:01:47.406 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/cloud-user/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:01:47.522 INFO GenomicsDBImport - ------------------------------------------------------------; 17:01:47.522 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.4.0; 17:01:47.522 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:01:47.522 INFO GenomicsDBImport - Executing as cloud-user@lustre-assembly-1 on Linux v3.10.0-862.3.3.el7.x86_64 amd64; 17:01:47.522 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-b10; 17:01:47.523 INFO GenomicsDBImport - Start Date/Time: 17 July 2018 17:01:47 BST; 17:01:47.523 INFO GenomicsDBImport - ------------------------------------------------------------; 17:01:47.523 INFO GenomicsDBImport - ------------------------------------------------------------; 17:01:47.523 INFO GenomicsDBImport - HTSJDK Version: 2.14.3; 17:01:47.523 INFO GenomicsDBImport - Picard,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5064
https://github.com/broadinstitute/gatk/issues/5064:174,Availability,error,error,174,Using 4.0.4.0 and got the following rather unhelpful error from a ImportGenomicsDB. I suspect it might be a variant in one of my VCFs being imported causing an issue but the error message doesn't even hint at which file it was working on. ```; ~/gatk-4.0.4.0/gatk GenomicsDBImport -R hs37d5.fa --sample-name-map gvcfs.samplemap --genomicsdb-workspace-path outputsb.workspace -L 6:29691241-33054015 --batch-size 50 --consolidate true; Using GATK jar /home/cloud-user/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/cloud-user/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar GenomicsDBImport -R hs37d5.fa --sample-name-map gvcfs.samplemap --genomicsdb-workspace-path outputsb.workspace -L 6:29691241-33054015 --batch-size 50 --consolidate true; 17:01:47.406 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/cloud-user/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:01:47.522 INFO GenomicsDBImport - ------------------------------------------------------------; 17:01:47.522 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.4.0; 17:01:47.522 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:01:47.522 INFO GenomicsDBImport - Executing as cloud-user@lustre-assembly-1 on Linux v3.10.0-862.3.3.el7.x86_64 amd64; 17:01:47.522 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-b10; 17:01:47.523 INFO GenomicsDBImport - Start Date/Time: 17 July 2018 17:01:47 BST; 17:01:47.523 INFO GenomicsDBImport - ------------------------------------------------------------; 17:01:47.523 INFO GenomicsDBImport - ------------------------------------------------------------; 17:01:47.523 INFO GenomicsDBImport - HTSJDK Version: 2.14.3; 17:01:47.523 INFO GenomicsDBImport - Picard,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5064
https://github.com/broadinstitute/gatk/issues/5064:2625,Deployability,patch,patch,2625,NFO GenomicsDBImport - Start Date/Time: 17 July 2018 17:01:47 BST; 17:01:47.523 INFO GenomicsDBImport - ------------------------------------------------------------; 17:01:47.523 INFO GenomicsDBImport - ------------------------------------------------------------; 17:01:47.523 INFO GenomicsDBImport - HTSJDK Version: 2.14.3; 17:01:47.523 INFO GenomicsDBImport - Picard Version: 2.18.2; 17:01:47.523 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:01:47.523 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:01:47.523 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:01:47.523 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:01:47.523 INFO GenomicsDBImport - Deflater: IntelDeflater; 17:01:47.523 INFO GenomicsDBImport - Inflater: IntelInflater; 17:01:47.523 INFO GenomicsDBImport - GCS max retries/reopens: 20; 17:01:47.523 INFO GenomicsDBImport - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:01:47.524 INFO GenomicsDBImport - Initializing engine; 17:01:47.959 INFO IntervalArgumentCollection - Processing 3362775 bp from intervals; 17:01:47.984 INFO GenomicsDBImport - Done initializing engine; Created workspace /mnt/chla/outputsb.workspace; 17:01:48.120 INFO GenomicsDBImport - Vid Map JSON file will be written to outputsb.workspace/vidmap.json; 17:01:48.120 INFO GenomicsDBImport - Callset Map JSON file will be written to outputsb.workspace/callset.json; 17:01:48.120 INFO GenomicsDBImport - Complete VCF Header will be written to outputsb.workspace/vcfheader.vcf; 17:01:48.120 INFO GenomicsDBImport - Importing to array - outputsb.workspace/genomicsdb_array; 17:01:48.136 INFO ProgressMeter - Starting traversal; 17:01:48.136 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 17:01:48.250 INFO GenomicsDBImport - Importing batch 1 with ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5064
https://github.com/broadinstitute/gatk/issues/5064:180,Integrability,message,message,180,Using 4.0.4.0 and got the following rather unhelpful error from a ImportGenomicsDB. I suspect it might be a variant in one of my VCFs being imported causing an issue but the error message doesn't even hint at which file it was working on. ```; ~/gatk-4.0.4.0/gatk GenomicsDBImport -R hs37d5.fa --sample-name-map gvcfs.samplemap --genomicsdb-workspace-path outputsb.workspace -L 6:29691241-33054015 --batch-size 50 --consolidate true; Using GATK jar /home/cloud-user/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/cloud-user/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar GenomicsDBImport -R hs37d5.fa --sample-name-map gvcfs.samplemap --genomicsdb-workspace-path outputsb.workspace -L 6:29691241-33054015 --batch-size 50 --consolidate true; 17:01:47.406 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/cloud-user/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:01:47.522 INFO GenomicsDBImport - ------------------------------------------------------------; 17:01:47.522 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.4.0; 17:01:47.522 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:01:47.522 INFO GenomicsDBImport - Executing as cloud-user@lustre-assembly-1 on Linux v3.10.0-862.3.3.el7.x86_64 amd64; 17:01:47.522 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-b10; 17:01:47.523 INFO GenomicsDBImport - Start Date/Time: 17 July 2018 17:01:47 BST; 17:01:47.523 INFO GenomicsDBImport - ------------------------------------------------------------; 17:01:47.523 INFO GenomicsDBImport - ------------------------------------------------------------; 17:01:47.523 INFO GenomicsDBImport - HTSJDK Version: 2.14.3; 17:01:47.523 INFO GenomicsDBImport - Picard,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5064
https://github.com/broadinstitute/gatk/issues/5064:957,Performance,Load,Loading,957,Using 4.0.4.0 and got the following rather unhelpful error from a ImportGenomicsDB. I suspect it might be a variant in one of my VCFs being imported causing an issue but the error message doesn't even hint at which file it was working on. ```; ~/gatk-4.0.4.0/gatk GenomicsDBImport -R hs37d5.fa --sample-name-map gvcfs.samplemap --genomicsdb-workspace-path outputsb.workspace -L 6:29691241-33054015 --batch-size 50 --consolidate true; Using GATK jar /home/cloud-user/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/cloud-user/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar GenomicsDBImport -R hs37d5.fa --sample-name-map gvcfs.samplemap --genomicsdb-workspace-path outputsb.workspace -L 6:29691241-33054015 --batch-size 50 --consolidate true; 17:01:47.406 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/cloud-user/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:01:47.522 INFO GenomicsDBImport - ------------------------------------------------------------; 17:01:47.522 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.4.0; 17:01:47.522 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:01:47.522 INFO GenomicsDBImport - Executing as cloud-user@lustre-assembly-1 on Linux v3.10.0-862.3.3.el7.x86_64 amd64; 17:01:47.522 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-b10; 17:01:47.523 INFO GenomicsDBImport - Start Date/Time: 17 July 2018 17:01:47 BST; 17:01:47.523 INFO GenomicsDBImport - ------------------------------------------------------------; 17:01:47.523 INFO GenomicsDBImport - ------------------------------------------------------------; 17:01:47.523 INFO GenomicsDBImport - HTSJDK Version: 2.14.3; 17:01:47.523 INFO GenomicsDBImport - Picard,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5064
https://github.com/broadinstitute/gatk/issues/5065:136,Performance,race condition,race condition,136,"The StreamingPythonExecutor unit test `testStderrOutput` fails intermittently on Travis. We should try flushing stderr to eliminate the race condition, or else just remove the test.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5065
https://github.com/broadinstitute/gatk/issues/5065:33,Testability,test,test,33,"The StreamingPythonExecutor unit test `testStderrOutput` fails intermittently on Travis. We should try flushing stderr to eliminate the race condition, or else just remove the test.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5065
https://github.com/broadinstitute/gatk/issues/5065:39,Testability,test,testStderrOutput,39,"The StreamingPythonExecutor unit test `testStderrOutput` fails intermittently on Travis. We should try flushing stderr to eliminate the race condition, or else just remove the test.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5065
https://github.com/broadinstitute/gatk/issues/5065:176,Testability,test,test,176,"The StreamingPythonExecutor unit test `testStderrOutput` fails intermittently on Travis. We should try flushing stderr to eliminate the race condition, or else just remove the test.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5065
https://github.com/broadinstitute/gatk/issues/5066:129,Availability,avail,available,129,"When a large number of intervals is specified at import time, a large number of arrays are created, which can lead to exhausting available open file handles. In addition, my informal tests indicate that querying a workspace created from an import that used a large number of intervals is pretty slow. @kgururaj suggests we might want issue a warning at a threshold of 100 intervals. See discussion in https://github.com/broadinstitute/gatk/pull/4997.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5066
https://github.com/broadinstitute/gatk/issues/5066:183,Testability,test,tests,183,"When a large number of intervals is specified at import time, a large number of arrays are created, which can lead to exhausting available open file handles. In addition, my informal tests indicate that querying a workspace created from an import that used a large number of intervals is pretty slow. @kgururaj suggests we might want issue a warning at a threshold of 100 intervals. See discussion in https://github.com/broadinstitute/gatk/pull/4997.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5066
https://github.com/broadinstitute/gatk/pull/5071:215,Integrability,synchroniz,synchronization,215,"Fixes #4661. I managed to run `genome_reads-pipeline_hdfs.sh` with this change, whereas before it was failing (see details in #4661). No unit test since it is very difficult to write a test for the effect of adding synchronization.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071
https://github.com/broadinstitute/gatk/pull/5071:142,Testability,test,test,142,"Fixes #4661. I managed to run `genome_reads-pipeline_hdfs.sh` with this change, whereas before it was failing (see details in #4661). No unit test since it is very difficult to write a test for the effect of adding synchronization.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071
https://github.com/broadinstitute/gatk/pull/5071:185,Testability,test,test,185,"Fixes #4661. I managed to run `genome_reads-pipeline_hdfs.sh` with this change, whereas before it was failing (see details in #4661). No unit test since it is very difficult to write a test for the effect of adding synchronization.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071
https://github.com/broadinstitute/gatk/issues/5072:271,Deployability,update,updated,271,"## Documentation request. ### Tool(s) or class(es) involved. LeftAlignIndels. ### Description . The example for LeftAlignIndels uses a parameter `-O` which doesn't exist for the tool - the parameter listed in the documentation is `--OUTPUT`. Either the example should be updated to use `--OUTPUT`, or the tool should be changed to accept `-O`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5072
https://github.com/broadinstitute/gatk/pull/5073:279,Performance,cache,cached,279,"@LeeTL1220 The first commit makes all concordance tools and MC3/M2 vcf merge much faster in Firecloud -- tasks that have been taking an hour will take a few minutes. The second commit makes `GetPileupSummaries`, hence the contamination task much faster. Previously that tool has cached all reads for the 100,000 bases around each site, so basically it had to do a whole bam's worth of I/O just to get ~60,000 pileups.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5073
https://github.com/broadinstitute/gatk/pull/5074:162,Energy Efficiency,allocate,allocated,162,"Closes #1493. @droazen @cmnbroad Is this what was intended by #1493 -- just replace `LinkedList` with `ArrayList` and `clear` the reservoir, keeping its capacity allocated, when possible?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5074
https://github.com/broadinstitute/gatk/pull/5074:119,Usability,clear,clear,119,"Closes #1493. @droazen @cmnbroad Is this what was intended by #1493 -- just replace `LinkedList` with `ArrayList` and `clear` the reservoir, keeping its capacity allocated, when possible?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5074
https://github.com/broadinstitute/gatk/issues/5075:146,Availability,down,downsampling,146,"## Feature request and Question. Original question was posted in GATK forum https://gatkforums.broadinstitute.org/gatk/discussion/12026/how-to-do-downsampling,; but it seems to me that the question should be posted here to ask the developer team. ### Tool(s) or class(es) involved; PrintReads. ### Description. In GATK4, printReads doesn't have an option to do downsample to coverage anymore. Is there any reason for that ? Or is there any update suggestions to do the same thing but migrating it from GATK3 to GATK4 ? The forum maintainer told me in original discussion that there is a `DownsampleSam` function in picard, but it can't be used to downsample to coverage directly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5075
https://github.com/broadinstitute/gatk/issues/5075:361,Availability,down,downsample,361,"## Feature request and Question. Original question was posted in GATK forum https://gatkforums.broadinstitute.org/gatk/discussion/12026/how-to-do-downsampling,; but it seems to me that the question should be posted here to ask the developer team. ### Tool(s) or class(es) involved; PrintReads. ### Description. In GATK4, printReads doesn't have an option to do downsample to coverage anymore. Is there any reason for that ? Or is there any update suggestions to do the same thing but migrating it from GATK3 to GATK4 ? The forum maintainer told me in original discussion that there is a `DownsampleSam` function in picard, but it can't be used to downsample to coverage directly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5075
https://github.com/broadinstitute/gatk/issues/5075:588,Availability,Down,DownsampleSam,588,"## Feature request and Question. Original question was posted in GATK forum https://gatkforums.broadinstitute.org/gatk/discussion/12026/how-to-do-downsampling,; but it seems to me that the question should be posted here to ask the developer team. ### Tool(s) or class(es) involved; PrintReads. ### Description. In GATK4, printReads doesn't have an option to do downsample to coverage anymore. Is there any reason for that ? Or is there any update suggestions to do the same thing but migrating it from GATK3 to GATK4 ? The forum maintainer told me in original discussion that there is a `DownsampleSam` function in picard, but it can't be used to downsample to coverage directly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5075
https://github.com/broadinstitute/gatk/issues/5075:647,Availability,down,downsample,647,"## Feature request and Question. Original question was posted in GATK forum https://gatkforums.broadinstitute.org/gatk/discussion/12026/how-to-do-downsampling,; but it seems to me that the question should be posted here to ask the developer team. ### Tool(s) or class(es) involved; PrintReads. ### Description. In GATK4, printReads doesn't have an option to do downsample to coverage anymore. Is there any reason for that ? Or is there any update suggestions to do the same thing but migrating it from GATK3 to GATK4 ? The forum maintainer told me in original discussion that there is a `DownsampleSam` function in picard, but it can't be used to downsample to coverage directly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5075
https://github.com/broadinstitute/gatk/issues/5075:440,Deployability,update,update,440,"## Feature request and Question. Original question was posted in GATK forum https://gatkforums.broadinstitute.org/gatk/discussion/12026/how-to-do-downsampling,; but it seems to me that the question should be posted here to ask the developer team. ### Tool(s) or class(es) involved; PrintReads. ### Description. In GATK4, printReads doesn't have an option to do downsample to coverage anymore. Is there any reason for that ? Or is there any update suggestions to do the same thing but migrating it from GATK3 to GATK4 ? The forum maintainer told me in original discussion that there is a `DownsampleSam` function in picard, but it can't be used to downsample to coverage directly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5075
https://github.com/broadinstitute/gatk/pull/5078:161,Availability,error,errors,161,@takutosato This change doesn't hurt sensitivity in our validations and made M2 25% faster. We were getting a lot of active regions based on single substitution errors in overlapping reads.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5078
https://github.com/broadinstitute/gatk/pull/5078:56,Security,validat,validations,56,@takutosato This change doesn't hurt sensitivity in our validations and made M2 25% faster. We were getting a lot of active regions based on single substitution errors in overlapping reads.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5078
https://github.com/broadinstitute/gatk/pull/5081:245,Deployability,update,update,245,"Fixes #https://github.com/broadinstitute/gatk/issues/4995 for the `StreamingScriptExecutor`. Still need to enable this for `PythonScriptExecutor`. Also includes an opportunistic fix for the localDevCondaEnv gradle task, which sometimes fails to update the Python package.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5081
https://github.com/broadinstitute/gatk/pull/5082:322,Integrability,depend,depend,322,"This PR is a replacement for https://github.com/broadinstitute/gatk/pull/5055 (includes the cherry-picked commit with the tests from that PR), and a fix for https://github.com/broadinstitute/hdf5-java-bindings/issues/11. I implemented this in GATK rather than in hdf5-java-bindings because we need a version that does not depend on (throw) if the hdf5 library is not supported on the current platform.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5082
https://github.com/broadinstitute/gatk/pull/5082:122,Testability,test,tests,122,"This PR is a replacement for https://github.com/broadinstitute/gatk/pull/5055 (includes the cherry-picked commit with the tests from that PR), and a fix for https://github.com/broadinstitute/hdf5-java-bindings/issues/11. I implemented this in GATK rather than in hdf5-java-bindings because we need a version that does not depend on (throw) if the hdf5 library is not supported on the current platform.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5082
https://github.com/broadinstitute/gatk/issues/5083:185,Modifiability,refactor,refactoring,185,## Feature request. ### Tool(s) or class(es) involved. `GATKSparkTool`. ### Description. `GATKTool` currently has them (and unfortunately takes `File` as input). It would great if some refactoring can happen so that methods provide common utilities (such as these) can be merged in a single base class.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5083
https://github.com/broadinstitute/gatk/issues/5085:58,Modifiability,variab,variable,58,"The model currently gives each read an independent latent variable indicating which haplotype it was derived from. This latent variable should be a property of fragments, not reads.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5085
https://github.com/broadinstitute/gatk/issues/5085:127,Modifiability,variab,variable,127,"The model currently gives each read an independent latent variable indicating which haplotype it was derived from. This latent variable should be a property of fragments, not reads.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5085
https://github.com/broadinstitute/gatk/issues/5087:50,Deployability,Update,UpdateVCFSequenceDictionary,50,"## Bug Report. ### Affected tool(s) or class(es); UpdateVCFSequenceDictionary. ### Affected version(s); - \[x] Latest master branch as of 8/3/18. ### Description ; I am running UpdateVCFSequenceDictionary on a vcf which should have a hg38 header but which (for unrelated unpleasant reasons) instead has a hg19 header. Using an hg38 dictionary as source dict to try to fix the header. If I ask to output a .vcf file, everything works fine. If I ask to output a .vcf.gz file, gatk crashes with; ```; java.lang.ArrayIndexOutOfBoundsException: 12922; 	at htsjdk.samtools.BinningIndexBuilder.processFeature(BinningIndexBuilder.java:89); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeIndex(TabixIndexCreator.java:129); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:146); 	at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:212); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.UpdateVCFSequenceDictionary.closeTool(UpdateVCFSequenceDictionary.java:174); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:983); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ``` . #### Steps to reproduce; Works: ``gatk UpdateVCFSequenceDictionary -V /dsde/working/ckachulis/UpdateVCFSequenceDictionary_Bug/na12878_hg38_giab_pg_hybrid_happy.vcf.gz -O corrected.dictionary.vcf --source-dictionary /seq/references/Homo_sapiens_as",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5087
https://github.com/broadinstitute/gatk/issues/5087:177,Deployability,Update,UpdateVCFSequenceDictionary,177,"## Bug Report. ### Affected tool(s) or class(es); UpdateVCFSequenceDictionary. ### Affected version(s); - \[x] Latest master branch as of 8/3/18. ### Description ; I am running UpdateVCFSequenceDictionary on a vcf which should have a hg38 header but which (for unrelated unpleasant reasons) instead has a hg19 header. Using an hg38 dictionary as source dict to try to fix the header. If I ask to output a .vcf file, everything works fine. If I ask to output a .vcf.gz file, gatk crashes with; ```; java.lang.ArrayIndexOutOfBoundsException: 12922; 	at htsjdk.samtools.BinningIndexBuilder.processFeature(BinningIndexBuilder.java:89); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeIndex(TabixIndexCreator.java:129); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:146); 	at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:212); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.UpdateVCFSequenceDictionary.closeTool(UpdateVCFSequenceDictionary.java:174); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:983); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ``` . #### Steps to reproduce; Works: ``gatk UpdateVCFSequenceDictionary -V /dsde/working/ckachulis/UpdateVCFSequenceDictionary_Bug/na12878_hg38_giab_pg_hybrid_happy.vcf.gz -O corrected.dictionary.vcf --source-dictionary /seq/references/Homo_sapiens_as",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5087
https://github.com/broadinstitute/gatk/issues/5087:1073,Deployability,Update,UpdateVCFSequenceDictionary,1073,"cted version(s); - \[x] Latest master branch as of 8/3/18. ### Description ; I am running UpdateVCFSequenceDictionary on a vcf which should have a hg38 header but which (for unrelated unpleasant reasons) instead has a hg19 header. Using an hg38 dictionary as source dict to try to fix the header. If I ask to output a .vcf file, everything works fine. If I ask to output a .vcf.gz file, gatk crashes with; ```; java.lang.ArrayIndexOutOfBoundsException: 12922; 	at htsjdk.samtools.BinningIndexBuilder.processFeature(BinningIndexBuilder.java:89); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeIndex(TabixIndexCreator.java:129); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:146); 	at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:212); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.UpdateVCFSequenceDictionary.closeTool(UpdateVCFSequenceDictionary.java:174); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:983); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ``` . #### Steps to reproduce; Works: ``gatk UpdateVCFSequenceDictionary -V /dsde/working/ckachulis/UpdateVCFSequenceDictionary_Bug/na12878_hg38_giab_pg_hybrid_happy.vcf.gz -O corrected.dictionary.vcf --source-dictionary /seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.dict --replace ``. Crashes: ``gatk UpdateVCFSequen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5087
https://github.com/broadinstitute/gatk/issues/5087:1111,Deployability,Update,UpdateVCFSequenceDictionary,1111,"ster branch as of 8/3/18. ### Description ; I am running UpdateVCFSequenceDictionary on a vcf which should have a hg38 header but which (for unrelated unpleasant reasons) instead has a hg19 header. Using an hg38 dictionary as source dict to try to fix the header. If I ask to output a .vcf file, everything works fine. If I ask to output a .vcf.gz file, gatk crashes with; ```; java.lang.ArrayIndexOutOfBoundsException: 12922; 	at htsjdk.samtools.BinningIndexBuilder.processFeature(BinningIndexBuilder.java:89); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeIndex(TabixIndexCreator.java:129); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:146); 	at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:212); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.UpdateVCFSequenceDictionary.closeTool(UpdateVCFSequenceDictionary.java:174); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:983); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ``` . #### Steps to reproduce; Works: ``gatk UpdateVCFSequenceDictionary -V /dsde/working/ckachulis/UpdateVCFSequenceDictionary_Bug/na12878_hg38_giab_pg_hybrid_happy.vcf.gz -O corrected.dictionary.vcf --source-dictionary /seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.dict --replace ``. Crashes: ``gatk UpdateVCFSequenceDictionary -V /dsde/working/cka",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5087
https://github.com/broadinstitute/gatk/issues/5087:1794,Deployability,Update,UpdateVCFSequenceDictionary,1794,".tabix.TabixIndexCreator.finalizeIndex(TabixIndexCreator.java:129); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:146); 	at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:212); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.UpdateVCFSequenceDictionary.closeTool(UpdateVCFSequenceDictionary.java:174); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:983); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ``` . #### Steps to reproduce; Works: ``gatk UpdateVCFSequenceDictionary -V /dsde/working/ckachulis/UpdateVCFSequenceDictionary_Bug/na12878_hg38_giab_pg_hybrid_happy.vcf.gz -O corrected.dictionary.vcf --source-dictionary /seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.dict --replace ``. Crashes: ``gatk UpdateVCFSequenceDictionary -V /dsde/working/ckachulis/UpdateVCFSequenceDictionary_Bug/na12878_hg38_giab_pg_hybrid_happy.vcf.gz -O corrected.dictionary.vcf.gz --source-dictionary /seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.dict --replace `` . This is caused by the Tabix tools in htsjdk being given the dictionary from the input vcf to use for indexing instead of the source-dictionary. Can be fixed by overriding ``getBestAvailableSequenceDictionary()`` in UpdateVCFSequenceDictionary to return ``sourceDictionary``. However, this requires making ``VariantWalkerBase::getBestAvailableSequenceDictionary()`` non-final, so perhaps there is a better option.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5087
https://github.com/broadinstitute/gatk/issues/5087:2072,Deployability,Update,UpdateVCFSequenceDictionary,2072,".tabix.TabixIndexCreator.finalizeIndex(TabixIndexCreator.java:129); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:146); 	at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:212); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.UpdateVCFSequenceDictionary.closeTool(UpdateVCFSequenceDictionary.java:174); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:983); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ``` . #### Steps to reproduce; Works: ``gatk UpdateVCFSequenceDictionary -V /dsde/working/ckachulis/UpdateVCFSequenceDictionary_Bug/na12878_hg38_giab_pg_hybrid_happy.vcf.gz -O corrected.dictionary.vcf --source-dictionary /seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.dict --replace ``. Crashes: ``gatk UpdateVCFSequenceDictionary -V /dsde/working/ckachulis/UpdateVCFSequenceDictionary_Bug/na12878_hg38_giab_pg_hybrid_happy.vcf.gz -O corrected.dictionary.vcf.gz --source-dictionary /seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.dict --replace `` . This is caused by the Tabix tools in htsjdk being given the dictionary from the input vcf to use for indexing instead of the source-dictionary. Can be fixed by overriding ``getBestAvailableSequenceDictionary()`` in UpdateVCFSequenceDictionary to return ``sourceDictionary``. However, this requires making ``VariantWalkerBase::getBestAvailableSequenceDictionary()`` non-final, so perhaps there is a better option.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5087
https://github.com/broadinstitute/gatk/issues/5087:2553,Deployability,Update,UpdateVCFSequenceDictionary,2553,".tabix.TabixIndexCreator.finalizeIndex(TabixIndexCreator.java:129); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:146); 	at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:212); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.UpdateVCFSequenceDictionary.closeTool(UpdateVCFSequenceDictionary.java:174); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:983); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ``` . #### Steps to reproduce; Works: ``gatk UpdateVCFSequenceDictionary -V /dsde/working/ckachulis/UpdateVCFSequenceDictionary_Bug/na12878_hg38_giab_pg_hybrid_happy.vcf.gz -O corrected.dictionary.vcf --source-dictionary /seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.dict --replace ``. Crashes: ``gatk UpdateVCFSequenceDictionary -V /dsde/working/ckachulis/UpdateVCFSequenceDictionary_Bug/na12878_hg38_giab_pg_hybrid_happy.vcf.gz -O corrected.dictionary.vcf.gz --source-dictionary /seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.dict --replace `` . This is caused by the Tabix tools in htsjdk being given the dictionary from the input vcf to use for indexing instead of the source-dictionary. Can be fixed by overriding ``getBestAvailableSequenceDictionary()`` in UpdateVCFSequenceDictionary to return ``sourceDictionary``. However, this requires making ``VariantWalkerBase::getBestAvailableSequenceDictionary()`` non-final, so perhaps there is a better option.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5087
https://github.com/broadinstitute/gatk/issues/5089:382,Usability,learn,learning,382,"We need to see how many variants from COSMIC a typical tumor contains, and of these what is the precision. If there are a decent number with high confidence we could use it as follows:. * Get high-confidence somatic sites for a tumor-in-normal tool. For such a tool we can't rely on the normal artifact filter and so a bit of extra help would be nice.; * Get training data for deep learning, perhaps to seed a semi-supervised approach.; * Modify the tumor lod threshold based on presence in COSMIC.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5089
https://github.com/broadinstitute/gatk/issues/5090:429,Availability,ERROR,ERROR,429,"Hi,; I use the gatk4.0.2.1 to detect variant and the command line:; time gatk-4.0.2.1/gatk --java-options ""-XX:ParallelGCThreads=5 -Xmx30G"" HaplotypeCaller --input rice.RGAP7.R01.dedup.bam --output rice.RGAP7.1.0.g.vcf --reference ref.genome.fa --native-pair-hmm-threads --emit-ref-confidence GVCF --indel-size-to-eliminate-in-ref-model 50 --sample-ploidy 2 --intervals rice.RGAP7.chr_allocation.1.list --TMP_DIR tmp --verbosity ERROR. **real	32m47.986s**; user	32m56.767s; sys	0m22.567s. I ues the gatk3.8 to detect variant and the command line:; time java -XX:ParallelGCThreads=5 -Djava.io.tmpdir=tmp -Xmx30G GenomeAnalysisTK/3.8/GenomeAnalysisTK.jar -T HaplotypeCaller -R ref.genome.fa --indelSizeToEliminateInRefModel 50 --emitRefConfidence GVCF --sample_ploidy 2 -nct 4 -o rice.RGAP7.1.0.g.vcf -L rice.RGAP7.chr_allocation.1.list -I rice.RGAP7.R01.dedup.realn.bam . **real	8m49.673s**; user	35m42.770s; sys	0m21.607s. Theoreticallythe gatk4 runtime is faster than gatk3.x .; Why do I get the opposite result?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5090
https://github.com/broadinstitute/gatk/issues/5090:30,Safety,detect,detect,30,"Hi,; I use the gatk4.0.2.1 to detect variant and the command line:; time gatk-4.0.2.1/gatk --java-options ""-XX:ParallelGCThreads=5 -Xmx30G"" HaplotypeCaller --input rice.RGAP7.R01.dedup.bam --output rice.RGAP7.1.0.g.vcf --reference ref.genome.fa --native-pair-hmm-threads --emit-ref-confidence GVCF --indel-size-to-eliminate-in-ref-model 50 --sample-ploidy 2 --intervals rice.RGAP7.chr_allocation.1.list --TMP_DIR tmp --verbosity ERROR. **real	32m47.986s**; user	32m56.767s; sys	0m22.567s. I ues the gatk3.8 to detect variant and the command line:; time java -XX:ParallelGCThreads=5 -Djava.io.tmpdir=tmp -Xmx30G GenomeAnalysisTK/3.8/GenomeAnalysisTK.jar -T HaplotypeCaller -R ref.genome.fa --indelSizeToEliminateInRefModel 50 --emitRefConfidence GVCF --sample_ploidy 2 -nct 4 -o rice.RGAP7.1.0.g.vcf -L rice.RGAP7.chr_allocation.1.list -I rice.RGAP7.R01.dedup.realn.bam . **real	8m49.673s**; user	35m42.770s; sys	0m21.607s. Theoreticallythe gatk4 runtime is faster than gatk3.x .; Why do I get the opposite result?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5090
https://github.com/broadinstitute/gatk/issues/5090:510,Safety,detect,detect,510,"Hi,; I use the gatk4.0.2.1 to detect variant and the command line:; time gatk-4.0.2.1/gatk --java-options ""-XX:ParallelGCThreads=5 -Xmx30G"" HaplotypeCaller --input rice.RGAP7.R01.dedup.bam --output rice.RGAP7.1.0.g.vcf --reference ref.genome.fa --native-pair-hmm-threads --emit-ref-confidence GVCF --indel-size-to-eliminate-in-ref-model 50 --sample-ploidy 2 --intervals rice.RGAP7.chr_allocation.1.list --TMP_DIR tmp --verbosity ERROR. **real	32m47.986s**; user	32m56.767s; sys	0m22.567s. I ues the gatk3.8 to detect variant and the command line:; time java -XX:ParallelGCThreads=5 -Djava.io.tmpdir=tmp -Xmx30G GenomeAnalysisTK/3.8/GenomeAnalysisTK.jar -T HaplotypeCaller -R ref.genome.fa --indelSizeToEliminateInRefModel 50 --emitRefConfidence GVCF --sample_ploidy 2 -nct 4 -o rice.RGAP7.1.0.g.vcf -L rice.RGAP7.chr_allocation.1.list -I rice.RGAP7.R01.dedup.realn.bam . **real	8m49.673s**; user	35m42.770s; sys	0m21.607s. Theoreticallythe gatk4 runtime is faster than gatk3.x .; Why do I get the opposite result?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5090
https://github.com/broadinstitute/gatk/pull/5091:117,Testability,test,test,117,"One of the variants is no longer emitted in vcf, potentially due to #5078. Somehow this was not caught by the remote test for #4895.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5091
https://github.com/broadinstitute/gatk/pull/5092:111,Modifiability,refactor,refactoring,111,@LeeTL1220 This gets rid of a bunch of false positives with no effect on sensitivity. It also does some useful refactoring of the filtering engine in order to exploit Takuto's two-pass formalism for more filters than just OB.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5092
https://github.com/broadinstitute/gatk/issues/5094:28,Availability,failure,failures,28,We're seeing a high rate of failures running BQSR pipelines in production with `ExecutionException`s. . The root cause seems to be an `UnknownHostException` thrown in the google storage api client. ```; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: www.googleapis.com; 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); 	at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:567); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:556); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:525); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331); 	at java.io.DataInputStream.read(DataInputStream.java:149); 	at htsjdk.samtools.util.BinaryCodec.readBytesOrFewer(BinaryCodec.java:404); 	at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:380); 	at htsjdk.samtools.util.BinaryCodec.readByteBuffer(BinaryCodec.java:490); 	at htsjdk.samtools.util.BinaryCodec.readInt(BinaryCodec.java:501); 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:198); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.jav,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094
https://github.com/broadinstitute/gatk/issues/5094:1102,Availability,avail,available,1102,eems to be an `UnknownHostException` thrown in the google storage api client. ```; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: www.googleapis.com; 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); 	at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:567); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:556); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:525); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331); 	at java.io.DataInputStream.read(DataInputStream.java:149); 	at htsjdk.samtools.util.BinaryCodec.readBytesOrFewer(BinaryCodec.java:404); 	at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:380); 	at htsjdk.samtools.util.BinaryCodec.readByteBuffer(BinaryCodec.java:490); 	at htsjdk.samtools.util.BinaryCodec.readInt(BinaryCodec.java:501); 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:198); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765); 	at htsjdk.samtools.BAMFileReade,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094
https://github.com/broadinstitute/gatk/issues/5094:50,Deployability,pipeline,pipelines,50,We're seeing a high rate of failures running BQSR pipelines in production with `ExecutionException`s. . The root cause seems to be an `UnknownHostException` thrown in the google storage api client. ```; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: www.googleapis.com; 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); 	at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:567); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:556); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:525); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331); 	at java.io.DataInputStream.read(DataInputStream.java:149); 	at htsjdk.samtools.util.BinaryCodec.readBytesOrFewer(BinaryCodec.java:404); 	at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:380); 	at htsjdk.samtools.util.BinaryCodec.readByteBuffer(BinaryCodec.java:490); 	at htsjdk.samtools.util.BinaryCodec.readInt(BinaryCodec.java:501); 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:198); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.jav,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094
https://github.com/broadinstitute/gatk/issues/5094:3363,Integrability,wrap,wrapAndCopyInto,3363,ools.SamReader$AssertingIterator.next(SamReader.java:576); 	at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:548); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextRecord(SamReaderQueryingIterator.java:114); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:151); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:15); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094
https://github.com/broadinstitute/gatk/issues/5094:6864,Integrability,protocol,protocol,6864,refetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.UnknownHostException: www.googleapis.com; 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:668); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527); 	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:264); 	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:367); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:191); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:177); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shade,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094
https://github.com/broadinstitute/gatk/issues/5094:6937,Integrability,protocol,protocol,6937,broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.UnknownHostException: www.googleapis.com; 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:668); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527); 	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:264); 	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:367); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:191); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:177); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClien,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094
https://github.com/broadinstitute/gatk/issues/5094:7007,Integrability,protocol,protocol,7007,Unit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.UnknownHostException: www.googleapis.com; 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:668); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527); 	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:264); 	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:367); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:191); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:177); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at sh,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094
https://github.com/broadinstitute/gatk/issues/5094:7136,Integrability,protocol,protocol,7136,ncurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.UnknownHostException: www.googleapis.com; 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:668); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527); 	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:264); 	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:367); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:191); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:177); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoogleClientRequest.jav,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094
https://github.com/broadinstitute/gatk/issues/5094:7228,Integrability,protocol,protocol,7228,t.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.UnknownHostException: www.googleapis.com; 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:668); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527); 	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:264); 	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:367); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:191); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:177); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoogleClientRequest.java:380); 	at shaded.cloud_nio.com.google.api.services.storage.Storage$Objects$Get.executeMedi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094
https://github.com/broadinstitute/gatk/issues/5094:7319,Integrability,protocol,protocol,7319,ad.java:745); Caused by: java.net.UnknownHostException: www.googleapis.com; 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:668); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527); 	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:264); 	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:367); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:191); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:177); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoogleClientRequest.java:380); 	at shaded.cloud_nio.com.google.api.services.storage.Storage$Objects$Get.executeMedia(Storage.java:6133); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094
https://github.com/broadinstitute/gatk/issues/5094:7439,Integrability,protocol,protocol,7439,sed by: java.net.UnknownHostException: www.googleapis.com; 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:668); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527); 	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:264); 	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:367); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:191); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:177); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoogleClientRequest.java:380); 	at shaded.cloud_nio.com.google.api.services.storage.Storage$Objects$Get.executeMedia(Storage.java:6133); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:505); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094
https://github.com/broadinstitute/gatk/issues/5094:241,Performance,concurren,concurrent,241,We're seeing a high rate of failures running BQSR pipelines in production with `ExecutionException`s. . The root cause seems to be an `UnknownHostException` thrown in the google storage api client. ```; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: www.googleapis.com; 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); 	at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:567); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:556); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:525); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331); 	at java.io.DataInputStream.read(DataInputStream.java:149); 	at htsjdk.samtools.util.BinaryCodec.readBytesOrFewer(BinaryCodec.java:404); 	at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:380); 	at htsjdk.samtools.util.BinaryCodec.readByteBuffer(BinaryCodec.java:490); 	at htsjdk.samtools.util.BinaryCodec.readInt(BinaryCodec.java:501); 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:198); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.jav,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094
https://github.com/broadinstitute/gatk/issues/5094:2588,Performance,load,loadNextRecord,2588,mtools.BAMRecordCodec.decode(BAMRecordCodec.java:198); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.advance(BAMFileReader.java:1034); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:1024); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:988); 	at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:576); 	at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:548); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextRecord(SamReaderQueryingIterator.java:114); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:151); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:15); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractP,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094
https://github.com/broadinstitute/gatk/issues/5094:4432,Performance,concurren,concurrent,4432,s$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:233); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: www.googleapis.com; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 45 more; Caused by: com.google.cloud.storage.StorageException: www.googleapis.com; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:526); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094
https://github.com/broadinstitute/gatk/issues/5094:4540,Performance,concurren,concurrent,4540,eSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:233); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: www.googleapis.com; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 45 more; Caused by: com.google.cloud.storage.StorageException: www.googleapis.com; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:526); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094
https://github.com/broadinstitute/gatk/issues/5094:4605,Performance,concurren,concurrent,4605,peline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:233); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: www.googleapis.com; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 45 more; Caused by: com.google.cloud.storage.StorageException: www.googleapis.com; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:526); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094
https://github.com/broadinstitute/gatk/issues/5094:6076,Performance,concurren,concurrent,6076,eException: www.googleapis.com; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:526); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:114); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.UnknownHostException: www.googleapis.com; 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:668); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527); 	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:264); 	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:367); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(Abstract,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094
https://github.com/broadinstitute/gatk/issues/5094:6138,Performance,concurren,concurrent,6138,pi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:526); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:114); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.UnknownHostException: www.googleapis.com; 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:668); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527); 	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:264); 	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:367); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:191); 	at sun.net.www.protocol,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094
https://github.com/broadinstitute/gatk/issues/5094:6223,Performance,concurren,concurrent,6223,.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:526); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:114); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.UnknownHostException: www.googleapis.com; 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:668); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527); 	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:264); 	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:367); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:191); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138); 	at sun.net.www.p,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094
https://github.com/broadinstitute/gatk/issues/5094:6597,Security,secur,security,6597,etryHelper.java:54); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:114); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.UnknownHostException: www.googleapis.com; 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:668); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527); 	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:264); 	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:367); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:191); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:177); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetH,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094
https://github.com/broadinstitute/gatk/issues/5094:2394,Testability,Assert,AssertingIterator,2394,ols.util.BinaryCodec.readBytes(BinaryCodec.java:380); 	at htsjdk.samtools.util.BinaryCodec.readByteBuffer(BinaryCodec.java:490); 	at htsjdk.samtools.util.BinaryCodec.readInt(BinaryCodec.java:501); 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:198); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.advance(BAMFileReader.java:1034); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:1024); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:988); 	at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:576); 	at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:548); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextRecord(SamReaderQueryingIterator.java:114); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:151); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:15); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094
https://github.com/broadinstitute/gatk/issues/5094:2468,Testability,Assert,AssertingIterator,2468,util.BinaryCodec.readByteBuffer(BinaryCodec.java:490); 	at htsjdk.samtools.util.BinaryCodec.readInt(BinaryCodec.java:501); 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:198); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.advance(BAMFileReader.java:1034); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:1024); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:988); 	at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:576); 	at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:548); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextRecord(SamReaderQueryingIterator.java:114); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:151); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:15); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForE,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094
https://github.com/broadinstitute/gatk/pull/5097:460,Availability,reliab,reliable,460,"Fixes https://github.com/broadinstitute/gatk/issues/5065. This test verifies that we receive output written to Python's stderr, but it fails occasionally due to an inherent race condition in the assert (occasionally the assert executes before the data is received and the test fails). I added a Python statement that explicitly flushes stderr first, and ran the test 1000 times and it still failed once for the same reason. Since I don't see any way to have a reliable test condition (that doesn't involve polling inside a loop and a long timeout), I'm just removing the test.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5097
https://github.com/broadinstitute/gatk/pull/5097:173,Performance,race condition,race condition,173,"Fixes https://github.com/broadinstitute/gatk/issues/5065. This test verifies that we receive output written to Python's stderr, but it fails occasionally due to an inherent race condition in the assert (occasionally the assert executes before the data is received and the test fails). I added a Python statement that explicitly flushes stderr first, and ran the test 1000 times and it still failed once for the same reason. Since I don't see any way to have a reliable test condition (that doesn't involve polling inside a loop and a long timeout), I'm just removing the test.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5097
https://github.com/broadinstitute/gatk/pull/5097:539,Safety,timeout,timeout,539,"Fixes https://github.com/broadinstitute/gatk/issues/5065. This test verifies that we receive output written to Python's stderr, but it fails occasionally due to an inherent race condition in the assert (occasionally the assert executes before the data is received and the test fails). I added a Python statement that explicitly flushes stderr first, and ran the test 1000 times and it still failed once for the same reason. Since I don't see any way to have a reliable test condition (that doesn't involve polling inside a loop and a long timeout), I'm just removing the test.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5097
https://github.com/broadinstitute/gatk/pull/5097:63,Testability,test,test,63,"Fixes https://github.com/broadinstitute/gatk/issues/5065. This test verifies that we receive output written to Python's stderr, but it fails occasionally due to an inherent race condition in the assert (occasionally the assert executes before the data is received and the test fails). I added a Python statement that explicitly flushes stderr first, and ran the test 1000 times and it still failed once for the same reason. Since I don't see any way to have a reliable test condition (that doesn't involve polling inside a loop and a long timeout), I'm just removing the test.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5097
https://github.com/broadinstitute/gatk/pull/5097:195,Testability,assert,assert,195,"Fixes https://github.com/broadinstitute/gatk/issues/5065. This test verifies that we receive output written to Python's stderr, but it fails occasionally due to an inherent race condition in the assert (occasionally the assert executes before the data is received and the test fails). I added a Python statement that explicitly flushes stderr first, and ran the test 1000 times and it still failed once for the same reason. Since I don't see any way to have a reliable test condition (that doesn't involve polling inside a loop and a long timeout), I'm just removing the test.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5097
https://github.com/broadinstitute/gatk/pull/5097:220,Testability,assert,assert,220,"Fixes https://github.com/broadinstitute/gatk/issues/5065. This test verifies that we receive output written to Python's stderr, but it fails occasionally due to an inherent race condition in the assert (occasionally the assert executes before the data is received and the test fails). I added a Python statement that explicitly flushes stderr first, and ran the test 1000 times and it still failed once for the same reason. Since I don't see any way to have a reliable test condition (that doesn't involve polling inside a loop and a long timeout), I'm just removing the test.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5097
https://github.com/broadinstitute/gatk/pull/5097:272,Testability,test,test,272,"Fixes https://github.com/broadinstitute/gatk/issues/5065. This test verifies that we receive output written to Python's stderr, but it fails occasionally due to an inherent race condition in the assert (occasionally the assert executes before the data is received and the test fails). I added a Python statement that explicitly flushes stderr first, and ran the test 1000 times and it still failed once for the same reason. Since I don't see any way to have a reliable test condition (that doesn't involve polling inside a loop and a long timeout), I'm just removing the test.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5097
https://github.com/broadinstitute/gatk/pull/5097:362,Testability,test,test,362,"Fixes https://github.com/broadinstitute/gatk/issues/5065. This test verifies that we receive output written to Python's stderr, but it fails occasionally due to an inherent race condition in the assert (occasionally the assert executes before the data is received and the test fails). I added a Python statement that explicitly flushes stderr first, and ran the test 1000 times and it still failed once for the same reason. Since I don't see any way to have a reliable test condition (that doesn't involve polling inside a loop and a long timeout), I'm just removing the test.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5097
https://github.com/broadinstitute/gatk/pull/5097:469,Testability,test,test,469,"Fixes https://github.com/broadinstitute/gatk/issues/5065. This test verifies that we receive output written to Python's stderr, but it fails occasionally due to an inherent race condition in the assert (occasionally the assert executes before the data is received and the test fails). I added a Python statement that explicitly flushes stderr first, and ran the test 1000 times and it still failed once for the same reason. Since I don't see any way to have a reliable test condition (that doesn't involve polling inside a loop and a long timeout), I'm just removing the test.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5097
https://github.com/broadinstitute/gatk/pull/5097:571,Testability,test,test,571,"Fixes https://github.com/broadinstitute/gatk/issues/5065. This test verifies that we receive output written to Python's stderr, but it fails occasionally due to an inherent race condition in the assert (occasionally the assert executes before the data is received and the test fails). I added a Python statement that explicitly flushes stderr first, and ran the test 1000 times and it still failed once for the same reason. Since I don't see any way to have a reliable test condition (that doesn't involve polling inside a loop and a long timeout), I'm just removing the test.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5097
https://github.com/broadinstitute/gatk/issues/5098:75,Availability,failure,failure,75,"When WDL tests fail on travis it can be hard to determine the cause of the failure. We should either always retain the cromwell workflow logs, maybe in a gcs bucket, or have some easy way to opt-in for retention to make it easier to diagnose.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5098
https://github.com/broadinstitute/gatk/issues/5098:9,Testability,test,tests,9,"When WDL tests fail on travis it can be hard to determine the cause of the failure. We should either always retain the cromwell workflow logs, maybe in a gcs bucket, or have some easy way to opt-in for retention to make it easier to diagnose.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5098
https://github.com/broadinstitute/gatk/issues/5098:137,Testability,log,logs,137,"When WDL tests fail on travis it can be hard to determine the cause of the failure. We should either always retain the cromwell workflow logs, maybe in a gcs bucket, or have some easy way to opt-in for retention to make it easier to diagnose.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5098
https://github.com/broadinstitute/gatk/pull/5099:72,Availability,error,errors,72,"This brings in some additional retries for UnknownHostException and 502 errors,; and moves us from a fork in my personal github repository to the fork in; https://github.com/broadinstitute/google-cloud-java. Resolves #4888; Resolves #5094",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5099
https://github.com/broadinstitute/gatk/issues/5100:27,Deployability,install,installs,27,"The `gatktool` Python code installs a system exception handler to catch unhandled Python exceptions, and sends a negative ack to the `StreamingProcessController` when it sees one. The controller then grabs the stdout/stderr contents; writes it to the log/journal file; and throws a java exception. However, there is a shutdown race condition where occasionally the GATK process will get the negative ack and terminate before the Python exception chain is finished processing, and the exception message never appears in the journal. We've seen this happen when the CNN Python inference code runs out of memory. It would be better to have the exception handler write the exception string directly to the ack FIFO, with a message length included, so the controller can deterministically retrieve the message for inclusion in the java exception without having to rely on std in/out.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5100
https://github.com/broadinstitute/gatk/issues/5100:494,Integrability,message,message,494,"The `gatktool` Python code installs a system exception handler to catch unhandled Python exceptions, and sends a negative ack to the `StreamingProcessController` when it sees one. The controller then grabs the stdout/stderr contents; writes it to the log/journal file; and throws a java exception. However, there is a shutdown race condition where occasionally the GATK process will get the negative ack and terminate before the Python exception chain is finished processing, and the exception message never appears in the journal. We've seen this happen when the CNN Python inference code runs out of memory. It would be better to have the exception handler write the exception string directly to the ack FIFO, with a message length included, so the controller can deterministically retrieve the message for inclusion in the java exception without having to rely on std in/out.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5100
https://github.com/broadinstitute/gatk/issues/5100:719,Integrability,message,message,719,"The `gatktool` Python code installs a system exception handler to catch unhandled Python exceptions, and sends a negative ack to the `StreamingProcessController` when it sees one. The controller then grabs the stdout/stderr contents; writes it to the log/journal file; and throws a java exception. However, there is a shutdown race condition where occasionally the GATK process will get the negative ack and terminate before the Python exception chain is finished processing, and the exception message never appears in the journal. We've seen this happen when the CNN Python inference code runs out of memory. It would be better to have the exception handler write the exception string directly to the ack FIFO, with a message length included, so the controller can deterministically retrieve the message for inclusion in the java exception without having to rely on std in/out.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5100
https://github.com/broadinstitute/gatk/issues/5100:797,Integrability,message,message,797,"The `gatktool` Python code installs a system exception handler to catch unhandled Python exceptions, and sends a negative ack to the `StreamingProcessController` when it sees one. The controller then grabs the stdout/stderr contents; writes it to the log/journal file; and throws a java exception. However, there is a shutdown race condition where occasionally the GATK process will get the negative ack and terminate before the Python exception chain is finished processing, and the exception message never appears in the journal. We've seen this happen when the CNN Python inference code runs out of memory. It would be better to have the exception handler write the exception string directly to the ack FIFO, with a message length included, so the controller can deterministically retrieve the message for inclusion in the java exception without having to rely on std in/out.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5100
https://github.com/broadinstitute/gatk/issues/5100:327,Performance,race condition,race condition,327,"The `gatktool` Python code installs a system exception handler to catch unhandled Python exceptions, and sends a negative ack to the `StreamingProcessController` when it sees one. The controller then grabs the stdout/stderr contents; writes it to the log/journal file; and throws a java exception. However, there is a shutdown race condition where occasionally the GATK process will get the negative ack and terminate before the Python exception chain is finished processing, and the exception message never appears in the journal. We've seen this happen when the CNN Python inference code runs out of memory. It would be better to have the exception handler write the exception string directly to the ack FIFO, with a message length included, so the controller can deterministically retrieve the message for inclusion in the java exception without having to rely on std in/out.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5100
https://github.com/broadinstitute/gatk/issues/5100:251,Testability,log,log,251,"The `gatktool` Python code installs a system exception handler to catch unhandled Python exceptions, and sends a negative ack to the `StreamingProcessController` when it sees one. The controller then grabs the stdout/stderr contents; writes it to the log/journal file; and throws a java exception. However, there is a shutdown race condition where occasionally the GATK process will get the negative ack and terminate before the Python exception chain is finished processing, and the exception message never appears in the journal. We've seen this happen when the CNN Python inference code runs out of memory. It would be better to have the exception handler write the exception string directly to the ack FIFO, with a message length included, so the controller can deterministically retrieve the message for inclusion in the java exception without having to rely on std in/out.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5100
https://github.com/broadinstitute/gatk/issues/5101:310,Deployability,install,install,310,"#Bug Report. ### Affected tool(s) or class(es); CNNScoreVariant. ### Affected version(s); 4.0.7.0. ### Description ; For 1,297,033 variant sites, the CNN_1D=-16.118. . #### Steps to reproduce; /run_cnn.sh adsp-5k.hg38.GATK.aws-batch_SNP_INDEL.chr22.4794samples.g.vcf.gz; Using GATK jar /share/pkg/gatk/4.0.7.0/install/bin/gatk-package-4.0.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /share/pkf/adsp-5k.hg38.GATK.aws-batch_SNP_INDEL.chr22.4794samples.g.vcf.gz -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O cnn/adsp-5k.hg38.GATK.aws-batch_SNP_INDEL.chr22.4794samples.g.vcf.gz; 11:29:43.339 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.7.0/install/bin/gatk-package-4.0.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:29:43.467 INFO CNNScoreVariants - ------------------------------------------------------------; 11:29:43.467 INFO CNNScoreVariants - The Genome Analysis Toolkit (GATK) v4.0.7.0; 11:29:43.467 INFO CNNScoreVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:29:43.468 INFO CNNScoreVariants - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-696.28.1.el6.x86_64 amd64; 11:29:43.468 INFO CNNScoreVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 11:29:43.469 INFO CNNScoreVariants - Start Date/Time: August 13, 2018 11:29:43 AM UTC; 11:29:43.469 INFO CNNScoreVariants - ------------------------------------------------------------; 11:29:43.469 INFO CNNScoreVariants - ------------------------------------------------------------; 11:29:43.469 INFO CNNScoreVariants - HTSJDK Version: 2.16.0; 11:29:43.469 INFO CNNScoreVariants - Picard Version: 2.18.7; 11:29:43.469 INFO CNNScoreVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:29:43",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5101
https://github.com/broadinstitute/gatk/issues/5101:904,Deployability,install,install,904,"#Bug Report. ### Affected tool(s) or class(es); CNNScoreVariant. ### Affected version(s); 4.0.7.0. ### Description ; For 1,297,033 variant sites, the CNN_1D=-16.118. . #### Steps to reproduce; /run_cnn.sh adsp-5k.hg38.GATK.aws-batch_SNP_INDEL.chr22.4794samples.g.vcf.gz; Using GATK jar /share/pkg/gatk/4.0.7.0/install/bin/gatk-package-4.0.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /share/pkf/adsp-5k.hg38.GATK.aws-batch_SNP_INDEL.chr22.4794samples.g.vcf.gz -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O cnn/adsp-5k.hg38.GATK.aws-batch_SNP_INDEL.chr22.4794samples.g.vcf.gz; 11:29:43.339 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.7.0/install/bin/gatk-package-4.0.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:29:43.467 INFO CNNScoreVariants - ------------------------------------------------------------; 11:29:43.467 INFO CNNScoreVariants - The Genome Analysis Toolkit (GATK) v4.0.7.0; 11:29:43.467 INFO CNNScoreVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:29:43.468 INFO CNNScoreVariants - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-696.28.1.el6.x86_64 amd64; 11:29:43.468 INFO CNNScoreVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 11:29:43.469 INFO CNNScoreVariants - Start Date/Time: August 13, 2018 11:29:43 AM UTC; 11:29:43.469 INFO CNNScoreVariants - ------------------------------------------------------------; 11:29:43.469 INFO CNNScoreVariants - ------------------------------------------------------------; 11:29:43.469 INFO CNNScoreVariants - HTSJDK Version: 2.16.0; 11:29:43.469 INFO CNNScoreVariants - Picard Version: 2.18.7; 11:29:43.469 INFO CNNScoreVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:29:43",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5101
https://github.com/broadinstitute/gatk/issues/5101:2523,Deployability,patch,patch,2523,"NScoreVariants - Start Date/Time: August 13, 2018 11:29:43 AM UTC; 11:29:43.469 INFO CNNScoreVariants - ------------------------------------------------------------; 11:29:43.469 INFO CNNScoreVariants - ------------------------------------------------------------; 11:29:43.469 INFO CNNScoreVariants - HTSJDK Version: 2.16.0; 11:29:43.469 INFO CNNScoreVariants - Picard Version: 2.18.7; 11:29:43.469 INFO CNNScoreVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:29:43.470 INFO CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:29:43.470 INFO CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:29:43.470 INFO CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:29:43.470 INFO CNNScoreVariants - Deflater: IntelDeflater; 11:29:43.470 INFO CNNScoreVariants - Inflater: IntelInflater; 11:29:43.470 INFO CNNScoreVariants - GCS max retries/reopens: 20; 11:29:43.470 INFO CNNScoreVariants - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 11:29:43.470 WARN CNNScoreVariants -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CNNScoreVariants is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 11:29:43.470 INFO CNNScoreVariants - Initializing engine; 11:29:44.086 INFO FeatureManager - Using codec VCFCodec to read file file:///restricted/projectnb/casa/wgs.hg38/sv/gatk.cnn/vcf/adsp-5k.hg38.GATK.aws-batch_SNP_INDEL.chr22.4794samples.g.vcf.gz; 11:29:44.281 INFO CNNScoreVariants - Done initializing engine; 11:29:52.451 INFO CNNScoreVariants - Using key:CNN_1D for CNN architecture:/tmp/farrell/1d_cnn_mix_train_full_bn.3573521081782697200.json and weights:/tmp/farrell/1d_cnn_mix_train_full_bn.1075881893743930029.hd5; 11:29:54.959 INFO ProgressMeter - Starting traversal; 11:29:54.960 INFO ProgressMeter - Current Locus Elapsed",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5101
https://github.com/broadinstitute/gatk/issues/5101:836,Performance,Load,Loading,836,"#Bug Report. ### Affected tool(s) or class(es); CNNScoreVariant. ### Affected version(s); 4.0.7.0. ### Description ; For 1,297,033 variant sites, the CNN_1D=-16.118. . #### Steps to reproduce; /run_cnn.sh adsp-5k.hg38.GATK.aws-batch_SNP_INDEL.chr22.4794samples.g.vcf.gz; Using GATK jar /share/pkg/gatk/4.0.7.0/install/bin/gatk-package-4.0.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /share/pkf/adsp-5k.hg38.GATK.aws-batch_SNP_INDEL.chr22.4794samples.g.vcf.gz -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O cnn/adsp-5k.hg38.GATK.aws-batch_SNP_INDEL.chr22.4794samples.g.vcf.gz; 11:29:43.339 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.7.0/install/bin/gatk-package-4.0.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:29:43.467 INFO CNNScoreVariants - ------------------------------------------------------------; 11:29:43.467 INFO CNNScoreVariants - The Genome Analysis Toolkit (GATK) v4.0.7.0; 11:29:43.467 INFO CNNScoreVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:29:43.468 INFO CNNScoreVariants - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-696.28.1.el6.x86_64 amd64; 11:29:43.468 INFO CNNScoreVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 11:29:43.469 INFO CNNScoreVariants - Start Date/Time: August 13, 2018 11:29:43 AM UTC; 11:29:43.469 INFO CNNScoreVariants - ------------------------------------------------------------; 11:29:43.469 INFO CNNScoreVariants - ------------------------------------------------------------; 11:29:43.469 INFO CNNScoreVariants - HTSJDK Version: 2.16.0; 11:29:43.469 INFO CNNScoreVariants - Picard Version: 2.18.7; 11:29:43.469 INFO CNNScoreVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:29:43",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5101
https://github.com/broadinstitute/gatk/issues/5101:3855,Testability,log,log,3855,"USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:29:43.470 INFO CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:29:43.470 INFO CNNScoreVariants - Deflater: IntelDeflater; 11:29:43.470 INFO CNNScoreVariants - Inflater: IntelInflater; 11:29:43.470 INFO CNNScoreVariants - GCS max retries/reopens: 20; 11:29:43.470 INFO CNNScoreVariants - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 11:29:43.470 WARN CNNScoreVariants -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CNNScoreVariants is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 11:29:43.470 INFO CNNScoreVariants - Initializing engine; 11:29:44.086 INFO FeatureManager - Using codec VCFCodec to read file file:///restricted/projectnb/casa/wgs.hg38/sv/gatk.cnn/vcf/adsp-5k.hg38.GATK.aws-batch_SNP_INDEL.chr22.4794samples.g.vcf.gz; 11:29:44.281 INFO CNNScoreVariants - Done initializing engine; 11:29:52.451 INFO CNNScoreVariants - Using key:CNN_1D for CNN architecture:/tmp/farrell/1d_cnn_mix_train_full_bn.3573521081782697200.json and weights:/tmp/farrell/1d_cnn_mix_train_full_bn.1075881893743930029.hd5; 11:29:54.959 INFO ProgressMeter - Starting traversal; 11:29:54.960 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 11:30:05.581 INFO ProgressMeter - chr22:10585117 0.2 5000 28251.2; 11:30:16.545 INFO ProgressMeter - chr22:10652425 0.4 9000 25018.5; 11:30:27.526 INFO ProgressMeter - chr22:10695391 0.5 14000 25793.8. #### Expected behavior; CNN_1D should have a range of values to indicate the log odds of the site being a true variant. When run on another VCF with just 1 subject, a range of values occurred. The VCF with just one CNN_1D value has 4794 subjects. . #### Actual behavior; The same CNN_1D value (-16.118) was assigned to both PASS and other tranches sites. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5101
https://github.com/broadinstitute/gatk/issues/5103:91,Deployability,pipeline,pipeline,91,Loading the known sites file (e.g. dbsnp_138.hg18.vcf) takes around 6 minutes in the Spark pipeline using `KnownSitesCache`. This ticket is to see if we can improve this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5103
https://github.com/broadinstitute/gatk/issues/5103:0,Performance,Load,Loading,0,Loading the known sites file (e.g. dbsnp_138.hg18.vcf) takes around 6 minutes in the Spark pipeline using `KnownSitesCache`. This ticket is to see if we can improve this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5103
https://github.com/broadinstitute/gatk/pull/5107:139,Deployability,integrat,integration,139,read name mangling should use `replaceAll` instead of `replace`. Discovered in the process of creating test data for more comprehensive SV integration tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5107
https://github.com/broadinstitute/gatk/pull/5107:139,Integrability,integrat,integration,139,read name mangling should use `replaceAll` instead of `replace`. Discovered in the process of creating test data for more comprehensive SV integration tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5107
https://github.com/broadinstitute/gatk/pull/5107:103,Testability,test,test,103,read name mangling should use `replaceAll` instead of `replace`. Discovered in the process of creating test data for more comprehensive SV integration tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5107
https://github.com/broadinstitute/gatk/pull/5107:151,Testability,test,tests,151,read name mangling should use `replaceAll` instead of `replace`. Discovered in the process of creating test data for more comprehensive SV integration tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5107
https://github.com/broadinstitute/gatk/pull/5108:61,Availability,failure,failure,61,"the first commit is the fix, the second is a deliberate test failure so we can validate that the fix works when the tests fail",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5108
https://github.com/broadinstitute/gatk/pull/5108:79,Security,validat,validate,79,"the first commit is the fix, the second is a deliberate test failure so we can validate that the fix works when the tests fail",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5108
https://github.com/broadinstitute/gatk/pull/5108:56,Testability,test,test,56,"the first commit is the fix, the second is a deliberate test failure so we can validate that the fix works when the tests fail",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5108
https://github.com/broadinstitute/gatk/pull/5108:116,Testability,test,tests,116,"the first commit is the fix, the second is a deliberate test failure so we can validate that the fix works when the tests fail",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5108
https://github.com/broadinstitute/gatk/issues/5111:684,Availability,down,downside,684,"## Feature request. ### Tool(s) or class(es) involved. SV pipeline, Funcotator, etc. ### Description. In trying to build test data for SV, time and time again we face the problem of not being able to find actual desired events on the two chromosomes 20 and 21, hence end up having to painfully perform all kinds of coordinate hacks in order to have enough test coverage. It seems that the Funcotator team is also facing a similar issue. Therefore it will be great if the whole reference genome for HG38, and maybe HG19 as well, can be included in the tests, so that tool developers spend less time worrying about hassles in moving real events to chr20 and chr21. One of the potential downside is obvious: it increases the repo size and time for running tests (downloading a bigger file) on Travis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5111
https://github.com/broadinstitute/gatk/issues/5111:760,Availability,down,downloading,760,"## Feature request. ### Tool(s) or class(es) involved. SV pipeline, Funcotator, etc. ### Description. In trying to build test data for SV, time and time again we face the problem of not being able to find actual desired events on the two chromosomes 20 and 21, hence end up having to painfully perform all kinds of coordinate hacks in order to have enough test coverage. It seems that the Funcotator team is also facing a similar issue. Therefore it will be great if the whole reference genome for HG38, and maybe HG19 as well, can be included in the tests, so that tool developers spend less time worrying about hassles in moving real events to chr20 and chr21. One of the potential downside is obvious: it increases the repo size and time for running tests (downloading a bigger file) on Travis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5111
https://github.com/broadinstitute/gatk/issues/5111:58,Deployability,pipeline,pipeline,58,"## Feature request. ### Tool(s) or class(es) involved. SV pipeline, Funcotator, etc. ### Description. In trying to build test data for SV, time and time again we face the problem of not being able to find actual desired events on the two chromosomes 20 and 21, hence end up having to painfully perform all kinds of coordinate hacks in order to have enough test coverage. It seems that the Funcotator team is also facing a similar issue. Therefore it will be great if the whole reference genome for HG38, and maybe HG19 as well, can be included in the tests, so that tool developers spend less time worrying about hassles in moving real events to chr20 and chr21. One of the potential downside is obvious: it increases the repo size and time for running tests (downloading a bigger file) on Travis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5111
https://github.com/broadinstitute/gatk/issues/5111:294,Performance,perform,perform,294,"## Feature request. ### Tool(s) or class(es) involved. SV pipeline, Funcotator, etc. ### Description. In trying to build test data for SV, time and time again we face the problem of not being able to find actual desired events on the two chromosomes 20 and 21, hence end up having to painfully perform all kinds of coordinate hacks in order to have enough test coverage. It seems that the Funcotator team is also facing a similar issue. Therefore it will be great if the whole reference genome for HG38, and maybe HG19 as well, can be included in the tests, so that tool developers spend less time worrying about hassles in moving real events to chr20 and chr21. One of the potential downside is obvious: it increases the repo size and time for running tests (downloading a bigger file) on Travis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5111
https://github.com/broadinstitute/gatk/issues/5111:121,Testability,test,test,121,"## Feature request. ### Tool(s) or class(es) involved. SV pipeline, Funcotator, etc. ### Description. In trying to build test data for SV, time and time again we face the problem of not being able to find actual desired events on the two chromosomes 20 and 21, hence end up having to painfully perform all kinds of coordinate hacks in order to have enough test coverage. It seems that the Funcotator team is also facing a similar issue. Therefore it will be great if the whole reference genome for HG38, and maybe HG19 as well, can be included in the tests, so that tool developers spend less time worrying about hassles in moving real events to chr20 and chr21. One of the potential downside is obvious: it increases the repo size and time for running tests (downloading a bigger file) on Travis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5111
https://github.com/broadinstitute/gatk/issues/5111:356,Testability,test,test,356,"## Feature request. ### Tool(s) or class(es) involved. SV pipeline, Funcotator, etc. ### Description. In trying to build test data for SV, time and time again we face the problem of not being able to find actual desired events on the two chromosomes 20 and 21, hence end up having to painfully perform all kinds of coordinate hacks in order to have enough test coverage. It seems that the Funcotator team is also facing a similar issue. Therefore it will be great if the whole reference genome for HG38, and maybe HG19 as well, can be included in the tests, so that tool developers spend less time worrying about hassles in moving real events to chr20 and chr21. One of the potential downside is obvious: it increases the repo size and time for running tests (downloading a bigger file) on Travis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5111
https://github.com/broadinstitute/gatk/issues/5111:551,Testability,test,tests,551,"## Feature request. ### Tool(s) or class(es) involved. SV pipeline, Funcotator, etc. ### Description. In trying to build test data for SV, time and time again we face the problem of not being able to find actual desired events on the two chromosomes 20 and 21, hence end up having to painfully perform all kinds of coordinate hacks in order to have enough test coverage. It seems that the Funcotator team is also facing a similar issue. Therefore it will be great if the whole reference genome for HG38, and maybe HG19 as well, can be included in the tests, so that tool developers spend less time worrying about hassles in moving real events to chr20 and chr21. One of the potential downside is obvious: it increases the repo size and time for running tests (downloading a bigger file) on Travis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5111
https://github.com/broadinstitute/gatk/issues/5111:753,Testability,test,tests,753,"## Feature request. ### Tool(s) or class(es) involved. SV pipeline, Funcotator, etc. ### Description. In trying to build test data for SV, time and time again we face the problem of not being able to find actual desired events on the two chromosomes 20 and 21, hence end up having to painfully perform all kinds of coordinate hacks in order to have enough test coverage. It seems that the Funcotator team is also facing a similar issue. Therefore it will be great if the whole reference genome for HG38, and maybe HG19 as well, can be included in the tests, so that tool developers spend less time worrying about hassles in moving real events to chr20 and chr21. One of the potential downside is obvious: it increases the repo size and time for running tests (downloading a bigger file) on Travis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5111
https://github.com/broadinstitute/gatk/pull/5112:42,Integrability,depend,dependencies,42,"this allows us to remove the compile time dependencies minicluster and testng and convert them to dependencies of this new artifact; moving utils.test package to testutils and a new source root. the new dependency structure looks like. main <- testUtils; ^ ^; test. one side effect was that commons.math is no longer imported, we were accidentally using this in some places instead of commons.math3 which is what we wanted.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5112
https://github.com/broadinstitute/gatk/pull/5112:98,Integrability,depend,dependencies,98,"this allows us to remove the compile time dependencies minicluster and testng and convert them to dependencies of this new artifact; moving utils.test package to testutils and a new source root. the new dependency structure looks like. main <- testUtils; ^ ^; test. one side effect was that commons.math is no longer imported, we were accidentally using this in some places instead of commons.math3 which is what we wanted.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5112
https://github.com/broadinstitute/gatk/pull/5112:203,Integrability,depend,dependency,203,"this allows us to remove the compile time dependencies minicluster and testng and convert them to dependencies of this new artifact; moving utils.test package to testutils and a new source root. the new dependency structure looks like. main <- testUtils; ^ ^; test. one side effect was that commons.math is no longer imported, we were accidentally using this in some places instead of commons.math3 which is what we wanted.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5112
https://github.com/broadinstitute/gatk/pull/5112:71,Testability,test,testng,71,"this allows us to remove the compile time dependencies minicluster and testng and convert them to dependencies of this new artifact; moving utils.test package to testutils and a new source root. the new dependency structure looks like. main <- testUtils; ^ ^; test. one side effect was that commons.math is no longer imported, we were accidentally using this in some places instead of commons.math3 which is what we wanted.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5112
https://github.com/broadinstitute/gatk/pull/5112:146,Testability,test,test,146,"this allows us to remove the compile time dependencies minicluster and testng and convert them to dependencies of this new artifact; moving utils.test package to testutils and a new source root. the new dependency structure looks like. main <- testUtils; ^ ^; test. one side effect was that commons.math is no longer imported, we were accidentally using this in some places instead of commons.math3 which is what we wanted.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5112
https://github.com/broadinstitute/gatk/pull/5112:162,Testability,test,testutils,162,"this allows us to remove the compile time dependencies minicluster and testng and convert them to dependencies of this new artifact; moving utils.test package to testutils and a new source root. the new dependency structure looks like. main <- testUtils; ^ ^; test. one side effect was that commons.math is no longer imported, we were accidentally using this in some places instead of commons.math3 which is what we wanted.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5112
https://github.com/broadinstitute/gatk/pull/5112:244,Testability,test,testUtils,244,"this allows us to remove the compile time dependencies minicluster and testng and convert them to dependencies of this new artifact; moving utils.test package to testutils and a new source root. the new dependency structure looks like. main <- testUtils; ^ ^; test. one side effect was that commons.math is no longer imported, we were accidentally using this in some places instead of commons.math3 which is what we wanted.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5112
https://github.com/broadinstitute/gatk/pull/5112:260,Testability,test,test,260,"this allows us to remove the compile time dependencies minicluster and testng and convert them to dependencies of this new artifact; moving utils.test package to testutils and a new source root. the new dependency structure looks like. main <- testUtils; ^ ^; test. one side effect was that commons.math is no longer imported, we were accidentally using this in some places instead of commons.math3 which is what we wanted.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5112
https://github.com/broadinstitute/gatk/pull/5115:19,Deployability,Update,Updates,19,- Closes #5114 ; - Updates the WDL as well to expose the new files.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5115
https://github.com/broadinstitute/gatk/pull/5115:46,Security,expose,expose,46,- Closes #5114 ; - Updates the WDL as well to expose the new files.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5115
https://github.com/broadinstitute/gatk/pull/5116:124,Deployability,integrat,integration,124,"Does:. * utils class cleanups; * removing SGA-related phantom test files (not sure why they were not removed previously); * integration test for new interpretation tool (precursor of #5077 ). Note: ; * to fully resolve #5077, we need to deal with #5111 first.; * the large number of files and lines change is mostly due to removing SGA-related phantom test files. @TedBrookings tagging you as the victim for reviewing.; Please feel free to reassign.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5116
https://github.com/broadinstitute/gatk/pull/5116:124,Integrability,integrat,integration,124,"Does:. * utils class cleanups; * removing SGA-related phantom test files (not sure why they were not removed previously); * integration test for new interpretation tool (precursor of #5077 ). Note: ; * to fully resolve #5077, we need to deal with #5111 first.; * the large number of files and lines change is mostly due to removing SGA-related phantom test files. @TedBrookings tagging you as the victim for reviewing.; Please feel free to reassign.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5116
https://github.com/broadinstitute/gatk/pull/5116:62,Testability,test,test,62,"Does:. * utils class cleanups; * removing SGA-related phantom test files (not sure why they were not removed previously); * integration test for new interpretation tool (precursor of #5077 ). Note: ; * to fully resolve #5077, we need to deal with #5111 first.; * the large number of files and lines change is mostly due to removing SGA-related phantom test files. @TedBrookings tagging you as the victim for reviewing.; Please feel free to reassign.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5116
https://github.com/broadinstitute/gatk/pull/5116:136,Testability,test,test,136,"Does:. * utils class cleanups; * removing SGA-related phantom test files (not sure why they were not removed previously); * integration test for new interpretation tool (precursor of #5077 ). Note: ; * to fully resolve #5077, we need to deal with #5111 first.; * the large number of files and lines change is mostly due to removing SGA-related phantom test files. @TedBrookings tagging you as the victim for reviewing.; Please feel free to reassign.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5116
https://github.com/broadinstitute/gatk/pull/5116:352,Testability,test,test,352,"Does:. * utils class cleanups; * removing SGA-related phantom test files (not sure why they were not removed previously); * integration test for new interpretation tool (precursor of #5077 ). Note: ; * to fully resolve #5077, we need to deal with #5111 first.; * the large number of files and lines change is mostly due to removing SGA-related phantom test files. @TedBrookings tagging you as the victim for reviewing.; Please feel free to reassign.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5116
https://github.com/broadinstitute/gatk/pull/5117:156,Performance,perform,perform,156,"Not quite ready to be production level, ; but I've polished it as I reviewed the concordance results given by this tool. Important limitation: this doesn't perform genotype concordance tests, and it's probably better to wait until @vruano 's genotyping module is stable. @cwhelan and @vruano tagging you two as the victim for reviewing.; Also please feel free to propose feature requests.; Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5117
https://github.com/broadinstitute/gatk/pull/5117:185,Testability,test,tests,185,"Not quite ready to be production level, ; but I've polished it as I reviewed the concordance results given by this tool. Important limitation: this doesn't perform genotype concordance tests, and it's probably better to wait until @vruano 's genotyping module is stable. @cwhelan and @vruano tagging you two as the victim for reviewing.; Also please feel free to propose feature requests.; Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5117
https://github.com/broadinstitute/gatk/pull/5119:70,Availability,error,errors,70,"Added a new catch block in `PathLineIterator` for character encoding; errors, along with a new error message to be given to the user for such; cases. Fixes #4006",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5119
https://github.com/broadinstitute/gatk/pull/5119:95,Availability,error,error,95,"Added a new catch block in `PathLineIterator` for character encoding; errors, along with a new error message to be given to the user for such; cases. Fixes #4006",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5119
https://github.com/broadinstitute/gatk/pull/5119:101,Integrability,message,message,101,"Added a new catch block in `PathLineIterator` for character encoding; errors, along with a new error message to be given to the user for such; cases. Fixes #4006",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5119
https://github.com/broadinstitute/gatk/pull/5121:89,Deployability,integrat,integration,89,@LeeTL1220 This fixes the bug preventing Beri from updating to 4.0.8.0. I will put in an integration test but could you start looking at it now?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5121
https://github.com/broadinstitute/gatk/pull/5121:89,Integrability,integrat,integration,89,@LeeTL1220 This fixes the bug preventing Beri from updating to 4.0.8.0. I will put in an integration test but could you start looking at it now?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5121
https://github.com/broadinstitute/gatk/pull/5121:101,Testability,test,test,101,@LeeTL1220 This fixes the bug preventing Beri from updating to 4.0.8.0. I will put in an integration test but could you start looking at it now?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5121
https://github.com/broadinstitute/gatk/pull/5124:70,Availability,error,errors,70,"Added a new catch block in `PathLineIterator` for character encoding; errors, along with a new error message to be given to the user for such; cases. Added unit test for malformed xsv locatable files. Fixes #4006",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5124
https://github.com/broadinstitute/gatk/pull/5124:95,Availability,error,error,95,"Added a new catch block in `PathLineIterator` for character encoding; errors, along with a new error message to be given to the user for such; cases. Added unit test for malformed xsv locatable files. Fixes #4006",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5124
https://github.com/broadinstitute/gatk/pull/5124:101,Integrability,message,message,101,"Added a new catch block in `PathLineIterator` for character encoding; errors, along with a new error message to be given to the user for such; cases. Added unit test for malformed xsv locatable files. Fixes #4006",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5124
https://github.com/broadinstitute/gatk/pull/5124:161,Testability,test,test,161,"Added a new catch block in `PathLineIterator` for character encoding; errors, along with a new error message to be given to the user for such; cases. Added unit test for malformed xsv locatable files. Fixes #4006",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5124
https://github.com/broadinstitute/gatk/pull/5125:7,Deployability,update,update,7,Simple update to use the correct version of Spark in the scripts.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5125
https://github.com/broadinstitute/gatk/pull/5125:0,Usability,Simpl,Simple,0,Simple update to use the correct version of Spark in the scripts.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5125
https://github.com/broadinstitute/gatk/issues/5126:331,Availability,error,error,331,"Hello!. When I try running gatk 4.0.7.0 on spark 2.2 Microsoft Azure hdinsight cluster (using data from here https://gatkforums.broadinstitute.org/gatk/discussion/6484/how-to-generate-an-unmapped-bam-from-fastq-or-aligned-bam#optionA) I get ""java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender"" error like the one below. The thing that gatk-package-4.0.7.0-spark.jar has that class as can be verified by ; $jar tvf gatk-package-4.0.7.0-spark.jar; but nonetheless it seems like it does not load it correctly somehow. $java -version; openjdk version ""1.8.0_171""; OpenJDK Runtime Environment (build 1.8.0_171-8u171-b11-0ubuntu0.16.04.1-b11); OpenJDK 64-Bit Server VM (build 25.171-b11, mixed mode). What would be the possible fix for it?. $./gatk PrintReadsSpark -I ../6484_snippet.bam -O ../output.bam -- --spark-runner SPARK --spark-master spark://10.0.0.21:7077; Using GATK jar /root/gatk-4.0.7.0/gatk-package-4.0.7.0-spark.jar; Running:; /usr/hdp/current/spark2-client/bin/spark-submit --master spark://10.0.0.21:7077 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /root/gatk-4.0.7.0/gatk-package-4.0.7.0-spark.jar PrintReadsSpark -I ../6484_snippet.bam -O ../output.bam --spark-master spark://10.0.0.21:7077; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.40-13/spark2/jars/sl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:5540,Deployability,deploy,deploy,5540,ntextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:230); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.appender.AbstractAppender; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 43 more. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/12624/java-lang-noclassdeffounderror-org-apache-logging-log4j-core-appender-abstractappender/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:5577,Deployability,deploy,deploy,5577,ntextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:230); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.appender.AbstractAppender; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 43 more. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/12624/java-lang-noclassdeffounderror-org-apache-logging-log4j-core-appender-abstractappender/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:5649,Deployability,deploy,deploy,5649,ntextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:230); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.appender.AbstractAppender; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 43 more. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/12624/java-lang-noclassdeffounderror-org-apache-logging-log4j-core-appender-abstractappender/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:5725,Deployability,deploy,deploy,5725,ntextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:230); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.appender.AbstractAppender; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 43 more. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/12624/java-lang-noclassdeffounderror-org-apache-logging-log4j-core-appender-abstractappender/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:5796,Deployability,deploy,deploy,5796,ntextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:230); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.appender.AbstractAppender; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 43 more. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/12624/java-lang-noclassdeffounderror-org-apache-logging-log4j-core-appender-abstractappender/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:5865,Deployability,deploy,deploy,5865,ntextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:230); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.appender.AbstractAppender; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 43 more. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/12624/java-lang-noclassdeffounderror-org-apache-logging-log4j-core-appender-abstractappender/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3294,Modifiability,config,config,3294,.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfigura,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3301,Modifiability,plugin,plugins,3301,: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<i,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3314,Modifiability,Plugin,PluginRegistry,3314, is of type [org.slf4j.impl.Log4jLoggerFactory]; java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3346,Modifiability,Plugin,PluginRegistry,3346,mpl.Log4jLoggerFactory]; java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at o,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3405,Modifiability,config,config,3405,ache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3412,Modifiability,plugin,plugins,3412,ging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3425,Modifiability,Plugin,PluginRegistry,3425,appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3464,Modifiability,Plugin,PluginRegistry,3464,at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3523,Modifiability,config,config,3523,ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelecto,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3530,Modifiability,plugin,plugins,3530,der.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3543,Modifiability,Plugin,PluginManager,3543,ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3572,Modifiability,Plugin,PluginManager,3572,; at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:4167,Modifiability,config,config,4167,loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:4275,Modifiability,config,config,4275,ging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Uti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:525,Performance,load,load,525,"Hello!. When I try running gatk 4.0.7.0 on spark 2.2 Microsoft Azure hdinsight cluster (using data from here https://gatkforums.broadinstitute.org/gatk/discussion/6484/how-to-generate-an-unmapped-bam-from-fastq-or-aligned-bam#optionA) I get ""java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender"" error like the one below. The thing that gatk-package-4.0.7.0-spark.jar has that class as can be verified by ; $jar tvf gatk-package-4.0.7.0-spark.jar; but nonetheless it seems like it does not load it correctly somehow. $java -version; openjdk version ""1.8.0_171""; OpenJDK Runtime Environment (build 1.8.0_171-8u171-b11-0ubuntu0.16.04.1-b11); OpenJDK 64-Bit Server VM (build 25.171-b11, mixed mode). What would be the possible fix for it?. $./gatk PrintReadsSpark -I ../6484_snippet.bam -O ../output.bam -- --spark-runner SPARK --spark-master spark://10.0.0.21:7077; Using GATK jar /root/gatk-4.0.7.0/gatk-package-4.0.7.0-spark.jar; Running:; /usr/hdp/current/spark2-client/bin/spark-submit --master spark://10.0.0.21:7077 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /root/gatk-4.0.7.0/gatk-package-4.0.7.0-spark.jar PrintReadsSpark -I ../6484_snippet.bam -O ../output.bam --spark-master spark://10.0.0.21:7077; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.40-13/spark2/jars/sl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3046,Performance,load,loadClass,3046,r.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.40-13/spark_llap/spark-llap-assembly-1.0.0.2.6.3.40-13.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.lo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3115,Performance,load,loadClass,3115,ark_llap/spark-llap-assembly-1.0.0.2.6.3.40-13.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3170,Performance,load,loadClass,3170,lf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.Abstrac,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3228,Performance,load,loadClass,3228,.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3440,Performance,load,loadFromMainClassLoader,3440,at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:6099,Performance,load,loadClass,6099,ntextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:230); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.appender.AbstractAppender; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 43 more. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/12624/java-lang-noclassdeffounderror-org-apache-logging-log4j-core-appender-abstractappender/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:6168,Performance,load,loadClass,6168,ntextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:230); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.appender.AbstractAppender; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 43 more. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/12624/java-lang-noclassdeffounderror-org-apache-logging-log4j-core-appender-abstractappender/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:6223,Performance,load,loadClass,6223,ntextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:230); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.appender.AbstractAppender; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 43 more. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/12624/java-lang-noclassdeffounderror-org-apache-logging-log4j-core-appender-abstractappender/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:2581,Security,secur,security,2581,_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /root/gatk-4.0.7.0/gatk-package-4.0.7.0-spark.jar PrintReadsSpark -I ../6484_snippet.bam -O ../output.bam --spark-master spark://10.0.0.21:7077; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.40-13/spark2/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.40-13/spark_llap/spark-llap-assembly-1.0.0.2.6.3.40-13.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:2590,Security,Secur,SecureClassLoader,2590,e_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /root/gatk-4.0.7.0/gatk-package-4.0.7.0-spark.jar PrintReadsSpark -I ../6484_snippet.bam -O ../output.bam --spark-master spark://10.0.0.21:7077; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.40-13/spark2/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.40-13/spark_llap/spark-llap-assembly-1.0.0.2.6.3.40-13.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:2620,Security,Secur,SecureClassLoader,2620,.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /root/gatk-4.0.7.0/gatk-package-4.0.7.0-spark.jar PrintReadsSpark -I ../6484_snippet.bam -O ../output.bam --spark-master spark://10.0.0.21:7077; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.40-13/spark2/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.40-13/spark_llap/spark-llap-assembly-1.0.0.2.6.3.40-13.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:2741,Security,access,access,2741,atk-package-4.0.7.0-spark.jar PrintReadsSpark -I ../6484_snippet.bam -O ../output.bam --spark-master spark://10.0.0.21:7077; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.40-13/spark2/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.40-13/spark_llap/spark-llap-assembly-1.0.0.2.6.3.40-13.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternP,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:2903,Security,secur,security,2903,ultiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.40-13/spark2/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.40-13/spark_llap/spark-llap-assembly-1.0.0.2.6.3.40-13.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layou,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:2912,Security,Access,AccessController,2912, bindings.; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.40-13/spark2/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.40-13/spark_llap/spark-llap-assembly-1.0.0.2.6.3.40-13.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:285,Testability,log,logging,285,"Hello!. When I try running gatk 4.0.7.0 on spark 2.2 Microsoft Azure hdinsight cluster (using data from here https://gatkforums.broadinstitute.org/gatk/discussion/6484/how-to-generate-an-unmapped-bam-from-fastq-or-aligned-bam#optionA) I get ""java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender"" error like the one below. The thing that gatk-package-4.0.7.0-spark.jar has that class as can be verified by ; $jar tvf gatk-package-4.0.7.0-spark.jar; but nonetheless it seems like it does not load it correctly somehow. $java -version; openjdk version ""1.8.0_171""; OpenJDK Runtime Environment (build 1.8.0_171-8u171-b11-0ubuntu0.16.04.1-b11); OpenJDK 64-Bit Server VM (build 25.171-b11, mixed mode). What would be the possible fix for it?. $./gatk PrintReadsSpark -I ../6484_snippet.bam -O ../output.bam -- --spark-runner SPARK --spark-master spark://10.0.0.21:7077; Using GATK jar /root/gatk-4.0.7.0/gatk-package-4.0.7.0-spark.jar; Running:; /usr/hdp/current/spark2-client/bin/spark-submit --master spark://10.0.0.21:7077 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /root/gatk-4.0.7.0/gatk-package-4.0.7.0-spark.jar PrintReadsSpark -I ../6484_snippet.bam -O ../output.bam --spark-master spark://10.0.0.21:7077; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.40-13/spark2/jars/sl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:2413,Testability,log,logging,2413,driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /root/gatk-4.0.7.0/gatk-package-4.0.7.0-spark.jar PrintReadsSpark -I ../6484_snippet.bam -O ../output.bam --spark-master spark://10.0.0.21:7077; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.40-13/spark2/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.40-13/spark_llap/spark-llap-assembly-1.0.0.2.6.3.40-13.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3275,Testability,log,logging,3275,for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.conf,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3386,Testability,log,logging,3386,FoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<i,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3504,Testability,log,logging,3504,od); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3611,Testability,log,logging,3611,lass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locate,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3698,Testability,log,logging,3698,.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3785,Testability,log,logging,3785,URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apac,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3884,Testability,log,logging,3884,:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.jav,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:3970,Testability,log,logging,3970,lassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jConte,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:4055,Testability,log,logging,4055,ssLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getCon,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:4148,Testability,log,logging,4148,.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManag,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:4256,Testability,log,logging,4256, at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.h,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:4355,Testability,log,logging,4355,y.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:4374,Testability,Log,LoggerContext,4374,ache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:4395,Testability,Log,LoggerContext,4395,j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Na,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:4433,Testability,log,logging,4433,ry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forNa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:4554,Testability,log,logging,4554,ollectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:230); at org.apache.spark.deploy.SparkSubmit,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:4675,Testability,log,logging,4675,31); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:230); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739); at org.apache.spark.deploy.SparkSubmit$.doRunMain$,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:4792,Testability,log,logging,4792,4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:230); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:4909,Testability,log,logging,4909,ternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:230); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caus,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:5009,Testability,log,logging,5009,it>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:230); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.appender.AbstractAppender; at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:5108,Testability,log,logging,5108,ternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:230); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.appender.AbstractAppender; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(Cla,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:5122,Testability,Log,LogManager,5122,:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:230); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.appender.AbstractAppender; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:42,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:5144,Testability,Log,LogManager,5144,che.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:230); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.appender.AbstractAppender; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:5180,Testability,log,logging,5180,nfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:230); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.appender.AbstractAppender; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launch,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:5194,Testability,Log,LogManager,5194,ToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:230); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.appender.AbstractAppender; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:5215,Testability,Log,LogManager,5215,ctConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:230); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.appender.AbstractAppender; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.Class,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:5965,Testability,log,logging,5965,ntextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:230); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.appender.AbstractAppender; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 43 more. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/12624/java-lang-noclassdeffounderror-org-apache-logging-log4j-core-appender-abstractappender/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/issues/5126:6427,Testability,log,logging-,6427,ntextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:230); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.appender.AbstractAppender; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 43 more. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/12624/java-lang-noclassdeffounderror-org-apache-logging-log4j-core-appender-abstractappender/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126
https://github.com/broadinstitute/gatk/pull/5129:137,Testability,test,tests,137,Attempt to eliminate lines in vcf with only * as ALT when using SelectVariants (change to SelectVariants). Also includes addition of new tests in SelectVariantsIntegrationTest to ensure this is true over many variations of arguments / assignments.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5129
https://github.com/broadinstitute/gatk/issues/5130:229,Deployability,release,release,229,"## Bug Report. ### Affected tool(s) or class(es). MuTect2 for the test case, but any caller using the Reference Bases annotation and calling bases near the end of chromosomes. ### Affected version(s). This occurs with the latest release (4.0.8.1) and not with the previous (4.0.7.0). It appears to be related to the addition of the Orientation Bias filter (#4895) and assessing sequence context:. https://github.com/broadinstitute/gatk/pull/4895/files#diff-07e3c8c33f865c5b32b362afe50cfd86R48. ### Description . When identifying variants near the end of chromosome boundaries, MuTect2 fails with:; ```; java.lang.StringIndexOutOfBoundsException: String index out of range: 369; at java.lang.String.substring(String.java:1963); at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:48); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:270); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:176); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:211); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:212); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:979); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.m",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5130
https://github.com/broadinstitute/gatk/issues/5130:66,Testability,test,test,66,"## Bug Report. ### Affected tool(s) or class(es). MuTect2 for the test case, but any caller using the Reference Bases annotation and calling bases near the end of chromosomes. ### Affected version(s). This occurs with the latest release (4.0.8.1) and not with the previous (4.0.7.0). It appears to be related to the addition of the Orientation Bias filter (#4895) and assessing sequence context:. https://github.com/broadinstitute/gatk/pull/4895/files#diff-07e3c8c33f865c5b32b362afe50cfd86R48. ### Description . When identifying variants near the end of chromosome boundaries, MuTect2 fails with:; ```; java.lang.StringIndexOutOfBoundsException: String index out of range: 369; at java.lang.String.substring(String.java:1963); at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:48); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:270); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:176); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:211); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:212); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:979); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.m",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5130
https://github.com/broadinstitute/gatk/issues/5130:2140,Testability,test,test,2140,lang.StringIndexOutOfBoundsException: String index out of range: 369; at java.lang.String.substring(String.java:1963); at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:48); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:270); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:176); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:211); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:212); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:979); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce. This is a self contained test case with a shell script that demonstrates the issue:. https://s3.amazonaws.com/chapmanb/testcases/gatk/gatk4_mutect2_indexerror.tar.gz. You can adjust the BED file (`TumorOnly-chr22_14000_20001-regions.bed`) to make it not fail. The current failing version is:; ```; chr22 14000 19995; ```; and reducing one base at the end will pass cleanly:; ```; chr22 14000 19994; ```. Thanks for looking at this and please let me know if I can provide any other information.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5130
https://github.com/broadinstitute/gatk/issues/5130:2234,Testability,test,testcases,2234,lang.StringIndexOutOfBoundsException: String index out of range: 369; at java.lang.String.substring(String.java:1963); at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:48); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:270); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:176); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:211); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:212); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:979); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce. This is a self contained test case with a shell script that demonstrates the issue:. https://s3.amazonaws.com/chapmanb/testcases/gatk/gatk4_mutect2_indexerror.tar.gz. You can adjust the BED file (`TumorOnly-chr22_14000_20001-regions.bed`) to make it not fail. The current failing version is:; ```; chr22 14000 19995; ```; and reducing one base at the end will pass cleanly:; ```; chr22 14000 19994; ```. Thanks for looking at this and please let me know if I can provide any other information.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5130
https://github.com/broadinstitute/gatk/pull/5131:141,Deployability,integrat,integration,141,Fixed logging in Funcotator.java; Added a method to the VcfOutputRenderer to fix indel boundaries.; Now VCF and MAF files are compared in an integration test.; Fixed a bug in how data from multiple alleles are parsed into funcotation maps. Fixes #4378,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5131
https://github.com/broadinstitute/gatk/pull/5131:141,Integrability,integrat,integration,141,Fixed logging in Funcotator.java; Added a method to the VcfOutputRenderer to fix indel boundaries.; Now VCF and MAF files are compared in an integration test.; Fixed a bug in how data from multiple alleles are parsed into funcotation maps. Fixes #4378,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5131
https://github.com/broadinstitute/gatk/pull/5131:6,Testability,log,logging,6,Fixed logging in Funcotator.java; Added a method to the VcfOutputRenderer to fix indel boundaries.; Now VCF and MAF files are compared in an integration test.; Fixed a bug in how data from multiple alleles are parsed into funcotation maps. Fixes #4378,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5131
https://github.com/broadinstitute/gatk/pull/5131:153,Testability,test,test,153,Fixed logging in Funcotator.java; Added a method to the VcfOutputRenderer to fix indel boundaries.; Now VCF and MAF files are compared in an integration test.; Fixed a bug in how data from multiple alleles are parsed into funcotation maps. Fixes #4378,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5131
https://github.com/broadinstitute/gatk/pull/5135:262,Deployability,update,updates,262,"The google-cloud-java maintainers have merged a fix for the longstanding issue; https://github.com/GoogleCloudPlatform/google-cloud-java/issues/2453 that prevented us; from running on a modern version of the library, and forced us to run off of a fork.; This PR updates us to the latest release, which incorporates the fix. Resolves #3591; Resolves #3500; Resolves #4986",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5135
https://github.com/broadinstitute/gatk/pull/5135:287,Deployability,release,release,287,"The google-cloud-java maintainers have merged a fix for the longstanding issue; https://github.com/GoogleCloudPlatform/google-cloud-java/issues/2453 that prevented us; from running on a modern version of the library, and forced us to run off of a fork.; This PR updates us to the latest release, which incorporates the fix. Resolves #3591; Resolves #3500; Resolves #4986",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5135
https://github.com/broadinstitute/gatk/issues/5139:452,Deployability,integrat,integration,452,"We've been implementing new features but the coverage in the `sv.utils` package has been non-pretty. Some of the classes like `ExternalCommandlineProgramModule.java` and `GATKSVVCFConstants.java` are OK (`ExternalCommandlineProgramModule` exists for historical reason, when we had SGA as the assembler; `GATKSVVCFConstants` is mostly holding String literals), but other classes are doing actual work and have low coverage. We should test them, not via integration tests only, but also unit tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5139
https://github.com/broadinstitute/gatk/issues/5139:452,Integrability,integrat,integration,452,"We've been implementing new features but the coverage in the `sv.utils` package has been non-pretty. Some of the classes like `ExternalCommandlineProgramModule.java` and `GATKSVVCFConstants.java` are OK (`ExternalCommandlineProgramModule` exists for historical reason, when we had SGA as the assembler; `GATKSVVCFConstants` is mostly holding String literals), but other classes are doing actual work and have low coverage. We should test them, not via integration tests only, but also unit tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5139
https://github.com/broadinstitute/gatk/issues/5139:433,Testability,test,test,433,"We've been implementing new features but the coverage in the `sv.utils` package has been non-pretty. Some of the classes like `ExternalCommandlineProgramModule.java` and `GATKSVVCFConstants.java` are OK (`ExternalCommandlineProgramModule` exists for historical reason, when we had SGA as the assembler; `GATKSVVCFConstants` is mostly holding String literals), but other classes are doing actual work and have low coverage. We should test them, not via integration tests only, but also unit tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5139
https://github.com/broadinstitute/gatk/issues/5139:464,Testability,test,tests,464,"We've been implementing new features but the coverage in the `sv.utils` package has been non-pretty. Some of the classes like `ExternalCommandlineProgramModule.java` and `GATKSVVCFConstants.java` are OK (`ExternalCommandlineProgramModule` exists for historical reason, when we had SGA as the assembler; `GATKSVVCFConstants` is mostly holding String literals), but other classes are doing actual work and have low coverage. We should test them, not via integration tests only, but also unit tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5139
https://github.com/broadinstitute/gatk/issues/5139:490,Testability,test,tests,490,"We've been implementing new features but the coverage in the `sv.utils` package has been non-pretty. Some of the classes like `ExternalCommandlineProgramModule.java` and `GATKSVVCFConstants.java` are OK (`ExternalCommandlineProgramModule` exists for historical reason, when we had SGA as the assembler; `GATKSVVCFConstants` is mostly holding String literals), but other classes are doing actual work and have low coverage. We should test them, not via integration tests only, but also unit tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5139
https://github.com/broadinstitute/gatk/pull/5140:284,Energy Efficiency,charge,charges,284,"Google Cloud Storage has ""requester pays"" buckets. When reading from; those buckets, the requester is billed for the bandwidth (normally, it's; the bucket owner who is billed). This pull request enables this feature with GATK. By default it is; turned off (so there are no unexpected charges). To turn it on, use the; command line argument ""--project-for-requester-pays"" (or; ""--requester-project"") to indicate which project to bill. Example:. $ ./gatk PrintReads --input $INPUT --output=/tmp/reads.bam; fails with: com.google.cloud.storage.StorageException: Bucket is requester pays bucket but no user project provided. $ ./gatk PrintReads --input $INPUT --output=/tmp/reads.bam --requester-project=$PROJECT; works. This PR also removes the argumentless version of; setGlobalNIODefaultOptions() because it was confusing (it uses the; default values, not those indicated by the user - usually we'd expect; the user's values to be taken into account).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5140
https://github.com/broadinstitute/gatk/pull/5142:171,Deployability,release,released,171,"The Intel-optimized version of TensorFlow 1.9 is now the default for Anaconda users. It now supports all processors with AVX - so everything since Sandy Bridge, which was released in 2011. With that in mind, I was thinking we could dispense with two different conda environments and fold everything into the ```gatk``` environment. @samuelklee , I'm the new guy on the Intel team you've been dealing with.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5142
https://github.com/broadinstitute/gatk/pull/5142:153,Integrability,Bridg,Bridge,153,"The Intel-optimized version of TensorFlow 1.9 is now the default for Anaconda users. It now supports all processors with AVX - so everything since Sandy Bridge, which was released in 2011. With that in mind, I was thinking we could dispense with two different conda environments and fold everything into the ```gatk``` environment. @samuelklee , I'm the new guy on the Intel team you've been dealing with.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5142
https://github.com/broadinstitute/gatk/pull/5142:10,Performance,optimiz,optimized,10,"The Intel-optimized version of TensorFlow 1.9 is now the default for Anaconda users. It now supports all processors with AVX - so everything since Sandy Bridge, which was released in 2011. With that in mind, I was thinking we could dispense with two different conda environments and fold everything into the ```gatk``` environment. @samuelklee , I'm the new guy on the Intel team you've been dealing with.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5142
https://github.com/broadinstitute/gatk/issues/5143:259,Performance,cache,cached,259,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests [failed](https://travis-ci.org/broadinstitute/gatk/builds/422089722), including a few of the Funcotator ones:. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance. These may be test artifacts, but we should audit the Funcotator cache access patterns and see if this is actually causing thrashing that affects performance. Since the FeatureCache caching strategy assumes queries will be forward-only, it might be an indication that Funcotator performance could be improved by either turning off caching or using an alternative cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5143
https://github.com/broadinstitute/gatk/issues/5143:687,Performance,cache,cache,687,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests [failed](https://travis-ci.org/broadinstitute/gatk/builds/422089722), including a few of the Funcotator ones:. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance. These may be test artifacts, but we should audit the Funcotator cache access patterns and see if this is actually causing thrashing that affects performance. Since the FeatureCache caching strategy assumes queries will be forward-only, it might be an indication that Funcotator performance could be improved by either turning off caching or using an alternative cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5143
https://github.com/broadinstitute/gatk/issues/5143:768,Performance,perform,performance,768,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests [failed](https://travis-ci.org/broadinstitute/gatk/builds/422089722), including a few of the Funcotator ones:. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance. These may be test artifacts, but we should audit the Funcotator cache access patterns and see if this is actually causing thrashing that affects performance. Since the FeatureCache caching strategy assumes queries will be forward-only, it might be an indication that Funcotator performance could be improved by either turning off caching or using an alternative cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5143
https://github.com/broadinstitute/gatk/issues/5143:901,Performance,perform,performance,901,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests [failed](https://travis-ci.org/broadinstitute/gatk/builds/422089722), including a few of the Funcotator ones:. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance. These may be test artifacts, but we should audit the Funcotator cache access patterns and see if this is actually causing thrashing that affects performance. Since the FeatureCache caching strategy assumes queries will be forward-only, it might be an indication that Funcotator performance could be improved by either turning off caching or using an alternative cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5143
https://github.com/broadinstitute/gatk/issues/5143:985,Performance,cache,cache,985,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests [failed](https://travis-ci.org/broadinstitute/gatk/builds/422089722), including a few of the Funcotator ones:. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance. These may be test artifacts, but we should audit the Funcotator cache access patterns and see if this is actually causing thrashing that affects performance. Since the FeatureCache caching strategy assumes queries will be forward-only, it might be an indication that Funcotator performance could be improved by either turning off caching or using an alternative cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5143
https://github.com/broadinstitute/gatk/issues/5143:666,Security,audit,audit,666,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests [failed](https://travis-ci.org/broadinstitute/gatk/builds/422089722), including a few of the Funcotator ones:. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance. These may be test artifacts, but we should audit the Funcotator cache access patterns and see if this is actually causing thrashing that affects performance. Since the FeatureCache caching strategy assumes queries will be forward-only, it might be an indication that Funcotator performance could be improved by either turning off caching or using an alternative cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5143
https://github.com/broadinstitute/gatk/issues/5143:693,Security,access,access,693,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests [failed](https://travis-ci.org/broadinstitute/gatk/builds/422089722), including a few of the Funcotator ones:. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance. These may be test artifacts, but we should audit the Funcotator cache access patterns and see if this is actually causing thrashing that affects performance. Since the FeatureCache caching strategy assumes queries will be forward-only, it might be an indication that Funcotator performance could be improved by either turning off caching or using an alternative cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5143
https://github.com/broadinstitute/gatk/issues/5143:15,Testability,test,test,15,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests [failed](https://travis-ci.org/broadinstitute/gatk/builds/422089722), including a few of the Funcotator ones:. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance. These may be test artifacts, but we should audit the Funcotator cache access patterns and see if this is actually causing thrashing that affects performance. Since the FeatureCache caching strategy assumes queries will be forward-only, it might be an indication that Funcotator performance could be improved by either turning off caching or using an alternative cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5143
https://github.com/broadinstitute/gatk/issues/5143:275,Testability,test,tests,275,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests [failed](https://travis-ci.org/broadinstitute/gatk/builds/422089722), including a few of the Funcotator ones:. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance. These may be test artifacts, but we should audit the Funcotator cache access patterns and see if this is actually causing thrashing that affects performance. Since the FeatureCache caching strategy assumes queries will be forward-only, it might be an indication that Funcotator performance could be improved by either turning off caching or using an alternative cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5143
https://github.com/broadinstitute/gatk/issues/5143:468,Testability,test,testFuncotatorWithoutValidatingResults,468,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests [failed](https://travis-ci.org/broadinstitute/gatk/builds/422089722), including a few of the Funcotator ones:. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance. These may be test artifacts, but we should audit the Funcotator cache access patterns and see if this is actually causing thrashing that affects performance. Since the FeatureCache caching strategy assumes queries will be forward-only, it might be an indication that Funcotator performance could be improved by either turning off caching or using an alternative cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5143
https://github.com/broadinstitute/gatk/issues/5143:534,Testability,test,testVcfDatasourceAccountsForAltAlleles,534,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests [failed](https://travis-ci.org/broadinstitute/gatk/builds/422089722), including a few of the Funcotator ones:. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance. These may be test artifacts, but we should audit the Funcotator cache access patterns and see if this is actually causing thrashing that affects performance. Since the FeatureCache caching strategy assumes queries will be forward-only, it might be an indication that Funcotator performance could be improved by either turning off caching or using an alternative cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5143
https://github.com/broadinstitute/gatk/issues/5143:600,Testability,test,testVcfMafConcordance,600,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests [failed](https://travis-ci.org/broadinstitute/gatk/builds/422089722), including a few of the Funcotator ones:. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance. These may be test artifacts, but we should audit the Funcotator cache access patterns and see if this is actually causing thrashing that affects performance. Since the FeatureCache caching strategy assumes queries will be forward-only, it might be an indication that Funcotator performance could be improved by either turning off caching or using an alternative cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5143
https://github.com/broadinstitute/gatk/issues/5143:636,Testability,test,test,636,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests [failed](https://travis-ci.org/broadinstitute/gatk/builds/422089722), including a few of the Funcotator ones:. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance. These may be test artifacts, but we should audit the Funcotator cache access patterns and see if this is actually causing thrashing that affects performance. Since the FeatureCache caching strategy assumes queries will be forward-only, it might be an indication that Funcotator performance could be improved by either turning off caching or using an alternative cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5143
https://github.com/broadinstitute/gatk/issues/5145:103,Availability,error,error,103,"### Bug Report; When running the StructuralVariationDiscoveryPipelineSpark, I am getting the following error:. ```; java.lang.UnsatisfiedLinkError:`/tmp/jp102/libfml.833188020007107749.jnilib: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/jp102/libfml.833188020007107749.jnilib); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at org.broadinstitute.hellbender.utils.fermi.FermiLiteAssembler.loadNativeLibrary(FermiLiteAssembler.java:157); at org.broadinstitute.hellbender.utils.fermi.FermiLiteAssembler.<init>(FermiLiteAssembler.java:24); at org.broadinstitute.hellbender.tools.spark.sv.evidence.FermiLiteAssemblyHandler.apply(FermiLiteAssemblyHandler.java:72); at org.broadinstitute.hellbender.tools.spark.sv.evidence.FermiLiteAssemblyHandler.apply(FermiLiteAssemblyHandler.java:23); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.sp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5145
https://github.com/broadinstitute/gatk/issues/5145:2742,Availability,ERROR,ERROR,2742, scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/08/29 10:20:49 ERROR Executor: Exception in task 12.0 in stage 12.0 (TID 3228); ```. I am running version 4.0.8.1 of GATK using openjdk version 1.8.0_212.; The command I am using is:. ```; gatk StructuralVariationDiscoveryPipelineSpark \; --aligner-index-image refrance.fasta.img \; --contig-sam-file contigs-aligned.sam \; --spark-master local[30] \; --kmers-to-ignore kmers_to_ignore.txt \; -R $fasta \; -I $sample.bam \; -O $sample.vcf; ```. Thanks for taking a look!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5145
https://github.com/broadinstitute/gatk/issues/5145:2333,Energy Efficiency,schedul,scheduler,2333, scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/08/29 10:20:49 ERROR Executor: Exception in task 12.0 in stage 12.0 (TID 3228); ```. I am running version 4.0.8.1 of GATK using openjdk version 1.8.0_212.; The command I am using is:. ```; gatk StructuralVariationDiscoveryPipelineSpark \; --aligner-index-image refrance.fasta.img \; --contig-sam-file contigs-aligned.sam \; --spark-master local[30] \; --kmers-to-ignore kmers_to_ignore.txt \; -R $fasta \; -I $sample.bam \; -O $sample.vcf; ```. Thanks for taking a look!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5145
https://github.com/broadinstitute/gatk/issues/5145:2404,Energy Efficiency,schedul,scheduler,2404, scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/08/29 10:20:49 ERROR Executor: Exception in task 12.0 in stage 12.0 (TID 3228); ```. I am running version 4.0.8.1 of GATK using openjdk version 1.8.0_212.; The command I am using is:. ```; gatk StructuralVariationDiscoveryPipelineSpark \; --aligner-index-image refrance.fasta.img \; --contig-sam-file contigs-aligned.sam \; --spark-master local[30] \; --kmers-to-ignore kmers_to_ignore.txt \; -R $fasta \; -I $sample.bam \; -O $sample.vcf; ```. Thanks for taking a look!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5145
https://github.com/broadinstitute/gatk/issues/5145:340,Performance,load,load,340,"### Bug Report; When running the StructuralVariationDiscoveryPipelineSpark, I am getting the following error:. ```; java.lang.UnsatisfiedLinkError:`/tmp/jp102/libfml.833188020007107749.jnilib: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/jp102/libfml.833188020007107749.jnilib); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at org.broadinstitute.hellbender.utils.fermi.FermiLiteAssembler.loadNativeLibrary(FermiLiteAssembler.java:157); at org.broadinstitute.hellbender.utils.fermi.FermiLiteAssembler.<init>(FermiLiteAssembler.java:24); at org.broadinstitute.hellbender.tools.spark.sv.evidence.FermiLiteAssemblyHandler.apply(FermiLiteAssemblyHandler.java:72); at org.broadinstitute.hellbender.tools.spark.sv.evidence.FermiLiteAssemblyHandler.apply(FermiLiteAssemblyHandler.java:23); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.sp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5145
https://github.com/broadinstitute/gatk/issues/5145:448,Performance,load,loadLibrary,448,"### Bug Report; When running the StructuralVariationDiscoveryPipelineSpark, I am getting the following error:. ```; java.lang.UnsatisfiedLinkError:`/tmp/jp102/libfml.833188020007107749.jnilib: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/jp102/libfml.833188020007107749.jnilib); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at org.broadinstitute.hellbender.utils.fermi.FermiLiteAssembler.loadNativeLibrary(FermiLiteAssembler.java:157); at org.broadinstitute.hellbender.utils.fermi.FermiLiteAssembler.<init>(FermiLiteAssembler.java:24); at org.broadinstitute.hellbender.tools.spark.sv.evidence.FermiLiteAssemblyHandler.apply(FermiLiteAssemblyHandler.java:72); at org.broadinstitute.hellbender.tools.spark.sv.evidence.FermiLiteAssemblyHandler.apply(FermiLiteAssemblyHandler.java:23); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.sp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5145
https://github.com/broadinstitute/gatk/issues/5145:550,Performance,load,load,550,"### Bug Report; When running the StructuralVariationDiscoveryPipelineSpark, I am getting the following error:. ```; java.lang.UnsatisfiedLinkError:`/tmp/jp102/libfml.833188020007107749.jnilib: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/jp102/libfml.833188020007107749.jnilib); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at org.broadinstitute.hellbender.utils.fermi.FermiLiteAssembler.loadNativeLibrary(FermiLiteAssembler.java:157); at org.broadinstitute.hellbender.utils.fermi.FermiLiteAssembler.<init>(FermiLiteAssembler.java:24); at org.broadinstitute.hellbender.tools.spark.sv.evidence.FermiLiteAssemblyHandler.apply(FermiLiteAssemblyHandler.java:72); at org.broadinstitute.hellbender.tools.spark.sv.evidence.FermiLiteAssemblyHandler.apply(FermiLiteAssemblyHandler.java:23); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.sp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5145
https://github.com/broadinstitute/gatk/issues/5145:638,Performance,load,loadNativeLibrary,638,"### Bug Report; When running the StructuralVariationDiscoveryPipelineSpark, I am getting the following error:. ```; java.lang.UnsatisfiedLinkError:`/tmp/jp102/libfml.833188020007107749.jnilib: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/jp102/libfml.833188020007107749.jnilib); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at org.broadinstitute.hellbender.utils.fermi.FermiLiteAssembler.loadNativeLibrary(FermiLiteAssembler.java:157); at org.broadinstitute.hellbender.utils.fermi.FermiLiteAssembler.<init>(FermiLiteAssembler.java:24); at org.broadinstitute.hellbender.tools.spark.sv.evidence.FermiLiteAssemblyHandler.apply(FermiLiteAssemblyHandler.java:72); at org.broadinstitute.hellbender.tools.spark.sv.evidence.FermiLiteAssemblyHandler.apply(FermiLiteAssemblyHandler.java:23); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.sp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5145
https://github.com/broadinstitute/gatk/issues/5145:2527,Performance,concurren,concurrent,2527, scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/08/29 10:20:49 ERROR Executor: Exception in task 12.0 in stage 12.0 (TID 3228); ```. I am running version 4.0.8.1 of GATK using openjdk version 1.8.0_212.; The command I am using is:. ```; gatk StructuralVariationDiscoveryPipelineSpark \; --aligner-index-image refrance.fasta.img \; --contig-sam-file contigs-aligned.sam \; --spark-master local[30] \; --kmers-to-ignore kmers_to_ignore.txt \; -R $fasta \; -I $sample.bam \; -O $sample.vcf; ```. Thanks for taking a look!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5145
https://github.com/broadinstitute/gatk/issues/5145:2611,Performance,concurren,concurrent,2611, scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/08/29 10:20:49 ERROR Executor: Exception in task 12.0 in stage 12.0 (TID 3228); ```. I am running version 4.0.8.1 of GATK using openjdk version 1.8.0_212.; The command I am using is:. ```; gatk StructuralVariationDiscoveryPipelineSpark \; --aligner-index-image refrance.fasta.img \; --contig-sam-file contigs-aligned.sam \; --spark-master local[30] \; --kmers-to-ignore kmers_to_ignore.txt \; -R $fasta \; -I $sample.bam \; -O $sample.vcf; ```. Thanks for taking a look!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5145
https://github.com/broadinstitute/gatk/pull/5146:57,Integrability,interface,interface,57,"Addded abstract class MachineLearningUtils to provide an interface and; handle common tasks. These include loading data, splitting data into; training and test sets, cross-validation, and optimizing classifier; hyperparameters. Also added XGBoostUtils which provides a concrete implemention of; MachineLearningUtils (by wrapping xgboost4j) and serves as an example; of how to provide access to a 3rd-party machine learning library. Finally, added an example tool: ExampleTrainXGBoostClassifier. This; demonstrates a typical training use case of loading data, training a; classifier, assessing accuracy, and saving the classifier. It also; demonstrates a typical filtering use case of loading a saved classifer,; and using it to calculate probabilities or class labels. This is working towards issue 4922 by providing the tools necessary to; train classifiers in general, but does not provide tools to train a; BreakpointEvidence filter, so does not resolve it. Additionally, this; framework should eventually be extended to provide a bayesian; hyperparameter optimizer. One outstanding problem with these changes is that xgboost4j threading; does not appear to work on OSX, resulting in slower training. However,; it does work on linux.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5146
https://github.com/broadinstitute/gatk/pull/5146:320,Integrability,wrap,wrapping,320,"Addded abstract class MachineLearningUtils to provide an interface and; handle common tasks. These include loading data, splitting data into; training and test sets, cross-validation, and optimizing classifier; hyperparameters. Also added XGBoostUtils which provides a concrete implemention of; MachineLearningUtils (by wrapping xgboost4j) and serves as an example; of how to provide access to a 3rd-party machine learning library. Finally, added an example tool: ExampleTrainXGBoostClassifier. This; demonstrates a typical training use case of loading data, training a; classifier, assessing accuracy, and saving the classifier. It also; demonstrates a typical filtering use case of loading a saved classifer,; and using it to calculate probabilities or class labels. This is working towards issue 4922 by providing the tools necessary to; train classifiers in general, but does not provide tools to train a; BreakpointEvidence filter, so does not resolve it. Additionally, this; framework should eventually be extended to provide a bayesian; hyperparameter optimizer. One outstanding problem with these changes is that xgboost4j threading; does not appear to work on OSX, resulting in slower training. However,; it does work on linux.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5146
https://github.com/broadinstitute/gatk/pull/5146:1012,Modifiability,extend,extended,1012,"Addded abstract class MachineLearningUtils to provide an interface and; handle common tasks. These include loading data, splitting data into; training and test sets, cross-validation, and optimizing classifier; hyperparameters. Also added XGBoostUtils which provides a concrete implemention of; MachineLearningUtils (by wrapping xgboost4j) and serves as an example; of how to provide access to a 3rd-party machine learning library. Finally, added an example tool: ExampleTrainXGBoostClassifier. This; demonstrates a typical training use case of loading data, training a; classifier, assessing accuracy, and saving the classifier. It also; demonstrates a typical filtering use case of loading a saved classifer,; and using it to calculate probabilities or class labels. This is working towards issue 4922 by providing the tools necessary to; train classifiers in general, but does not provide tools to train a; BreakpointEvidence filter, so does not resolve it. Additionally, this; framework should eventually be extended to provide a bayesian; hyperparameter optimizer. One outstanding problem with these changes is that xgboost4j threading; does not appear to work on OSX, resulting in slower training. However,; it does work on linux.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5146
https://github.com/broadinstitute/gatk/pull/5146:107,Performance,load,loading,107,"Addded abstract class MachineLearningUtils to provide an interface and; handle common tasks. These include loading data, splitting data into; training and test sets, cross-validation, and optimizing classifier; hyperparameters. Also added XGBoostUtils which provides a concrete implemention of; MachineLearningUtils (by wrapping xgboost4j) and serves as an example; of how to provide access to a 3rd-party machine learning library. Finally, added an example tool: ExampleTrainXGBoostClassifier. This; demonstrates a typical training use case of loading data, training a; classifier, assessing accuracy, and saving the classifier. It also; demonstrates a typical filtering use case of loading a saved classifer,; and using it to calculate probabilities or class labels. This is working towards issue 4922 by providing the tools necessary to; train classifiers in general, but does not provide tools to train a; BreakpointEvidence filter, so does not resolve it. Additionally, this; framework should eventually be extended to provide a bayesian; hyperparameter optimizer. One outstanding problem with these changes is that xgboost4j threading; does not appear to work on OSX, resulting in slower training. However,; it does work on linux.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5146
https://github.com/broadinstitute/gatk/pull/5146:188,Performance,optimiz,optimizing,188,"Addded abstract class MachineLearningUtils to provide an interface and; handle common tasks. These include loading data, splitting data into; training and test sets, cross-validation, and optimizing classifier; hyperparameters. Also added XGBoostUtils which provides a concrete implemention of; MachineLearningUtils (by wrapping xgboost4j) and serves as an example; of how to provide access to a 3rd-party machine learning library. Finally, added an example tool: ExampleTrainXGBoostClassifier. This; demonstrates a typical training use case of loading data, training a; classifier, assessing accuracy, and saving the classifier. It also; demonstrates a typical filtering use case of loading a saved classifer,; and using it to calculate probabilities or class labels. This is working towards issue 4922 by providing the tools necessary to; train classifiers in general, but does not provide tools to train a; BreakpointEvidence filter, so does not resolve it. Additionally, this; framework should eventually be extended to provide a bayesian; hyperparameter optimizer. One outstanding problem with these changes is that xgboost4j threading; does not appear to work on OSX, resulting in slower training. However,; it does work on linux.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5146
https://github.com/broadinstitute/gatk/pull/5146:545,Performance,load,loading,545,"Addded abstract class MachineLearningUtils to provide an interface and; handle common tasks. These include loading data, splitting data into; training and test sets, cross-validation, and optimizing classifier; hyperparameters. Also added XGBoostUtils which provides a concrete implemention of; MachineLearningUtils (by wrapping xgboost4j) and serves as an example; of how to provide access to a 3rd-party machine learning library. Finally, added an example tool: ExampleTrainXGBoostClassifier. This; demonstrates a typical training use case of loading data, training a; classifier, assessing accuracy, and saving the classifier. It also; demonstrates a typical filtering use case of loading a saved classifer,; and using it to calculate probabilities or class labels. This is working towards issue 4922 by providing the tools necessary to; train classifiers in general, but does not provide tools to train a; BreakpointEvidence filter, so does not resolve it. Additionally, this; framework should eventually be extended to provide a bayesian; hyperparameter optimizer. One outstanding problem with these changes is that xgboost4j threading; does not appear to work on OSX, resulting in slower training. However,; it does work on linux.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5146
https://github.com/broadinstitute/gatk/pull/5146:684,Performance,load,loading,684,"Addded abstract class MachineLearningUtils to provide an interface and; handle common tasks. These include loading data, splitting data into; training and test sets, cross-validation, and optimizing classifier; hyperparameters. Also added XGBoostUtils which provides a concrete implemention of; MachineLearningUtils (by wrapping xgboost4j) and serves as an example; of how to provide access to a 3rd-party machine learning library. Finally, added an example tool: ExampleTrainXGBoostClassifier. This; demonstrates a typical training use case of loading data, training a; classifier, assessing accuracy, and saving the classifier. It also; demonstrates a typical filtering use case of loading a saved classifer,; and using it to calculate probabilities or class labels. This is working towards issue 4922 by providing the tools necessary to; train classifiers in general, but does not provide tools to train a; BreakpointEvidence filter, so does not resolve it. Additionally, this; framework should eventually be extended to provide a bayesian; hyperparameter optimizer. One outstanding problem with these changes is that xgboost4j threading; does not appear to work on OSX, resulting in slower training. However,; it does work on linux.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5146
https://github.com/broadinstitute/gatk/pull/5146:1059,Performance,optimiz,optimizer,1059,"Addded abstract class MachineLearningUtils to provide an interface and; handle common tasks. These include loading data, splitting data into; training and test sets, cross-validation, and optimizing classifier; hyperparameters. Also added XGBoostUtils which provides a concrete implemention of; MachineLearningUtils (by wrapping xgboost4j) and serves as an example; of how to provide access to a 3rd-party machine learning library. Finally, added an example tool: ExampleTrainXGBoostClassifier. This; demonstrates a typical training use case of loading data, training a; classifier, assessing accuracy, and saving the classifier. It also; demonstrates a typical filtering use case of loading a saved classifer,; and using it to calculate probabilities or class labels. This is working towards issue 4922 by providing the tools necessary to; train classifiers in general, but does not provide tools to train a; BreakpointEvidence filter, so does not resolve it. Additionally, this; framework should eventually be extended to provide a bayesian; hyperparameter optimizer. One outstanding problem with these changes is that xgboost4j threading; does not appear to work on OSX, resulting in slower training. However,; it does work on linux.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5146
https://github.com/broadinstitute/gatk/pull/5146:172,Security,validat,validation,172,"Addded abstract class MachineLearningUtils to provide an interface and; handle common tasks. These include loading data, splitting data into; training and test sets, cross-validation, and optimizing classifier; hyperparameters. Also added XGBoostUtils which provides a concrete implemention of; MachineLearningUtils (by wrapping xgboost4j) and serves as an example; of how to provide access to a 3rd-party machine learning library. Finally, added an example tool: ExampleTrainXGBoostClassifier. This; demonstrates a typical training use case of loading data, training a; classifier, assessing accuracy, and saving the classifier. It also; demonstrates a typical filtering use case of loading a saved classifer,; and using it to calculate probabilities or class labels. This is working towards issue 4922 by providing the tools necessary to; train classifiers in general, but does not provide tools to train a; BreakpointEvidence filter, so does not resolve it. Additionally, this; framework should eventually be extended to provide a bayesian; hyperparameter optimizer. One outstanding problem with these changes is that xgboost4j threading; does not appear to work on OSX, resulting in slower training. However,; it does work on linux.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5146
https://github.com/broadinstitute/gatk/pull/5146:384,Security,access,access,384,"Addded abstract class MachineLearningUtils to provide an interface and; handle common tasks. These include loading data, splitting data into; training and test sets, cross-validation, and optimizing classifier; hyperparameters. Also added XGBoostUtils which provides a concrete implemention of; MachineLearningUtils (by wrapping xgboost4j) and serves as an example; of how to provide access to a 3rd-party machine learning library. Finally, added an example tool: ExampleTrainXGBoostClassifier. This; demonstrates a typical training use case of loading data, training a; classifier, assessing accuracy, and saving the classifier. It also; demonstrates a typical filtering use case of loading a saved classifer,; and using it to calculate probabilities or class labels. This is working towards issue 4922 by providing the tools necessary to; train classifiers in general, but does not provide tools to train a; BreakpointEvidence filter, so does not resolve it. Additionally, this; framework should eventually be extended to provide a bayesian; hyperparameter optimizer. One outstanding problem with these changes is that xgboost4j threading; does not appear to work on OSX, resulting in slower training. However,; it does work on linux.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5146
https://github.com/broadinstitute/gatk/pull/5146:155,Testability,test,test,155,"Addded abstract class MachineLearningUtils to provide an interface and; handle common tasks. These include loading data, splitting data into; training and test sets, cross-validation, and optimizing classifier; hyperparameters. Also added XGBoostUtils which provides a concrete implemention of; MachineLearningUtils (by wrapping xgboost4j) and serves as an example; of how to provide access to a 3rd-party machine learning library. Finally, added an example tool: ExampleTrainXGBoostClassifier. This; demonstrates a typical training use case of loading data, training a; classifier, assessing accuracy, and saving the classifier. It also; demonstrates a typical filtering use case of loading a saved classifer,; and using it to calculate probabilities or class labels. This is working towards issue 4922 by providing the tools necessary to; train classifiers in general, but does not provide tools to train a; BreakpointEvidence filter, so does not resolve it. Additionally, this; framework should eventually be extended to provide a bayesian; hyperparameter optimizer. One outstanding problem with these changes is that xgboost4j threading; does not appear to work on OSX, resulting in slower training. However,; it does work on linux.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5146
https://github.com/broadinstitute/gatk/pull/5146:414,Usability,learn,learning,414,"Addded abstract class MachineLearningUtils to provide an interface and; handle common tasks. These include loading data, splitting data into; training and test sets, cross-validation, and optimizing classifier; hyperparameters. Also added XGBoostUtils which provides a concrete implemention of; MachineLearningUtils (by wrapping xgboost4j) and serves as an example; of how to provide access to a 3rd-party machine learning library. Finally, added an example tool: ExampleTrainXGBoostClassifier. This; demonstrates a typical training use case of loading data, training a; classifier, assessing accuracy, and saving the classifier. It also; demonstrates a typical filtering use case of loading a saved classifer,; and using it to calculate probabilities or class labels. This is working towards issue 4922 by providing the tools necessary to; train classifiers in general, but does not provide tools to train a; BreakpointEvidence filter, so does not resolve it. Additionally, this; framework should eventually be extended to provide a bayesian; hyperparameter optimizer. One outstanding problem with these changes is that xgboost4j threading; does not appear to work on OSX, resulting in slower training. However,; it does work on linux.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5146
https://github.com/broadinstitute/gatk/issues/5148:680,Availability,failure,failures,680,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests failed, including a few of the Mutect2/HC ones:. Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly ; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode. The FeatureCache assumes that queries are always increasing along a contig; the failures in this branch indicate that the caller is attempting to back up and re-query territory that has already been cached and then trimmed. I didn't track down all of these cases, but the general pattern appears to be that active region determination results in initial caching and trimming, and then the same/similar territory is traversed again during calling, resulting in cache misses. It happens pretty frequently when running M2 tests, at least for pon and germline resource inputs; we should investigate how much a better caching strategy would help performance. If it would, we'd need https://github.com/broadinstitute/gatk/pull/4902 at a mimimum in order to use a alternate cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5148
https://github.com/broadinstitute/gatk/issues/5148:839,Availability,down,down,839,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests failed, including a few of the Mutect2/HC ones:. Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly ; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode. The FeatureCache assumes that queries are always increasing along a contig; the failures in this branch indicate that the caller is attempting to back up and re-query territory that has already been cached and then trimmed. I didn't track down all of these cases, but the general pattern appears to be that active region determination results in initial caching and trimming, and then the same/similar territory is traversed again during calling, resulting in cache misses. It happens pretty frequently when running M2 tests, at least for pon and germline resource inputs; we should investigate how much a better caching strategy would help performance. If it would, we'd need https://github.com/broadinstitute/gatk/pull/4902 at a mimimum in order to use a alternate cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5148
https://github.com/broadinstitute/gatk/issues/5148:259,Performance,cache,cached,259,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests failed, including a few of the Mutect2/HC ones:. Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly ; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode. The FeatureCache assumes that queries are always increasing along a contig; the failures in this branch indicate that the caller is attempting to back up and re-query territory that has already been cached and then trimmed. I didn't track down all of these cases, but the general pattern appears to be that active region determination results in initial caching and trimming, and then the same/similar territory is traversed again during calling, resulting in cache misses. It happens pretty frequently when running M2 tests, at least for pon and germline resource inputs; we should investigate how much a better caching strategy would help performance. If it would, we'd need https://github.com/broadinstitute/gatk/pull/4902 at a mimimum in order to use a alternate cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5148
https://github.com/broadinstitute/gatk/issues/5148:799,Performance,cache,cached,799,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests failed, including a few of the Mutect2/HC ones:. Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly ; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode. The FeatureCache assumes that queries are always increasing along a contig; the failures in this branch indicate that the caller is attempting to back up and re-query territory that has already been cached and then trimmed. I didn't track down all of these cases, but the general pattern appears to be that active region determination results in initial caching and trimming, and then the same/similar territory is traversed again during calling, resulting in cache misses. It happens pretty frequently when running M2 tests, at least for pon and germline resource inputs; we should investigate how much a better caching strategy would help performance. If it would, we'd need https://github.com/broadinstitute/gatk/pull/4902 at a mimimum in order to use a alternate cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5148
https://github.com/broadinstitute/gatk/issues/5148:1060,Performance,cache,cache,1060,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests failed, including a few of the Mutect2/HC ones:. Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly ; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode. The FeatureCache assumes that queries are always increasing along a contig; the failures in this branch indicate that the caller is attempting to back up and re-query territory that has already been cached and then trimmed. I didn't track down all of these cases, but the general pattern appears to be that active region determination results in initial caching and trimming, and then the same/similar territory is traversed again during calling, resulting in cache misses. It happens pretty frequently when running M2 tests, at least for pon and germline resource inputs; we should investigate how much a better caching strategy would help performance. If it would, we'd need https://github.com/broadinstitute/gatk/pull/4902 at a mimimum in order to use a alternate cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5148
https://github.com/broadinstitute/gatk/issues/5148:1241,Performance,perform,performance,1241,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests failed, including a few of the Mutect2/HC ones:. Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly ; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode. The FeatureCache assumes that queries are always increasing along a contig; the failures in this branch indicate that the caller is attempting to back up and re-query territory that has already been cached and then trimmed. I didn't track down all of these cases, but the general pattern appears to be that active region determination results in initial caching and trimming, and then the same/similar territory is traversed again during calling, resulting in cache misses. It happens pretty frequently when running M2 tests, at least for pon and germline resource inputs; we should investigate how much a better caching strategy would help performance. If it would, we'd need https://github.com/broadinstitute/gatk/pull/4902 at a mimimum in order to use a alternate cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5148
https://github.com/broadinstitute/gatk/issues/5148:1367,Performance,cache,cache,1367,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests failed, including a few of the Mutect2/HC ones:. Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly ; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode. The FeatureCache assumes that queries are always increasing along a contig; the failures in this branch indicate that the caller is attempting to back up and re-query territory that has already been cached and then trimmed. I didn't track down all of these cases, but the general pattern appears to be that active region determination results in initial caching and trimming, and then the same/similar territory is traversed again during calling, resulting in cache misses. It happens pretty frequently when running M2 tests, at least for pon and germline resource inputs; we should investigate how much a better caching strategy would help performance. If it would, we'd need https://github.com/broadinstitute/gatk/pull/4902 at a mimimum in order to use a alternate cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5148
https://github.com/broadinstitute/gatk/issues/5148:15,Testability,test,test,15,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests failed, including a few of the Mutect2/HC ones:. Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly ; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode. The FeatureCache assumes that queries are always increasing along a contig; the failures in this branch indicate that the caller is attempting to back up and re-query territory that has already been cached and then trimmed. I didn't track down all of these cases, but the general pattern appears to be that active region determination results in initial caching and trimming, and then the same/similar territory is traversed again during calling, resulting in cache misses. It happens pretty frequently when running M2 tests, at least for pon and germline resource inputs; we should investigate how much a better caching strategy would help performance. If it would, we'd need https://github.com/broadinstitute/gatk/pull/4902 at a mimimum in order to use a alternate cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5148
https://github.com/broadinstitute/gatk/issues/5148:275,Testability,test,tests,275,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests failed, including a few of the Mutect2/HC ones:. Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly ; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode. The FeatureCache assumes that queries are always increasing along a contig; the failures in this branch indicate that the caller is attempting to back up and re-query territory that has already been cached and then trimmed. I didn't track down all of these cases, but the general pattern appears to be that active region determination results in initial caching and trimming, and then the same/similar territory is traversed again during calling, resulting in cache misses. It happens pretty frequently when running M2 tests, at least for pon and germline resource inputs; we should investigate how much a better caching strategy would help performance. If it would, we'd need https://github.com/broadinstitute/gatk/pull/4902 at a mimimum in order to use a alternate cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5148
https://github.com/broadinstitute/gatk/issues/5148:353,Testability,test,testContaminationFilter,353,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests failed, including a few of the Mutect2/HC ones:. Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly ; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode. The FeatureCache assumes that queries are always increasing along a contig; the failures in this branch indicate that the caller is attempting to back up and re-query territory that has already been cached and then trimmed. I didn't track down all of these cases, but the general pattern appears to be that active region determination results in initial caching and trimming, and then the same/similar territory is traversed again during calling, resulting in cache misses. It happens pretty frequently when running M2 tests, at least for pon and germline resource inputs; we should investigate how much a better caching strategy would help performance. If it would, we'd need https://github.com/broadinstitute/gatk/pull/4902 at a mimimum in order to use a alternate cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5148
https://github.com/broadinstitute/gatk/issues/5148:401,Testability,test,testDreamTumorNormal,401,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests failed, including a few of the Mutect2/HC ones:. Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly ; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode. The FeatureCache assumes that queries are always increasing along a contig; the failures in this branch indicate that the caller is attempting to back up and re-query territory that has already been cached and then trimmed. I didn't track down all of these cases, but the general pattern appears to be that active region determination results in initial caching and trimming, and then the same/similar territory is traversed again during calling, resulting in cache misses. It happens pretty frequently when running M2 tests, at least for pon and germline resource inputs; we should investigate how much a better caching strategy would help performance. If it would, we'd need https://github.com/broadinstitute/gatk/pull/4902 at a mimimum in order to use a alternate cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5148
https://github.com/broadinstitute/gatk/issues/5148:446,Testability,test,testGivenAllelesMode,446,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests failed, including a few of the Mutect2/HC ones:. Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly ; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode. The FeatureCache assumes that queries are always increasing along a contig; the failures in this branch indicate that the caller is attempting to back up and re-query territory that has already been cached and then trimmed. I didn't track down all of these cases, but the general pattern appears to be that active region determination results in initial caching and trimming, and then the same/similar territory is traversed again during calling, resulting in cache misses. It happens pretty frequently when running M2 tests, at least for pon and germline resource inputs; we should investigate how much a better caching strategy would help performance. If it would, we'd need https://github.com/broadinstitute/gatk/pull/4902 at a mimimum in order to use a alternate cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5148
https://github.com/broadinstitute/gatk/issues/5148:491,Testability,test,testPon,491,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests failed, including a few of the Mutect2/HC ones:. Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly ; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode. The FeatureCache assumes that queries are always increasing along a contig; the failures in this branch indicate that the caller is attempting to back up and re-query territory that has already been cached and then trimmed. I didn't track down all of these cases, but the general pattern appears to be that active region determination results in initial caching and trimming, and then the same/similar territory is traversed again during calling, resulting in cache misses. It happens pretty frequently when running M2 tests, at least for pon and germline resource inputs; we should investigate how much a better caching strategy would help performance. If it would, we'd need https://github.com/broadinstitute/gatk/pull/4902 at a mimimum in order to use a alternate cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5148
https://github.com/broadinstitute/gatk/issues/5148:523,Testability,test,testTumorOnly,523,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests failed, including a few of the Mutect2/HC ones:. Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly ; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode. The FeatureCache assumes that queries are always increasing along a contig; the failures in this branch indicate that the caller is attempting to back up and re-query territory that has already been cached and then trimmed. I didn't track down all of these cases, but the general pattern appears to be that active region determination results in initial caching and trimming, and then the same/similar territory is traversed again during calling, resulting in cache misses. It happens pretty frequently when running M2 tests, at least for pon and germline resource inputs; we should investigate how much a better caching strategy would help performance. If it would, we'd need https://github.com/broadinstitute/gatk/pull/4902 at a mimimum in order to use a alternate cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5148
https://github.com/broadinstitute/gatk/issues/5148:570,Testability,test,testGenotypeGivenAllelesMode,570,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests failed, including a few of the Mutect2/HC ones:. Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly ; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode. The FeatureCache assumes that queries are always increasing along a contig; the failures in this branch indicate that the caller is attempting to back up and re-query territory that has already been cached and then trimmed. I didn't track down all of these cases, but the general pattern appears to be that active region determination results in initial caching and trimming, and then the same/similar territory is traversed again during calling, resulting in cache misses. It happens pretty frequently when running M2 tests, at least for pon and germline resource inputs; we should investigate how much a better caching strategy would help performance. If it would, we'd need https://github.com/broadinstitute/gatk/pull/4902 at a mimimum in order to use a alternate cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5148
https://github.com/broadinstitute/gatk/issues/5148:1119,Testability,test,tests,1119,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests failed, including a few of the Mutect2/HC ones:. Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly ; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode. The FeatureCache assumes that queries are always increasing along a contig; the failures in this branch indicate that the caller is attempting to back up and re-query territory that has already been cached and then trimmed. I didn't track down all of these cases, but the general pattern appears to be that active region determination results in initial caching and trimming, and then the same/similar territory is traversed again during calling, resulting in cache misses. It happens pretty frequently when running M2 tests, at least for pon and germline resource inputs; we should investigate how much a better caching strategy would help performance. If it would, we'd need https://github.com/broadinstitute/gatk/pull/4902 at a mimimum in order to use a alternate cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5148
https://github.com/broadinstitute/gatk/pull/5149:147,Testability,test,test,147,"Now looks at manifest file, not readme.; Now supports version decorators (e.g. `somatic`) after version numbers in manifest file.; Added in a unit test to check the version regex. Fixes #4582 ; Fixes #4692",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5149
https://github.com/broadinstitute/gatk/pull/5150:134,Availability,error,error,134,Created NioFileCopier that copies files using nio paths (includes; optional progress indicator and integrity validation).; Updated an error message in funcotator to make it more descriptive. Fixes #4549,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5150
https://github.com/broadinstitute/gatk/pull/5150:123,Deployability,Update,Updated,123,Created NioFileCopier that copies files using nio paths (includes; optional progress indicator and integrity validation).; Updated an error message in funcotator to make it more descriptive. Fixes #4549,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5150
https://github.com/broadinstitute/gatk/pull/5150:140,Integrability,message,message,140,Created NioFileCopier that copies files using nio paths (includes; optional progress indicator and integrity validation).; Updated an error message in funcotator to make it more descriptive. Fixes #4549,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5150
https://github.com/broadinstitute/gatk/pull/5150:99,Security,integrity,integrity,99,Created NioFileCopier that copies files using nio paths (includes; optional progress indicator and integrity validation).; Updated an error message in funcotator to make it more descriptive. Fixes #4549,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5150
https://github.com/broadinstitute/gatk/pull/5150:109,Security,validat,validation,109,Created NioFileCopier that copies files using nio paths (includes; optional progress indicator and integrity validation).; Updated an error message in funcotator to make it more descriptive. Fixes #4549,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5150
https://github.com/broadinstitute/gatk/pull/5151:211,Availability,error,error,211,"(Reported in #5130). We had a user report that when a variant is within 10 base pairs from the end of the chromosome (as defined by the reference dictionary), `ReferenceBases` throws a String index out of range error. This bug was in the annotation prior to #4895. What #4895 did was that it made the `ReferenceBases` annotation part of `StandardMutectAnnotation,` which turned on `RefernceBases` by default, and that led to the user getting the error.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5151
https://github.com/broadinstitute/gatk/pull/5151:446,Availability,error,error,446,"(Reported in #5130). We had a user report that when a variant is within 10 base pairs from the end of the chromosome (as defined by the reference dictionary), `ReferenceBases` throws a String index out of range error. This bug was in the annotation prior to #4895. What #4895 did was that it made the `ReferenceBases` annotation part of `StandardMutectAnnotation,` which turned on `RefernceBases` by default, and that led to the user getting the error.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5151
https://github.com/broadinstitute/gatk/issues/5152:0,Testability,Test,Test,0,Test the scripts running on Dataproc 1.3. See https://github.com/broadinstitute/gatk/pull/5125#issuecomment-417345290 for background.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5152
https://github.com/broadinstitute/gatk/issues/5153:1201,Availability,failure,failure,1201,"used. I suspect I'd have a lot of ""YAGNI"" comments if I knew.; For example, you are basing all your implementations on Apache's AbstractIntegerDistribution. That class, it seems to me, is really intended to allow you to do sampling from a distribution. But I suspect you won't be sampling, you'll only be asking questions about density. If so, there's a lot of baggage that gets pulled into your anonymous implementations of this class: random number generators, boundary information, etc. Lots of extra boilerplate. Couldn't this be clearer if reorganized as an abstract class implementing AbstractIntegerDistribution, 3 concrete classes for each case (rather than the current anonymous classes), a factory that takes a spec and returns the correct distribution, and a simple enum class?. It seems weird that the distributions you allow users to realize using a spec are both two-tailed distributions, when fragment size is a one-tailed distribution. It seems awkward that failure to parse a distribution spec leads to a code path where you try to extract a file name and read serialized read metadata. Wouldn't it be clearer to have two completely distinct code paths with a different program argument for the empirical case?. The read metadata gives per library distributions. It seems suspect that you are folding them all together. Different libraries can have rather different fragment size stats. Still don't like that you're providing the possibility of reading the metadata text file. Seems fragile. Why don't you modify the ReadMetadata code to always produce just the data you need. Then you could eliminate the text-file code. And you could simplify the code that processes the serialized ReadMetadata which now has this awkward code path: CDF -> density -> sum across libs -> density+CDF stored in memory. If you have the CDF you can trivially produce density on demand. Notwithstanding all this, if you're happy with the code as it stands, feel free to merge.; Back to you, review done.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5153
https://github.com/broadinstitute/gatk/issues/5153:761,Usability,clear,clearer,761,"As noted by @tedsharpe in the corresponding pull request #4827 there is stuff that can be done to improve the current code:. This is difficult to review because there isn't any client code: I don't know how this is going to be used. I suspect I'd have a lot of ""YAGNI"" comments if I knew.; For example, you are basing all your implementations on Apache's AbstractIntegerDistribution. That class, it seems to me, is really intended to allow you to do sampling from a distribution. But I suspect you won't be sampling, you'll only be asking questions about density. If so, there's a lot of baggage that gets pulled into your anonymous implementations of this class: random number generators, boundary information, etc. Lots of extra boilerplate. Couldn't this be clearer if reorganized as an abstract class implementing AbstractIntegerDistribution, 3 concrete classes for each case (rather than the current anonymous classes), a factory that takes a spec and returns the correct distribution, and a simple enum class?. It seems weird that the distributions you allow users to realize using a spec are both two-tailed distributions, when fragment size is a one-tailed distribution. It seems awkward that failure to parse a distribution spec leads to a code path where you try to extract a file name and read serialized read metadata. Wouldn't it be clearer to have two completely distinct code paths with a different program argument for the empirical case?. The read metadata gives per library distributions. It seems suspect that you are folding them all together. Different libraries can have rather different fragment size stats. Still don't like that you're providing the possibility of reading the metadata text file. Seems fragile. Why don't you modify the ReadMetadata code to always produce just the data you need. Then you could eliminate the text-file code. And you could simplify the code that processes the serialized ReadMetadata which now has this awkward code path: CDF -> density -> sum ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5153
https://github.com/broadinstitute/gatk/issues/5153:997,Usability,simpl,simple,997,"As noted by @tedsharpe in the corresponding pull request #4827 there is stuff that can be done to improve the current code:. This is difficult to review because there isn't any client code: I don't know how this is going to be used. I suspect I'd have a lot of ""YAGNI"" comments if I knew.; For example, you are basing all your implementations on Apache's AbstractIntegerDistribution. That class, it seems to me, is really intended to allow you to do sampling from a distribution. But I suspect you won't be sampling, you'll only be asking questions about density. If so, there's a lot of baggage that gets pulled into your anonymous implementations of this class: random number generators, boundary information, etc. Lots of extra boilerplate. Couldn't this be clearer if reorganized as an abstract class implementing AbstractIntegerDistribution, 3 concrete classes for each case (rather than the current anonymous classes), a factory that takes a spec and returns the correct distribution, and a simple enum class?. It seems weird that the distributions you allow users to realize using a spec are both two-tailed distributions, when fragment size is a one-tailed distribution. It seems awkward that failure to parse a distribution spec leads to a code path where you try to extract a file name and read serialized read metadata. Wouldn't it be clearer to have two completely distinct code paths with a different program argument for the empirical case?. The read metadata gives per library distributions. It seems suspect that you are folding them all together. Different libraries can have rather different fragment size stats. Still don't like that you're providing the possibility of reading the metadata text file. Seems fragile. Why don't you modify the ReadMetadata code to always produce just the data you need. Then you could eliminate the text-file code. And you could simplify the code that processes the serialized ReadMetadata which now has this awkward code path: CDF -> density -> sum ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5153
https://github.com/broadinstitute/gatk/issues/5153:1346,Usability,clear,clearer,1346,"used. I suspect I'd have a lot of ""YAGNI"" comments if I knew.; For example, you are basing all your implementations on Apache's AbstractIntegerDistribution. That class, it seems to me, is really intended to allow you to do sampling from a distribution. But I suspect you won't be sampling, you'll only be asking questions about density. If so, there's a lot of baggage that gets pulled into your anonymous implementations of this class: random number generators, boundary information, etc. Lots of extra boilerplate. Couldn't this be clearer if reorganized as an abstract class implementing AbstractIntegerDistribution, 3 concrete classes for each case (rather than the current anonymous classes), a factory that takes a spec and returns the correct distribution, and a simple enum class?. It seems weird that the distributions you allow users to realize using a spec are both two-tailed distributions, when fragment size is a one-tailed distribution. It seems awkward that failure to parse a distribution spec leads to a code path where you try to extract a file name and read serialized read metadata. Wouldn't it be clearer to have two completely distinct code paths with a different program argument for the empirical case?. The read metadata gives per library distributions. It seems suspect that you are folding them all together. Different libraries can have rather different fragment size stats. Still don't like that you're providing the possibility of reading the metadata text file. Seems fragile. Why don't you modify the ReadMetadata code to always produce just the data you need. Then you could eliminate the text-file code. And you could simplify the code that processes the serialized ReadMetadata which now has this awkward code path: CDF -> density -> sum across libs -> density+CDF stored in memory. If you have the CDF you can trivially produce density on demand. Notwithstanding all this, if you're happy with the code as it stands, feel free to merge.; Back to you, review done.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5153
https://github.com/broadinstitute/gatk/issues/5153:1880,Usability,simpl,simplify,1880,"used. I suspect I'd have a lot of ""YAGNI"" comments if I knew.; For example, you are basing all your implementations on Apache's AbstractIntegerDistribution. That class, it seems to me, is really intended to allow you to do sampling from a distribution. But I suspect you won't be sampling, you'll only be asking questions about density. If so, there's a lot of baggage that gets pulled into your anonymous implementations of this class: random number generators, boundary information, etc. Lots of extra boilerplate. Couldn't this be clearer if reorganized as an abstract class implementing AbstractIntegerDistribution, 3 concrete classes for each case (rather than the current anonymous classes), a factory that takes a spec and returns the correct distribution, and a simple enum class?. It seems weird that the distributions you allow users to realize using a spec are both two-tailed distributions, when fragment size is a one-tailed distribution. It seems awkward that failure to parse a distribution spec leads to a code path where you try to extract a file name and read serialized read metadata. Wouldn't it be clearer to have two completely distinct code paths with a different program argument for the empirical case?. The read metadata gives per library distributions. It seems suspect that you are folding them all together. Different libraries can have rather different fragment size stats. Still don't like that you're providing the possibility of reading the metadata text file. Seems fragile. Why don't you modify the ReadMetadata code to always produce just the data you need. Then you could eliminate the text-file code. And you could simplify the code that processes the serialized ReadMetadata which now has this awkward code path: CDF -> density -> sum across libs -> density+CDF stored in memory. If you have the CDF you can trivially produce density on demand. Notwithstanding all this, if you're happy with the code as it stands, feel free to merge.; Back to you, review done.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5153
https://github.com/broadinstitute/gatk/issues/5154:199,Deployability,update,update,199,"Turns out I've been self-consistently using SVIntervals as BED intervals, which is the wrong interpretation according to its methods `toBEDInterval(...)` and `toSimpleInterval(...)`. Note to self to update the interpretation in package `sv.discovery`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5154
https://github.com/broadinstitute/gatk/pull/5155:122,Performance,load,loaded,122,Previously there was a scary duplicated block that needed to be changed in tandem in two files. this pulls it out so it's loaded as a separate gradle file and doesn't need to be duplicated.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5155
https://github.com/broadinstitute/gatk/pull/5156:0,Testability,test,testing,0,testing if this skips the push builds.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5156
https://github.com/broadinstitute/gatk/pull/5157:70,Performance,perform,performance,70,@tedsharpe ; not sure if the args checking would significantly affect performance.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5157
https://github.com/broadinstitute/gatk/issues/5158:2950,Availability,error,error,2950,"ing each allele"">; ##FORMAT=<ID=F2R1,Number=R,Type=Integer,Description=""Count of reads in F2R1 pair orientation supporting each allele"">; ##FORMAT=<ID=FT,Number=1,Type=String,Description=""Genotype-level filter"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MBQ,Number=A,Type=Integer,Description=""median base quality"">; ##FORMAT=<ID=MFRL,Number=R,Type=Integer,Description=""median fragment length"">; ##FORMAT=<ID=MMQ,Number=A,Type=Integer,Description=""median mapping quality"">; ##FORMAT=<ID=MPOS,Number=A,Type=Integer,Description=""median distance from end of read"">; ##FORMAT=<ID=OBAM,Number=A,Type=String,Description=""Whether the variant can be one of the given REF/ALT artifact modes."">; ##FORMAT=<ID=OBAMRC,Number=A,Type=String,Description=""Whether the variant can be one of the given REF/ALT artifact mode complements."">; ##FORMAT=<ID=OBF,Number=A,Type=Float,Description=""Fraction of alt reads indicating orientation bias error (taking into account artifact mode complement)."">; ##FORMAT=<ID=OBP,Number=A,Type=Float,Description=""Orientation bias p value for the given REF/ALT artifact or its complement."">; ##FORMAT=<ID=OBQ,Number=A,Type=Float,Description=""Measure (across entire bam file) of orientation bias for a given REF/ALT error."">; ##FORMAT=<ID=OBQRC,Number=A,Type=Float,Description=""Measure (across entire bam file) of orientation bias for the complement of a given REF/ALT error."">; ##FORMAT=<ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another"">; ##FORMAT=<ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5158
https://github.com/broadinstitute/gatk/issues/5158:3258,Availability,error,error,3258,"Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MBQ,Number=A,Type=Integer,Description=""median base quality"">; ##FORMAT=<ID=MFRL,Number=R,Type=Integer,Description=""median fragment length"">; ##FORMAT=<ID=MMQ,Number=A,Type=Integer,Description=""median mapping quality"">; ##FORMAT=<ID=MPOS,Number=A,Type=Integer,Description=""median distance from end of read"">; ##FORMAT=<ID=OBAM,Number=A,Type=String,Description=""Whether the variant can be one of the given REF/ALT artifact modes."">; ##FORMAT=<ID=OBAMRC,Number=A,Type=String,Description=""Whether the variant can be one of the given REF/ALT artifact mode complements."">; ##FORMAT=<ID=OBF,Number=A,Type=Float,Description=""Fraction of alt reads indicating orientation bias error (taking into account artifact mode complement)."">; ##FORMAT=<ID=OBP,Number=A,Type=Float,Description=""Orientation bias p value for the given REF/ALT artifact or its complement."">; ##FORMAT=<ID=OBQ,Number=A,Type=Float,Description=""Measure (across entire bam file) of orientation bias for a given REF/ALT error."">; ##FORMAT=<ID=OBQRC,Number=A,Type=Float,Description=""Measure (across entire bam file) of orientation bias for the complement of a given REF/ALT error."">; ##FORMAT=<ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another"">; ##FORMAT=<ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">; ##FORMAT=<ID=SA_MAP_AF,Number=3,Type=Float,Description=""MAP estimates of allele fraction given z"">; ##FORMAT=<ID=SA_POST_PROB,Number=3,Type=Float,Description=""posterior probabilities of the presence of strand artifact"">; etc..; etc..; etc..; 1 237752 . A G . artifact_in_n",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5158
https://github.com/broadinstitute/gatk/issues/5158:3411,Availability,error,error,3411,"y"">; ##FORMAT=<ID=MFRL,Number=R,Type=Integer,Description=""median fragment length"">; ##FORMAT=<ID=MMQ,Number=A,Type=Integer,Description=""median mapping quality"">; ##FORMAT=<ID=MPOS,Number=A,Type=Integer,Description=""median distance from end of read"">; ##FORMAT=<ID=OBAM,Number=A,Type=String,Description=""Whether the variant can be one of the given REF/ALT artifact modes."">; ##FORMAT=<ID=OBAMRC,Number=A,Type=String,Description=""Whether the variant can be one of the given REF/ALT artifact mode complements."">; ##FORMAT=<ID=OBF,Number=A,Type=Float,Description=""Fraction of alt reads indicating orientation bias error (taking into account artifact mode complement)."">; ##FORMAT=<ID=OBP,Number=A,Type=Float,Description=""Orientation bias p value for the given REF/ALT artifact or its complement."">; ##FORMAT=<ID=OBQ,Number=A,Type=Float,Description=""Measure (across entire bam file) of orientation bias for a given REF/ALT error."">; ##FORMAT=<ID=OBQRC,Number=A,Type=Float,Description=""Measure (across entire bam file) of orientation bias for the complement of a given REF/ALT error."">; ##FORMAT=<ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another"">; ##FORMAT=<ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">; ##FORMAT=<ID=SA_MAP_AF,Number=3,Type=Float,Description=""MAP estimates of allele fraction given z"">; ##FORMAT=<ID=SA_POST_PROB,Number=3,Type=Float,Description=""posterior probabilities of the presence of strand artifact"">; etc..; etc..; etc..; 1 237752 . A G . artifact_in_normal;clustered_events;mapping_quality;panel_of_normals;strand_artifact DP=369;ECNT=3;IN_PON;NLOD=14.51;N_ART_LOD=9.23;POP_AF=0.168;P_GERMLINE=",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5158
https://github.com/broadinstitute/gatk/issues/5158:1251,Integrability,contract,contraction,1251,""">; ##FILTER=<ID=orientation_bias,Description=""Orientation bias (in one of the specified artifact mode(s) or complement) seen in one or more samples."">; ##FILTER=<ID=panel_of_normals,Description=""Blacklisted site in panel of normals"">; ##FILTER=<ID=read_position,Description=""median distance of alt variants from end of reads"">; ##FILTER=<ID=str_contraction,Description=""Site filtered due to contraction of short tandem repeat region"">; ##FILTER=<ID=strand_artifact,Description=""Evidence for alt allele comes from one read direction only"">; ##FILTER=<ID=t_lod,Description=""Tumor does not meet likelihood threshold"">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=AF,Number=A,Type=Float,Description=""Allele fractions of alternate alleles in the tumor"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=F1R2,Number=R,Type=Integer,Description=""Count of reads in F1R2 pair orientation supporting each allele"">; ##FORMAT=<ID=F2R1,Number=R,Type=Integer,Description=""Count of reads in F2R1 pair orientation supporting each allele"">; ##FORMAT=<ID=FT,Number=1,Type=String,Description=""Genotype-level filter"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MBQ,Number=A,Type=Integer,Description=""median base quality"">; ##FORMAT=<ID=MFRL,Number=R,Type=Integer,Description=""median fragment length"">; ##FORMAT=<ID=MMQ,Number=A,Type=Integer,Description=""median mapping quality"">; ##FORMAT=<ID=MPOS,Number=A,Type=Integer,Description=""median distance from end of read"">; ##FORMAT=<ID=OBAM,Number=A,Type=String,Description=""Whether the variant can be one of the given REF/ALT artifact modes."">; ##FORMAT=<ID=OBAMRC,Number=A,Type=String,Description=""Whether the variant can be one of the given REF/ALT artifact mode complements."">; ##FORMAT=",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5158
https://github.com/broadinstitute/gatk/issues/5160:962,Testability,test,test,962,"When https://github.com/broadinstitute/gatk/pull/4963 is merged, HaplotypeCaller will produce more spanning deletions, and in some instances will produce spanning deletion alleles for deletions that are overlapped by other deletions. In those instances, CombineGVCFs and GenomicsDBImport do not properly compute the genotype FORMAT AD and DP tags for sites overlapped by the deletions. For example, for the input GVCF lines. ```; 20	10068158	.	GTGTATATATATA	G,<NON_REF>	66.73	.	BaseQRankSum=-0.652;ClippingRankSum=0.000;DP=29;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-0.253	GT:AD:DP:GQ:PL:SB	0/1:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20	10068160	.	GTATATATATATGTA	G,*,<NON_REF>	697.73	.	DP=28;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQ=87005.00	GT:AD:DP:GQ:PL:SB	1/2:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,2,4; ```. Combine GVCFs run as follows:. ```; ./gatk CombineGVCFs -V src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.testGVCFMode.gatk4.g.vcf -O test_gdb_import_combine.g.vcf -R src/test/resources/large/human_g1k_v37.20.21.fasta; ```. Returns the following output:. ```; 20 10068158 . GTGTATATATATA G,<NON_REF> . . BaseQRankSum=-6.520e-01;ClippingRankSum=0.00;DP=29;ExcessHet=3.01;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-2.530e-01 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068159 . T *,<NON_REF> . . DP=29 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068160 . GTATATATATATGTA G,*,<NON_REF> . . DP=28;ExcessHet=3.01;RAW_MQ=87005.00 GT:AD:DP:GQ:PL:SB ./.:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,2,4; 20 10068161 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068162 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068163 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5160
https://github.com/broadinstitute/gatk/issues/5160:1038,Testability,test,testGVCFMode,1038,"pull/4963 is merged, HaplotypeCaller will produce more spanning deletions, and in some instances will produce spanning deletion alleles for deletions that are overlapped by other deletions. In those instances, CombineGVCFs and GenomicsDBImport do not properly compute the genotype FORMAT AD and DP tags for sites overlapped by the deletions. For example, for the input GVCF lines. ```; 20	10068158	.	GTGTATATATATA	G,<NON_REF>	66.73	.	BaseQRankSum=-0.652;ClippingRankSum=0.000;DP=29;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-0.253	GT:AD:DP:GQ:PL:SB	0/1:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20	10068160	.	GTATATATATATGTA	G,*,<NON_REF>	697.73	.	DP=28;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQ=87005.00	GT:AD:DP:GQ:PL:SB	1/2:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,2,4; ```. Combine GVCFs run as follows:. ```; ./gatk CombineGVCFs -V src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.testGVCFMode.gatk4.g.vcf -O test_gdb_import_combine.g.vcf -R src/test/resources/large/human_g1k_v37.20.21.fasta; ```. Returns the following output:. ```; 20 10068158 . GTGTATATATATA G,<NON_REF> . . BaseQRankSum=-6.520e-01;ClippingRankSum=0.00;DP=29;ExcessHet=3.01;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-2.530e-01 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068159 . T *,<NON_REF> . . DP=29 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068160 . GTATATATATATGTA G,*,<NON_REF> . . DP=28;ExcessHet=3.01;RAW_MQ=87005.00 GT:AD:DP:GQ:PL:SB ./.:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,2,4; 20 10068161 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068162 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068163 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068164 . A *,<NON_REF> . . DP=28 GT:AD:DP:G",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5160
https://github.com/broadinstitute/gatk/issues/5160:1103,Testability,test,test,1103,"ns, and in some instances will produce spanning deletion alleles for deletions that are overlapped by other deletions. In those instances, CombineGVCFs and GenomicsDBImport do not properly compute the genotype FORMAT AD and DP tags for sites overlapped by the deletions. For example, for the input GVCF lines. ```; 20	10068158	.	GTGTATATATATA	G,<NON_REF>	66.73	.	BaseQRankSum=-0.652;ClippingRankSum=0.000;DP=29;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-0.253	GT:AD:DP:GQ:PL:SB	0/1:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20	10068160	.	GTATATATATATGTA	G,*,<NON_REF>	697.73	.	DP=28;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQ=87005.00	GT:AD:DP:GQ:PL:SB	1/2:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,2,4; ```. Combine GVCFs run as follows:. ```; ./gatk CombineGVCFs -V src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.testGVCFMode.gatk4.g.vcf -O test_gdb_import_combine.g.vcf -R src/test/resources/large/human_g1k_v37.20.21.fasta; ```. Returns the following output:. ```; 20 10068158 . GTGTATATATATA G,<NON_REF> . . BaseQRankSum=-6.520e-01;ClippingRankSum=0.00;DP=29;ExcessHet=3.01;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-2.530e-01 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068159 . T *,<NON_REF> . . DP=29 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068160 . GTATATATATATGTA G,*,<NON_REF> . . DP=28;ExcessHet=3.01;RAW_MQ=87005.00 GT:AD:DP:GQ:PL:SB ./.:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,2,4; 20 10068161 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068162 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068163 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068164 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068165 . T",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5160
https://github.com/broadinstitute/gatk/issues/5160:3204,Testability,test,test,3204,"10068166 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068167 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068168 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068169 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068170 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068171 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068172 . G *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068173 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068174 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; ```. GenomicsDBImport run like this:. ```; ./gatk GenomicsDBImport -R src/test/resources/large/human_g1k_v37.20.21.fasta -L 20 -V src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.testGVCFMode.gatk4.g.vcf -genomicsdb-workspace-path spanDelWorkspace; ./gatk SelectVariants -V gendb://spanDelWorkspace -R src/test/resources/large/human_g1k_v37.20.21.fasta -O test.g.vcf -L 20; ```. Returns the following output:. ```; 20 10068158 . GTGTATATATATA G,<NON_REF> . . BaseQRankSum=-6.520e-01;ClippingRankSum=0.00;DP=29;ExcessHet=3.01;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-2.530e-0; 1 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068159 . T *,<NON_REF> . . DP=29 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068160 . GTATATATATATGTA G,*,<NON_REF> . . DP=28;ExcessHet=3.01;RAW_MQ=87005.00 GT:AD:DP:GQ:PL:SB ./.:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,; 2,4; 20 10068161 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,472:0,0,2,4; 20 10068162 . A *,<NON_REF>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5160
https://github.com/broadinstitute/gatk/issues/5160:3264,Testability,test,test,3264,",472:0,0,2,4; 20 10068167 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068168 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068169 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068170 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068171 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068172 . G *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068173 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068174 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; ```. GenomicsDBImport run like this:. ```; ./gatk GenomicsDBImport -R src/test/resources/large/human_g1k_v37.20.21.fasta -L 20 -V src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.testGVCFMode.gatk4.g.vcf -genomicsdb-workspace-path spanDelWorkspace; ./gatk SelectVariants -V gendb://spanDelWorkspace -R src/test/resources/large/human_g1k_v37.20.21.fasta -O test.g.vcf -L 20; ```. Returns the following output:. ```; 20 10068158 . GTGTATATATATA G,<NON_REF> . . BaseQRankSum=-6.520e-01;ClippingRankSum=0.00;DP=29;ExcessHet=3.01;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-2.530e-0; 1 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068159 . T *,<NON_REF> . . DP=29 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068160 . GTATATATATATGTA G,*,<NON_REF> . . DP=28;ExcessHet=3.01;RAW_MQ=87005.00 GT:AD:DP:GQ:PL:SB ./.:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,; 2,4; 20 10068161 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,472:0,0,2,4; 20 10068162 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,472:0,0,2,4; 20 100681",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5160
https://github.com/broadinstitute/gatk/issues/5160:3340,Testability,test,testGVCFMode,3340,":AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068168 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068169 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068170 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068171 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068172 . G *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068173 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068174 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; ```. GenomicsDBImport run like this:. ```; ./gatk GenomicsDBImport -R src/test/resources/large/human_g1k_v37.20.21.fasta -L 20 -V src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.testGVCFMode.gatk4.g.vcf -genomicsdb-workspace-path spanDelWorkspace; ./gatk SelectVariants -V gendb://spanDelWorkspace -R src/test/resources/large/human_g1k_v37.20.21.fasta -O test.g.vcf -L 20; ```. Returns the following output:. ```; 20 10068158 . GTGTATATATATA G,<NON_REF> . . BaseQRankSum=-6.520e-01;ClippingRankSum=0.00;DP=29;ExcessHet=3.01;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-2.530e-0; 1 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068159 . T *,<NON_REF> . . DP=29 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068160 . GTATATATATATGTA G,*,<NON_REF> . . DP=28;ExcessHet=3.01;RAW_MQ=87005.00 GT:AD:DP:GQ:PL:SB ./.:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,; 2,4; 20 10068161 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,472:0,0,2,4; 20 10068162 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,472:0,0,2,4; 20 10068163 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5160
https://github.com/broadinstitute/gatk/issues/5160:3467,Testability,test,test,3467,"GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068169 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068170 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068171 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068172 . G *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068173 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068174 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; ```. GenomicsDBImport run like this:. ```; ./gatk GenomicsDBImport -R src/test/resources/large/human_g1k_v37.20.21.fasta -L 20 -V src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.testGVCFMode.gatk4.g.vcf -genomicsdb-workspace-path spanDelWorkspace; ./gatk SelectVariants -V gendb://spanDelWorkspace -R src/test/resources/large/human_g1k_v37.20.21.fasta -O test.g.vcf -L 20; ```. Returns the following output:. ```; 20 10068158 . GTGTATATATATA G,<NON_REF> . . BaseQRankSum=-6.520e-01;ClippingRankSum=0.00;DP=29;ExcessHet=3.01;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-2.530e-0; 1 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068159 . T *,<NON_REF> . . DP=29 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068160 . GTATATATATATGTA G,*,<NON_REF> . . DP=28;ExcessHet=3.01;RAW_MQ=87005.00 GT:AD:DP:GQ:PL:SB ./.:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,; 2,4; 20 10068161 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,472:0,0,2,4; 20 10068162 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,472:0,0,2,4; 20 10068163 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,472:0,0,2,4; 20 10068164 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:5",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5160
https://github.com/broadinstitute/gatk/issues/5160:3517,Testability,test,test,3517,"10068169 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068170 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068171 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068172 . G *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068173 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068174 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; ```. GenomicsDBImport run like this:. ```; ./gatk GenomicsDBImport -R src/test/resources/large/human_g1k_v37.20.21.fasta -L 20 -V src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.testGVCFMode.gatk4.g.vcf -genomicsdb-workspace-path spanDelWorkspace; ./gatk SelectVariants -V gendb://spanDelWorkspace -R src/test/resources/large/human_g1k_v37.20.21.fasta -O test.g.vcf -L 20; ```. Returns the following output:. ```; 20 10068158 . GTGTATATATATA G,<NON_REF> . . BaseQRankSum=-6.520e-01;ClippingRankSum=0.00;DP=29;ExcessHet=3.01;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-2.530e-0; 1 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068159 . T *,<NON_REF> . . DP=29 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068160 . GTATATATATATGTA G,*,<NON_REF> . . DP=28;ExcessHet=3.01;RAW_MQ=87005.00 GT:AD:DP:GQ:PL:SB ./.:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,; 2,4; 20 10068161 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,472:0,0,2,4; 20 10068162 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,472:0,0,2,4; 20 10068163 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,472:0,0,2,4; 20 10068164 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,472:0,0,2,4; 20 10068165 . T *,<NON_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5160
https://github.com/broadinstitute/gatk/pull/5161:64,Deployability,pipeline,pipeline,64,"Adds a tool for calling SVs using various SV evidence (BNDs, SV pipeline DEL calls, discordant read pairs, split reads) and copy number calls from gCNV. Possible haplotypes are enumerated using a traversal of the sequence graph supported by the SV evidence. The copy number posteriors are integrated to call likely events associated with the possible genotypes. Unit and integration tests forthcoming.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5161
https://github.com/broadinstitute/gatk/pull/5161:289,Deployability,integrat,integrated,289,"Adds a tool for calling SVs using various SV evidence (BNDs, SV pipeline DEL calls, discordant read pairs, split reads) and copy number calls from gCNV. Possible haplotypes are enumerated using a traversal of the sequence graph supported by the SV evidence. The copy number posteriors are integrated to call likely events associated with the possible genotypes. Unit and integration tests forthcoming.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5161
https://github.com/broadinstitute/gatk/pull/5161:371,Deployability,integrat,integration,371,"Adds a tool for calling SVs using various SV evidence (BNDs, SV pipeline DEL calls, discordant read pairs, split reads) and copy number calls from gCNV. Possible haplotypes are enumerated using a traversal of the sequence graph supported by the SV evidence. The copy number posteriors are integrated to call likely events associated with the possible genotypes. Unit and integration tests forthcoming.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5161
https://github.com/broadinstitute/gatk/pull/5161:289,Integrability,integrat,integrated,289,"Adds a tool for calling SVs using various SV evidence (BNDs, SV pipeline DEL calls, discordant read pairs, split reads) and copy number calls from gCNV. Possible haplotypes are enumerated using a traversal of the sequence graph supported by the SV evidence. The copy number posteriors are integrated to call likely events associated with the possible genotypes. Unit and integration tests forthcoming.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5161
https://github.com/broadinstitute/gatk/pull/5161:371,Integrability,integrat,integration,371,"Adds a tool for calling SVs using various SV evidence (BNDs, SV pipeline DEL calls, discordant read pairs, split reads) and copy number calls from gCNV. Possible haplotypes are enumerated using a traversal of the sequence graph supported by the SV evidence. The copy number posteriors are integrated to call likely events associated with the possible genotypes. Unit and integration tests forthcoming.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5161
https://github.com/broadinstitute/gatk/pull/5161:383,Testability,test,tests,383,"Adds a tool for calling SVs using various SV evidence (BNDs, SV pipeline DEL calls, discordant read pairs, split reads) and copy number calls from gCNV. Possible haplotypes are enumerated using a traversal of the sequence graph supported by the SV evidence. The copy number posteriors are integrated to call likely events associated with the possible genotypes. Unit and integration tests forthcoming.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5161
https://github.com/broadinstitute/gatk/pull/5162:231,Deployability,pipeline,pipeline,231,"This produces a resource that will be used as input to an upcoming tool to filter intervals based on these annotations (as well as coverage statistics). Currently, we have an external python script performing this step in the gCNV pipeline. I also updated the AnnotateIntervals task and calls in WDL, but these changes are untested; the reviewer should check carefully for typos. Currently, all annotations are of double type, but I've added code that can support all types supported by the TSV code as well. Additional tracks can also be added relatively easily. Currently, allowed annotations and their corresponding types are hardcoded; we could possibly move this information to the SAM-style header in the future. For the Umap hg19 k100 single-read mappability track and the segmental-duplication track used by the Talkowski lab, annotation of 1kb bins on hg19 takes less than a minute with the default feature lookahead (which is exposed as a parameter). I tested using the Umap multi-read mappability track (which is orders of magnitude larger, but is actually what is used in the external script), but this is much slower (documentation indicates that the single-read track should be used to dissuade this). We should evaluate whether or not using the single-read track suffices for filtering.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5162
https://github.com/broadinstitute/gatk/pull/5162:248,Deployability,update,updated,248,"This produces a resource that will be used as input to an upcoming tool to filter intervals based on these annotations (as well as coverage statistics). Currently, we have an external python script performing this step in the gCNV pipeline. I also updated the AnnotateIntervals task and calls in WDL, but these changes are untested; the reviewer should check carefully for typos. Currently, all annotations are of double type, but I've added code that can support all types supported by the TSV code as well. Additional tracks can also be added relatively easily. Currently, allowed annotations and their corresponding types are hardcoded; we could possibly move this information to the SAM-style header in the future. For the Umap hg19 k100 single-read mappability track and the segmental-duplication track used by the Talkowski lab, annotation of 1kb bins on hg19 takes less than a minute with the default feature lookahead (which is exposed as a parameter). I tested using the Umap multi-read mappability track (which is orders of magnitude larger, but is actually what is used in the external script), but this is much slower (documentation indicates that the single-read track should be used to dissuade this). We should evaluate whether or not using the single-read track suffices for filtering.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5162
https://github.com/broadinstitute/gatk/pull/5162:198,Performance,perform,performing,198,"This produces a resource that will be used as input to an upcoming tool to filter intervals based on these annotations (as well as coverage statistics). Currently, we have an external python script performing this step in the gCNV pipeline. I also updated the AnnotateIntervals task and calls in WDL, but these changes are untested; the reviewer should check carefully for typos. Currently, all annotations are of double type, but I've added code that can support all types supported by the TSV code as well. Additional tracks can also be added relatively easily. Currently, allowed annotations and their corresponding types are hardcoded; we could possibly move this information to the SAM-style header in the future. For the Umap hg19 k100 single-read mappability track and the segmental-duplication track used by the Talkowski lab, annotation of 1kb bins on hg19 takes less than a minute with the default feature lookahead (which is exposed as a parameter). I tested using the Umap multi-read mappability track (which is orders of magnitude larger, but is actually what is used in the external script), but this is much slower (documentation indicates that the single-read track should be used to dissuade this). We should evaluate whether or not using the single-read track suffices for filtering.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5162
https://github.com/broadinstitute/gatk/pull/5162:936,Security,expose,exposed,936,"This produces a resource that will be used as input to an upcoming tool to filter intervals based on these annotations (as well as coverage statistics). Currently, we have an external python script performing this step in the gCNV pipeline. I also updated the AnnotateIntervals task and calls in WDL, but these changes are untested; the reviewer should check carefully for typos. Currently, all annotations are of double type, but I've added code that can support all types supported by the TSV code as well. Additional tracks can also be added relatively easily. Currently, allowed annotations and their corresponding types are hardcoded; we could possibly move this information to the SAM-style header in the future. For the Umap hg19 k100 single-read mappability track and the segmental-duplication track used by the Talkowski lab, annotation of 1kb bins on hg19 takes less than a minute with the default feature lookahead (which is exposed as a parameter). I tested using the Umap multi-read mappability track (which is orders of magnitude larger, but is actually what is used in the external script), but this is much slower (documentation indicates that the single-read track should be used to dissuade this). We should evaluate whether or not using the single-read track suffices for filtering.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5162
https://github.com/broadinstitute/gatk/pull/5162:963,Testability,test,tested,963,"This produces a resource that will be used as input to an upcoming tool to filter intervals based on these annotations (as well as coverage statistics). Currently, we have an external python script performing this step in the gCNV pipeline. I also updated the AnnotateIntervals task and calls in WDL, but these changes are untested; the reviewer should check carefully for typos. Currently, all annotations are of double type, but I've added code that can support all types supported by the TSV code as well. Additional tracks can also be added relatively easily. Currently, allowed annotations and their corresponding types are hardcoded; we could possibly move this information to the SAM-style header in the future. For the Umap hg19 k100 single-read mappability track and the segmental-duplication track used by the Talkowski lab, annotation of 1kb bins on hg19 takes less than a minute with the default feature lookahead (which is exposed as a parameter). I tested using the Umap multi-read mappability track (which is orders of magnitude larger, but is actually what is used in the external script), but this is much slower (documentation indicates that the single-read track should be used to dissuade this). We should evaluate whether or not using the single-read track suffices for filtering.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5162
https://github.com/broadinstitute/gatk/pull/5164:605,Deployability,integrat,integration,605,"@vruano ; Since there's some a major change of implementation following your suggestions (single class instead of abstract-base-and-sole-inheritor, remove over-classing), I'm issuing this PR to replace #5117, so the comments you made there are easier to be kept track of. Basically, ; * the first commit is trivial; * the second commit is to address some comments you have about various utils classes; * the third commit is what's contained in #5117 ; * the fourth commit is the re-implementation, which replaces the two old classes with a new class so it's easier to read; * the fifth commit is a simple integration test for this new tool. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5164
https://github.com/broadinstitute/gatk/pull/5164:605,Integrability,integrat,integration,605,"@vruano ; Since there's some a major change of implementation following your suggestions (single class instead of abstract-base-and-sole-inheritor, remove over-classing), I'm issuing this PR to replace #5117, so the comments you made there are easier to be kept track of. Basically, ; * the first commit is trivial; * the second commit is to address some comments you have about various utils classes; * the third commit is what's contained in #5117 ; * the fourth commit is the re-implementation, which replaces the two old classes with a new class so it's easier to read; * the fifth commit is a simple integration test for this new tool. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5164
https://github.com/broadinstitute/gatk/pull/5164:137,Modifiability,inherit,inheritor,137,"@vruano ; Since there's some a major change of implementation following your suggestions (single class instead of abstract-base-and-sole-inheritor, remove over-classing), I'm issuing this PR to replace #5117, so the comments you made there are easier to be kept track of. Basically, ; * the first commit is trivial; * the second commit is to address some comments you have about various utils classes; * the third commit is what's contained in #5117 ; * the fourth commit is the re-implementation, which replaces the two old classes with a new class so it's easier to read; * the fifth commit is a simple integration test for this new tool. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5164
https://github.com/broadinstitute/gatk/pull/5164:617,Testability,test,test,617,"@vruano ; Since there's some a major change of implementation following your suggestions (single class instead of abstract-base-and-sole-inheritor, remove over-classing), I'm issuing this PR to replace #5117, so the comments you made there are easier to be kept track of. Basically, ; * the first commit is trivial; * the second commit is to address some comments you have about various utils classes; * the third commit is what's contained in #5117 ; * the fourth commit is the re-implementation, which replaces the two old classes with a new class so it's easier to read; * the fifth commit is a simple integration test for this new tool. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5164
https://github.com/broadinstitute/gatk/pull/5164:598,Usability,simpl,simple,598,"@vruano ; Since there's some a major change of implementation following your suggestions (single class instead of abstract-base-and-sole-inheritor, remove over-classing), I'm issuing this PR to replace #5117, so the comments you made there are easier to be kept track of. Basically, ; * the first commit is trivial; * the second commit is to address some comments you have about various utils classes; * the third commit is what's contained in #5117 ; * the fourth commit is the re-implementation, which replaces the two old classes with a new class so it's easier to read; * the fifth commit is a simple integration test for this new tool. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5164
https://github.com/broadinstitute/gatk/pull/5166:22,Deployability,update,updated,22,"In light of the newly updated MarkDuplicatesSpark which is finally reaching the point where we trust it, we have evaluated that the work of maintaining both this tool and MarkDuplicatesSpark was too great considering how little code they could directly reuse relative to eachother. . Resolves #4896 #3705",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5166
https://github.com/broadinstitute/gatk/issues/5169:5400,Availability,failure,failure,5400,"); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). #### Steps to reproduce; Run the MarkDuplicatesSpark in a local SPARK cluster. The following function return a null, which cause the exception. public static String getLibrary( final GATKRead read, final SAMFileHeader header, String defaultLibrary) {; final SAMReadGroupRecord readGroup = getSAMReadGroupRecord(read, header);; String library = readGroup != null ? readGroup.getLibrary() : null;; return library==null? defaultLibrary : library;; }. public EmptyFragment(GATKRead read, SAMFileHeader header, Map<String, Byte> headerLibraryMap) {; super(0, null);; this.R1R = read.isReverseStrand();; this.key = ReadsKey.getKeyForFragment(ReadUtils.getStrandedUnclippedStart(read),; isRead1ReverseStrand(),; ReadUtils.getReferenceIndex(read, header),; headerLibraryMap.get(ReadUtils.getLibrary(read, header, LibraryIdGenerator.UNKNOWN_LIBRARY)));; }. #### Expected behavior; MarkDuplicatesSpark should success finish. #### Actual behavior; The MarkDuplicatesSpark run failure. ----. ## Feature request. ### Tool(s) or class(es) involved; SPARK, Hadoop HDFS. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169
https://github.com/broadinstitute/gatk/issues/5169:116,Deployability,release,release,116,"## Bug Report. ### Affected tool(s) or class(es); MarkDuplicatesSpark. ### Affected version(s); - [ ] Latest public release version [version?]; 4.0.8.1; - [ ] Latest master branch as of [date of test?]; Sep 10, 2018. ### Description ; 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 515, localhost, executor 1, partition 0, NODE_LOCAL, 5270 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 516, localhost, executor 2, partition 1, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 517, localhost, executor 1, partition 2, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.0 (TID 518, localhost, executor 2, partition 3, NODE_LOCAL, 5594 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.0 (TID 519, localhost, executor 1, partition 4, NODE_LOCAL, 5317 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.0 (TID 520, localhost, executor 2, partition 5, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 6.0 (TID 521, localhost, executor 1, partition 6, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 6.0 (TID 522, localhost, executor 2, partition 7, NODE_LOCAL, 5316 bytes); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:37617 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:45786 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:50 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:37617 (size: 9.0 B, free: 2004.5 MB); 18/09/08 10:32:50 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 6.0 (TID 523, localhost, executor 1, partition 8, NODE_LOCAL, 5604 bytes)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169
https://github.com/broadinstitute/gatk/issues/5169:258,Energy Efficiency,schedul,scheduler,258,"## Bug Report. ### Affected tool(s) or class(es); MarkDuplicatesSpark. ### Affected version(s); - [ ] Latest public release version [version?]; 4.0.8.1; - [ ] Latest master branch as of [date of test?]; Sep 10, 2018. ### Description ; 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 515, localhost, executor 1, partition 0, NODE_LOCAL, 5270 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 516, localhost, executor 2, partition 1, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 517, localhost, executor 1, partition 2, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.0 (TID 518, localhost, executor 2, partition 3, NODE_LOCAL, 5594 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.0 (TID 519, localhost, executor 1, partition 4, NODE_LOCAL, 5317 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.0 (TID 520, localhost, executor 2, partition 5, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 6.0 (TID 521, localhost, executor 1, partition 6, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 6.0 (TID 522, localhost, executor 2, partition 7, NODE_LOCAL, 5316 bytes); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:37617 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:45786 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:50 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:37617 (size: 9.0 B, free: 2004.5 MB); 18/09/08 10:32:50 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 6.0 (TID 523, localhost, executor 1, partition 8, NODE_LOCAL, 5604 bytes)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169
https://github.com/broadinstitute/gatk/issues/5169:409,Energy Efficiency,schedul,scheduler,409,"## Bug Report. ### Affected tool(s) or class(es); MarkDuplicatesSpark. ### Affected version(s); - [ ] Latest public release version [version?]; 4.0.8.1; - [ ] Latest master branch as of [date of test?]; Sep 10, 2018. ### Description ; 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 515, localhost, executor 1, partition 0, NODE_LOCAL, 5270 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 516, localhost, executor 2, partition 1, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 517, localhost, executor 1, partition 2, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.0 (TID 518, localhost, executor 2, partition 3, NODE_LOCAL, 5594 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.0 (TID 519, localhost, executor 1, partition 4, NODE_LOCAL, 5317 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.0 (TID 520, localhost, executor 2, partition 5, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 6.0 (TID 521, localhost, executor 1, partition 6, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 6.0 (TID 522, localhost, executor 2, partition 7, NODE_LOCAL, 5316 bytes); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:37617 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:45786 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:50 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:37617 (size: 9.0 B, free: 2004.5 MB); 18/09/08 10:32:50 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 6.0 (TID 523, localhost, executor 1, partition 8, NODE_LOCAL, 5604 bytes)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169
https://github.com/broadinstitute/gatk/issues/5169:560,Energy Efficiency,schedul,scheduler,560,"## Bug Report. ### Affected tool(s) or class(es); MarkDuplicatesSpark. ### Affected version(s); - [ ] Latest public release version [version?]; 4.0.8.1; - [ ] Latest master branch as of [date of test?]; Sep 10, 2018. ### Description ; 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 515, localhost, executor 1, partition 0, NODE_LOCAL, 5270 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 516, localhost, executor 2, partition 1, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 517, localhost, executor 1, partition 2, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.0 (TID 518, localhost, executor 2, partition 3, NODE_LOCAL, 5594 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.0 (TID 519, localhost, executor 1, partition 4, NODE_LOCAL, 5317 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.0 (TID 520, localhost, executor 2, partition 5, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 6.0 (TID 521, localhost, executor 1, partition 6, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 6.0 (TID 522, localhost, executor 2, partition 7, NODE_LOCAL, 5316 bytes); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:37617 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:45786 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:50 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:37617 (size: 9.0 B, free: 2004.5 MB); 18/09/08 10:32:50 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 6.0 (TID 523, localhost, executor 1, partition 8, NODE_LOCAL, 5604 bytes)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169
https://github.com/broadinstitute/gatk/issues/5169:711,Energy Efficiency,schedul,scheduler,711,"## Bug Report. ### Affected tool(s) or class(es); MarkDuplicatesSpark. ### Affected version(s); - [ ] Latest public release version [version?]; 4.0.8.1; - [ ] Latest master branch as of [date of test?]; Sep 10, 2018. ### Description ; 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 515, localhost, executor 1, partition 0, NODE_LOCAL, 5270 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 516, localhost, executor 2, partition 1, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 517, localhost, executor 1, partition 2, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.0 (TID 518, localhost, executor 2, partition 3, NODE_LOCAL, 5594 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.0 (TID 519, localhost, executor 1, partition 4, NODE_LOCAL, 5317 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.0 (TID 520, localhost, executor 2, partition 5, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 6.0 (TID 521, localhost, executor 1, partition 6, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 6.0 (TID 522, localhost, executor 2, partition 7, NODE_LOCAL, 5316 bytes); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:37617 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:45786 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:50 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:37617 (size: 9.0 B, free: 2004.5 MB); 18/09/08 10:32:50 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 6.0 (TID 523, localhost, executor 1, partition 8, NODE_LOCAL, 5604 bytes)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169
https://github.com/broadinstitute/gatk/issues/5169:862,Energy Efficiency,schedul,scheduler,862,"## Bug Report. ### Affected tool(s) or class(es); MarkDuplicatesSpark. ### Affected version(s); - [ ] Latest public release version [version?]; 4.0.8.1; - [ ] Latest master branch as of [date of test?]; Sep 10, 2018. ### Description ; 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 515, localhost, executor 1, partition 0, NODE_LOCAL, 5270 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 516, localhost, executor 2, partition 1, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 517, localhost, executor 1, partition 2, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.0 (TID 518, localhost, executor 2, partition 3, NODE_LOCAL, 5594 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.0 (TID 519, localhost, executor 1, partition 4, NODE_LOCAL, 5317 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.0 (TID 520, localhost, executor 2, partition 5, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 6.0 (TID 521, localhost, executor 1, partition 6, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 6.0 (TID 522, localhost, executor 2, partition 7, NODE_LOCAL, 5316 bytes); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:37617 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:45786 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:50 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:37617 (size: 9.0 B, free: 2004.5 MB); 18/09/08 10:32:50 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 6.0 (TID 523, localhost, executor 1, partition 8, NODE_LOCAL, 5604 bytes)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169
https://github.com/broadinstitute/gatk/issues/5169:1013,Energy Efficiency,schedul,scheduler,1013,"## Bug Report. ### Affected tool(s) or class(es); MarkDuplicatesSpark. ### Affected version(s); - [ ] Latest public release version [version?]; 4.0.8.1; - [ ] Latest master branch as of [date of test?]; Sep 10, 2018. ### Description ; 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 515, localhost, executor 1, partition 0, NODE_LOCAL, 5270 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 516, localhost, executor 2, partition 1, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 517, localhost, executor 1, partition 2, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.0 (TID 518, localhost, executor 2, partition 3, NODE_LOCAL, 5594 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.0 (TID 519, localhost, executor 1, partition 4, NODE_LOCAL, 5317 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.0 (TID 520, localhost, executor 2, partition 5, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 6.0 (TID 521, localhost, executor 1, partition 6, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 6.0 (TID 522, localhost, executor 2, partition 7, NODE_LOCAL, 5316 bytes); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:37617 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:45786 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:50 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:37617 (size: 9.0 B, free: 2004.5 MB); 18/09/08 10:32:50 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 6.0 (TID 523, localhost, executor 1, partition 8, NODE_LOCAL, 5604 bytes)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169
https://github.com/broadinstitute/gatk/issues/5169:1164,Energy Efficiency,schedul,scheduler,1164,"se version [version?]; 4.0.8.1; - [ ] Latest master branch as of [date of test?]; Sep 10, 2018. ### Description ; 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 515, localhost, executor 1, partition 0, NODE_LOCAL, 5270 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 516, localhost, executor 2, partition 1, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 517, localhost, executor 1, partition 2, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.0 (TID 518, localhost, executor 2, partition 3, NODE_LOCAL, 5594 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.0 (TID 519, localhost, executor 1, partition 4, NODE_LOCAL, 5317 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.0 (TID 520, localhost, executor 2, partition 5, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 6.0 (TID 521, localhost, executor 1, partition 6, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 6.0 (TID 522, localhost, executor 2, partition 7, NODE_LOCAL, 5316 bytes); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:37617 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:45786 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:50 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:37617 (size: 9.0 B, free: 2004.5 MB); 18/09/08 10:32:50 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 6.0 (TID 523, localhost, executor 1, partition 8, NODE_LOCAL, 5604 bytes); 18/09/08 10:32:50 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 6.0 (TID 524, localhost, executor 1, parti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169
https://github.com/broadinstitute/gatk/issues/5169:1315,Energy Efficiency,schedul,scheduler,1315,"SetManager: Starting task 0.0 in stage 6.0 (TID 515, localhost, executor 1, partition 0, NODE_LOCAL, 5270 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 516, localhost, executor 2, partition 1, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 517, localhost, executor 1, partition 2, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.0 (TID 518, localhost, executor 2, partition 3, NODE_LOCAL, 5594 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.0 (TID 519, localhost, executor 1, partition 4, NODE_LOCAL, 5317 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.0 (TID 520, localhost, executor 2, partition 5, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 6.0 (TID 521, localhost, executor 1, partition 6, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 6.0 (TID 522, localhost, executor 2, partition 7, NODE_LOCAL, 5316 bytes); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:37617 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:45786 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:50 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:37617 (size: 9.0 B, free: 2004.5 MB); 18/09/08 10:32:50 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 6.0 (TID 523, localhost, executor 1, partition 8, NODE_LOCAL, 5604 bytes); 18/09/08 10:32:50 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 6.0 (TID 524, localhost, executor 1, partition 9, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:50 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 6.0 (TID 515, localhost, executor 1): java.lan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169
https://github.com/broadinstitute/gatk/issues/5169:1875,Energy Efficiency,schedul,scheduler,1875,"eduler.TaskSetManager: Starting task 4.0 in stage 6.0 (TID 519, localhost, executor 1, partition 4, NODE_LOCAL, 5317 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.0 (TID 520, localhost, executor 2, partition 5, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 6.0 (TID 521, localhost, executor 1, partition 6, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 6.0 (TID 522, localhost, executor 2, partition 7, NODE_LOCAL, 5316 bytes); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:37617 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:45786 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:50 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:37617 (size: 9.0 B, free: 2004.5 MB); 18/09/08 10:32:50 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 6.0 (TID 523, localhost, executor 1, partition 8, NODE_LOCAL, 5604 bytes); 18/09/08 10:32:50 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 6.0 (TID 524, localhost, executor 1, partition 9, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:50 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 6.0 (TID 515, localhost, executor 1): java.lang.NullPointerException; 	at org.broadinstitute.hellbender.utils.read.markduplicates.sparkrecords.EmptyFragment.<init>(EmptyFragment.java:35); 	at org.broadinstitute.hellbender.utils.read.markduplicates.sparkrecords.MarkDuplicatesSparkRecord.newEmptyFragment(MarkDuplicatesSparkRecord.java:37); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$null$0(MarkDuplicatesSparkUtils.java:114); 	at java.util.stream.ReferencePipeline$11$1.accept(ReferencePipeline.java:372); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169
https://github.com/broadinstitute/gatk/issues/5169:2026,Energy Efficiency,schedul,scheduler,2026,"ytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.0 (TID 520, localhost, executor 2, partition 5, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 6.0 (TID 521, localhost, executor 1, partition 6, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 6.0 (TID 522, localhost, executor 2, partition 7, NODE_LOCAL, 5316 bytes); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:37617 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:45786 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:50 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:37617 (size: 9.0 B, free: 2004.5 MB); 18/09/08 10:32:50 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 6.0 (TID 523, localhost, executor 1, partition 8, NODE_LOCAL, 5604 bytes); 18/09/08 10:32:50 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 6.0 (TID 524, localhost, executor 1, partition 9, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:50 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 6.0 (TID 515, localhost, executor 1): java.lang.NullPointerException; 	at org.broadinstitute.hellbender.utils.read.markduplicates.sparkrecords.EmptyFragment.<init>(EmptyFragment.java:35); 	at org.broadinstitute.hellbender.utils.read.markduplicates.sparkrecords.MarkDuplicatesSparkRecord.newEmptyFragment(MarkDuplicatesSparkRecord.java:37); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$null$0(MarkDuplicatesSparkUtils.java:114); 	at java.util.stream.ReferencePipeline$11$1.accept(ReferencePipeline.java:372); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169
https://github.com/broadinstitute/gatk/issues/5169:2177,Energy Efficiency,schedul,scheduler,2177,"ytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 6.0 (TID 521, localhost, executor 1, partition 6, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 6.0 (TID 522, localhost, executor 2, partition 7, NODE_LOCAL, 5316 bytes); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:37617 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:45786 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:50 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:37617 (size: 9.0 B, free: 2004.5 MB); 18/09/08 10:32:50 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 6.0 (TID 523, localhost, executor 1, partition 8, NODE_LOCAL, 5604 bytes); 18/09/08 10:32:50 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 6.0 (TID 524, localhost, executor 1, partition 9, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:50 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 6.0 (TID 515, localhost, executor 1): java.lang.NullPointerException; 	at org.broadinstitute.hellbender.utils.read.markduplicates.sparkrecords.EmptyFragment.<init>(EmptyFragment.java:35); 	at org.broadinstitute.hellbender.utils.read.markduplicates.sparkrecords.MarkDuplicatesSparkRecord.newEmptyFragment(MarkDuplicatesSparkRecord.java:37); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$null$0(MarkDuplicatesSparkUtils.java:114); 	at java.util.stream.ReferencePipeline$11$1.accept(ReferencePipeline.java:372); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169
https://github.com/broadinstitute/gatk/issues/5169:3055,Energy Efficiency,Reduce,ReduceOps,3055,"k 9.0 in stage 6.0 (TID 524, localhost, executor 1, partition 9, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:50 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 6.0 (TID 515, localhost, executor 1): java.lang.NullPointerException; 	at org.broadinstitute.hellbender.utils.read.markduplicates.sparkrecords.EmptyFragment.<init>(EmptyFragment.java:35); 	at org.broadinstitute.hellbender.utils.read.markduplicates.sparkrecords.MarkDuplicatesSparkRecord.newEmptyFragment(MarkDuplicatesSparkRecord.java:37); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$null$0(MarkDuplicatesSparkUtils.java:114); 	at java.util.stream.ReferencePipeline$11$1.accept(ReferencePipeline.java:372); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$transformToDuplicateNames$17d832cf$1(MarkDuplicatesSparkUtils.java:123); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(S",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169
https://github.com/broadinstitute/gatk/issues/5169:3065,Energy Efficiency,Reduce,ReduceOp,3065,"k 9.0 in stage 6.0 (TID 524, localhost, executor 1, partition 9, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:50 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 6.0 (TID 515, localhost, executor 1): java.lang.NullPointerException; 	at org.broadinstitute.hellbender.utils.read.markduplicates.sparkrecords.EmptyFragment.<init>(EmptyFragment.java:35); 	at org.broadinstitute.hellbender.utils.read.markduplicates.sparkrecords.MarkDuplicatesSparkRecord.newEmptyFragment(MarkDuplicatesSparkRecord.java:37); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$null$0(MarkDuplicatesSparkUtils.java:114); 	at java.util.stream.ReferencePipeline$11$1.accept(ReferencePipeline.java:372); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$transformToDuplicateNames$17d832cf$1(MarkDuplicatesSparkUtils.java:123); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(S",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169
https://github.com/broadinstitute/gatk/issues/5169:3093,Energy Efficiency,Reduce,ReduceOps,3093,"524, localhost, executor 1, partition 9, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:50 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 6.0 (TID 515, localhost, executor 1): java.lang.NullPointerException; 	at org.broadinstitute.hellbender.utils.read.markduplicates.sparkrecords.EmptyFragment.<init>(EmptyFragment.java:35); 	at org.broadinstitute.hellbender.utils.read.markduplicates.sparkrecords.MarkDuplicatesSparkRecord.newEmptyFragment(MarkDuplicatesSparkRecord.java:37); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$null$0(MarkDuplicatesSparkUtils.java:114); 	at java.util.stream.ReferencePipeline$11$1.accept(ReferencePipeline.java:372); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$transformToDuplicateNames$17d832cf$1(MarkDuplicatesSparkUtils.java:123); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53);",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169
https://github.com/broadinstitute/gatk/issues/5169:3951,Energy Efficiency,schedul,scheduler,3951,"java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$transformToDuplicateNames$17d832cf$1(MarkDuplicatesSparkUtils.java:123); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). #### Steps to reproduce; Run the MarkDuplicatesSpark in a local SPARK cluster. The following function return a null, which cause the exception. public static String getLibrary( final GATKRead read, final SAMFileHeader header, String defaultLibrary) {; final SAMReadGroupRecord readGroup = getSAMReadGroupRecord(read, header);; String library = readGroup != null ? readGroup.getLibrary() : null;; return library==null? defaultLibrary : library;; }. public EmptyFragment(GATKRead read, SAMFileHeader header, Map<String, Byt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169
https://github.com/broadinstitute/gatk/issues/5169:4031,Energy Efficiency,schedul,scheduler,4031,"t java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$transformToDuplicateNames$17d832cf$1(MarkDuplicatesSparkUtils.java:123); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). #### Steps to reproduce; Run the MarkDuplicatesSpark in a local SPARK cluster. The following function return a null, which cause the exception. public static String getLibrary( final GATKRead read, final SAMFileHeader header, String defaultLibrary) {; final SAMReadGroupRecord readGroup = getSAMReadGroupRecord(read, header);; String library = readGroup != null ? readGroup.getLibrary() : null;; return library==null? defaultLibrary : library;; }. public EmptyFragment(GATKRead read, SAMFileHeader header, Map<String, Byte> headerLibraryMap) {; super(0, null);; this.R1R = read.isReverseStrand();; thi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169
https://github.com/broadinstitute/gatk/issues/5169:4111,Energy Efficiency,schedul,scheduler,4111,"t java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$transformToDuplicateNames$17d832cf$1(MarkDuplicatesSparkUtils.java:123); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). #### Steps to reproduce; Run the MarkDuplicatesSpark in a local SPARK cluster. The following function return a null, which cause the exception. public static String getLibrary( final GATKRead read, final SAMFileHeader header, String defaultLibrary) {; final SAMReadGroupRecord readGroup = getSAMReadGroupRecord(read, header);; String library = readGroup != null ? readGroup.getLibrary() : null;; return library==null? defaultLibrary : library;; }. public EmptyFragment(GATKRead read, SAMFileHeader header, Map<String, Byte> headerLibraryMap) {; super(0, null);; this.R1R = read.isReverseStrand();; this.key = ReadsKey.getKeyForFragment(ReadUtils.getStrandedUnclippedStart(read),; i",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169
https://github.com/broadinstitute/gatk/issues/5169:2990,Integrability,wrap,wrapAndCopyInto,2990,"09/08 10:32:50 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 6.0 (TID 524, localhost, executor 1, partition 9, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:50 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 6.0 (TID 515, localhost, executor 1): java.lang.NullPointerException; 	at org.broadinstitute.hellbender.utils.read.markduplicates.sparkrecords.EmptyFragment.<init>(EmptyFragment.java:35); 	at org.broadinstitute.hellbender.utils.read.markduplicates.sparkrecords.MarkDuplicatesSparkRecord.newEmptyFragment(MarkDuplicatesSparkRecord.java:37); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$null$0(MarkDuplicatesSparkUtils.java:114); 	at java.util.stream.ReferencePipeline$11$1.accept(ReferencePipeline.java:372); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$transformToDuplicateNames$17d832cf$1(MarkDuplicatesSparkUtils.java:123); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169
https://github.com/broadinstitute/gatk/issues/5169:4236,Performance,concurren,concurrent,4236,"encePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$transformToDuplicateNames$17d832cf$1(MarkDuplicatesSparkUtils.java:123); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). #### Steps to reproduce; Run the MarkDuplicatesSpark in a local SPARK cluster. The following function return a null, which cause the exception. public static String getLibrary( final GATKRead read, final SAMFileHeader header, String defaultLibrary) {; final SAMReadGroupRecord readGroup = getSAMReadGroupRecord(read, header);; String library = readGroup != null ? readGroup.getLibrary() : null;; return library==null? defaultLibrary : library;; }. public EmptyFragment(GATKRead read, SAMFileHeader header, Map<String, Byte> headerLibraryMap) {; super(0, null);; this.R1R = read.isReverseStrand();; this.key = ReadsKey.getKeyForFragment(ReadUtils.getStrandedUnclippedStart(read),; isRead1ReverseStrand(),; ReadUtils.getReferenceIndex(read, header),; headerLibraryMap.get(ReadUtils.getLibrary(read, header, Li",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169
https://github.com/broadinstitute/gatk/issues/5169:4321,Performance,concurren,concurrent,4321,"duplicates.MarkDuplicatesSparkUtils.lambda$transformToDuplicateNames$17d832cf$1(MarkDuplicatesSparkUtils.java:123); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). #### Steps to reproduce; Run the MarkDuplicatesSpark in a local SPARK cluster. The following function return a null, which cause the exception. public static String getLibrary( final GATKRead read, final SAMFileHeader header, String defaultLibrary) {; final SAMReadGroupRecord readGroup = getSAMReadGroupRecord(read, header);; String library = readGroup != null ? readGroup.getLibrary() : null;; return library==null? defaultLibrary : library;; }. public EmptyFragment(GATKRead read, SAMFileHeader header, Map<String, Byte> headerLibraryMap) {; super(0, null);; this.R1R = read.isReverseStrand();; this.key = ReadsKey.getKeyForFragment(ReadUtils.getStrandedUnclippedStart(read),; isRead1ReverseStrand(),; ReadUtils.getReferenceIndex(read, header),; headerLibraryMap.get(ReadUtils.getLibrary(read, header, LibraryIdGenerator.UNKNOWN_LIBRARY)));; }. #### Expected behavior; MarkDuplicatesSpark ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169
https://github.com/broadinstitute/gatk/issues/5169:195,Testability,test,test,195,"## Bug Report. ### Affected tool(s) or class(es); MarkDuplicatesSpark. ### Affected version(s); - [ ] Latest public release version [version?]; 4.0.8.1; - [ ] Latest master branch as of [date of test?]; Sep 10, 2018. ### Description ; 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 515, localhost, executor 1, partition 0, NODE_LOCAL, 5270 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 516, localhost, executor 2, partition 1, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 517, localhost, executor 1, partition 2, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.0 (TID 518, localhost, executor 2, partition 3, NODE_LOCAL, 5594 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.0 (TID 519, localhost, executor 1, partition 4, NODE_LOCAL, 5317 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.0 (TID 520, localhost, executor 2, partition 5, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 6.0 (TID 521, localhost, executor 1, partition 6, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 6.0 (TID 522, localhost, executor 2, partition 7, NODE_LOCAL, 5316 bytes); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:37617 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:45786 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:50 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:37617 (size: 9.0 B, free: 2004.5 MB); 18/09/08 10:32:50 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 6.0 (TID 523, localhost, executor 1, partition 8, NODE_LOCAL, 5604 bytes)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169
https://github.com/broadinstitute/gatk/pull/5170:16,Integrability,message,message,16,This adds a new message to the StreamingProcessController ack FIFO protocol to allow additional message detail to be passed as part of a negative ack. Fixes https://github.com/broadinstitute/gatk/issues/5100.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5170
https://github.com/broadinstitute/gatk/pull/5170:67,Integrability,protocol,protocol,67,This adds a new message to the StreamingProcessController ack FIFO protocol to allow additional message detail to be passed as part of a negative ack. Fixes https://github.com/broadinstitute/gatk/issues/5100.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5170
https://github.com/broadinstitute/gatk/pull/5170:96,Integrability,message,message,96,This adds a new message to the StreamingProcessController ack FIFO protocol to allow additional message detail to be passed as part of a negative ack. Fixes https://github.com/broadinstitute/gatk/issues/5100.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5170
https://github.com/broadinstitute/gatk/issues/5171:46,Testability,test,test,46,"At some positions (such as 20:10001430 in src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.testGVCFMode.gatk4.g.vcf) reads with mismatches or indels in the BWA-aligned pileup get realigned during local assembly (where they will get evaluated as variants in the active region). However, the flanking regions adjacent to the active region are evaluated using the original alignments. This can lead to het calls outside the active region that get capped to GQ0 homRef calls. The solution is not entirely trivial since we hard clip reads after we trim the active region and compute the likelihoods used for their realignments based on this clipped sequence. One relatively easy hack that would be a partial improvement would be to omit reads that had been realigned from the pileup in the flanking regions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5171
https://github.com/broadinstitute/gatk/issues/5171:122,Testability,test,testGVCFMode,122,"At some positions (such as 20:10001430 in src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.testGVCFMode.gatk4.g.vcf) reads with mismatches or indels in the BWA-aligned pileup get realigned during local assembly (where they will get evaluated as variants in the active region). However, the flanking regions adjacent to the active region are evaluated using the original alignments. This can lead to het calls outside the active region that get capped to GQ0 homRef calls. The solution is not entirely trivial since we hard clip reads after we trim the active region and compute the likelihoods used for their realignments based on this clipped sequence. One relatively easy hack that would be a partial improvement would be to omit reads that had been realigned from the pileup in the flanking regions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5171
https://github.com/broadinstitute/gatk/pull/5172:397,Performance,perform,performed,397,"Previous behavior generated some PL=0,0,0 no-calls because CIGAR of reads containing indels wasn't taken into account when determining which reads were informative for the indel ref conf model. The local realignment wasn't being used inside the active region previously either, which has been fixed. A related change considers bases on either side of indels informative if local assembly has been performed (but not during active region detection). Both result in far fewer 0,0,0 calls. Unfortunately there are still some 0,0,X homRef calls related to #5171.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5172
https://github.com/broadinstitute/gatk/pull/5172:437,Safety,detect,detection,437,"Previous behavior generated some PL=0,0,0 no-calls because CIGAR of reads containing indels wasn't taken into account when determining which reads were informative for the indel ref conf model. The local realignment wasn't being used inside the active region previously either, which has been fixed. A related change considers bases on either side of indels informative if local assembly has been performed (but not during active region detection). Both result in far fewer 0,0,0 calls. Unfortunately there are still some 0,0,X homRef calls related to #5171.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5172
https://github.com/broadinstitute/gatk/pull/5175:46,Deployability,update,updated,46,"Code cleanup and technical debt payback, plus updated models.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5175
https://github.com/broadinstitute/gatk/pull/5176:68,Energy Efficiency,efficient,efficiently,68,"Addresses #4397 and #5054. Restructured gCNV WDLs to pass data more efficiently to postprocessing tasks, and added a wrapper workflow for gCNV case WDL that scatters samples in multiple blocks. Also cleaned up some of the unused cromwell travis tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5176
https://github.com/broadinstitute/gatk/pull/5176:117,Integrability,wrap,wrapper,117,"Addresses #4397 and #5054. Restructured gCNV WDLs to pass data more efficiently to postprocessing tasks, and added a wrapper workflow for gCNV case WDL that scatters samples in multiple blocks. Also cleaned up some of the unused cromwell travis tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5176
https://github.com/broadinstitute/gatk/pull/5176:245,Testability,test,tests,245,"Addresses #4397 and #5054. Restructured gCNV WDLs to pass data more efficiently to postprocessing tasks, and added a wrapper workflow for gCNV case WDL that scatters samples in multiple blocks. Also cleaned up some of the unused cromwell travis tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5176
https://github.com/broadinstitute/gatk/issues/5178:373,Deployability,update,updates,373,"## Feature request. ### Tool(s) or class(es) involved; _org.broadinstitute.hellbender.engine.ProgressMeter_; _org.broadinstitute.hellbender.utils.nio.NioFileCopierWithProgressMeter_. ### Description; One `ProgressBar` to rule them all. One `ProgressBar` class hierarchy to bind them. Progress bars/meters should be consolidated into a single class hierarchy, with threaded updates that are triggered by both a `time interval` and a `percentage/# of records completed count` (whichever occurs first). This class hierarchy should have an abstract base `ProgressMeter` class, which has at least 2 concrete child classes - `GenomicProgressMeter` (equivalent to `ProgressMeter`), and `NumericProgressMeter` (which encapsulates the progress meter functionality inside `NioFileCopierWithProgressMeter`). . The progress meter functionality inside `NioFileCopierWithProgressMeter` should be replaced with the resulting progress meter class. The `ProgressMeter` class should be similarly updated / replaced within `GATKTool` to leverage the new class hierarchy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5178
https://github.com/broadinstitute/gatk/issues/5178:978,Deployability,update,updated,978,"## Feature request. ### Tool(s) or class(es) involved; _org.broadinstitute.hellbender.engine.ProgressMeter_; _org.broadinstitute.hellbender.utils.nio.NioFileCopierWithProgressMeter_. ### Description; One `ProgressBar` to rule them all. One `ProgressBar` class hierarchy to bind them. Progress bars/meters should be consolidated into a single class hierarchy, with threaded updates that are triggered by both a `time interval` and a `percentage/# of records completed count` (whichever occurs first). This class hierarchy should have an abstract base `ProgressMeter` class, which has at least 2 concrete child classes - `GenomicProgressMeter` (equivalent to `ProgressMeter`), and `NumericProgressMeter` (which encapsulates the progress meter functionality inside `NioFileCopierWithProgressMeter`). . The progress meter functionality inside `NioFileCopierWithProgressMeter` should be replaced with the resulting progress meter class. The `ProgressMeter` class should be similarly updated / replaced within `GATKTool` to leverage the new class hierarchy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5178
https://github.com/broadinstitute/gatk/issues/5178:298,Energy Efficiency,meter,meters,298,"## Feature request. ### Tool(s) or class(es) involved; _org.broadinstitute.hellbender.engine.ProgressMeter_; _org.broadinstitute.hellbender.utils.nio.NioFileCopierWithProgressMeter_. ### Description; One `ProgressBar` to rule them all. One `ProgressBar` class hierarchy to bind them. Progress bars/meters should be consolidated into a single class hierarchy, with threaded updates that are triggered by both a `time interval` and a `percentage/# of records completed count` (whichever occurs first). This class hierarchy should have an abstract base `ProgressMeter` class, which has at least 2 concrete child classes - `GenomicProgressMeter` (equivalent to `ProgressMeter`), and `NumericProgressMeter` (which encapsulates the progress meter functionality inside `NioFileCopierWithProgressMeter`). . The progress meter functionality inside `NioFileCopierWithProgressMeter` should be replaced with the resulting progress meter class. The `ProgressMeter` class should be similarly updated / replaced within `GATKTool` to leverage the new class hierarchy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5178
https://github.com/broadinstitute/gatk/issues/5178:735,Energy Efficiency,meter,meter,735,"## Feature request. ### Tool(s) or class(es) involved; _org.broadinstitute.hellbender.engine.ProgressMeter_; _org.broadinstitute.hellbender.utils.nio.NioFileCopierWithProgressMeter_. ### Description; One `ProgressBar` to rule them all. One `ProgressBar` class hierarchy to bind them. Progress bars/meters should be consolidated into a single class hierarchy, with threaded updates that are triggered by both a `time interval` and a `percentage/# of records completed count` (whichever occurs first). This class hierarchy should have an abstract base `ProgressMeter` class, which has at least 2 concrete child classes - `GenomicProgressMeter` (equivalent to `ProgressMeter`), and `NumericProgressMeter` (which encapsulates the progress meter functionality inside `NioFileCopierWithProgressMeter`). . The progress meter functionality inside `NioFileCopierWithProgressMeter` should be replaced with the resulting progress meter class. The `ProgressMeter` class should be similarly updated / replaced within `GATKTool` to leverage the new class hierarchy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5178
https://github.com/broadinstitute/gatk/issues/5178:812,Energy Efficiency,meter,meter,812,"## Feature request. ### Tool(s) or class(es) involved; _org.broadinstitute.hellbender.engine.ProgressMeter_; _org.broadinstitute.hellbender.utils.nio.NioFileCopierWithProgressMeter_. ### Description; One `ProgressBar` to rule them all. One `ProgressBar` class hierarchy to bind them. Progress bars/meters should be consolidated into a single class hierarchy, with threaded updates that are triggered by both a `time interval` and a `percentage/# of records completed count` (whichever occurs first). This class hierarchy should have an abstract base `ProgressMeter` class, which has at least 2 concrete child classes - `GenomicProgressMeter` (equivalent to `ProgressMeter`), and `NumericProgressMeter` (which encapsulates the progress meter functionality inside `NioFileCopierWithProgressMeter`). . The progress meter functionality inside `NioFileCopierWithProgressMeter` should be replaced with the resulting progress meter class. The `ProgressMeter` class should be similarly updated / replaced within `GATKTool` to leverage the new class hierarchy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5178
https://github.com/broadinstitute/gatk/issues/5178:919,Energy Efficiency,meter,meter,919,"## Feature request. ### Tool(s) or class(es) involved; _org.broadinstitute.hellbender.engine.ProgressMeter_; _org.broadinstitute.hellbender.utils.nio.NioFileCopierWithProgressMeter_. ### Description; One `ProgressBar` to rule them all. One `ProgressBar` class hierarchy to bind them. Progress bars/meters should be consolidated into a single class hierarchy, with threaded updates that are triggered by both a `time interval` and a `percentage/# of records completed count` (whichever occurs first). This class hierarchy should have an abstract base `ProgressMeter` class, which has at least 2 concrete child classes - `GenomicProgressMeter` (equivalent to `ProgressMeter`), and `NumericProgressMeter` (which encapsulates the progress meter functionality inside `NioFileCopierWithProgressMeter`). . The progress meter functionality inside `NioFileCopierWithProgressMeter` should be replaced with the resulting progress meter class. The `ProgressMeter` class should be similarly updated / replaced within `GATKTool` to leverage the new class hierarchy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5178
https://github.com/broadinstitute/gatk/issues/5178:284,Usability,Progress bar,Progress bars,284,"## Feature request. ### Tool(s) or class(es) involved; _org.broadinstitute.hellbender.engine.ProgressMeter_; _org.broadinstitute.hellbender.utils.nio.NioFileCopierWithProgressMeter_. ### Description; One `ProgressBar` to rule them all. One `ProgressBar` class hierarchy to bind them. Progress bars/meters should be consolidated into a single class hierarchy, with threaded updates that are triggered by both a `time interval` and a `percentage/# of records completed count` (whichever occurs first). This class hierarchy should have an abstract base `ProgressMeter` class, which has at least 2 concrete child classes - `GenomicProgressMeter` (equivalent to `ProgressMeter`), and `NumericProgressMeter` (which encapsulates the progress meter functionality inside `NioFileCopierWithProgressMeter`). . The progress meter functionality inside `NioFileCopierWithProgressMeter` should be replaced with the resulting progress meter class. The `ProgressMeter` class should be similarly updated / replaced within `GATKTool` to leverage the new class hierarchy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5178
https://github.com/broadinstitute/gatk/issues/5179:407,Testability,log,logged,407,"## Feature request. ### Tool(s) or class(es) involved; _org.broadinstitute.hellbender.engine.ProgressMeter_. ### Description; `ProgressMeter` does not currently let the user know how much longer to expect a tool to run on its data. . This can be computed by keeping a tally of the number of entries complete, how long they take, and how many there are in total. . The expected remaining time should then be logged with each log statement.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5179
https://github.com/broadinstitute/gatk/issues/5179:424,Testability,log,log,424,"## Feature request. ### Tool(s) or class(es) involved; _org.broadinstitute.hellbender.engine.ProgressMeter_. ### Description; `ProgressMeter` does not currently let the user know how much longer to expect a tool to run on its data. . This can be computed by keeping a tally of the number of entries complete, how long they take, and how many there are in total. . The expected remaining time should then be logged with each log statement.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5179
https://github.com/broadinstitute/gatk/issues/5180:204,Deployability,update,updates,204,There are a lot of sites where a `ReferenceSequenceFile` is created only to load a sequence dictionary from it. These should be replaced with calls to `ReferenceUtils.loadFastaDictionary()`. There may be updates that can be made to `ReferenceUtils` that make use of new methods in `ReferenceSequenceFileFactory`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5180
https://github.com/broadinstitute/gatk/issues/5180:76,Performance,load,load,76,There are a lot of sites where a `ReferenceSequenceFile` is created only to load a sequence dictionary from it. These should be replaced with calls to `ReferenceUtils.loadFastaDictionary()`. There may be updates that can be made to `ReferenceUtils` that make use of new methods in `ReferenceSequenceFileFactory`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5180
https://github.com/broadinstitute/gatk/issues/5180:167,Performance,load,loadFastaDictionary,167,There are a lot of sites where a `ReferenceSequenceFile` is created only to load a sequence dictionary from it. These should be replaced with calls to `ReferenceUtils.loadFastaDictionary()`. There may be updates that can be made to `ReferenceUtils` that make use of new methods in `ReferenceSequenceFileFactory`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5180
https://github.com/broadinstitute/gatk/pull/5182:389,Deployability,pipeline,pipeline,389,"joint-genotyping from the resulting GVCFs. @ldgauthier & @davidbenjamin this PR is a follow up from our short conversation in #4650 a couple of months ago, where I was wanting to generate GVCFs with MNP support. My goal here is that I really want to be able to generate a single VCF that a) gives me reference confidence and b) gives me MNPs for close by variants. This is for a clinical pipeline where all calling is done one sample at a time, so the problem of joint-genotyping from different MNP representations doesn't come up. I did briefly look at using `--emit-ref-confidence BP_RESOLUTION` but that has two issues that make me prefer this route:. 1) The generated files are really very large because they have a row for every single BP; 2) More problematic, is that when there is a MNP of say `ACG/GCT` two things happen that are less than ideal from my perspective. The first is that rows are emitted into the VCF for all three positions (the variant at A's position, and two `<NON_REF>` lines at the positions for the C and T respectively). Secondly, when one or more bases is the same in both MNP alleles (the C in this case) that base is output with a very high hom-ref GQ, which feels wrong!. I'm more than happy to modify this PR to address any concerns you have (e.g. adding a `--force-mnps-with-gvcfs` parameter that has to be specified, or requiring `--unsafe` to enable this). I'm also open to other solutions, but this seemed expedient and reasonable for folks running single-sample pipelines like you see in clinical settings.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5182
https://github.com/broadinstitute/gatk/pull/5182:1503,Deployability,pipeline,pipelines,1503,"joint-genotyping from the resulting GVCFs. @ldgauthier & @davidbenjamin this PR is a follow up from our short conversation in #4650 a couple of months ago, where I was wanting to generate GVCFs with MNP support. My goal here is that I really want to be able to generate a single VCF that a) gives me reference confidence and b) gives me MNPs for close by variants. This is for a clinical pipeline where all calling is done one sample at a time, so the problem of joint-genotyping from different MNP representations doesn't come up. I did briefly look at using `--emit-ref-confidence BP_RESOLUTION` but that has two issues that make me prefer this route:. 1) The generated files are really very large because they have a row for every single BP; 2) More problematic, is that when there is a MNP of say `ACG/GCT` two things happen that are less than ideal from my perspective. The first is that rows are emitted into the VCF for all three positions (the variant at A's position, and two `<NON_REF>` lines at the positions for the C and T respectively). Secondly, when one or more bases is the same in both MNP alleles (the C in this case) that base is output with a very high hom-ref GQ, which feels wrong!. I'm more than happy to modify this PR to address any concerns you have (e.g. adding a `--force-mnps-with-gvcfs` parameter that has to be specified, or requiring `--unsafe` to enable this). I'm also open to other solutions, but this seemed expedient and reasonable for folks running single-sample pipelines like you see in clinical settings.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5182
https://github.com/broadinstitute/gatk/pull/5182:648,Integrability,rout,route,648,"joint-genotyping from the resulting GVCFs. @ldgauthier & @davidbenjamin this PR is a follow up from our short conversation in #4650 a couple of months ago, where I was wanting to generate GVCFs with MNP support. My goal here is that I really want to be able to generate a single VCF that a) gives me reference confidence and b) gives me MNPs for close by variants. This is for a clinical pipeline where all calling is done one sample at a time, so the problem of joint-genotyping from different MNP representations doesn't come up. I did briefly look at using `--emit-ref-confidence BP_RESOLUTION` but that has two issues that make me prefer this route:. 1) The generated files are really very large because they have a row for every single BP; 2) More problematic, is that when there is a MNP of say `ACG/GCT` two things happen that are less than ideal from my perspective. The first is that rows are emitted into the VCF for all three positions (the variant at A's position, and two `<NON_REF>` lines at the positions for the C and T respectively). Secondly, when one or more bases is the same in both MNP alleles (the C in this case) that base is output with a very high hom-ref GQ, which feels wrong!. I'm more than happy to modify this PR to address any concerns you have (e.g. adding a `--force-mnps-with-gvcfs` parameter that has to be specified, or requiring `--unsafe` to enable this). I'm also open to other solutions, but this seemed expedient and reasonable for folks running single-sample pipelines like you see in clinical settings.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5182
https://github.com/broadinstitute/gatk/pull/5182:1371,Safety,unsafe,unsafe,1371,"joint-genotyping from the resulting GVCFs. @ldgauthier & @davidbenjamin this PR is a follow up from our short conversation in #4650 a couple of months ago, where I was wanting to generate GVCFs with MNP support. My goal here is that I really want to be able to generate a single VCF that a) gives me reference confidence and b) gives me MNPs for close by variants. This is for a clinical pipeline where all calling is done one sample at a time, so the problem of joint-genotyping from different MNP representations doesn't come up. I did briefly look at using `--emit-ref-confidence BP_RESOLUTION` but that has two issues that make me prefer this route:. 1) The generated files are really very large because they have a row for every single BP; 2) More problematic, is that when there is a MNP of say `ACG/GCT` two things happen that are less than ideal from my perspective. The first is that rows are emitted into the VCF for all three positions (the variant at A's position, and two `<NON_REF>` lines at the positions for the C and T respectively). Secondly, when one or more bases is the same in both MNP alleles (the C in this case) that base is output with a very high hom-ref GQ, which feels wrong!. I'm more than happy to modify this PR to address any concerns you have (e.g. adding a `--force-mnps-with-gvcfs` parameter that has to be specified, or requiring `--unsafe` to enable this). I'm also open to other solutions, but this seemed expedient and reasonable for folks running single-sample pipelines like you see in clinical settings.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5182
https://github.com/broadinstitute/gatk/issues/5183:549,Availability,down,downstream,549,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); - [x] Latest public release version [2.1]; - [x] Latest master branch as of [2018-09-13]. ### Description ; The VCF header line; ""##Mutect Version=x.y""; causes problems for some VCF readers. Each header line is required to be a key-value pair and a space character is not expected in the key. (The VCF specification is not clear on this matter, but I've never encountered a space character in a VCF header key before.); Making VCF files that are easily readable by downstream tools should be in the interest of Mutect2. #### Steps to reproduce; Create a VCF file using Mutect2 and look at the header. #### Expected behavior; output; ""##MutectVersion=2.1"". #### Actual behavior; output; ""##Mutect Version=2.1""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5183
https://github.com/broadinstitute/gatk/issues/5183:104,Deployability,release,release,104,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); - [x] Latest public release version [2.1]; - [x] Latest master branch as of [2018-09-13]. ### Description ; The VCF header line; ""##Mutect Version=x.y""; causes problems for some VCF readers. Each header line is required to be a key-value pair and a space character is not expected in the key. (The VCF specification is not clear on this matter, but I've never encountered a space character in a VCF header key before.); Making VCF files that are easily readable by downstream tools should be in the interest of Mutect2. #### Steps to reproduce; Create a VCF file using Mutect2 and look at the header. #### Expected behavior; output; ""##MutectVersion=2.1"". #### Actual behavior; output; ""##Mutect Version=2.1""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5183
https://github.com/broadinstitute/gatk/issues/5183:407,Usability,clear,clear,407,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); - [x] Latest public release version [2.1]; - [x] Latest master branch as of [2018-09-13]. ### Description ; The VCF header line; ""##Mutect Version=x.y""; causes problems for some VCF readers. Each header line is required to be a key-value pair and a space character is not expected in the key. (The VCF specification is not clear on this matter, but I've never encountered a space character in a VCF header key before.); Making VCF files that are easily readable by downstream tools should be in the interest of Mutect2. #### Steps to reproduce; Create a VCF file using Mutect2 and look at the header. #### Expected behavior; output; ""##MutectVersion=2.1"". #### Actual behavior; output; ""##Mutect Version=2.1""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5183
https://github.com/broadinstitute/gatk/issues/5186:444,Deployability,pipeline,pipeline,444,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator, VcfFuncotationFactory_. ### Description; When using a VCF file as a data source, `Funcotator` should include the `ID` column as a separate funcotation. The specific use case is for CLINVAR, but should apply to all VCF data sources. The annotation can be added as `<NAME>_ID` in the VCF data source's output annotations. This feature is requested specifically for the clinical pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5186
https://github.com/broadinstitute/gatk/issues/5187:737,Deployability,pipeline,pipeline,737,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator,GencodeFuncotationFactory_. ### Description; When annotating a variant and the `VariantClassification` is `SPLICE_SITE, INTRON`, Funcotator should include a non-empty `cDNA` annotation. This `cDNA` annotation should include the number of bases that the start of the variant is away from the exon, as well as the reference and alternate alleles of the variant in question. Concretely:; `c.e[EXON NUMBER][+|-][BASES FROM EXON][REF ALLELE]>[ALT ALLELE]`. For example:; c.e2-1A>G; Where:; 2 = the number of the closest exon (1-based); -1 = number of bases away from the exon (1 before); A = Reference allele; G = Alternate allele. This feature is requested for the clinical pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5187
https://github.com/broadinstitute/gatk/issues/5189:155,Availability,error,error,155,```; Using GATK 4.0.2.1. gatk FastqToSam --FASTQ=test_R1.fq --FASTQ2=test_R2.fq --RUN_DATE=2011-04-30T01:00:00+0100 --OUTPUT=test.bam --SM=test; Gives the error No value found for tagged argument: RUN_DATE=2011-04-30T01:00:00+0100. Using Picard 2.18.1-SNAPSHOT. java -jar XXXX/build/libs/picard.jar FastqToSam FASTQ=test_R1.fq FASTQ2=test_R2.fq RUN_DATE=2011-04-30T01:00:00+0100 OUTPUT=test.bam SM=test; Works without issues; ```. See https://gatkforums.broadinstitute.org/gatk/discussion/comment/51988#Comment_51988,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5189
https://github.com/broadinstitute/gatk/issues/5189:125,Testability,test,test,125,```; Using GATK 4.0.2.1. gatk FastqToSam --FASTQ=test_R1.fq --FASTQ2=test_R2.fq --RUN_DATE=2011-04-30T01:00:00+0100 --OUTPUT=test.bam --SM=test; Gives the error No value found for tagged argument: RUN_DATE=2011-04-30T01:00:00+0100. Using Picard 2.18.1-SNAPSHOT. java -jar XXXX/build/libs/picard.jar FastqToSam FASTQ=test_R1.fq FASTQ2=test_R2.fq RUN_DATE=2011-04-30T01:00:00+0100 OUTPUT=test.bam SM=test; Works without issues; ```. See https://gatkforums.broadinstitute.org/gatk/discussion/comment/51988#Comment_51988,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5189
https://github.com/broadinstitute/gatk/issues/5189:139,Testability,test,test,139,```; Using GATK 4.0.2.1. gatk FastqToSam --FASTQ=test_R1.fq --FASTQ2=test_R2.fq --RUN_DATE=2011-04-30T01:00:00+0100 --OUTPUT=test.bam --SM=test; Gives the error No value found for tagged argument: RUN_DATE=2011-04-30T01:00:00+0100. Using Picard 2.18.1-SNAPSHOT. java -jar XXXX/build/libs/picard.jar FastqToSam FASTQ=test_R1.fq FASTQ2=test_R2.fq RUN_DATE=2011-04-30T01:00:00+0100 OUTPUT=test.bam SM=test; Works without issues; ```. See https://gatkforums.broadinstitute.org/gatk/discussion/comment/51988#Comment_51988,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5189
https://github.com/broadinstitute/gatk/issues/5189:386,Testability,test,test,386,```; Using GATK 4.0.2.1. gatk FastqToSam --FASTQ=test_R1.fq --FASTQ2=test_R2.fq --RUN_DATE=2011-04-30T01:00:00+0100 --OUTPUT=test.bam --SM=test; Gives the error No value found for tagged argument: RUN_DATE=2011-04-30T01:00:00+0100. Using Picard 2.18.1-SNAPSHOT. java -jar XXXX/build/libs/picard.jar FastqToSam FASTQ=test_R1.fq FASTQ2=test_R2.fq RUN_DATE=2011-04-30T01:00:00+0100 OUTPUT=test.bam SM=test; Works without issues; ```. See https://gatkforums.broadinstitute.org/gatk/discussion/comment/51988#Comment_51988,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5189
https://github.com/broadinstitute/gatk/issues/5189:398,Testability,test,test,398,```; Using GATK 4.0.2.1. gatk FastqToSam --FASTQ=test_R1.fq --FASTQ2=test_R2.fq --RUN_DATE=2011-04-30T01:00:00+0100 --OUTPUT=test.bam --SM=test; Gives the error No value found for tagged argument: RUN_DATE=2011-04-30T01:00:00+0100. Using Picard 2.18.1-SNAPSHOT. java -jar XXXX/build/libs/picard.jar FastqToSam FASTQ=test_R1.fq FASTQ2=test_R2.fq RUN_DATE=2011-04-30T01:00:00+0100 OUTPUT=test.bam SM=test; Works without issues; ```. See https://gatkforums.broadinstitute.org/gatk/discussion/comment/51988#Comment_51988,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5189
https://github.com/broadinstitute/gatk/pull/5190:249,Availability,error,error,249,"The current implementation does not check whether `histo.get(testStatU)` returns a valid pointer or not. If runtime optimization is used (such as the case in J9 JIT), JVM may remove that bin if there is nothing in it. The outcome is a `NullPointer` error at runtime for some corner cases. Adding a pointer check can solve the problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5190
https://github.com/broadinstitute/gatk/pull/5190:116,Performance,optimiz,optimization,116,"The current implementation does not check whether `histo.get(testStatU)` returns a valid pointer or not. If runtime optimization is used (such as the case in J9 JIT), JVM may remove that bin if there is nothing in it. The outcome is a `NullPointer` error at runtime for some corner cases. Adding a pointer check can solve the problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5190
https://github.com/broadinstitute/gatk/pull/5190:61,Testability,test,testStatU,61,"The current implementation does not check whether `histo.get(testStatU)` returns a valid pointer or not. If runtime optimization is used (such as the case in J9 JIT), JVM may remove that bin if there is nothing in it. The outcome is a `NullPointer` error at runtime for some corner cases. Adding a pointer check can solve the problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5190
https://github.com/broadinstitute/gatk/pull/5192:152,Usability,clear,clear,152,"-Move the Spark reference datasource classes from the engine.datasources package into the; engine.spark.datasources package, and rename them to make it clear that they are for use; on Spark. This fixes a longstanding problem where they were getting confused with the; walker ReferenceDataSource/ReferenceFileSource classes. -Delete the unused/unmaintained experimental tool BaseRecalibratorSparkSharded, which has; fallen out-of-date relative to BaseRecalibratorSpark, as well as its unused companion `AddContextDataToReadsSparkOptimized`. -Delete an extra ""VariantSource"" class that is now unused (note: this is not the same as; VariantSparkSource, which is used extensively and retained here)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5192
https://github.com/broadinstitute/gatk/pull/5193:376,Deployability,pipeline,pipeline,376,"The workflow for mitochondria will include running `AddOriginalAlignmentTags` on a bam, realigning it to the mitochondria contig only, then running `Mutect2` with `--annotation OriginalAlignment` and `--median-autosomal-coverage` to get the appropriate annotations. Then running `FilterMitochondrialMutectCalls`. . I don't think any of these changes should effect the somatic pipeline. @ldgauthier I didn't change the name of `TLOD` or `tumor sample` in the mitochondria vcf. Maybe we can talk next week about how to best do that?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5193
https://github.com/broadinstitute/gatk/pull/5194:78,Availability,error,errors,78,* updating google-cloud-java 0.59.0 -> 0.62.0; * this includes retries on 502 errors see https://github.com/GoogleCloudPlatform/google-cloud-java/pull/3557,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5194
https://github.com/broadinstitute/gatk/pull/5197:949,Modifiability,variab,variable,949,"This PR is a finalized version of https://github.com/broadinstitute/gatk/pull/5017. I've copied the branch into our repo so that travis will run cloud tests on it. Currently, only Posix filesystem paths can be passed as workspaces and arrays to GenomicsDB via GenomicsDBImport and SelectVariants. This PR will allow for hdfs and gcs (and emrfs/s3) URIs to be supported as well. ; Examples; ```; ./gatk GenomicsDBImport -V /vcfs/sample.vcf.gz --genomicsdb-workspace-path hdfs://master:9000/gdb_ws -L 1:500-10000; ./gatk GenomicsDBImport -V /vcfs/sample.vcf.gz --genomicsdb-workspace-path gs://my_bucket/gdb_ws -L 1:500-10000; ```; ```; ./gatk SelectVariants -V gendb.hdfs://master:9000/gdb_ws -R hs37d5.fa -O out.vcf; ./gatk SelectVariants -V gendb.gs://my_bucket/gdb_ws -R hs37d5.fa -O out.vcf; ```; GenomicsDB supports GCS via the [Cloud Storage Connector](https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage). Set environment variable GOOGLE_APPLICATION_CREDENTIALS to point to the GCS Service Account json file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5197
https://github.com/broadinstitute/gatk/pull/5197:151,Testability,test,tests,151,"This PR is a finalized version of https://github.com/broadinstitute/gatk/pull/5017. I've copied the branch into our repo so that travis will run cloud tests on it. Currently, only Posix filesystem paths can be passed as workspaces and arrays to GenomicsDB via GenomicsDBImport and SelectVariants. This PR will allow for hdfs and gcs (and emrfs/s3) URIs to be supported as well. ; Examples; ```; ./gatk GenomicsDBImport -V /vcfs/sample.vcf.gz --genomicsdb-workspace-path hdfs://master:9000/gdb_ws -L 1:500-10000; ./gatk GenomicsDBImport -V /vcfs/sample.vcf.gz --genomicsdb-workspace-path gs://my_bucket/gdb_ws -L 1:500-10000; ```; ```; ./gatk SelectVariants -V gendb.hdfs://master:9000/gdb_ws -R hs37d5.fa -O out.vcf; ./gatk SelectVariants -V gendb.gs://my_bucket/gdb_ws -R hs37d5.fa -O out.vcf; ```; GenomicsDB supports GCS via the [Cloud Storage Connector](https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage). Set environment variable GOOGLE_APPLICATION_CREDENTIALS to point to the GCS Service Account json file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5197
https://github.com/broadinstitute/gatk/issues/5212:96,Deployability,release,release,96,The changes in #5112 introduced several problems that have prevented us from performing a maven release of 4.0.9.0. I believe that the two issues are:. 1. The sources jar is misspelled as `source` jar; 2. The generated pom files are missing project level information. . We should:. - [x] Manually fix the files and peform a release of 4.0.9.0.; - [ ] Patch the build.gradle so that it's correct for future release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5212
https://github.com/broadinstitute/gatk/issues/5212:324,Deployability,release,release,324,The changes in #5112 introduced several problems that have prevented us from performing a maven release of 4.0.9.0. I believe that the two issues are:. 1. The sources jar is misspelled as `source` jar; 2. The generated pom files are missing project level information. . We should:. - [x] Manually fix the files and peform a release of 4.0.9.0.; - [ ] Patch the build.gradle so that it's correct for future release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5212
https://github.com/broadinstitute/gatk/issues/5212:351,Deployability,Patch,Patch,351,The changes in #5112 introduced several problems that have prevented us from performing a maven release of 4.0.9.0. I believe that the two issues are:. 1. The sources jar is misspelled as `source` jar; 2. The generated pom files are missing project level information. . We should:. - [x] Manually fix the files and peform a release of 4.0.9.0.; - [ ] Patch the build.gradle so that it's correct for future release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5212
https://github.com/broadinstitute/gatk/issues/5212:406,Deployability,release,release,406,The changes in #5112 introduced several problems that have prevented us from performing a maven release of 4.0.9.0. I believe that the two issues are:. 1. The sources jar is misspelled as `source` jar; 2. The generated pom files are missing project level information. . We should:. - [x] Manually fix the files and peform a release of 4.0.9.0.; - [ ] Patch the build.gradle so that it's correct for future release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5212
https://github.com/broadinstitute/gatk/issues/5212:77,Performance,perform,performing,77,The changes in #5112 introduced several problems that have prevented us from performing a maven release of 4.0.9.0. I believe that the two issues are:. 1. The sources jar is misspelled as `source` jar; 2. The generated pom files are missing project level information. . We should:. - [x] Manually fix the files and peform a release of 4.0.9.0.; - [ ] Patch the build.gradle so that it's correct for future release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5212
https://github.com/broadinstitute/gatk/issues/5213:255,Safety,detect,detect,255,"Skipping the push builds that was introduced in #5156 has caused an unintended side effect when there is a merge conflict. In this case the pr build cant run, so no build is run but it reports a pass. This is unhelpful. . We should investigate how we can detect if there is a merge conflict and run the push builds in that case.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5213
https://github.com/broadinstitute/gatk/pull/5214:34,Availability,down,download,34,"* changing the key hash we use to download R package keys on travis from an insecure 32 bit hash that has been compromised to a more secure longer hash; * we will no longer be installing the ""Totally Legit Signing Key""; * see https://evil32.com/ for a summary of the problem",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5214
https://github.com/broadinstitute/gatk/pull/5214:176,Deployability,install,installing,176,"* changing the key hash we use to download R package keys on travis from an insecure 32 bit hash that has been compromised to a more secure longer hash; * we will no longer be installing the ""Totally Legit Signing Key""; * see https://evil32.com/ for a summary of the problem",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5214
https://github.com/broadinstitute/gatk/pull/5214:19,Security,hash,hash,19,"* changing the key hash we use to download R package keys on travis from an insecure 32 bit hash that has been compromised to a more secure longer hash; * we will no longer be installing the ""Totally Legit Signing Key""; * see https://evil32.com/ for a summary of the problem",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5214
https://github.com/broadinstitute/gatk/pull/5214:92,Security,hash,hash,92,"* changing the key hash we use to download R package keys on travis from an insecure 32 bit hash that has been compromised to a more secure longer hash; * we will no longer be installing the ""Totally Legit Signing Key""; * see https://evil32.com/ for a summary of the problem",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5214
https://github.com/broadinstitute/gatk/pull/5214:133,Security,secur,secure,133,"* changing the key hash we use to download R package keys on travis from an insecure 32 bit hash that has been compromised to a more secure longer hash; * we will no longer be installing the ""Totally Legit Signing Key""; * see https://evil32.com/ for a summary of the problem",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5214
https://github.com/broadinstitute/gatk/pull/5214:147,Security,hash,hash,147,"* changing the key hash we use to download R package keys on travis from an insecure 32 bit hash that has been compromised to a more secure longer hash; * we will no longer be installing the ""Totally Legit Signing Key""; * see https://evil32.com/ for a summary of the problem",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5214
https://github.com/broadinstitute/gatk/issues/5217:353,Deployability,update,update,353,I used the very last version of the WDL using the last master commit for the docker and the name of the output files seem to be a random permutation of the actual sample names. The name inside the file (i.e. the one listed in the #CHROM line) seems to be correct. A previous run using a earlier version of the WDL (probably the one just before the last update) and the latest official gatk docker didn't have this issue.; .,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5217
https://github.com/broadinstitute/gatk/issues/5218:51,Availability,avail,available,51,"GATK Version: 4.0.9.0. The issue: ; The previously available ""beta"" tool MarkDuplicatesGATK introduced reliable support for CRAM formatted files into the GATK tool chain. . Since that was removed again, I noticed that the default/picard-derived MarkDuplicates does not work reliably on CRAM formatted files. I am getting:. java.lang.IllegalStateException: A valid CRAM reference was not supplied and one cannot be acquired via the property settings reference_fasta or use_cram_ref_download. using this call:; gatk MarkDuplicates -I Sample_AS-230151.clean.cram -O test.cram -M metrics -R /ifs/data/nfs_share/ikmb_repository/references/gatk/bundle/2.8/b37/human_g1k_v37.clean.fasta. Files in folder: Sample_AS-230151.clean.cram Sample_AS-230151.clean.cram.bai Sample_AS-230151.clean.cram.crai. Since CRAM provides a *significant* storage saving over BAM, we would very much like use CRAM exclusively. But this still seems to be an issue, despite reports over at the Picard-side of things dating back to 2016 and beyond. . Any chance of this ever getting fixed?. Cheers,; Marc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5218
https://github.com/broadinstitute/gatk/issues/5218:103,Availability,reliab,reliable,103,"GATK Version: 4.0.9.0. The issue: ; The previously available ""beta"" tool MarkDuplicatesGATK introduced reliable support for CRAM formatted files into the GATK tool chain. . Since that was removed again, I noticed that the default/picard-derived MarkDuplicates does not work reliably on CRAM formatted files. I am getting:. java.lang.IllegalStateException: A valid CRAM reference was not supplied and one cannot be acquired via the property settings reference_fasta or use_cram_ref_download. using this call:; gatk MarkDuplicates -I Sample_AS-230151.clean.cram -O test.cram -M metrics -R /ifs/data/nfs_share/ikmb_repository/references/gatk/bundle/2.8/b37/human_g1k_v37.clean.fasta. Files in folder: Sample_AS-230151.clean.cram Sample_AS-230151.clean.cram.bai Sample_AS-230151.clean.cram.crai. Since CRAM provides a *significant* storage saving over BAM, we would very much like use CRAM exclusively. But this still seems to be an issue, despite reports over at the Picard-side of things dating back to 2016 and beyond. . Any chance of this ever getting fixed?. Cheers,; Marc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5218
https://github.com/broadinstitute/gatk/issues/5218:274,Availability,reliab,reliably,274,"GATK Version: 4.0.9.0. The issue: ; The previously available ""beta"" tool MarkDuplicatesGATK introduced reliable support for CRAM formatted files into the GATK tool chain. . Since that was removed again, I noticed that the default/picard-derived MarkDuplicates does not work reliably on CRAM formatted files. I am getting:. java.lang.IllegalStateException: A valid CRAM reference was not supplied and one cannot be acquired via the property settings reference_fasta or use_cram_ref_download. using this call:; gatk MarkDuplicates -I Sample_AS-230151.clean.cram -O test.cram -M metrics -R /ifs/data/nfs_share/ikmb_repository/references/gatk/bundle/2.8/b37/human_g1k_v37.clean.fasta. Files in folder: Sample_AS-230151.clean.cram Sample_AS-230151.clean.cram.bai Sample_AS-230151.clean.cram.crai. Since CRAM provides a *significant* storage saving over BAM, we would very much like use CRAM exclusively. But this still seems to be an issue, despite reports over at the Picard-side of things dating back to 2016 and beyond. . Any chance of this ever getting fixed?. Cheers,; Marc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5218
https://github.com/broadinstitute/gatk/issues/5218:563,Testability,test,test,563,"GATK Version: 4.0.9.0. The issue: ; The previously available ""beta"" tool MarkDuplicatesGATK introduced reliable support for CRAM formatted files into the GATK tool chain. . Since that was removed again, I noticed that the default/picard-derived MarkDuplicates does not work reliably on CRAM formatted files. I am getting:. java.lang.IllegalStateException: A valid CRAM reference was not supplied and one cannot be acquired via the property settings reference_fasta or use_cram_ref_download. using this call:; gatk MarkDuplicates -I Sample_AS-230151.clean.cram -O test.cram -M metrics -R /ifs/data/nfs_share/ikmb_repository/references/gatk/bundle/2.8/b37/human_g1k_v37.clean.fasta. Files in folder: Sample_AS-230151.clean.cram Sample_AS-230151.clean.cram.bai Sample_AS-230151.clean.cram.crai. Since CRAM provides a *significant* storage saving over BAM, we would very much like use CRAM exclusively. But this still seems to be an issue, despite reports over at the Picard-side of things dating back to 2016 and beyond. . Any chance of this ever getting fixed?. Cheers,; Marc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5218
https://github.com/broadinstitute/gatk/pull/5219:92,Performance,perform,performance,92,"Fixes https://github.com/broadinstitute/gatk/issues/2865. To match GATK without sacrificing performance for existing GenotypeGVCF modes, this adds a new traversal type `VariantWalkerGroupedByLocus` which can process variants one-at-a-time, or optionally grouped by Locus. GenotypeGVCFs itself then has to further filter the variants to emulate the ""prioritize start location"" behavior in GATK3. If we keep this traversal, I'll add some unit tests, and we should find a better name. In order to match GATK3 output for sites where the only alt allele is a spanning deletion, I had to use `OutputMode.EMIT_ALL_SITES` when emitting nonvariant sites only (GATK3 uses `OutputMode.EMIT_ALL_CONFIDENT_SITES` in that case). Specifically, its necessary to to prevent GenotypingEngine from entering [this](https://github.com/broadinstitute/gatk/blob/33af6de5fb990f4642245b7a80b76d251371ef09/src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java#L285) recently added code block, and short circuiting out. @ldgauthier Any opinion on whether this is too permissive ? Another option would be to special-case this, perhaps by adding another OutputMode state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5219
https://github.com/broadinstitute/gatk/pull/5219:441,Testability,test,tests,441,"Fixes https://github.com/broadinstitute/gatk/issues/2865. To match GATK without sacrificing performance for existing GenotypeGVCF modes, this adds a new traversal type `VariantWalkerGroupedByLocus` which can process variants one-at-a-time, or optionally grouped by Locus. GenotypeGVCFs itself then has to further filter the variants to emulate the ""prioritize start location"" behavior in GATK3. If we keep this traversal, I'll add some unit tests, and we should find a better name. In order to match GATK3 output for sites where the only alt allele is a spanning deletion, I had to use `OutputMode.EMIT_ALL_SITES` when emitting nonvariant sites only (GATK3 uses `OutputMode.EMIT_ALL_CONFIDENT_SITES` in that case). Specifically, its necessary to to prevent GenotypingEngine from entering [this](https://github.com/broadinstitute/gatk/blob/33af6de5fb990f4642245b7a80b76d251371ef09/src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java#L285) recently added code block, and short circuiting out. @ldgauthier Any opinion on whether this is too permissive ? Another option would be to special-case this, perhaps by adding another OutputMode state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5219
https://github.com/broadinstitute/gatk/issues/5220:5434,Availability,error,error,5434,"ava HotSpot(TM) 64-Bit Server VM v1.8.0_181-b13; 14:35:47.080 INFO SelectVariants - Start Date/Time: September 24, 2018 2:35:45 PM EET; 14:35:47.080 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.081 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.082 INFO SelectVariants - HTSJDK Version: 2.16.1; 14:35:47.082 INFO SelectVariants - Picard Version: 2.18.13; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:35:47.082 INFO SelectVariants - Deflater: IntelDeflater; 14:35:47.082 INFO SelectVariants - Inflater: IntelInflater; ```. From @jean-philippe-martin . > This error message is related to GATK's ability to load files on Google buckets (""gcs://bucket/file.bam""). This ability is enabled even when running locally (this aspect is intentional, because it's useful to be able to run a local GATK instance to process remote data without having to fire up a VM).; > ; > As the bucket-reading code (""NIO"") initializes, it looks for credentials to use. Those can be set via an environment variable or via gcloud auth, as described in GATK's README. If neither of these are set, it checks whether it's currently running in a Google virtual machine (so it can figure out who owns the virtual machine that it's running on, and use those credentials). Apparently this code throws an exception if it runs out of ways to find credentials, and our code prints it out and moves on.; > ; > The message is useful, for if we were running in a google VM and the credential-finding failed, we'd certainly like to know. Whether we need the full stack trace, now, that's a choice we have to make.; > . We should tone down the e",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5220
https://github.com/broadinstitute/gatk/issues/5220:6468,Availability,down,down,6468,"ver VM v1.8.0_181-b13; 14:35:47.080 INFO SelectVariants - Start Date/Time: September 24, 2018 2:35:45 PM EET; 14:35:47.080 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.081 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.082 INFO SelectVariants - HTSJDK Version: 2.16.1; 14:35:47.082 INFO SelectVariants - Picard Version: 2.18.13; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:35:47.082 INFO SelectVariants - Deflater: IntelDeflater; 14:35:47.082 INFO SelectVariants - Inflater: IntelInflater; ```. From @jean-philippe-martin . > This error message is related to GATK's ability to load files on Google buckets (""gcs://bucket/file.bam""). This ability is enabled even when running locally (this aspect is intentional, because it's useful to be able to run a local GATK instance to process remote data without having to fire up a VM).; > ; > As the bucket-reading code (""NIO"") initializes, it looks for credentials to use. Those can be set via an environment variable or via gcloud auth, as described in GATK's README. If neither of these are set, it checks whether it's currently running in a Google virtual machine (so it can figure out who owns the virtual machine that it's running on, and use those credentials). Apparently this code throws an exception if it runs out of ways to find credentials, and our code prints it out and moves on.; > ; > The message is useful, for if we were running in a google VM and the credential-finding failed, we'd certainly like to know. Whether we need the full stack trace, now, that's a choice we have to make.; > . We should tone down the error message if possible.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5220
https://github.com/broadinstitute/gatk/issues/5220:6477,Availability,error,error,6477,"ver VM v1.8.0_181-b13; 14:35:47.080 INFO SelectVariants - Start Date/Time: September 24, 2018 2:35:45 PM EET; 14:35:47.080 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.081 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.082 INFO SelectVariants - HTSJDK Version: 2.16.1; 14:35:47.082 INFO SelectVariants - Picard Version: 2.18.13; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:35:47.082 INFO SelectVariants - Deflater: IntelDeflater; 14:35:47.082 INFO SelectVariants - Inflater: IntelInflater; ```. From @jean-philippe-martin . > This error message is related to GATK's ability to load files on Google buckets (""gcs://bucket/file.bam""). This ability is enabled even when running locally (this aspect is intentional, because it's useful to be able to run a local GATK instance to process remote data without having to fire up a VM).; > ; > As the bucket-reading code (""NIO"") initializes, it looks for credentials to use. Those can be set via an environment variable or via gcloud auth, as described in GATK's README. If neither of these are set, it checks whether it's currently running in a Google virtual machine (so it can figure out who owns the virtual machine that it's running on, and use those credentials). Apparently this code throws an exception if it runs out of ways to find credentials, and our code prints it out and moves on.; > ; > The message is useful, for if we were running in a google VM and the credential-finding failed, we'd certainly like to know. Whether we need the full stack trace, now, that's a choice we have to make.; > . We should tone down the error message if possible.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5220
https://github.com/broadinstitute/gatk/issues/5220:1010,Integrability,rout,route,1010,"ing the following unhelpful stack trace when running a local job. . ```; Gokalps-Mac-mini:1000GVCFs sky$ gatk SelectVariants -V 1000G_CEU_chr16.vcf.gz -O 1000G_CEU_AFfilt_chr16.vcf.gz -select ""AF > 0.0""; Using GATK jar /Users/sky/scripts/gatk-package-4.0.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /Users/sky/scripts/gatk-package-4.0.9.0-local.jar SelectVariants -V 1000G_CEU_chr16.vcf.gz -O 1000G_CEU_AFfilt_chr16.vcf.gz -select AF > 0.0; 14:35:45.842 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/sky/scripts/gatk-package-4.0.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 24, 2018 2:35:47 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: No route to host (connect failed); at java.net.PlainSocketImpl.socketConnect(Native Method); at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); at java.net.Socket.connect(Socket.java:589); at sun.net.NetworkClient.doConnect(NetworkClient.java:175); at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); at sun.net.www.http.HttpClient.New(HttpClient.java:339); at sun.net.www.http.HttpClient.New(HttpClient.java:357); at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220); at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156); at sun.net.www.pr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5220
https://github.com/broadinstitute/gatk/issues/5220:1833,Integrability,protocol,protocol,1833,oogle.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: No route to host (connect failed); at java.net.PlainSocketImpl.socketConnect(Native Method); at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); at java.net.Socket.connect(Socket.java:589); at sun.net.NetworkClient.doConnect(NetworkClient.java:175); at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); at sun.net.www.http.HttpClient.New(HttpClient.java:339); at sun.net.www.http.HttpClient.New(HttpClient.java:357); at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220); at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156); at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1050); at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:984); at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:104); at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.runningOnComputeEngine(ComputeEngineCredentials.java:210); at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.tryGetComputeCredentials(DefaultCredentialsProvider.java:290); at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentialsUnsynchronized(DefaultCredentialsProvider.java:207); at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsPr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5220
https://github.com/broadinstitute/gatk/issues/5220:1927,Integrability,protocol,protocol,1927,hether we are running on Google Compute Engine.; java.net.ConnectException: No route to host (connect failed); at java.net.PlainSocketImpl.socketConnect(Native Method); at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); at java.net.Socket.connect(Socket.java:589); at sun.net.NetworkClient.doConnect(NetworkClient.java:175); at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); at sun.net.www.http.HttpClient.New(HttpClient.java:339); at sun.net.www.http.HttpClient.New(HttpClient.java:357); at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220); at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156); at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1050); at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:984); at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:104); at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.runningOnComputeEngine(ComputeEngineCredentials.java:210); at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.tryGetComputeCredentials(DefaultCredentialsProvider.java:290); at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentialsUnsynchronized(DefaultCredentialsProvider.java:207); at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentials(DefaultCredentialsProvider.java:124); at shaded.cloud_nio.com.goo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5220
https://github.com/broadinstitute/gatk/issues/5220:2018,Integrability,protocol,protocol,2018,t (connect failed); at java.net.PlainSocketImpl.socketConnect(Native Method); at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); at java.net.Socket.connect(Socket.java:589); at sun.net.NetworkClient.doConnect(NetworkClient.java:175); at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); at sun.net.www.http.HttpClient.New(HttpClient.java:339); at sun.net.www.http.HttpClient.New(HttpClient.java:357); at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220); at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156); at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1050); at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:984); at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:104); at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.runningOnComputeEngine(ComputeEngineCredentials.java:210); at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.tryGetComputeCredentials(DefaultCredentialsProvider.java:290); at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentialsUnsynchronized(DefaultCredentialsProvider.java:207); at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentials(DefaultCredentialsProvider.java:124); at shaded.cloud_nio.com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:127); at sha,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5220
https://github.com/broadinstitute/gatk/issues/5220:2108,Integrability,protocol,protocol,2108,AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); at java.net.Socket.connect(Socket.java:589); at sun.net.NetworkClient.doConnect(NetworkClient.java:175); at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); at sun.net.www.http.HttpClient.New(HttpClient.java:339); at sun.net.www.http.HttpClient.New(HttpClient.java:357); at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220); at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156); at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1050); at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:984); at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:104); at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.runningOnComputeEngine(ComputeEngineCredentials.java:210); at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.tryGetComputeCredentials(DefaultCredentialsProvider.java:290); at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentialsUnsynchronized(DefaultCredentialsProvider.java:207); at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentials(DefaultCredentialsProvider.java:124); at shaded.cloud_nio.com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:127); at shaded.cloud_nio.com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredent,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5220
https://github.com/broadinstitute/gatk/issues/5220:5440,Integrability,message,message,5440,"ava HotSpot(TM) 64-Bit Server VM v1.8.0_181-b13; 14:35:47.080 INFO SelectVariants - Start Date/Time: September 24, 2018 2:35:45 PM EET; 14:35:47.080 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.081 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.082 INFO SelectVariants - HTSJDK Version: 2.16.1; 14:35:47.082 INFO SelectVariants - Picard Version: 2.18.13; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:35:47.082 INFO SelectVariants - Deflater: IntelDeflater; 14:35:47.082 INFO SelectVariants - Inflater: IntelInflater; ```. From @jean-philippe-martin . > This error message is related to GATK's ability to load files on Google buckets (""gcs://bucket/file.bam""). This ability is enabled even when running locally (this aspect is intentional, because it's useful to be able to run a local GATK instance to process remote data without having to fire up a VM).; > ; > As the bucket-reading code (""NIO"") initializes, it looks for credentials to use. Those can be set via an environment variable or via gcloud auth, as described in GATK's README. If neither of these are set, it checks whether it's currently running in a Google virtual machine (so it can figure out who owns the virtual machine that it's running on, and use those credentials). Apparently this code throws an exception if it runs out of ways to find credentials, and our code prints it out and moves on.; > ; > The message is useful, for if we were running in a google VM and the credential-finding failed, we'd certainly like to know. Whether we need the full stack trace, now, that's a choice we have to make.; > . We should tone down the e",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5220
https://github.com/broadinstitute/gatk/issues/5220:6251,Integrability,message,message,6251,"ver VM v1.8.0_181-b13; 14:35:47.080 INFO SelectVariants - Start Date/Time: September 24, 2018 2:35:45 PM EET; 14:35:47.080 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.081 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.082 INFO SelectVariants - HTSJDK Version: 2.16.1; 14:35:47.082 INFO SelectVariants - Picard Version: 2.18.13; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:35:47.082 INFO SelectVariants - Deflater: IntelDeflater; 14:35:47.082 INFO SelectVariants - Inflater: IntelInflater; ```. From @jean-philippe-martin . > This error message is related to GATK's ability to load files on Google buckets (""gcs://bucket/file.bam""). This ability is enabled even when running locally (this aspect is intentional, because it's useful to be able to run a local GATK instance to process remote data without having to fire up a VM).; > ; > As the bucket-reading code (""NIO"") initializes, it looks for credentials to use. Those can be set via an environment variable or via gcloud auth, as described in GATK's README. If neither of these are set, it checks whether it's currently running in a Google virtual machine (so it can figure out who owns the virtual machine that it's running on, and use those credentials). Apparently this code throws an exception if it runs out of ways to find credentials, and our code prints it out and moves on.; > ; > The message is useful, for if we were running in a google VM and the credential-finding failed, we'd certainly like to know. Whether we need the full stack trace, now, that's a choice we have to make.; > . We should tone down the error message if possible.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5220
https://github.com/broadinstitute/gatk/issues/5220:6483,Integrability,message,message,6483,"ver VM v1.8.0_181-b13; 14:35:47.080 INFO SelectVariants - Start Date/Time: September 24, 2018 2:35:45 PM EET; 14:35:47.080 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.081 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.082 INFO SelectVariants - HTSJDK Version: 2.16.1; 14:35:47.082 INFO SelectVariants - Picard Version: 2.18.13; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:35:47.082 INFO SelectVariants - Deflater: IntelDeflater; 14:35:47.082 INFO SelectVariants - Inflater: IntelInflater; ```. From @jean-philippe-martin . > This error message is related to GATK's ability to load files on Google buckets (""gcs://bucket/file.bam""). This ability is enabled even when running locally (this aspect is intentional, because it's useful to be able to run a local GATK instance to process remote data without having to fire up a VM).; > ; > As the bucket-reading code (""NIO"") initializes, it looks for credentials to use. Those can be set via an environment variable or via gcloud auth, as described in GATK's README. If neither of these are set, it checks whether it's currently running in a Google virtual machine (so it can figure out who owns the virtual machine that it's running on, and use those credentials). Apparently this code throws an exception if it runs out of ways to find credentials, and our code prints it out and moves on.; > ; > The message is useful, for if we were running in a google VM and the credential-finding failed, we'd certainly like to know. Whether we need the full stack trace, now, that's a choice we have to make.; > . We should tone down the error message if possible.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5220
https://github.com/broadinstitute/gatk/issues/5220:5855,Modifiability,variab,variable,5855,"ver VM v1.8.0_181-b13; 14:35:47.080 INFO SelectVariants - Start Date/Time: September 24, 2018 2:35:45 PM EET; 14:35:47.080 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.081 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.082 INFO SelectVariants - HTSJDK Version: 2.16.1; 14:35:47.082 INFO SelectVariants - Picard Version: 2.18.13; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:35:47.082 INFO SelectVariants - Deflater: IntelDeflater; 14:35:47.082 INFO SelectVariants - Inflater: IntelInflater; ```. From @jean-philippe-martin . > This error message is related to GATK's ability to load files on Google buckets (""gcs://bucket/file.bam""). This ability is enabled even when running locally (this aspect is intentional, because it's useful to be able to run a local GATK instance to process remote data without having to fire up a VM).; > ; > As the bucket-reading code (""NIO"") initializes, it looks for credentials to use. Those can be set via an environment variable or via gcloud auth, as described in GATK's README. If neither of these are set, it checks whether it's currently running in a Google virtual machine (so it can figure out who owns the virtual machine that it's running on, and use those credentials). Apparently this code throws an exception if it runs out of ways to find credentials, and our code prints it out and moves on.; > ; > The message is useful, for if we were running in a google VM and the credential-finding failed, we'd certainly like to know. Whether we need the full stack trace, now, that's a choice we have to make.; > . We should tone down the error message if possible.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5220
https://github.com/broadinstitute/gatk/issues/5220:646,Performance,Load,Loading,646,"A user reports getting the following unhelpful stack trace when running a local job. . ```; Gokalps-Mac-mini:1000GVCFs sky$ gatk SelectVariants -V 1000G_CEU_chr16.vcf.gz -O 1000G_CEU_AFfilt_chr16.vcf.gz -select ""AF > 0.0""; Using GATK jar /Users/sky/scripts/gatk-package-4.0.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /Users/sky/scripts/gatk-package-4.0.9.0-local.jar SelectVariants -V 1000G_CEU_chr16.vcf.gz -O 1000G_CEU_AFfilt_chr16.vcf.gz -select AF > 0.0; 14:35:45.842 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/sky/scripts/gatk-package-4.0.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 24, 2018 2:35:47 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: No route to host (connect failed); at java.net.PlainSocketImpl.socketConnect(Native Method); at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); at java.net.Socket.connect(Socket.java:589); at sun.net.NetworkClient.doConnect(NetworkClient.java:175); at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); at sun.net.www.http.HttpClient.New(HttpClient.java:339); at sun.net.www.http.HttpClient.New(HttpClient.java:357); at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220); at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5220
https://github.com/broadinstitute/gatk/issues/5220:5480,Performance,load,load,5480,"ava HotSpot(TM) 64-Bit Server VM v1.8.0_181-b13; 14:35:47.080 INFO SelectVariants - Start Date/Time: September 24, 2018 2:35:45 PM EET; 14:35:47.080 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.081 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.082 INFO SelectVariants - HTSJDK Version: 2.16.1; 14:35:47.082 INFO SelectVariants - Picard Version: 2.18.13; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:35:47.082 INFO SelectVariants - Deflater: IntelDeflater; 14:35:47.082 INFO SelectVariants - Inflater: IntelInflater; ```. From @jean-philippe-martin . > This error message is related to GATK's ability to load files on Google buckets (""gcs://bucket/file.bam""). This ability is enabled even when running locally (this aspect is intentional, because it's useful to be able to run a local GATK instance to process remote data without having to fire up a VM).; > ; > As the bucket-reading code (""NIO"") initializes, it looks for credentials to use. Those can be set via an environment variable or via gcloud auth, as described in GATK's README. If neither of these are set, it checks whether it's currently running in a Google virtual machine (so it can figure out who owns the virtual machine that it's running on, and use those credentials). Apparently this code throws an exception if it runs out of ways to find credentials, and our code prints it out and moves on.; > ; > The message is useful, for if we were running in a google VM and the credential-finding failed, we'd certainly like to know. Whether we need the full stack trace, now, that's a choice we have to make.; > . We should tone down the e",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5220
https://github.com/broadinstitute/gatk/issues/5220:923,Safety,detect,detect,923,"A user reports getting the following unhelpful stack trace when running a local job. . ```; Gokalps-Mac-mini:1000GVCFs sky$ gatk SelectVariants -V 1000G_CEU_chr16.vcf.gz -O 1000G_CEU_AFfilt_chr16.vcf.gz -select ""AF > 0.0""; Using GATK jar /Users/sky/scripts/gatk-package-4.0.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /Users/sky/scripts/gatk-package-4.0.9.0-local.jar SelectVariants -V 1000G_CEU_chr16.vcf.gz -O 1000G_CEU_AFfilt_chr16.vcf.gz -select AF > 0.0; 14:35:45.842 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/sky/scripts/gatk-package-4.0.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 24, 2018 2:35:47 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: No route to host (connect failed); at java.net.PlainSocketImpl.socketConnect(Native Method); at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); at java.net.Socket.connect(Socket.java:589); at sun.net.NetworkClient.doConnect(NetworkClient.java:175); at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); at sun.net.www.http.HttpClient.New(HttpClient.java:339); at sun.net.www.http.HttpClient.New(HttpClient.java:357); at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220); at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5220
https://github.com/broadinstitute/gatk/pull/5222:516,Energy Efficiency,efficient,efficient,516,"This PR fixes #3823 opened by @LeeTL1220 . There are several series of commits applied here:. 1. The first set of commits rebase `ll_CollectAllelicCountsSpark` on `master`.; 2. Then there's a commit (https://github.com/broadinstitute/gatk/commit/21e1dcfb88fc6543f6ba3e6095eba512a33f9f8d) to fix up some changes to make `CollectAllelicCountsSpark` work on `master`.; 3. The next set of commits applies the changes from #5127 and #5221 (since they have not yet been merged) to make passing the reference in Spark more efficient. These changes are needed for the fix below to work.; 4. The actual fix is in https://github.com/broadinstitute/gatk/commit/3326b9093246ff6fb51ad5537951b4646411d80f.; 5. There's also a new test for `ExampleLocusWalkerSpark` in addition to the one for `CollectAllelicCountsSpark`. The problem was that `emitEmptyLoci()` in `LocusWalkerSpark` was not working properly. Any intervals that didn't overlap with reads were being dropped, which meant that those loci were not being passed to the `LocusWalkerSpark` subclass.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5222
https://github.com/broadinstitute/gatk/pull/5222:715,Testability,test,test,715,"This PR fixes #3823 opened by @LeeTL1220 . There are several series of commits applied here:. 1. The first set of commits rebase `ll_CollectAllelicCountsSpark` on `master`.; 2. Then there's a commit (https://github.com/broadinstitute/gatk/commit/21e1dcfb88fc6543f6ba3e6095eba512a33f9f8d) to fix up some changes to make `CollectAllelicCountsSpark` work on `master`.; 3. The next set of commits applies the changes from #5127 and #5221 (since they have not yet been merged) to make passing the reference in Spark more efficient. These changes are needed for the fix below to work.; 4. The actual fix is in https://github.com/broadinstitute/gatk/commit/3326b9093246ff6fb51ad5537951b4646411d80f.; 5. There's also a new test for `ExampleLocusWalkerSpark` in addition to the one for `CollectAllelicCountsSpark`. The problem was that `emitEmptyLoci()` in `LocusWalkerSpark` was not working properly. Any intervals that didn't overlap with reads were being dropped, which meant that those loci were not being passed to the `LocusWalkerSpark` subclass.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5222
https://github.com/broadinstitute/gatk/issues/5223:109,Testability,test,test,109,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_. ### Description. Need to add more indel test cases for HG38 for completeness. The set of test cases should be: . {3' and 5' UTRs, exons, introns, intronic and exonic splice sites, 3' and 5' flanks}; x; lengths in bases of {1,2,3,5} (maybe a subset of these, TBD); x; {PIK3CA, MUC16 transcripts, HLA/alternate contig targets}; X; {insertion, deletion}; x; {hg38}",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5223
https://github.com/broadinstitute/gatk/issues/5223:158,Testability,test,test,158,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_. ### Description. Need to add more indel test cases for HG38 for completeness. The set of test cases should be: . {3' and 5' UTRs, exons, introns, intronic and exonic splice sites, 3' and 5' flanks}; x; lengths in bases of {1,2,3,5} (maybe a subset of these, TBD); x; {PIK3CA, MUC16 transcripts, HLA/alternate contig targets}; X; {insertion, deletion}; x; {hg38}",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5223
https://github.com/broadinstitute/gatk/pull/5224:113,Modifiability,config,configure,113,* fixing two issues that prevented us from publishing to maven central; - fixing sourceJar typo -> sourcesJar; - configure the pom for all artifact filters; * partial fix for https://github.com/broadinstitute/gatk/issues/5212,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5224
https://github.com/broadinstitute/gatk/pull/5225:116,Testability,test,tested,116,"Closes #5217. @vruano, I've verified that cohort and scattered-case mode WDLs run correctly on FC (to be precise, I tested the WDLs from my sl_filter branch rebased on this branch, but this branch alone should be fine as well). However, #4397 should still be addressed at some point in the future. This can wait until we have v1 of the FC evaluation in place.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5225
https://github.com/broadinstitute/gatk/pull/5229:123,Availability,failure,failure,123,This removes some symlinks that were checked into git-lfs and puts them back into normal git. It stops lfs from outputting failure messages on checkout.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5229
https://github.com/broadinstitute/gatk/pull/5229:131,Integrability,message,messages,131,This removes some symlinks that were checked into git-lfs and puts them back into normal git. It stops lfs from outputting failure messages on checkout.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5229
https://github.com/broadinstitute/gatk/issues/5230:1765,Integrability,wrap,wrapAndCopyInto,1765,verhangFixingManager.addReadGroup(OverhangFixingManager.java:209); 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.splitNCigarRead(SplitNCigarReads.java:270); 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.firstPassApply(SplitNCigarReads.java:180); 	at org.broadinstitute.hellbender.engine.TwoPassReadWalker.lambda$traverseReads$0(TwoPassReadWalker.java:62); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.TwoPassReadWalker.traverseReads(TwoPassReadWalker.java:60); 	at org.broadinstitute.hellbender.engine.TwoPassReadWalker.traverse(TwoPassReadWalker.java:42); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:979); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLinePro,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5230
https://github.com/broadinstitute/gatk/issues/5230:187,Security,Validat,ValidateSamFile,187,"@jamesemery, researcher has uploaded data to </humgen/gsa-scr1/pub/incoming/Exception_in_SplitNCigarReads.tgz> and has clarified a few other details within the forum thread, e.g. running ValidateSamFile `IGNORE=MISSING_TAG_NM IGNORE=MATE_NOT_FOUND` allows for validation. Thanks for looking into this. ---. Hello,. I am getting the following exception when running SplitNCigarReads on RNA-Seq data using GATK 4.0.8.1:; ```; java.lang.ArrayIndexOutOfBoundsException: 100; 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.overhangingBasesMismatch(OverhangFixingManager.java:313); 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.fixSplit(OverhangFixingManager.java:252); 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.addReadGroup(OverhangFixingManager.java:209); 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.splitNCigarRead(SplitNCigarReads.java:270); 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.firstPassApply(SplitNCigarReads.java:180); 	at org.broadinstitute.hellbender.engine.TwoPassReadWalker.lambda$traverseReads$0(TwoPassReadWalker.java:62); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5230
https://github.com/broadinstitute/gatk/issues/5230:260,Security,validat,validation,260,"@jamesemery, researcher has uploaded data to </humgen/gsa-scr1/pub/incoming/Exception_in_SplitNCigarReads.tgz> and has clarified a few other details within the forum thread, e.g. running ValidateSamFile `IGNORE=MISSING_TAG_NM IGNORE=MATE_NOT_FOUND` allows for validation. Thanks for looking into this. ---. Hello,. I am getting the following exception when running SplitNCigarReads on RNA-Seq data using GATK 4.0.8.1:; ```; java.lang.ArrayIndexOutOfBoundsException: 100; 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.overhangingBasesMismatch(OverhangFixingManager.java:313); 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.fixSplit(OverhangFixingManager.java:252); 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.addReadGroup(OverhangFixingManager.java:209); 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.splitNCigarRead(SplitNCigarReads.java:270); 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.firstPassApply(SplitNCigarReads.java:180); 	at org.broadinstitute.hellbender.engine.TwoPassReadWalker.lambda$traverseReads$0(TwoPassReadWalker.java:62); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5230
https://github.com/broadinstitute/gatk/issues/5230:3099,Security,Validat,ValidateSamFile,3099,stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.TwoPassReadWalker.traverseReads(TwoPassReadWalker.java:60); 	at org.broadinstitute.hellbender.engine.TwoPassReadWalker.traverse(TwoPassReadWalker.java:42); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:979); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Command:; ```; ./gatk/gatk \; SplitNCigarReads \; --reference $REF \; --input test3.bam \; --output output.bam \; --verbosity DEBUG \; > split.log 2>&1; ```; Running ValidateSamFile does not reveal anything suspicious and visual inspection of the reads also appears to be fine. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/12801/exception-in-splitncigarreads/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5230
https://github.com/broadinstitute/gatk/issues/5230:3076,Testability,log,log,3076,stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.TwoPassReadWalker.traverseReads(TwoPassReadWalker.java:60); 	at org.broadinstitute.hellbender.engine.TwoPassReadWalker.traverse(TwoPassReadWalker.java:42); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:979); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Command:; ```; ./gatk/gatk \; SplitNCigarReads \; --reference $REF \; --input test3.bam \; --output output.bam \; --verbosity DEBUG \; > split.log 2>&1; ```; Running ValidateSamFile does not reveal anything suspicious and visual inspection of the reads also appears to be fine. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/12801/exception-in-splitncigarreads/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5230
https://github.com/broadinstitute/gatk/issues/5233:1062,Deployability,install,install,1062,"Profiling using JBuilder remotely on gsa5 (with a large load from other programs) seem to show that close to or over 50% of the CPU effort is dedicated to filter ""bad"" reads. <img width=""1006"" alt=""screen shot 2018-09-27 at 2 37 59 pm"" src=""https://user-images.githubusercontent.com/791104/46167159-09fe1f00-c263-11e8-8ea0-02621146659b.png"">. To reproduce you may run (or better make your copy and run on a different profiling port and folder):; ```; cd /dsde/working/valentin/crc-profiling; sh run.sh; ```; ```; #run.sh contents:; java -agentpath:./bin/linux-x64/libjprofilerti.so=port=5006,nowait -jar ./gatk-local.jar \ ; CollectReadCounts \ ; -I /dsde/working/CHM/33remap/msb2.m38.bam \; -R /seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta \; -O /tmp/test.tsv -L hg38.interval_list -imr OVERLAPPING_ONLY; ```. The sample was chosen kinda at random , is a CHM pseudo diploid sample but you could use an alternative. Please change profiling port and output file. On the profiling machine (presumably your laptop or desktop) you need to install JProfiler (Broad owns a license for that).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5233
https://github.com/broadinstitute/gatk/issues/5233:56,Performance,load,load,56,"Profiling using JBuilder remotely on gsa5 (with a large load from other programs) seem to show that close to or over 50% of the CPU effort is dedicated to filter ""bad"" reads. <img width=""1006"" alt=""screen shot 2018-09-27 at 2 37 59 pm"" src=""https://user-images.githubusercontent.com/791104/46167159-09fe1f00-c263-11e8-8ea0-02621146659b.png"">. To reproduce you may run (or better make your copy and run on a different profiling port and folder):; ```; cd /dsde/working/valentin/crc-profiling; sh run.sh; ```; ```; #run.sh contents:; java -agentpath:./bin/linux-x64/libjprofilerti.so=port=5006,nowait -jar ./gatk-local.jar \ ; CollectReadCounts \ ; -I /dsde/working/CHM/33remap/msb2.m38.bam \; -R /seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta \; -O /tmp/test.tsv -L hg38.interval_list -imr OVERLAPPING_ONLY; ```. The sample was chosen kinda at random , is a CHM pseudo diploid sample but you could use an alternative. Please change profiling port and output file. On the profiling machine (presumably your laptop or desktop) you need to install JProfiler (Broad owns a license for that).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5233
https://github.com/broadinstitute/gatk/issues/5233:779,Testability,test,test,779,"Profiling using JBuilder remotely on gsa5 (with a large load from other programs) seem to show that close to or over 50% of the CPU effort is dedicated to filter ""bad"" reads. <img width=""1006"" alt=""screen shot 2018-09-27 at 2 37 59 pm"" src=""https://user-images.githubusercontent.com/791104/46167159-09fe1f00-c263-11e8-8ea0-02621146659b.png"">. To reproduce you may run (or better make your copy and run on a different profiling port and folder):; ```; cd /dsde/working/valentin/crc-profiling; sh run.sh; ```; ```; #run.sh contents:; java -agentpath:./bin/linux-x64/libjprofilerti.so=port=5006,nowait -jar ./gatk-local.jar \ ; CollectReadCounts \ ; -I /dsde/working/CHM/33remap/msb2.m38.bam \; -R /seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta \; -O /tmp/test.tsv -L hg38.interval_list -imr OVERLAPPING_ONLY; ```. The sample was chosen kinda at random , is a CHM pseudo diploid sample but you could use an alternative. Please change profiling port and output file. On the profiling machine (presumably your laptop or desktop) you need to install JProfiler (Broad owns a license for that).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5233
https://github.com/broadinstitute/gatk/issues/5234:356,Availability,avail,available,356,"## Documentation system request. Currently, the tooldoc generation system does not separate arguments that relate to deployment decisions like compute platform (eg `--gcs-project-for-requester-pays`) from the ones that modify the analytical or processing behavior of the tools. This adds to the cognitive burden involved in sorting through all the options available for a given tool. We'd like to have a separate category for these arguments so that they would be isolated from the rest. . In addition, there are a bunch of convenience arguments in the common args section that have more to do with how we're running the tool than its analysis behavior, and could also be consolidated into this separate category (or their own category but that might be too granular). Examples below are from the popular tool [SelectVariants](https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_walkers_variantutils_SelectVariants.php):. #### Arguments that would be stratified as platform args. `--cloud-index-prefetch-buffer`; `--cloud-prefetch-buffer`; `--disable-bam-index-caching`; `--gcs-max-retries`; `--gcs-project-for-requester-pays`. #### Arguments that would be stratified as convenience args. `--arguments_file` ; `--help` ; `--version` ; `--create-output-bam-index` ; `--create-output-bam-md5`; `--create-output-variant-index`; `--create-output-variant-md5`; `--gatk-config-file`; `--QUIET`; `--seconds-between-progress-updates`; `--tmp-dir`; `--use-jdk-deflater`; `--use-jdk-inflater`; `--verbosity`; `--showHidden` -> I thought we had got rid of hidden args??. These could also be stratified as convenience but one could argue they affect tool behavior qualitatively:. `--disable-sequence-dictionary-validation`; `--lenient`; `--read-validation-stringency`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5234
https://github.com/broadinstitute/gatk/issues/5234:117,Deployability,deploy,deployment,117,"## Documentation system request. Currently, the tooldoc generation system does not separate arguments that relate to deployment decisions like compute platform (eg `--gcs-project-for-requester-pays`) from the ones that modify the analytical or processing behavior of the tools. This adds to the cognitive burden involved in sorting through all the options available for a given tool. We'd like to have a separate category for these arguments so that they would be isolated from the rest. . In addition, there are a bunch of convenience arguments in the common args section that have more to do with how we're running the tool than its analysis behavior, and could also be consolidated into this separate category (or their own category but that might be too granular). Examples below are from the popular tool [SelectVariants](https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_walkers_variantutils_SelectVariants.php):. #### Arguments that would be stratified as platform args. `--cloud-index-prefetch-buffer`; `--cloud-prefetch-buffer`; `--disable-bam-index-caching`; `--gcs-max-retries`; `--gcs-project-for-requester-pays`. #### Arguments that would be stratified as convenience args. `--arguments_file` ; `--help` ; `--version` ; `--create-output-bam-index` ; `--create-output-bam-md5`; `--create-output-variant-index`; `--create-output-variant-md5`; `--gatk-config-file`; `--QUIET`; `--seconds-between-progress-updates`; `--tmp-dir`; `--use-jdk-deflater`; `--use-jdk-inflater`; `--verbosity`; `--showHidden` -> I thought we had got rid of hidden args??. These could also be stratified as convenience but one could argue they affect tool behavior qualitatively:. `--disable-sequence-dictionary-validation`; `--lenient`; `--read-validation-stringency`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5234
https://github.com/broadinstitute/gatk/issues/5234:1473,Deployability,update,updates,1473,"## Documentation system request. Currently, the tooldoc generation system does not separate arguments that relate to deployment decisions like compute platform (eg `--gcs-project-for-requester-pays`) from the ones that modify the analytical or processing behavior of the tools. This adds to the cognitive burden involved in sorting through all the options available for a given tool. We'd like to have a separate category for these arguments so that they would be isolated from the rest. . In addition, there are a bunch of convenience arguments in the common args section that have more to do with how we're running the tool than its analysis behavior, and could also be consolidated into this separate category (or their own category but that might be too granular). Examples below are from the popular tool [SelectVariants](https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_walkers_variantutils_SelectVariants.php):. #### Arguments that would be stratified as platform args. `--cloud-index-prefetch-buffer`; `--cloud-prefetch-buffer`; `--disable-bam-index-caching`; `--gcs-max-retries`; `--gcs-project-for-requester-pays`. #### Arguments that would be stratified as convenience args. `--arguments_file` ; `--help` ; `--version` ; `--create-output-bam-index` ; `--create-output-bam-md5`; `--create-output-variant-index`; `--create-output-variant-md5`; `--gatk-config-file`; `--QUIET`; `--seconds-between-progress-updates`; `--tmp-dir`; `--use-jdk-deflater`; `--use-jdk-inflater`; `--verbosity`; `--showHidden` -> I thought we had got rid of hidden args??. These could also be stratified as convenience but one could argue they affect tool behavior qualitatively:. `--disable-sequence-dictionary-validation`; `--lenient`; `--read-validation-stringency`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5234
https://github.com/broadinstitute/gatk/issues/5234:1420,Modifiability,config,config-file,1420,"## Documentation system request. Currently, the tooldoc generation system does not separate arguments that relate to deployment decisions like compute platform (eg `--gcs-project-for-requester-pays`) from the ones that modify the analytical or processing behavior of the tools. This adds to the cognitive burden involved in sorting through all the options available for a given tool. We'd like to have a separate category for these arguments so that they would be isolated from the rest. . In addition, there are a bunch of convenience arguments in the common args section that have more to do with how we're running the tool than its analysis behavior, and could also be consolidated into this separate category (or their own category but that might be too granular). Examples below are from the popular tool [SelectVariants](https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_walkers_variantutils_SelectVariants.php):. #### Arguments that would be stratified as platform args. `--cloud-index-prefetch-buffer`; `--cloud-prefetch-buffer`; `--disable-bam-index-caching`; `--gcs-max-retries`; `--gcs-project-for-requester-pays`. #### Arguments that would be stratified as convenience args. `--arguments_file` ; `--help` ; `--version` ; `--create-output-bam-index` ; `--create-output-bam-md5`; `--create-output-variant-index`; `--create-output-variant-md5`; `--gatk-config-file`; `--QUIET`; `--seconds-between-progress-updates`; `--tmp-dir`; `--use-jdk-deflater`; `--use-jdk-inflater`; `--verbosity`; `--showHidden` -> I thought we had got rid of hidden args??. These could also be stratified as convenience but one could argue they affect tool behavior qualitatively:. `--disable-sequence-dictionary-validation`; `--lenient`; `--read-validation-stringency`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5234
https://github.com/broadinstitute/gatk/issues/5234:1755,Security,validat,validation,1755,"## Documentation system request. Currently, the tooldoc generation system does not separate arguments that relate to deployment decisions like compute platform (eg `--gcs-project-for-requester-pays`) from the ones that modify the analytical or processing behavior of the tools. This adds to the cognitive burden involved in sorting through all the options available for a given tool. We'd like to have a separate category for these arguments so that they would be isolated from the rest. . In addition, there are a bunch of convenience arguments in the common args section that have more to do with how we're running the tool than its analysis behavior, and could also be consolidated into this separate category (or their own category but that might be too granular). Examples below are from the popular tool [SelectVariants](https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_walkers_variantutils_SelectVariants.php):. #### Arguments that would be stratified as platform args. `--cloud-index-prefetch-buffer`; `--cloud-prefetch-buffer`; `--disable-bam-index-caching`; `--gcs-max-retries`; `--gcs-project-for-requester-pays`. #### Arguments that would be stratified as convenience args. `--arguments_file` ; `--help` ; `--version` ; `--create-output-bam-index` ; `--create-output-bam-md5`; `--create-output-variant-index`; `--create-output-variant-md5`; `--gatk-config-file`; `--QUIET`; `--seconds-between-progress-updates`; `--tmp-dir`; `--use-jdk-deflater`; `--use-jdk-inflater`; `--verbosity`; `--showHidden` -> I thought we had got rid of hidden args??. These could also be stratified as convenience but one could argue they affect tool behavior qualitatively:. `--disable-sequence-dictionary-validation`; `--lenient`; `--read-validation-stringency`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5234
https://github.com/broadinstitute/gatk/issues/5234:1789,Security,validat,validation-stringency,1789,"## Documentation system request. Currently, the tooldoc generation system does not separate arguments that relate to deployment decisions like compute platform (eg `--gcs-project-for-requester-pays`) from the ones that modify the analytical or processing behavior of the tools. This adds to the cognitive burden involved in sorting through all the options available for a given tool. We'd like to have a separate category for these arguments so that they would be isolated from the rest. . In addition, there are a bunch of convenience arguments in the common args section that have more to do with how we're running the tool than its analysis behavior, and could also be consolidated into this separate category (or their own category but that might be too granular). Examples below are from the popular tool [SelectVariants](https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_walkers_variantutils_SelectVariants.php):. #### Arguments that would be stratified as platform args. `--cloud-index-prefetch-buffer`; `--cloud-prefetch-buffer`; `--disable-bam-index-caching`; `--gcs-max-retries`; `--gcs-project-for-requester-pays`. #### Arguments that would be stratified as convenience args. `--arguments_file` ; `--help` ; `--version` ; `--create-output-bam-index` ; `--create-output-bam-md5`; `--create-output-variant-index`; `--create-output-variant-md5`; `--gatk-config-file`; `--QUIET`; `--seconds-between-progress-updates`; `--tmp-dir`; `--use-jdk-deflater`; `--use-jdk-inflater`; `--verbosity`; `--showHidden` -> I thought we had got rid of hidden args??. These could also be stratified as convenience but one could argue they affect tool behavior qualitatively:. `--disable-sequence-dictionary-validation`; `--lenient`; `--read-validation-stringency`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5234
https://github.com/broadinstitute/gatk/issues/5236:174,Usability,simpl,simple,174,"GATK version: 4.0.9.0. The issue: I wanted to use the built-in Tool SplitIntervals to partition the WGS calling intervals (GATK bundle). These are in interval_list format. A simple line count (excluding the headers) shows vast differences (much fewer intervals across all the splits than were in the original interval list), which seems to suggest a problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5236
https://github.com/broadinstitute/gatk/issues/5240:248,Modifiability,flexible,flexible,248,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_, _MafOutputRenderer_, _VcfOutputRenderer_. ### Description; For VCFs, we render each funcotation separately, then concatenate strings. This approach has the drawback of being less flexible in terms of ordering the fields (and setting up aliases), but that has not mattered yet. Regardless, it means that all string operations (e.g. excluding fields and sanitizing values) must be in the same method (in this case renderSanitizedFuncotationForVcf) and that method must work on a funcotation. For MAFs, we flatten out the funcotations and put the fields into a giant map. Then we do the changes to field names and values on that map. But by the time I want to exclude fields and sanitize, the map is already made, so we do not render individual funcotations. Therefore no need for a renderSantiziedFuncotationForMaf. We should investigate how easy it would be to generalize an output renderer to use the `map` convention like in `MafOutputRenderer` so we can bubble up that functionality. Since there are only 2 output types now, it might be best to do it before we get more of them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5240
https://github.com/broadinstitute/gatk/issues/5240:421,Security,sanitiz,sanitizing,421,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_, _MafOutputRenderer_, _VcfOutputRenderer_. ### Description; For VCFs, we render each funcotation separately, then concatenate strings. This approach has the drawback of being less flexible in terms of ordering the fields (and setting up aliases), but that has not mattered yet. Regardless, it means that all string operations (e.g. excluding fields and sanitizing values) must be in the same method (in this case renderSanitizedFuncotationForVcf) and that method must work on a funcotation. For MAFs, we flatten out the funcotations and put the fields into a giant map. Then we do the changes to field names and values on that map. But by the time I want to exclude fields and sanitize, the map is already made, so we do not render individual funcotations. Therefore no need for a renderSantiziedFuncotationForMaf. We should investigate how easy it would be to generalize an output renderer to use the `map` convention like in `MafOutputRenderer` so we can bubble up that functionality. Since there are only 2 output types now, it might be best to do it before we get more of them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5240
https://github.com/broadinstitute/gatk/issues/5240:745,Security,sanitiz,sanitize,745,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_, _MafOutputRenderer_, _VcfOutputRenderer_. ### Description; For VCFs, we render each funcotation separately, then concatenate strings. This approach has the drawback of being less flexible in terms of ordering the fields (and setting up aliases), but that has not mattered yet. Regardless, it means that all string operations (e.g. excluding fields and sanitizing values) must be in the same method (in this case renderSanitizedFuncotationForVcf) and that method must work on a funcotation. For MAFs, we flatten out the funcotations and put the fields into a giant map. Then we do the changes to field names and values on that map. But by the time I want to exclude fields and sanitize, the map is already made, so we do not render individual funcotations. Therefore no need for a renderSantiziedFuncotationForMaf. We should investigate how easy it would be to generalize an output renderer to use the `map` convention like in `MafOutputRenderer` so we can bubble up that functionality. Since there are only 2 output types now, it might be best to do it before we get more of them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5240
https://github.com/broadinstitute/gatk/pull/5241:185,Deployability,release,release,185,"The palindrome artifact read filter wasn't checking for an edge case. Now it is. @takutosato Could you review? And if you approve and it's Monday or Tuesday, could you merge before the release?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5241
https://github.com/broadinstitute/gatk/pull/5242:128,Deployability,update,updated,128,- M2 WDL has explicit optional parameter for a list of fields that should be excluded from the output.; - Both M2 WDL files are updated. Manually tested mutect2.wdl on local backend. Closes #5141,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5242
https://github.com/broadinstitute/gatk/pull/5242:146,Testability,test,tested,146,- M2 WDL has explicit optional parameter for a list of fields that should be excluded from the output.; - Both M2 WDL files are updated. Manually tested mutect2.wdl on local backend. Closes #5141,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5242
https://github.com/broadinstitute/gatk/issues/5243:206,Testability,test,test,206,## Bug Report. ### Affected tool(s) or class(es); _Funcotator_. ### Affected version(s); - [X] Latest master branch as of [20181001]. ### Description ; There appears to be an extra folder in the funcotator test datasources in `large`:. `large/funcotator/funcotator_dataSources/cancer_gene_census/hg19/hg38`; `large/funcotator/funcotator_dataSources/cancer_gene_census/hg38/hg38`. This should be removed.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5243
https://github.com/broadinstitute/gatk/issues/5250:335,Testability,test,tests,335,"This should be possible in the FeatureDataSource using code similar to https://github.com/Intel-HLS/GenomicsDB/blob/master/src/main/java/com/intel/genomicsdb/importer/extensions/VidMapExtensions.java#L69. We don't have a use case for this right now, so I'm calling it low priority. I started a draft at ldg_addAScombineOp but it needs tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5250
https://github.com/broadinstitute/gatk/pull/5252:364,Availability,error,error,364,"Several experimental changes that improve precision results, and expand possible evaluations, of GATK CNV:. - `combine_tracks.wdl` for post-processing somatic CNV calls. This wdl will perform two operations:; - Increase precision by removing:; - germline segments. As a result, the WDL requires the matched normal segments.; - Areas of common germline activity or error from other cancer studies.; - Convert the tumor model seg file to the same format as AllelicCapSeg, which can be read by ABSOLUTE. This is currently done inline in the WDL. ; - This is not a trivial conversion, since each segment must be called whether it is balanced or not (MAF =? 0.5). The current algorithm relies on hard filtering and may need updating pending evaluation.; - For more information about AllelicCapSeg and ABSOLUTE, see: ; - Carter et al. *Absolute quantification of somatic DNA alterations in human cancer*, Nat Biotechnol. 2012 May; 30(5): 413421 ; - https://software.broadinstitute.org/cancer/cga/absolute ; - Brastianos, P.K., Carter S.L., et al. *Genomic Characterization of Brain Metastases Reveals Branched Evolution and Potential Therapeutic Targets* (2015) Cancer Discovery PMID:26410082. - Changes to GATK tools to support the above:; - `SimpleGermlineTagger` now uses reciprocal overlap to in addition to breakpoint matching when determining a possible germline event. This greatly improved results in areas near centromeres.; - Added tool `MergeAnnotatedRegionsByAnnotation`. This simple tool will merge genomic regions (specified in a tsv) when given annotations (columns) contain exact values in neighboring segments and the segments are within a specified maximum genomic distance. . - `multi_combine_tracks.wdl` and `aggregate_combine_tracks.wdl` which run `combine_tracks.wdl` on multiple pairs and combine the results into one seg file for easy consumption by IGV.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5252
https://github.com/broadinstitute/gatk/pull/5252:1854,Energy Efficiency,consumption,consumption,1854,"Several experimental changes that improve precision results, and expand possible evaluations, of GATK CNV:. - `combine_tracks.wdl` for post-processing somatic CNV calls. This wdl will perform two operations:; - Increase precision by removing:; - germline segments. As a result, the WDL requires the matched normal segments.; - Areas of common germline activity or error from other cancer studies.; - Convert the tumor model seg file to the same format as AllelicCapSeg, which can be read by ABSOLUTE. This is currently done inline in the WDL. ; - This is not a trivial conversion, since each segment must be called whether it is balanced or not (MAF =? 0.5). The current algorithm relies on hard filtering and may need updating pending evaluation.; - For more information about AllelicCapSeg and ABSOLUTE, see: ; - Carter et al. *Absolute quantification of somatic DNA alterations in human cancer*, Nat Biotechnol. 2012 May; 30(5): 413421 ; - https://software.broadinstitute.org/cancer/cga/absolute ; - Brastianos, P.K., Carter S.L., et al. *Genomic Characterization of Brain Metastases Reveals Branched Evolution and Potential Therapeutic Targets* (2015) Cancer Discovery PMID:26410082. - Changes to GATK tools to support the above:; - `SimpleGermlineTagger` now uses reciprocal overlap to in addition to breakpoint matching when determining a possible germline event. This greatly improved results in areas near centromeres.; - Added tool `MergeAnnotatedRegionsByAnnotation`. This simple tool will merge genomic regions (specified in a tsv) when given annotations (columns) contain exact values in neighboring segments and the segments are within a specified maximum genomic distance. . - `multi_combine_tracks.wdl` and `aggregate_combine_tracks.wdl` which run `combine_tracks.wdl` on multiple pairs and combine the results into one seg file for easy consumption by IGV.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5252
https://github.com/broadinstitute/gatk/pull/5252:184,Performance,perform,perform,184,"Several experimental changes that improve precision results, and expand possible evaluations, of GATK CNV:. - `combine_tracks.wdl` for post-processing somatic CNV calls. This wdl will perform two operations:; - Increase precision by removing:; - germline segments. As a result, the WDL requires the matched normal segments.; - Areas of common germline activity or error from other cancer studies.; - Convert the tumor model seg file to the same format as AllelicCapSeg, which can be read by ABSOLUTE. This is currently done inline in the WDL. ; - This is not a trivial conversion, since each segment must be called whether it is balanced or not (MAF =? 0.5). The current algorithm relies on hard filtering and may need updating pending evaluation.; - For more information about AllelicCapSeg and ABSOLUTE, see: ; - Carter et al. *Absolute quantification of somatic DNA alterations in human cancer*, Nat Biotechnol. 2012 May; 30(5): 413421 ; - https://software.broadinstitute.org/cancer/cga/absolute ; - Brastianos, P.K., Carter S.L., et al. *Genomic Characterization of Brain Metastases Reveals Branched Evolution and Potential Therapeutic Targets* (2015) Cancer Discovery PMID:26410082. - Changes to GATK tools to support the above:; - `SimpleGermlineTagger` now uses reciprocal overlap to in addition to breakpoint matching when determining a possible germline event. This greatly improved results in areas near centromeres.; - Added tool `MergeAnnotatedRegionsByAnnotation`. This simple tool will merge genomic regions (specified in a tsv) when given annotations (columns) contain exact values in neighboring segments and the segments are within a specified maximum genomic distance. . - `multi_combine_tracks.wdl` and `aggregate_combine_tracks.wdl` which run `combine_tracks.wdl` on multiple pairs and combine the results into one seg file for easy consumption by IGV.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5252
https://github.com/broadinstitute/gatk/pull/5252:1239,Usability,Simpl,SimpleGermlineTagger,1239,"Several experimental changes that improve precision results, and expand possible evaluations, of GATK CNV:. - `combine_tracks.wdl` for post-processing somatic CNV calls. This wdl will perform two operations:; - Increase precision by removing:; - germline segments. As a result, the WDL requires the matched normal segments.; - Areas of common germline activity or error from other cancer studies.; - Convert the tumor model seg file to the same format as AllelicCapSeg, which can be read by ABSOLUTE. This is currently done inline in the WDL. ; - This is not a trivial conversion, since each segment must be called whether it is balanced or not (MAF =? 0.5). The current algorithm relies on hard filtering and may need updating pending evaluation.; - For more information about AllelicCapSeg and ABSOLUTE, see: ; - Carter et al. *Absolute quantification of somatic DNA alterations in human cancer*, Nat Biotechnol. 2012 May; 30(5): 413421 ; - https://software.broadinstitute.org/cancer/cga/absolute ; - Brastianos, P.K., Carter S.L., et al. *Genomic Characterization of Brain Metastases Reveals Branched Evolution and Potential Therapeutic Targets* (2015) Cancer Discovery PMID:26410082. - Changes to GATK tools to support the above:; - `SimpleGermlineTagger` now uses reciprocal overlap to in addition to breakpoint matching when determining a possible germline event. This greatly improved results in areas near centromeres.; - Added tool `MergeAnnotatedRegionsByAnnotation`. This simple tool will merge genomic regions (specified in a tsv) when given annotations (columns) contain exact values in neighboring segments and the segments are within a specified maximum genomic distance. . - `multi_combine_tracks.wdl` and `aggregate_combine_tracks.wdl` which run `combine_tracks.wdl` on multiple pairs and combine the results into one seg file for easy consumption by IGV.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5252
https://github.com/broadinstitute/gatk/pull/5252:1484,Usability,simpl,simple,1484,"Several experimental changes that improve precision results, and expand possible evaluations, of GATK CNV:. - `combine_tracks.wdl` for post-processing somatic CNV calls. This wdl will perform two operations:; - Increase precision by removing:; - germline segments. As a result, the WDL requires the matched normal segments.; - Areas of common germline activity or error from other cancer studies.; - Convert the tumor model seg file to the same format as AllelicCapSeg, which can be read by ABSOLUTE. This is currently done inline in the WDL. ; - This is not a trivial conversion, since each segment must be called whether it is balanced or not (MAF =? 0.5). The current algorithm relies on hard filtering and may need updating pending evaluation.; - For more information about AllelicCapSeg and ABSOLUTE, see: ; - Carter et al. *Absolute quantification of somatic DNA alterations in human cancer*, Nat Biotechnol. 2012 May; 30(5): 413421 ; - https://software.broadinstitute.org/cancer/cga/absolute ; - Brastianos, P.K., Carter S.L., et al. *Genomic Characterization of Brain Metastases Reveals Branched Evolution and Potential Therapeutic Targets* (2015) Cancer Discovery PMID:26410082. - Changes to GATK tools to support the above:; - `SimpleGermlineTagger` now uses reciprocal overlap to in addition to breakpoint matching when determining a possible germline event. This greatly improved results in areas near centromeres.; - Added tool `MergeAnnotatedRegionsByAnnotation`. This simple tool will merge genomic regions (specified in a tsv) when given annotations (columns) contain exact values in neighboring segments and the segments are within a specified maximum genomic distance. . - `multi_combine_tracks.wdl` and `aggregate_combine_tracks.wdl` which run `combine_tracks.wdl` on multiple pairs and combine the results into one seg file for easy consumption by IGV.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5252
https://github.com/broadinstitute/gatk/issues/5255:272,Deployability,continuous,continuous,272,"We ship an Intel conda environment that uses the Intel optimized tensorflow implementation for those with AVX-enabled hardware, but the environment requires AVX, so there are no tests since we can't assume we have AVX hardware on Travis. We need to provision some kind of continuous test that ensures that the CNN tools continue to work in this environment as we upgrade our dependencies for things like tensorflow, keras, etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5255
https://github.com/broadinstitute/gatk/issues/5255:363,Deployability,upgrade,upgrade,363,"We ship an Intel conda environment that uses the Intel optimized tensorflow implementation for those with AVX-enabled hardware, but the environment requires AVX, so there are no tests since we can't assume we have AVX hardware on Travis. We need to provision some kind of continuous test that ensures that the CNN tools continue to work in this environment as we upgrade our dependencies for things like tensorflow, keras, etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5255
https://github.com/broadinstitute/gatk/issues/5255:375,Integrability,depend,dependencies,375,"We ship an Intel conda environment that uses the Intel optimized tensorflow implementation for those with AVX-enabled hardware, but the environment requires AVX, so there are no tests since we can't assume we have AVX hardware on Travis. We need to provision some kind of continuous test that ensures that the CNN tools continue to work in this environment as we upgrade our dependencies for things like tensorflow, keras, etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5255
https://github.com/broadinstitute/gatk/issues/5255:55,Performance,optimiz,optimized,55,"We ship an Intel conda environment that uses the Intel optimized tensorflow implementation for those with AVX-enabled hardware, but the environment requires AVX, so there are no tests since we can't assume we have AVX hardware on Travis. We need to provision some kind of continuous test that ensures that the CNN tools continue to work in this environment as we upgrade our dependencies for things like tensorflow, keras, etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5255
https://github.com/broadinstitute/gatk/issues/5255:178,Testability,test,tests,178,"We ship an Intel conda environment that uses the Intel optimized tensorflow implementation for those with AVX-enabled hardware, but the environment requires AVX, so there are no tests since we can't assume we have AVX hardware on Travis. We need to provision some kind of continuous test that ensures that the CNN tools continue to work in this environment as we upgrade our dependencies for things like tensorflow, keras, etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5255
https://github.com/broadinstitute/gatk/issues/5255:283,Testability,test,test,283,"We ship an Intel conda environment that uses the Intel optimized tensorflow implementation for those with AVX-enabled hardware, but the environment requires AVX, so there are no tests since we can't assume we have AVX hardware on Travis. We need to provision some kind of continuous test that ensures that the CNN tools continue to work in this environment as we upgrade our dependencies for things like tensorflow, keras, etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5255
https://github.com/broadinstitute/gatk/pull/5256:2,Usability,simpl,simplifying,2,* simplifying a few line in build.gradle and making all JavaCompile tasks apply the same compiler arguments; * previously this didn't apply to compileJavaTestUtils,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5256
https://github.com/broadinstitute/gatk/issues/5258:151,Testability,test,tests,151,`GatkSparkTool` doesn't propagate the '.gzi' file when passing fasta files around. This means it can't read fasta.gz files. We should fix this and add tests for fasta.gz. Fasta support was recently introduced here: https://github.com/broadinstitute/gatk/pull/5127,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5258
https://github.com/broadinstitute/gatk/issues/5259:295,Availability,down,downloader,295,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_. ### Description; Currently the data sources for the clinical pipeline work contain ExAC. This must be updated to use gnomAD. The change will require a new release of the data sources which must be connected to the data source downloader tool. Additionally, these new data sources must be validated in four ways:; - By visually inspecting the gnomAD source file for correctness.; - By verifying that the source file for gnomAD does not contain special characters.; - By validating that the source file for gnomAD is a valid VCF (assuming it is used VCF format).; - By running a large file and spot checking at least 10 variants for correctness. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5259
https://github.com/broadinstitute/gatk/issues/5259:130,Deployability,pipeline,pipeline,130,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_. ### Description; Currently the data sources for the clinical pipeline work contain ExAC. This must be updated to use gnomAD. The change will require a new release of the data sources which must be connected to the data source downloader tool. Additionally, these new data sources must be validated in four ways:; - By visually inspecting the gnomAD source file for correctness.; - By verifying that the source file for gnomAD does not contain special characters.; - By validating that the source file for gnomAD is a valid VCF (assuming it is used VCF format).; - By running a large file and spot checking at least 10 variants for correctness. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5259
https://github.com/broadinstitute/gatk/issues/5259:171,Deployability,update,updated,171,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_. ### Description; Currently the data sources for the clinical pipeline work contain ExAC. This must be updated to use gnomAD. The change will require a new release of the data sources which must be connected to the data source downloader tool. Additionally, these new data sources must be validated in four ways:; - By visually inspecting the gnomAD source file for correctness.; - By verifying that the source file for gnomAD does not contain special characters.; - By validating that the source file for gnomAD is a valid VCF (assuming it is used VCF format).; - By running a large file and spot checking at least 10 variants for correctness. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5259
https://github.com/broadinstitute/gatk/issues/5259:224,Deployability,release,release,224,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_. ### Description; Currently the data sources for the clinical pipeline work contain ExAC. This must be updated to use gnomAD. The change will require a new release of the data sources which must be connected to the data source downloader tool. Additionally, these new data sources must be validated in four ways:; - By visually inspecting the gnomAD source file for correctness.; - By verifying that the source file for gnomAD does not contain special characters.; - By validating that the source file for gnomAD is a valid VCF (assuming it is used VCF format).; - By running a large file and spot checking at least 10 variants for correctness. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5259
https://github.com/broadinstitute/gatk/issues/5259:357,Security,validat,validated,357,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_. ### Description; Currently the data sources for the clinical pipeline work contain ExAC. This must be updated to use gnomAD. The change will require a new release of the data sources which must be connected to the data source downloader tool. Additionally, these new data sources must be validated in four ways:; - By visually inspecting the gnomAD source file for correctness.; - By verifying that the source file for gnomAD does not contain special characters.; - By validating that the source file for gnomAD is a valid VCF (assuming it is used VCF format).; - By running a large file and spot checking at least 10 variants for correctness. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5259
https://github.com/broadinstitute/gatk/issues/5259:538,Security,validat,validating,538,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_. ### Description; Currently the data sources for the clinical pipeline work contain ExAC. This must be updated to use gnomAD. The change will require a new release of the data sources which must be connected to the data source downloader tool. Additionally, these new data sources must be validated in four ways:; - By visually inspecting the gnomAD source file for correctness.; - By verifying that the source file for gnomAD does not contain special characters.; - By validating that the source file for gnomAD is a valid VCF (assuming it is used VCF format).; - By running a large file and spot checking at least 10 variants for correctness. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5259
https://github.com/broadinstitute/gatk/issues/5260:87,Testability,test,tested,87,"## Feature request. ### Tool(s) or class(es) involved; SelectVariants (all versions -- tested most recently with 4.0.9.0). ### Description; SelectVariants has an option for excluding specific sites by rsID, `--exclude-ids`. Currently the matching is done by exact string matching. The limitation is that if I have a site with two rsIDs (as appears in the 1000 Genomes dataset), I can't filter against just one of them. For example, if I want to exclude this site from 1000G participant HG00096 (because it makes my synthetic data generator choke, if you must know):. 3523879:16	68401338	rs554836707;rs56090907	CGTGCGCTGCT	CTGCT,C	100	PASS <snip>. I have to specify `--exclude-ids 'rs554836707;rs56090907'` in my SelectVariants command. I would like to be able to specify just `--exclude-ids rs554836707` or `--exclude-ids rs56090907` with the same result (the site is excluded). Right now if I do use just one like that, the site is not excluded because the pattern fails to match. The reason I want to be able to do that is because there are several other similar sites that I need to exclude, so I make an array of rsIDs to exclude, e.g. provide . ````; Array[String] = [; ""rs575450111"",; ""rs151266607"",; ""rs557010312"",; ""rs564559134"",; ""rs560466806"",; ""rs140802196"",; ""rs554836707"",; ""rs56090907""; ]; ````. as an input to my WDL, which handles it in the command block as . --exclude-ids ${sep=' --exclude-ids ' exclusionList}. Right now I have to use . ````; [; ""'rs575450111;rs151266607'"",; ""'rs557010312;rs564559134'"",; ""'rs560466806;rs140802196'"",; ""'rs554836707;rs56090907'""; ]; ````. which is stylistically ugly and would not cover cases where only one rsID is annotated for whatever reason.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5260
https://github.com/broadinstitute/gatk/issues/5264:17,Performance,perform,performance,17,Whatever further performance improvements we can get out of `HaplotypeCallerEngine` will benefit both `HaplotypeCaller` and `HaplotypeCallerSpark` (see https://github.com/broadinstitute/gatk/issues/5263),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5264
https://github.com/broadinstitute/gatk/issues/5280:308,Deployability,update,update,308,"## Bug Report. ### Affected tool(s) or class(es); `combine_tracks.wdl`, `CombineSegmentBreakpoints`. ### Affected version(s); - [ ] Latest master branch as of October 4, 2018. ### Description ; `CombineSegmentBreakpoints` does not have any notion of the num probes or num het columns. As a result, it cannot update those values as the segments are broken into pieces. ; This will get more complicated if both segment files have NUM_POINT columns.; #### Steps to reproduce; Seg File 1; ```; CONTIG START END NUM_POINTS_COPY_RATIO; 1 100 200 25; ```. Seg File 2; ```; CONTIG START END type; 1 100 150 centromere; ```. Result:; ```; CONTIG START END NUM_POINTS_COPY_RATIO type; 1 100 150 25 centromere; 1 151 200 25 ; ```. This result is incorrect, since we have suddenly doubled the number of points b/w 1:100-200. #### Expected behavior; Result:; ```; CONTIG START END NUM_POINTS_COPY_RATIO type; 1 100 150 12 centromere; 1 151 200 13 ; ```; Assuming that there are 12 points in 1:100-150 and 13 points in 1:151-200",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5280
https://github.com/broadinstitute/gatk/issues/5283:205,Testability,test,tested,205,"## Feature request. ### Tool(s) or class(es) involved; combine_tracks.wdl. ### Description; In order for outputs from GATK CNV to be usable by GISTIC2, we need to have a conversion step. Here is mostly un-tested WDL that should work:. ```; #UNSUPPORTED -- simple conversion of a merged & pruned seg file (from the CNV postprocessing workflow) to the GISTIC2 format.; # No column headers printed. Each column is:; #; #(1) Sample (sample name); #(2) Chromosome (chromosome number); #(3) Start Position (segment start position, in bases); #(4) End Position (segment end position, in bases); #(5) Num markers (number of markers in segment); #(6) Seg.CN (log2() -1 of copy number); #; # This has barely been tested; #; workflow ConvertMergedPrunedSegsToGistic2 {; File cnv_postprocessing_tumor_with_tracks_pruned_merged_seg; String docker; call Gistic2Convert {; input:; input_file = cnv_postprocessing_tumor_with_tracks_pruned_merged_seg,; docker = docker; }. output {; File cnv_postprocessing_tumor_with_tracks_pruned_merged_seg_gistic2 = Gistic2Convert.output_file_gistic2; }; }. task Gistic2Convert {; File input_file; String docker; String output_file = basename(input_file) + "".gistic2.seg"". command <<<; set -e; python <<EOF; import csv; input_file = ""${input_file}""; output_file = ""${output_file}"". """"""; The column headers are:. (1) Sample (sample name); (2) Chromosome (chromosome number); (3) Start Position (segment start position, in bases); (4) End Position (segment end position, in bases); (5) Num markers (number of markers in segment); (6) Seg.CN (log2() -1 of copy number); """""". if __name__ == ""__main__"":; with open(input_file, 'rb') as tsvinfp, open(output_file, 'wb') as tsvoutfp:; tsvin = csv.DictReader(tsvinfp, delimiter='\t'); tsvout = csv.writer(tsvoutfp, delimiter=""\t""); for r in tsvin:; int_ify_num_points = r[""NUM_POINTS_COPY_RATIO""].replace("".0"", """"); outrow = [r[""SAMPLE""], r[""Chromosome""], r[""Start""], r[""End""], int_ify_num_points, r[""MEAN_LOG2_COPY_RATIO""]]; print(outrow)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5283
https://github.com/broadinstitute/gatk/issues/5283:703,Testability,test,tested,703,"## Feature request. ### Tool(s) or class(es) involved; combine_tracks.wdl. ### Description; In order for outputs from GATK CNV to be usable by GISTIC2, we need to have a conversion step. Here is mostly un-tested WDL that should work:. ```; #UNSUPPORTED -- simple conversion of a merged & pruned seg file (from the CNV postprocessing workflow) to the GISTIC2 format.; # No column headers printed. Each column is:; #; #(1) Sample (sample name); #(2) Chromosome (chromosome number); #(3) Start Position (segment start position, in bases); #(4) End Position (segment end position, in bases); #(5) Num markers (number of markers in segment); #(6) Seg.CN (log2() -1 of copy number); #; # This has barely been tested; #; workflow ConvertMergedPrunedSegsToGistic2 {; File cnv_postprocessing_tumor_with_tracks_pruned_merged_seg; String docker; call Gistic2Convert {; input:; input_file = cnv_postprocessing_tumor_with_tracks_pruned_merged_seg,; docker = docker; }. output {; File cnv_postprocessing_tumor_with_tracks_pruned_merged_seg_gistic2 = Gistic2Convert.output_file_gistic2; }; }. task Gistic2Convert {; File input_file; String docker; String output_file = basename(input_file) + "".gistic2.seg"". command <<<; set -e; python <<EOF; import csv; input_file = ""${input_file}""; output_file = ""${output_file}"". """"""; The column headers are:. (1) Sample (sample name); (2) Chromosome (chromosome number); (3) Start Position (segment start position, in bases); (4) End Position (segment end position, in bases); (5) Num markers (number of markers in segment); (6) Seg.CN (log2() -1 of copy number); """""". if __name__ == ""__main__"":; with open(input_file, 'rb') as tsvinfp, open(output_file, 'wb') as tsvoutfp:; tsvin = csv.DictReader(tsvinfp, delimiter='\t'); tsvout = csv.writer(tsvoutfp, delimiter=""\t""); for r in tsvin:; int_ify_num_points = r[""NUM_POINTS_COPY_RATIO""].replace("".0"", """"); outrow = [r[""SAMPLE""], r[""Chromosome""], r[""Start""], r[""End""], int_ify_num_points, r[""MEAN_LOG2_COPY_RATIO""]]; print(outrow)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5283
https://github.com/broadinstitute/gatk/issues/5283:133,Usability,usab,usable,133,"## Feature request. ### Tool(s) or class(es) involved; combine_tracks.wdl. ### Description; In order for outputs from GATK CNV to be usable by GISTIC2, we need to have a conversion step. Here is mostly un-tested WDL that should work:. ```; #UNSUPPORTED -- simple conversion of a merged & pruned seg file (from the CNV postprocessing workflow) to the GISTIC2 format.; # No column headers printed. Each column is:; #; #(1) Sample (sample name); #(2) Chromosome (chromosome number); #(3) Start Position (segment start position, in bases); #(4) End Position (segment end position, in bases); #(5) Num markers (number of markers in segment); #(6) Seg.CN (log2() -1 of copy number); #; # This has barely been tested; #; workflow ConvertMergedPrunedSegsToGistic2 {; File cnv_postprocessing_tumor_with_tracks_pruned_merged_seg; String docker; call Gistic2Convert {; input:; input_file = cnv_postprocessing_tumor_with_tracks_pruned_merged_seg,; docker = docker; }. output {; File cnv_postprocessing_tumor_with_tracks_pruned_merged_seg_gistic2 = Gistic2Convert.output_file_gistic2; }; }. task Gistic2Convert {; File input_file; String docker; String output_file = basename(input_file) + "".gistic2.seg"". command <<<; set -e; python <<EOF; import csv; input_file = ""${input_file}""; output_file = ""${output_file}"". """"""; The column headers are:. (1) Sample (sample name); (2) Chromosome (chromosome number); (3) Start Position (segment start position, in bases); (4) End Position (segment end position, in bases); (5) Num markers (number of markers in segment); (6) Seg.CN (log2() -1 of copy number); """""". if __name__ == ""__main__"":; with open(input_file, 'rb') as tsvinfp, open(output_file, 'wb') as tsvoutfp:; tsvin = csv.DictReader(tsvinfp, delimiter='\t'); tsvout = csv.writer(tsvoutfp, delimiter=""\t""); for r in tsvin:; int_ify_num_points = r[""NUM_POINTS_COPY_RATIO""].replace("".0"", """"); outrow = [r[""SAMPLE""], r[""Chromosome""], r[""Start""], r[""End""], int_ify_num_points, r[""MEAN_LOG2_COPY_RATIO""]]; print(outrow)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5283
https://github.com/broadinstitute/gatk/issues/5283:256,Usability,simpl,simple,256,"## Feature request. ### Tool(s) or class(es) involved; combine_tracks.wdl. ### Description; In order for outputs from GATK CNV to be usable by GISTIC2, we need to have a conversion step. Here is mostly un-tested WDL that should work:. ```; #UNSUPPORTED -- simple conversion of a merged & pruned seg file (from the CNV postprocessing workflow) to the GISTIC2 format.; # No column headers printed. Each column is:; #; #(1) Sample (sample name); #(2) Chromosome (chromosome number); #(3) Start Position (segment start position, in bases); #(4) End Position (segment end position, in bases); #(5) Num markers (number of markers in segment); #(6) Seg.CN (log2() -1 of copy number); #; # This has barely been tested; #; workflow ConvertMergedPrunedSegsToGistic2 {; File cnv_postprocessing_tumor_with_tracks_pruned_merged_seg; String docker; call Gistic2Convert {; input:; input_file = cnv_postprocessing_tumor_with_tracks_pruned_merged_seg,; docker = docker; }. output {; File cnv_postprocessing_tumor_with_tracks_pruned_merged_seg_gistic2 = Gistic2Convert.output_file_gistic2; }; }. task Gistic2Convert {; File input_file; String docker; String output_file = basename(input_file) + "".gistic2.seg"". command <<<; set -e; python <<EOF; import csv; input_file = ""${input_file}""; output_file = ""${output_file}"". """"""; The column headers are:. (1) Sample (sample name); (2) Chromosome (chromosome number); (3) Start Position (segment start position, in bases); (4) End Position (segment end position, in bases); (5) Num markers (number of markers in segment); (6) Seg.CN (log2() -1 of copy number); """""". if __name__ == ""__main__"":; with open(input_file, 'rb') as tsvinfp, open(output_file, 'wb') as tsvoutfp:; tsvin = csv.DictReader(tsvinfp, delimiter='\t'); tsvout = csv.writer(tsvoutfp, delimiter=""\t""); for r in tsvin:; int_ify_num_points = r[""NUM_POINTS_COPY_RATIO""].replace("".0"", """"); outrow = [r[""SAMPLE""], r[""Chromosome""], r[""Start""], r[""End""], int_ify_num_points, r[""MEAN_LOG2_COPY_RATIO""]]; print(outrow)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5283
https://github.com/broadinstitute/gatk/issues/5284:612,Integrability,wrap,wrapper,612,"## Bug Report. ### Affected tool(s) or class(es); combine_tracks.wdl. ### Description ; PrototypeACSConversion has a bug when `model_segments_seg_pd['MINOR_ALLELE_FRACTION_POSTERIOR_90'` is NaN. On line:; ```; result[model_segments_seg_pd['MINOR_ALLELE_FRACTION_POSTERIOR_90'] > HAM_FIST_THRESHOLD] = 0.5; ```. ```; Traceback (most recent call last): File ""<stdin>"", line 114, in <module> File ""<stdin>"", line 74, in convert_model_segments_to_alleliccapseg File ""<stdin>"", line 55, in simple_determine_allelic_fraction File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pandas/core/ops.py"", line 879, in wrapper res = na_op(values, other) ; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pandas/core/ops.py"", line 783, in na_op result = _comp_method_OBJECT_ARRAY(op, x, y) ; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pandas/core/ops.py"", line 763, in _comp_method_OBJECT_ARRAY result = lib.scalar_compare(x, y, op) ; File ""pandas/_libs/lib.pyx"", line 706, in pandas._libs.lib.scalar_compare TypeError: '>' not supported between instances of 'str' and 'float'; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5284
https://github.com/broadinstitute/gatk/pull/5286:626,Availability,avail,available,626,"This PR includes two changes:; 1. Provide a command line argument to toggle the overlapping base quality correction (i.e. min(bq, 20)) before reassembly, which happens in FragmentUtils. I've found, however, that by the time SomaticGenotypingEngine runs, those the quality of these bases get bumped up to what they used to be, so this may be a no-op. I included it in case I missed something, and to be consistent with the branch @fleharty and @madduran have been using.; 2. Provide a command line argument to count the two reads in an overlapping pair separately in StrandArtfiact and StrandBiasBySample. This feature is only available in Mutect i.e. it won't affect other tools that use StrandBiasBySample",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5286
https://github.com/broadinstitute/gatk/pull/5286:69,Deployability,toggle,toggle,69,"This PR includes two changes:; 1. Provide a command line argument to toggle the overlapping base quality correction (i.e. min(bq, 20)) before reassembly, which happens in FragmentUtils. I've found, however, that by the time SomaticGenotypingEngine runs, those the quality of these bases get bumped up to what they used to be, so this may be a no-op. I included it in case I missed something, and to be consistent with the branch @fleharty and @madduran have been using.; 2. Provide a command line argument to count the two reads in an overlapping pair separately in StrandArtfiact and StrandBiasBySample. This feature is only available in Mutect i.e. it won't affect other tools that use StrandBiasBySample",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5286
https://github.com/broadinstitute/gatk/pull/5287:684,Deployability,configurat,configurations,684,"There are no Java code changes in this PR. Tests were done manually. As a reminder, the modified files are still considered experimental. Changes:; - combine_tracks.wdl: Fixes bug where string was compared to a float. Closes #5284 ; - combine_tracks.wdl: Converts the processed seg file into a format for GISTIC2. This is a trivial conversion. Closes #5283 ; - Other changes in `aggregate_combine_tracks.wdl` to support the above, including aggregation of individual GISTIC2 seg files into a single GISTIC2 seg file.; - Added gs urls for necessary auxiliary files in the documentation.; - Added multiple output types for the ABSOLUTE skew parameter to support heterogeneous execution configurations. File, Float, and String. All are the same value.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5287
https://github.com/broadinstitute/gatk/pull/5287:684,Modifiability,config,configurations,684,"There are no Java code changes in this PR. Tests were done manually. As a reminder, the modified files are still considered experimental. Changes:; - combine_tracks.wdl: Fixes bug where string was compared to a float. Closes #5284 ; - combine_tracks.wdl: Converts the processed seg file into a format for GISTIC2. This is a trivial conversion. Closes #5283 ; - Other changes in `aggregate_combine_tracks.wdl` to support the above, including aggregation of individual GISTIC2 seg files into a single GISTIC2 seg file.; - Added gs urls for necessary auxiliary files in the documentation.; - Added multiple output types for the ABSOLUTE skew parameter to support heterogeneous execution configurations. File, Float, and String. All are the same value.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5287
https://github.com/broadinstitute/gatk/pull/5287:43,Testability,Test,Tests,43,"There are no Java code changes in this PR. Tests were done manually. As a reminder, the modified files are still considered experimental. Changes:; - combine_tracks.wdl: Fixes bug where string was compared to a float. Closes #5284 ; - combine_tracks.wdl: Converts the processed seg file into a format for GISTIC2. This is a trivial conversion. Closes #5283 ; - Other changes in `aggregate_combine_tracks.wdl` to support the above, including aggregation of individual GISTIC2 seg files into a single GISTIC2 seg file.; - Added gs urls for necessary auxiliary files in the documentation.; - Added multiple output types for the ABSOLUTE skew parameter to support heterogeneous execution configurations. File, Float, and String. All are the same value.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5287
https://github.com/broadinstitute/gatk/pull/5289:11,Testability,test,test,11,"Includes a test for the non-Spark version, and tests the two produce the same result. Fixes #5276.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5289
https://github.com/broadinstitute/gatk/pull/5289:47,Testability,test,tests,47,"Includes a test for the non-Spark version, and tests the two produce the same result. Fixes #5276.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5289
https://github.com/broadinstitute/gatk/pull/5291:310,Availability,avail,availability,310,"CNNScoreVariant relies on a computationally demanding operation - a deep neural network. Using an Intel-optimized version of TensorFlow gives a 10X improvement in performance (e.g. 50 hours to 5 hours for a typical input). However, these improvements mean that we now have a minimum hardware requirement - the availability of AVX.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291
https://github.com/broadinstitute/gatk/pull/5291:104,Performance,optimiz,optimized,104,"CNNScoreVariant relies on a computationally demanding operation - a deep neural network. Using an Intel-optimized version of TensorFlow gives a 10X improvement in performance (e.g. 50 hours to 5 hours for a typical input). However, these improvements mean that we now have a minimum hardware requirement - the availability of AVX.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291
https://github.com/broadinstitute/gatk/pull/5291:163,Performance,perform,performance,163,"CNNScoreVariant relies on a computationally demanding operation - a deep neural network. Using an Intel-optimized version of TensorFlow gives a 10X improvement in performance (e.g. 50 hours to 5 hours for a typical input). However, these improvements mean that we now have a minimum hardware requirement - the availability of AVX.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291
https://github.com/broadinstitute/gatk/issues/5293:168,Availability,error,error,168,"## Bug Report. Hi @jamesemery and @sooheelee ,. Thanks very much for looking into: https://github.com/broadinstitute/gatk/issues/5230. I am running into one additional error. Reads that contain an insert spanning the full length of the read are causing an exception in SplitNCigarReads. ### Affected tool(s) or class(es); SplitNCigarReads. ### Affected version(s); - Tested on 4.0.3.0 and also branch: je_splitNCigarReadsSplitError (gatk-4.0.10.0-4-gb0f0ab3). ### Description ; SplitNCigarReads gives an Exception when a read that is entirely an insertion is encountered. By contrast, HaplotypeCaller does not seem to have a problem with these reads. Example read:; ```; seq.1028598	163	chr20	3146413	60	100I	=	3146307	-106	CCAATAATTCGACCCTATAAATGATGACCTCCGTTATCGGAAGGGCACAGAACCGTCAGCCGCAACACCAGCAGCTGTAGGCCCTGCTGGGCGCGCTGGG	8;72442435768::8443224764768:84:7534457962;99:787;628:7557;::7:72878:7;:7;:8754;9:::87:8799:7:7:87::	YA:Z:chr20:3145675:600M138N208I599M	MC:Z:100M	PG:Z:MarkDuplicates	RG:Z:1	NH:i:1	HI:i:1	YM:i:0	nM:i:100	YO:Z:chr20:3146278:+:56S44M	MQ:i:60	AS:i:140	YX:i:49	mc:i:3146406	ms:i:2300; ```. #### Steps to reproduce; Command line:; ```; gatk \; --java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true' \; SplitNCigarReads \; --reference $REF \; --input 100I_rna.bam \; --output gatk.split.bam \; > split.log 2>&1; ```. A tiny BAM file illustrating the problem is attached (it is gzipped to allow Github upload).; [100I_rna.bam.gz](https://github.com/broadinstitute/gatk/files/2456955/100I_rna.bam.gz). #### Actual behavior; Here is the stacktrace:; ```; ***********************************************************************. A USER ERROR has occurred: Badly formed genome unclippedLoc: Parameters to GenomeLocParser are incorrect:The stop position 3146412 is less than start 3146413 in contig chr20. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: Badly formed genome unclippedLo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5293
https://github.com/broadinstitute/gatk/issues/5293:1654,Availability,ERROR,ERROR,1654,CGACCCTATAAATGATGACCTCCGTTATCGGAAGGGCACAGAACCGTCAGCCGCAACACCAGCAGCTGTAGGCCCTGCTGGGCGCGCTGGG	8;72442435768::8443224764768:84:7534457962;99:787;628:7557;::7:72878:7;:7;:8754;9:::87:8799:7:7:87::	YA:Z:chr20:3145675:600M138N208I599M	MC:Z:100M	PG:Z:MarkDuplicates	RG:Z:1	NH:i:1	HI:i:1	YM:i:0	nM:i:100	YO:Z:chr20:3146278:+:56S44M	MQ:i:60	AS:i:140	YX:i:49	mc:i:3146406	ms:i:2300; ```. #### Steps to reproduce; Command line:; ```; gatk \; --java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true' \; SplitNCigarReads \; --reference $REF \; --input 100I_rna.bam \; --output gatk.split.bam \; > split.log 2>&1; ```. A tiny BAM file illustrating the problem is attached (it is gzipped to allow Github upload).; [100I_rna.bam.gz](https://github.com/broadinstitute/gatk/files/2456955/100I_rna.bam.gz). #### Actual behavior; Here is the stacktrace:; ```; ***********************************************************************. A USER ERROR has occurred: Badly formed genome unclippedLoc: Parameters to GenomeLocParser are incorrect:The stop position 3146412 is less than start 3146413 in contig chr20. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: Badly formed genome unclippedLoc: Parameters to GenomeLocParser are incorrect:The stop position 3146412 is less than start 3146413 in contig chr20; at org.broadinstitute.hellbender.utils.GenomeLocParser.vglHelper(GenomeLocParser.java:280); at org.broadinstitute.hellbender.utils.GenomeLocParser.validateGenomeLoc(GenomeLocParser.java:226); at org.broadinstitute.hellbender.utils.GenomeLocParser.createGenomeLoc(GenomeLocParser.java:185); at org.broadinstitute.hellbender.utils.GenomeLocParser.createGenomeLoc(GenomeLocParser.java:169); at org.broadinstitute.hellbender.utils.GenomeLocParser.createGenomeLoc(GenomeLocParser.java:150); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager$SplitRead.setRead(OverhangFixingManager.java:402); at ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5293
https://github.com/broadinstitute/gatk/issues/5293:3362,Energy Efficiency,Reduce,ReduceOps,3362,GenomeLoc(GenomeLocParser.java:185); at org.broadinstitute.hellbender.utils.GenomeLocParser.createGenomeLoc(GenomeLocParser.java:169); at org.broadinstitute.hellbender.utils.GenomeLocParser.createGenomeLoc(GenomeLocParser.java:150); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager$SplitRead.setRead(OverhangFixingManager.java:402); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager$SplitRead.<init>(OverhangFixingManager.java:396); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.getSplitRead(OverhangFixingManager.java:467); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.Collections$2.tryAdvance(Collections.java:4717); at java.util.Collections$2.forEachRemaining(Collections.java:4725); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.addReadGroup(OverhangFixingManager.java:207); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.splitNCigarRead(SplitNCigarReads.java:259); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.firstPassApply(SplitNCigarReads.java:180); at org.broadinstitute.hellbender.engine.TwoPassReadWalker.lambda$traverseReads$0(TwoPassReadWalker.java:62); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.Iterator.forEachRem,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5293
https://github.com/broadinstitute/gatk/issues/5293:3372,Energy Efficiency,Reduce,ReduceOp,3372,GenomeLoc(GenomeLocParser.java:185); at org.broadinstitute.hellbender.utils.GenomeLocParser.createGenomeLoc(GenomeLocParser.java:169); at org.broadinstitute.hellbender.utils.GenomeLocParser.createGenomeLoc(GenomeLocParser.java:150); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager$SplitRead.setRead(OverhangFixingManager.java:402); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager$SplitRead.<init>(OverhangFixingManager.java:396); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.getSplitRead(OverhangFixingManager.java:467); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.Collections$2.tryAdvance(Collections.java:4717); at java.util.Collections$2.forEachRemaining(Collections.java:4725); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.addReadGroup(OverhangFixingManager.java:207); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.splitNCigarRead(SplitNCigarReads.java:259); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.firstPassApply(SplitNCigarReads.java:180); at org.broadinstitute.hellbender.engine.TwoPassReadWalker.lambda$traverseReads$0(TwoPassReadWalker.java:62); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.Iterator.forEachRem,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5293
https://github.com/broadinstitute/gatk/issues/5293:3400,Energy Efficiency,Reduce,ReduceOps,3400,r.java:185); at org.broadinstitute.hellbender.utils.GenomeLocParser.createGenomeLoc(GenomeLocParser.java:169); at org.broadinstitute.hellbender.utils.GenomeLocParser.createGenomeLoc(GenomeLocParser.java:150); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager$SplitRead.setRead(OverhangFixingManager.java:402); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager$SplitRead.<init>(OverhangFixingManager.java:396); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.getSplitRead(OverhangFixingManager.java:467); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.Collections$2.tryAdvance(Collections.java:4717); at java.util.Collections$2.forEachRemaining(Collections.java:4725); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.addReadGroup(OverhangFixingManager.java:207); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.splitNCigarRead(SplitNCigarReads.java:259); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.firstPassApply(SplitNCigarReads.java:180); at org.broadinstitute.hellbender.engine.TwoPassReadWalker.lambda$traverseReads$0(TwoPassReadWalker.java:62); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.Iterator.forEachRemaining(Iterator.java:116,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5293
https://github.com/broadinstitute/gatk/issues/5293:3298,Integrability,wrap,wrapAndCopyInto,3298,rg.broadinstitute.hellbender.utils.GenomeLocParser.createGenomeLoc(GenomeLocParser.java:185); at org.broadinstitute.hellbender.utils.GenomeLocParser.createGenomeLoc(GenomeLocParser.java:169); at org.broadinstitute.hellbender.utils.GenomeLocParser.createGenomeLoc(GenomeLocParser.java:150); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager$SplitRead.setRead(OverhangFixingManager.java:402); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager$SplitRead.<init>(OverhangFixingManager.java:396); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.getSplitRead(OverhangFixingManager.java:467); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.Collections$2.tryAdvance(Collections.java:4717); at java.util.Collections$2.forEachRemaining(Collections.java:4725); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.addReadGroup(OverhangFixingManager.java:207); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.splitNCigarRead(SplitNCigarReads.java:259); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.firstPassApply(SplitNCigarReads.java:180); at org.broadinstitute.hellbender.engine.TwoPassReadWalker.lambda$traverseReads$0(TwoPassReadWalker.java:62); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.stream.ReferencePipeline$3$1.accept(Refe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5293
https://github.com/broadinstitute/gatk/issues/5293:4598,Integrability,wrap,wrapAndCopyInto,4598,rs.rnaseq.OverhangFixingManager.addReadGroup(OverhangFixingManager.java:207); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.splitNCigarRead(SplitNCigarReads.java:259); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.firstPassApply(SplitNCigarReads.java:180); at org.broadinstitute.hellbender.engine.TwoPassReadWalker.lambda$traverseReads$0(TwoPassReadWalker.java:62); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.TwoPassReadWalker.traverseReads(TwoPassReadWalker.java:60); at org.broadinstitute.hellbender.engine.TwoPassReadWalker.traverse(TwoPassReadWalker.java:42); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5293
https://github.com/broadinstitute/gatk/issues/5293:2265,Security,validat,validateGenomeLoc,2265,bam \; --output gatk.split.bam \; > split.log 2>&1; ```. A tiny BAM file illustrating the problem is attached (it is gzipped to allow Github upload).; [100I_rna.bam.gz](https://github.com/broadinstitute/gatk/files/2456955/100I_rna.bam.gz). #### Actual behavior; Here is the stacktrace:; ```; ***********************************************************************. A USER ERROR has occurred: Badly formed genome unclippedLoc: Parameters to GenomeLocParser are incorrect:The stop position 3146412 is less than start 3146413 in contig chr20. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: Badly formed genome unclippedLoc: Parameters to GenomeLocParser are incorrect:The stop position 3146412 is less than start 3146413 in contig chr20; at org.broadinstitute.hellbender.utils.GenomeLocParser.vglHelper(GenomeLocParser.java:280); at org.broadinstitute.hellbender.utils.GenomeLocParser.validateGenomeLoc(GenomeLocParser.java:226); at org.broadinstitute.hellbender.utils.GenomeLocParser.createGenomeLoc(GenomeLocParser.java:185); at org.broadinstitute.hellbender.utils.GenomeLocParser.createGenomeLoc(GenomeLocParser.java:169); at org.broadinstitute.hellbender.utils.GenomeLocParser.createGenomeLoc(GenomeLocParser.java:150); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager$SplitRead.setRead(OverhangFixingManager.java:402); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager$SplitRead.<init>(OverhangFixingManager.java:396); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.getSplitRead(OverhangFixingManager.java:467); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.Collections$2.tryAdvance(Collections.java:4717); at java.util.Collections$2.forEachRemaining(Collections.java:4725); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.A,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5293
https://github.com/broadinstitute/gatk/issues/5293:367,Testability,Test,Tested,367,"## Bug Report. Hi @jamesemery and @sooheelee ,. Thanks very much for looking into: https://github.com/broadinstitute/gatk/issues/5230. I am running into one additional error. Reads that contain an insert spanning the full length of the read are causing an exception in SplitNCigarReads. ### Affected tool(s) or class(es); SplitNCigarReads. ### Affected version(s); - Tested on 4.0.3.0 and also branch: je_splitNCigarReadsSplitError (gatk-4.0.10.0-4-gb0f0ab3). ### Description ; SplitNCigarReads gives an Exception when a read that is entirely an insertion is encountered. By contrast, HaplotypeCaller does not seem to have a problem with these reads. Example read:; ```; seq.1028598	163	chr20	3146413	60	100I	=	3146307	-106	CCAATAATTCGACCCTATAAATGATGACCTCCGTTATCGGAAGGGCACAGAACCGTCAGCCGCAACACCAGCAGCTGTAGGCCCTGCTGGGCGCGCTGGG	8;72442435768::8443224764768:84:7534457962;99:787;628:7557;::7:72878:7;:7;:8754;9:::87:8799:7:7:87::	YA:Z:chr20:3145675:600M138N208I599M	MC:Z:100M	PG:Z:MarkDuplicates	RG:Z:1	NH:i:1	HI:i:1	YM:i:0	nM:i:100	YO:Z:chr20:3146278:+:56S44M	MQ:i:60	AS:i:140	YX:i:49	mc:i:3146406	ms:i:2300; ```. #### Steps to reproduce; Command line:; ```; gatk \; --java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true' \; SplitNCigarReads \; --reference $REF \; --input 100I_rna.bam \; --output gatk.split.bam \; > split.log 2>&1; ```. A tiny BAM file illustrating the problem is attached (it is gzipped to allow Github upload).; [100I_rna.bam.gz](https://github.com/broadinstitute/gatk/files/2456955/100I_rna.bam.gz). #### Actual behavior; Here is the stacktrace:; ```; ***********************************************************************. A USER ERROR has occurred: Badly formed genome unclippedLoc: Parameters to GenomeLocParser are incorrect:The stop position 3146412 is less than start 3146413 in contig chr20. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: Badly formed genome unclippedLo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5293
https://github.com/broadinstitute/gatk/issues/5293:1324,Testability,log,log,1324,"arReads. ### Affected version(s); - Tested on 4.0.3.0 and also branch: je_splitNCigarReadsSplitError (gatk-4.0.10.0-4-gb0f0ab3). ### Description ; SplitNCigarReads gives an Exception when a read that is entirely an insertion is encountered. By contrast, HaplotypeCaller does not seem to have a problem with these reads. Example read:; ```; seq.1028598	163	chr20	3146413	60	100I	=	3146307	-106	CCAATAATTCGACCCTATAAATGATGACCTCCGTTATCGGAAGGGCACAGAACCGTCAGCCGCAACACCAGCAGCTGTAGGCCCTGCTGGGCGCGCTGGG	8;72442435768::8443224764768:84:7534457962;99:787;628:7557;::7:72878:7;:7;:8754;9:::87:8799:7:7:87::	YA:Z:chr20:3145675:600M138N208I599M	MC:Z:100M	PG:Z:MarkDuplicates	RG:Z:1	NH:i:1	HI:i:1	YM:i:0	nM:i:100	YO:Z:chr20:3146278:+:56S44M	MQ:i:60	AS:i:140	YX:i:49	mc:i:3146406	ms:i:2300; ```. #### Steps to reproduce; Command line:; ```; gatk \; --java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true' \; SplitNCigarReads \; --reference $REF \; --input 100I_rna.bam \; --output gatk.split.bam \; > split.log 2>&1; ```. A tiny BAM file illustrating the problem is attached (it is gzipped to allow Github upload).; [100I_rna.bam.gz](https://github.com/broadinstitute/gatk/files/2456955/100I_rna.bam.gz). #### Actual behavior; Here is the stacktrace:; ```; ***********************************************************************. A USER ERROR has occurred: Badly formed genome unclippedLoc: Parameters to GenomeLocParser are incorrect:The stop position 3146412 is less than start 3146413 in contig chr20. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: Badly formed genome unclippedLoc: Parameters to GenomeLocParser are incorrect:The stop position 3146412 is less than start 3146413 in contig chr20; at org.broadinstitute.hellbender.utils.GenomeLocParser.vglHelper(GenomeLocParser.java:280); at org.broadinstitute.hellbender.utils.GenomeLocParser.validateGenomeLoc(GenomeLocParser.java:226); at org.broadinstitute",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5293
https://github.com/broadinstitute/gatk/issues/5295:21,Testability,test,test,21,"There's currently no test coverage for ""all transcripts"" mode in Funcotator, as far as I can discover, and the GTF files used for MUC16/PIK3CA tests don't even have multiple transcripts.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5295
https://github.com/broadinstitute/gatk/issues/5295:143,Testability,test,tests,143,"There's currently no test coverage for ""all transcripts"" mode in Funcotator, as far as I can discover, and the GTF files used for MUC16/PIK3CA tests don't even have multiple transcripts.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5295
https://github.com/broadinstitute/gatk/issues/5299:521,Deployability,pipeline,pipeline,521,"GATK Version 4.0.9.0. What is the issue:; When using the -OBI flag in e.g. ApplyBQSR, GATK uses an inconsistent pattern for adding index extensions. When using BAM format as output:; my_file.bam; my_file.bai. When using CRAM format as output:; my_file.cram; my_file.cram.bai. Since e.g. samtools uses the second pattern for both BAM and CRAM (well, actually they have .crai, but that is a different discssion), I think it would be sensible to adopt that schema. It's not a huge deal, but I tripped over it when writing a pipeline where the user could specify the desired output format - and noticed that there is this odd difference.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5299
https://github.com/broadinstitute/gatk/issues/5300:31,Availability,error,error,31,"@vdauwera reports getting this error when running `GenomicsDBImport` with a large interval list as the `-L` input:. ```; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; ```. Looking at the code that produces this error, this seems like a ""should never happen"" type of error that would likely only be produced by a race condition of some kind. Full stderr log follows:. ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.876625c8; 04:37:28.801 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.0.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 04:37:29.319 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.319 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.9.0; 04:37:29.320 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 04:37:29.320 INFO GenomicsDBImport - Executing as root@7a7880aef99b on Linux v4.9.0-0.bpo.6-amd64 amd64; 04:37:29.321 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-8u171-b11-0ubuntu0.16.04.1-b11; 04:37:29.321 INFO GenomicsDBImport - Start Date/Time: October 8, 2018 4:37:28 AM UTC; 04:37:29.321 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.322 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.322 INFO GenomicsDBImport - HTSJDK Version: 2.16.1; 04:37:29.323 INFO GenomicsDBImport - Picard Version: 2.18.13; 04:37:29.323 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 04:37:29.323 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 04:37:29.324 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 04:37:29.324 INFO GenomicsDBImport - HTSJDK Defaults.US",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300
https://github.com/broadinstitute/gatk/issues/5300:358,Availability,error,error,358,"@vdauwera reports getting this error when running `GenomicsDBImport` with a large interval list as the `-L` input:. ```; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; ```. Looking at the code that produces this error, this seems like a ""should never happen"" type of error that would likely only be produced by a race condition of some kind. Full stderr log follows:. ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.876625c8; 04:37:28.801 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.0.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 04:37:29.319 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.319 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.9.0; 04:37:29.320 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 04:37:29.320 INFO GenomicsDBImport - Executing as root@7a7880aef99b on Linux v4.9.0-0.bpo.6-amd64 amd64; 04:37:29.321 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-8u171-b11-0ubuntu0.16.04.1-b11; 04:37:29.321 INFO GenomicsDBImport - Start Date/Time: October 8, 2018 4:37:28 AM UTC; 04:37:29.321 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.322 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.322 INFO GenomicsDBImport - HTSJDK Version: 2.16.1; 04:37:29.323 INFO GenomicsDBImport - Picard Version: 2.18.13; 04:37:29.323 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 04:37:29.323 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 04:37:29.324 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 04:37:29.324 INFO GenomicsDBImport - HTSJDK Defaults.US",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300
https://github.com/broadinstitute/gatk/issues/5300:413,Availability,error,error,413,"@vdauwera reports getting this error when running `GenomicsDBImport` with a large interval list as the `-L` input:. ```; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; ```. Looking at the code that produces this error, this seems like a ""should never happen"" type of error that would likely only be produced by a race condition of some kind. Full stderr log follows:. ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.876625c8; 04:37:28.801 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.0.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 04:37:29.319 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.319 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.9.0; 04:37:29.320 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 04:37:29.320 INFO GenomicsDBImport - Executing as root@7a7880aef99b on Linux v4.9.0-0.bpo.6-amd64 amd64; 04:37:29.321 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-8u171-b11-0ubuntu0.16.04.1-b11; 04:37:29.321 INFO GenomicsDBImport - Start Date/Time: October 8, 2018 4:37:28 AM UTC; 04:37:29.321 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.322 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.322 INFO GenomicsDBImport - HTSJDK Version: 2.16.1; 04:37:29.323 INFO GenomicsDBImport - Picard Version: 2.18.13; 04:37:29.323 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 04:37:29.323 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 04:37:29.324 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 04:37:29.324 INFO GenomicsDBImport - HTSJDK Defaults.US",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300
https://github.com/broadinstitute/gatk/issues/5300:3966,Availability,down,down,3966,"7:30.677 INFO GenomicsDBImport - Vid Map JSON file will be written to genomicsdb/vidmap.json; 04:37:30.677 INFO GenomicsDBImport - Callset Map JSON file will be written to genomicsdb/callset.json; 04:37:30.677 INFO GenomicsDBImport - Complete VCF Header will be written to genomicsdb/vcfheader.vcf; 04:37:30.678 INFO GenomicsDBImport - Importing to array - genomicsdb/genomicsdb_array; 04:37:30.680 INFO ProgressMeter - Starting traversal; 04:37:30.680 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 04:37:33.253 INFO GenomicsDBImport - Starting batch input file preload; 04:37:35.079 INFO GenomicsDBImport - Finished batch preload; 04:37:35.079 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:37.079 INFO GenomicsDBImport - Starting batch input file preload; 04:37:38.712 INFO GenomicsDBImport - Finished batch preload; 04:37:38.712 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:39.162 INFO GenomicsDBImport - Shutting down engine; [October 8, 2018 4:37:39 AM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=4116185088; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300
https://github.com/broadinstitute/gatk/issues/5300:131,Performance,concurren,concurrent,131,"@vdauwera reports getting this error when running `GenomicsDBImport` with a large interval list as the `-L` input:. ```; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; ```. Looking at the code that produces this error, this seems like a ""should never happen"" type of error that would likely only be produced by a race condition of some kind. Full stderr log follows:. ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.876625c8; 04:37:28.801 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.0.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 04:37:29.319 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.319 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.9.0; 04:37:29.320 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 04:37:29.320 INFO GenomicsDBImport - Executing as root@7a7880aef99b on Linux v4.9.0-0.bpo.6-amd64 amd64; 04:37:29.321 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-8u171-b11-0ubuntu0.16.04.1-b11; 04:37:29.321 INFO GenomicsDBImport - Start Date/Time: October 8, 2018 4:37:28 AM UTC; 04:37:29.321 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.322 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.322 INFO GenomicsDBImport - HTSJDK Version: 2.16.1; 04:37:29.323 INFO GenomicsDBImport - Picard Version: 2.18.13; 04:37:29.323 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 04:37:29.323 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 04:37:29.324 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 04:37:29.324 INFO GenomicsDBImport - HTSJDK Defaults.US",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300
https://github.com/broadinstitute/gatk/issues/5300:459,Performance,race condition,race condition,459,"@vdauwera reports getting this error when running `GenomicsDBImport` with a large interval list as the `-L` input:. ```; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; ```. Looking at the code that produces this error, this seems like a ""should never happen"" type of error that would likely only be produced by a race condition of some kind. Full stderr log follows:. ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.876625c8; 04:37:28.801 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.0.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 04:37:29.319 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.319 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.9.0; 04:37:29.320 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 04:37:29.320 INFO GenomicsDBImport - Executing as root@7a7880aef99b on Linux v4.9.0-0.bpo.6-amd64 amd64; 04:37:29.321 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-8u171-b11-0ubuntu0.16.04.1-b11; 04:37:29.321 INFO GenomicsDBImport - Start Date/Time: October 8, 2018 4:37:28 AM UTC; 04:37:29.321 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.322 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.322 INFO GenomicsDBImport - HTSJDK Version: 2.16.1; 04:37:29.323 INFO GenomicsDBImport - Picard Version: 2.18.13; 04:37:29.323 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 04:37:29.323 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 04:37:29.324 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 04:37:29.324 INFO GenomicsDBImport - HTSJDK Defaults.US",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300
https://github.com/broadinstitute/gatk/issues/5300:630,Performance,Load,Loading,630,"@vdauwera reports getting this error when running `GenomicsDBImport` with a large interval list as the `-L` input:. ```; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; ```. Looking at the code that produces this error, this seems like a ""should never happen"" type of error that would likely only be produced by a race condition of some kind. Full stderr log follows:. ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.876625c8; 04:37:28.801 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.0.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 04:37:29.319 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.319 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.9.0; 04:37:29.320 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 04:37:29.320 INFO GenomicsDBImport - Executing as root@7a7880aef99b on Linux v4.9.0-0.bpo.6-amd64 amd64; 04:37:29.321 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-8u171-b11-0ubuntu0.16.04.1-b11; 04:37:29.321 INFO GenomicsDBImport - Start Date/Time: October 8, 2018 4:37:28 AM UTC; 04:37:29.321 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.322 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.322 INFO GenomicsDBImport - HTSJDK Version: 2.16.1; 04:37:29.323 INFO GenomicsDBImport - Picard Version: 2.18.13; 04:37:29.323 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 04:37:29.323 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 04:37:29.324 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 04:37:29.324 INFO GenomicsDBImport - HTSJDK Defaults.US",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300
https://github.com/broadinstitute/gatk/issues/5300:2779,Performance,perform,performance,2779,_LEVEL : 2; 04:37:29.323 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 04:37:29.324 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 04:37:29.324 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 04:37:29.324 INFO GenomicsDBImport - Deflater: IntelDeflater; 04:37:29.324 INFO GenomicsDBImport - Inflater: IntelInflater; 04:37:29.325 INFO GenomicsDBImport - GCS max retries/reopens: 20; 04:37:29.325 INFO GenomicsDBImport - Requester pays: disabled; 04:37:29.325 INFO GenomicsDBImport - Initializing engine; 04:37:29.993 INFO FeatureManager - Using codec BEDCodec to read file file:///cromwell_root/gatk-test-data/intervals/Broad.human.exome.scattered/Broad.human.exome.b37_1_nozero.bed; 04:37:30.330 INFO IntervalArgumentCollection - Processing 18455956 bp from intervals; 04:37:30.340 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. It is recommended that intervals be aggregated together.; 04:37:30.380 INFO GenomicsDBImport - Done initializing engine; Created workspace /cromwell_root/genomicsdb; 04:37:30.677 INFO GenomicsDBImport - Vid Map JSON file will be written to genomicsdb/vidmap.json; 04:37:30.677 INFO GenomicsDBImport - Callset Map JSON file will be written to genomicsdb/callset.json; 04:37:30.677 INFO GenomicsDBImport - Complete VCF Header will be written to genomicsdb/vcfheader.vcf; 04:37:30.678 INFO GenomicsDBImport - Importing to array - genomicsdb/genomicsdb_array; 04:37:30.680 INFO ProgressMeter - Starting traversal; 04:37:30.680 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 04:37:33.253 INFO GenomicsDBImport - Starting batch input file preload; 04:37:35.079 INFO GenomicsDBImport - Finished batch preload; 04:37:35.079 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:37.079 INFO GenomicsDBImport,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300
https://github.com/broadinstitute/gatk/issues/5300:4155,Performance,concurren,concurrent,4155,"et.json; 04:37:30.677 INFO GenomicsDBImport - Complete VCF Header will be written to genomicsdb/vcfheader.vcf; 04:37:30.678 INFO GenomicsDBImport - Importing to array - genomicsdb/genomicsdb_array; 04:37:30.680 INFO ProgressMeter - Starting traversal; 04:37:30.680 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 04:37:33.253 INFO GenomicsDBImport - Starting batch input file preload; 04:37:35.079 INFO GenomicsDBImport - Finished batch preload; 04:37:35.079 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:37.079 INFO GenomicsDBImport - Starting batch input file preload; 04:37:38.712 INFO GenomicsDBImport - Finished batch preload; 04:37:38.712 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:39.162 INFO GenomicsDBImport - Shutting down engine; [October 8, 2018 4:37:39 AM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=4116185088; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$InitializedQueryWrapper.query(GenomicsDBImport.java:769); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(Gen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300
https://github.com/broadinstitute/gatk/issues/5300:4351,Performance,concurren,concurrent,4351,"; 04:37:30.680 INFO ProgressMeter - Starting traversal; 04:37:30.680 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 04:37:33.253 INFO GenomicsDBImport - Starting batch input file preload; 04:37:35.079 INFO GenomicsDBImport - Finished batch preload; 04:37:35.079 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:37.079 INFO GenomicsDBImport - Starting batch input file preload; 04:37:38.712 INFO GenomicsDBImport - Finished batch preload; 04:37:38.712 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:39.162 INFO GenomicsDBImport - Shutting down engine; [October 8, 2018 4:37:39 AM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=4116185088; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$InitializedQueryWrapper.query(GenomicsDBImport.java:769); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:165); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:604); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFutu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300
https://github.com/broadinstitute/gatk/issues/5300:4438,Performance,concurren,concurrent,4438," - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 04:37:33.253 INFO GenomicsDBImport - Starting batch input file preload; 04:37:35.079 INFO GenomicsDBImport - Finished batch preload; 04:37:35.079 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:37.079 INFO GenomicsDBImport - Starting batch input file preload; 04:37:38.712 INFO GenomicsDBImport - Finished batch preload; 04:37:38.712 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:39.162 INFO GenomicsDBImport - Shutting down engine; [October 8, 2018 4:37:39 AM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=4116185088; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$InitializedQueryWrapper.query(GenomicsDBImport.java:769); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:165); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:604); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 3 more; 04:37:39.167 INFO GenomicsDBImport - Starting batch input fi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300
https://github.com/broadinstitute/gatk/issues/5300:4527,Performance,concurren,concurrent,4527,"micsDBImport - Starting batch input file preload; 04:37:35.079 INFO GenomicsDBImport - Finished batch preload; 04:37:35.079 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:37.079 INFO GenomicsDBImport - Starting batch input file preload; 04:37:38.712 INFO GenomicsDBImport - Finished batch preload; 04:37:38.712 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:39.162 INFO GenomicsDBImport - Shutting down engine; [October 8, 2018 4:37:39 AM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=4116185088; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$InitializedQueryWrapper.query(GenomicsDBImport.java:769); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:165); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:604); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 3 more; 04:37:39.167 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.169 INFO GenomicsDBImport - Starting batch input file preload; 04:37",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300
https://github.com/broadinstitute/gatk/issues/5300:4615,Performance,concurren,concurrent,4615,"inished batch preload; 04:37:35.079 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:37.079 INFO GenomicsDBImport - Starting batch input file preload; 04:37:38.712 INFO GenomicsDBImport - Finished batch preload; 04:37:38.712 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:39.162 INFO GenomicsDBImport - Shutting down engine; [October 8, 2018 4:37:39 AM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=4116185088; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$InitializedQueryWrapper.query(GenomicsDBImport.java:769); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:165); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:604); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 3 more; 04:37:39.167 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.169 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.169 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.169 INFO Gen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300
https://github.com/broadinstitute/gatk/issues/5300:4699,Performance,concurren,concurrent,4699,"0 samples; 04:37:37.079 INFO GenomicsDBImport - Starting batch input file preload; 04:37:38.712 INFO GenomicsDBImport - Finished batch preload; 04:37:38.712 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:39.162 INFO GenomicsDBImport - Shutting down engine; [October 8, 2018 4:37:39 AM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=4116185088; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$InitializedQueryWrapper.query(GenomicsDBImport.java:769); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:165); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:604); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 3 more; 04:37:39.167 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.169 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.169 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.169 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.170 INFO GenomicsDBImpor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300
https://github.com/broadinstitute/gatk/issues/5300:5297,Performance,concurren,concurrent,5297,"-31003 queried with: 1:68590-70510; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$InitializedQueryWrapper.query(GenomicsDBImport.java:769); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:165); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:604); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 3 more; 04:37:39.167 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.169 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.169 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.169 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.170 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.170 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.171 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.171 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.171 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.171 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.172 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.172 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.172 INFO GenomicsDBImport - Starting batch inp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300
https://github.com/broadinstitute/gatk/issues/5300:500,Testability,log,log,500,"@vdauwera reports getting this error when running `GenomicsDBImport` with a large interval list as the `-L` input:. ```; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; ```. Looking at the code that produces this error, this seems like a ""should never happen"" type of error that would likely only be produced by a race condition of some kind. Full stderr log follows:. ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.876625c8; 04:37:28.801 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.0.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 04:37:29.319 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.319 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.9.0; 04:37:29.320 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 04:37:29.320 INFO GenomicsDBImport - Executing as root@7a7880aef99b on Linux v4.9.0-0.bpo.6-amd64 amd64; 04:37:29.321 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-8u171-b11-0ubuntu0.16.04.1-b11; 04:37:29.321 INFO GenomicsDBImport - Start Date/Time: October 8, 2018 4:37:28 AM UTC; 04:37:29.321 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.322 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.322 INFO GenomicsDBImport - HTSJDK Version: 2.16.1; 04:37:29.323 INFO GenomicsDBImport - Picard Version: 2.18.13; 04:37:29.323 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 04:37:29.323 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 04:37:29.324 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 04:37:29.324 INFO GenomicsDBImport - HTSJDK Defaults.US",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300
https://github.com/broadinstitute/gatk/issues/5300:2446,Testability,test,test-data,2446,---------------------------------------------; 04:37:29.322 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.322 INFO GenomicsDBImport - HTSJDK Version: 2.16.1; 04:37:29.323 INFO GenomicsDBImport - Picard Version: 2.18.13; 04:37:29.323 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 04:37:29.323 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 04:37:29.324 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 04:37:29.324 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 04:37:29.324 INFO GenomicsDBImport - Deflater: IntelDeflater; 04:37:29.324 INFO GenomicsDBImport - Inflater: IntelInflater; 04:37:29.325 INFO GenomicsDBImport - GCS max retries/reopens: 20; 04:37:29.325 INFO GenomicsDBImport - Requester pays: disabled; 04:37:29.325 INFO GenomicsDBImport - Initializing engine; 04:37:29.993 INFO FeatureManager - Using codec BEDCodec to read file file:///cromwell_root/gatk-test-data/intervals/Broad.human.exome.scattered/Broad.human.exome.b37_1_nozero.bed; 04:37:30.330 INFO IntervalArgumentCollection - Processing 18455956 bp from intervals; 04:37:30.340 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. It is recommended that intervals be aggregated together.; 04:37:30.380 INFO GenomicsDBImport - Done initializing engine; Created workspace /cromwell_root/genomicsdb; 04:37:30.677 INFO GenomicsDBImport - Vid Map JSON file will be written to genomicsdb/vidmap.json; 04:37:30.677 INFO GenomicsDBImport - Callset Map JSON file will be written to genomicsdb/callset.json; 04:37:30.677 INFO GenomicsDBImport - Complete VCF Header will be written to genomicsdb/vcfheader.vcf; 04:37:30.678 INFO GenomicsDBImport - Importing to array - genomicsdb/genomicsdb_array; 04:37:30.680 INFO ProgressMeter - Starting traversal; 04:3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300
https://github.com/broadinstitute/gatk/issues/5300:8494,Testability,test,test-data,8494,port - Starting batch input file preload; 04:37:39.174 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.174 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.174 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.175 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.175 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.175 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.176 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.177 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.177 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.178 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.179 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.180 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.180 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.180 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.180 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.181 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.181 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.181 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.181 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.187 INFO GenomicsDBImport - Starting batch input file preload; Using GATK jar /gatk/gatk-package-4.0.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -Xms4g -jar /gatk/gatk-package-4.0.9.0-local.jar GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L /cromwell_root/gatk-test-data/intervals/Broad.human.exome.scattered/Broad.human.exome.b37_1_nozero.bed --sample-name-map inputs.list --reader-threads 5 -ip 500; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300
https://github.com/broadinstitute/gatk/pull/5302:27,Deployability,update,updates,27,"MASSIVE bug fixes and test updates. (Rebased 61 commits). - Fixed a bug when variants overlap the end of transcripts. - Updated logging in FuncotatorUtils::getAlignedRefAllele. - Fixed a bug in identifying splice sites for intronic regions. - Fixed a bug in FuncotatorUtils::isIndelBetweenCodons that caused issues; on reverse stranded variants. - Added in regression test data input files and expected files.; - Updated regression test sets to include only unique variants.; - Added in a 5' flank and a Start Codon insertion to regression test set. - Fixed a bug in FuncotatorUtils::isIndelBetweenCodons. - Finally fixed a bug with indels and start codons:; Now indels in start codons will not have protein renderings, nor will; they have codon change strings. This brings Funcotator closer to; Oncotator functionality (in Oncotator, start codon insertions/deletions; do not have protein change strings or codon change strings). - Fixed a bug in ordering transcripts by appris ranking. - Fixed a minor bug in how other transcripts are generated:.; With RNA/LINCRNA transcripts, the protein change would be null and was; append ed to the end of each `other transcript`. Now the null is no; longer appended. - Fixed a bug in insertions on the - strand:; All insertions on - strand had incorrectly rendered genome changes,; coding sequence changes, and protein changes.; This is due to how these fields are generated from the reference; sequence. - Fixed a bug (insertions on - strand):; Insertions on the - strand would have incorrect reference; sequences/alleles.; Now they are handled as a special case when computing the aligned; reference allele. - Fixed a bug in transcript selection for GencodeFuncotationFactory:; The LocusLevel / Curation Level was being incorrectly pulled from the; GENE features, rather than the TRANSCRIPT features that contain each; variant. As a result, the order in which representative transcripts; were chosen was wrong. The TRANSCRIPT feature is now being used to; det",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:120,Deployability,Update,Updated,120,"MASSIVE bug fixes and test updates. (Rebased 61 commits). - Fixed a bug when variants overlap the end of transcripts. - Updated logging in FuncotatorUtils::getAlignedRefAllele. - Fixed a bug in identifying splice sites for intronic regions. - Fixed a bug in FuncotatorUtils::isIndelBetweenCodons that caused issues; on reverse stranded variants. - Added in regression test data input files and expected files.; - Updated regression test sets to include only unique variants.; - Added in a 5' flank and a Start Codon insertion to regression test set. - Fixed a bug in FuncotatorUtils::isIndelBetweenCodons. - Finally fixed a bug with indels and start codons:; Now indels in start codons will not have protein renderings, nor will; they have codon change strings. This brings Funcotator closer to; Oncotator functionality (in Oncotator, start codon insertions/deletions; do not have protein change strings or codon change strings). - Fixed a bug in ordering transcripts by appris ranking. - Fixed a minor bug in how other transcripts are generated:.; With RNA/LINCRNA transcripts, the protein change would be null and was; append ed to the end of each `other transcript`. Now the null is no; longer appended. - Fixed a bug in insertions on the - strand:; All insertions on - strand had incorrectly rendered genome changes,; coding sequence changes, and protein changes.; This is due to how these fields are generated from the reference; sequence. - Fixed a bug (insertions on - strand):; Insertions on the - strand would have incorrect reference; sequences/alleles.; Now they are handled as a special case when computing the aligned; reference allele. - Fixed a bug in transcript selection for GencodeFuncotationFactory:; The LocusLevel / Curation Level was being incorrectly pulled from the; GENE features, rather than the TRANSCRIPT features that contain each; variant. As a result, the order in which representative transcripts; were chosen was wrong. The TRANSCRIPT feature is now being used to; det",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:413,Deployability,Update,Updated,413,"MASSIVE bug fixes and test updates. (Rebased 61 commits). - Fixed a bug when variants overlap the end of transcripts. - Updated logging in FuncotatorUtils::getAlignedRefAllele. - Fixed a bug in identifying splice sites for intronic regions. - Fixed a bug in FuncotatorUtils::isIndelBetweenCodons that caused issues; on reverse stranded variants. - Added in regression test data input files and expected files.; - Updated regression test sets to include only unique variants.; - Added in a 5' flank and a Start Codon insertion to regression test set. - Fixed a bug in FuncotatorUtils::isIndelBetweenCodons. - Finally fixed a bug with indels and start codons:; Now indels in start codons will not have protein renderings, nor will; they have codon change strings. This brings Funcotator closer to; Oncotator functionality (in Oncotator, start codon insertions/deletions; do not have protein change strings or codon change strings). - Fixed a bug in ordering transcripts by appris ranking. - Fixed a minor bug in how other transcripts are generated:.; With RNA/LINCRNA transcripts, the protein change would be null and was; append ed to the end of each `other transcript`. Now the null is no; longer appended. - Fixed a bug in insertions on the - strand:; All insertions on - strand had incorrectly rendered genome changes,; coding sequence changes, and protein changes.; This is due to how these fields are generated from the reference; sequence. - Fixed a bug (insertions on - strand):; Insertions on the - strand would have incorrect reference; sequences/alleles.; Now they are handled as a special case when computing the aligned; reference allele. - Fixed a bug in transcript selection for GencodeFuncotationFactory:; The LocusLevel / Curation Level was being incorrectly pulled from the; GENE features, rather than the TRANSCRIPT features that contain each; variant. As a result, the order in which representative transcripts; were chosen was wrong. The TRANSCRIPT feature is now being used to; det",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:3939,Deployability,Update,Updated,3939,"oncotator that was fixed in Funcotator involving cdna; strings for - strand indels. In Oncotator the positions reported are off by 1 (they; should be one less) and the base reported is also wrong.; This is now fixed. - Removed some old code that had been taken out of the main codepath. - Fixed a bug in how the gencode reference contexts are created.; - Fixed a bug in how the end points for the gencode annotations are; created. - Ref context field is now consistent for indels.; The reference context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:4526,Deployability,Update,Updated,4526,"nce context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updated test suite to use full, checked-in references. - Updated many tests to use regression test data sources rather than small data; sources specifically for PIK3CA and MUC16. - Removed some now unused data files (primarily old data sources). - Updated regression tests to work on local data sources and references. - Now `FuncotatorIntegrationTest::regressionTest` works on local, checked-out copies of the data sources so that; they can be run from any checkout. - Fixes #4344 ; - Fixes #5295",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:4599,Deployability,Update,Updated,4599,"nce context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updated test suite to use full, checked-in references. - Updated many tests to use regression test data sources rather than small data; sources specifically for PIK3CA and MUC16. - Removed some now unused data files (primarily old data sources). - Updated regression tests to work on local data sources and references. - Now `FuncotatorIntegrationTest::regressionTest` works on local, checked-out copies of the data sources so that; they can be run from any checkout. - Fixes #4344 ; - Fixes #5295",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:4684,Deployability,Update,Updated,4684,"nce context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updated test suite to use full, checked-in references. - Updated many tests to use regression test data sources rather than small data; sources specifically for PIK3CA and MUC16. - Removed some now unused data files (primarily old data sources). - Updated regression tests to work on local data sources and references. - Now `FuncotatorIntegrationTest::regressionTest` works on local, checked-out copies of the data sources so that; they can be run from any checkout. - Fixes #4344 ; - Fixes #5295",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:4961,Deployability,Update,Updated,4961,"nce context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updated test suite to use full, checked-in references. - Updated many tests to use regression test data sources rather than small data; sources specifically for PIK3CA and MUC16. - Removed some now unused data files (primarily old data sources). - Updated regression tests to work on local data sources and references. - Now `FuncotatorIntegrationTest::regressionTest` works on local, checked-out copies of the data sources so that; they can be run from any checkout. - Fixes #4344 ; - Fixes #5295",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:5018,Deployability,Update,Updated,5018,"nce context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updated test suite to use full, checked-in references. - Updated many tests to use regression test data sources rather than small data; sources specifically for PIK3CA and MUC16. - Removed some now unused data files (primarily old data sources). - Updated regression tests to work on local data sources and references. - Now `FuncotatorIntegrationTest::regressionTest` works on local, checked-out copies of the data sources so that; they can be run from any checkout. - Fixes #4344 ; - Fixes #5295",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:5209,Deployability,Update,Updated,5209,"nce context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updated test suite to use full, checked-in references. - Updated many tests to use regression test data sources rather than small data; sources specifically for PIK3CA and MUC16. - Removed some now unused data files (primarily old data sources). - Updated regression tests to work on local data sources and references. - Now `FuncotatorIntegrationTest::regressionTest` works on local, checked-out copies of the data sources so that; they can be run from any checkout. - Fixes #4344 ; - Fixes #5295",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:2273,Modifiability,Refactor,Refactored,2273,"genome changes,; coding sequence changes, and protein changes.; This is due to how these fields are generated from the reference; sequence. - Fixed a bug (insertions on - strand):; Insertions on the - strand would have incorrect reference; sequences/alleles.; Now they are handled as a special case when computing the aligned; reference allele. - Fixed a bug in transcript selection for GencodeFuncotationFactory:; The LocusLevel / Curation Level was being incorrectly pulled from the; GENE features, rather than the TRANSCRIPT features that contain each; variant. As a result, the order in which representative transcripts; were chosen was wrong. The TRANSCRIPT feature is now being used to; determine the Locus/Curation Level. - `TranscriptType` now determined by transcript annotation, not gene annotation; - Start/stop codon overlapping now corrected for preceding indel bases (is now correct for more cases).; - Changed algorithm for how 5'UTRs are determined. - Refactored how frameshift indels have codon change strings created. - Added in helper some scripts for testing funcotator. - Fixed how codon change strings are rendered to be consistent and more; correct. - Fixed Protein Change strings to be consistent and more; correct. - Implemented tests for CreateProteinChangeInfo; - Implemented tests for RenderProteinChangeString; - Implemented tests for IsIndelBetweenCodons; - Implemented tests for GetCodonChangeString. - Added a unit test for; testCreateGencodeFuncotationBuilderWithTrivialFieldsPopulated. - Fixed a bug when variant ref allele doesn't match reference genome. - Fixed test cases for - strand indel cdna strings:; There is a bug in oncotator that was fixed in Funcotator involving cdna; strings for - strand indels. In Oncotator the positions reported are off by 1 (they; should be one less) and the base reported is also wrong.; This is now fixed. - Removed some old code that had been taken out of the main codepath. - Fixed a bug in how the gencode reference contexts ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:4653,Modifiability,variab,variable,4653,"nce context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updated test suite to use full, checked-in references. - Updated many tests to use regression test data sources rather than small data; sources specifically for PIK3CA and MUC16. - Removed some now unused data files (primarily old data sources). - Updated regression tests to work on local data sources and references. - Now `FuncotatorIntegrationTest::regressionTest` works on local, checked-out copies of the data sources so that; they can be run from any checkout. - Fixes #4344 ; - Fixes #5295",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:4810,Modifiability,Refactor,Refactored,4810,"nce context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updated test suite to use full, checked-in references. - Updated many tests to use regression test data sources rather than small data; sources specifically for PIK3CA and MUC16. - Removed some now unused data files (primarily old data sources). - Updated regression tests to work on local data sources and references. - Now `FuncotatorIntegrationTest::regressionTest` works on local, checked-out copies of the data sources so that; they can be run from any checkout. - Fixes #4344 ; - Fixes #5295",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:4674,Safety,safe,safely,4674,"nce context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updated test suite to use full, checked-in references. - Updated many tests to use regression test data sources rather than small data; sources specifically for PIK3CA and MUC16. - Removed some now unused data files (primarily old data sources). - Updated regression tests to work on local data sources and references. - Now `FuncotatorIntegrationTest::regressionTest` works on local, checked-out copies of the data sources so that; they can be run from any checkout. - Fixes #4344 ; - Fixes #5295",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:22,Testability,test,test,22,"MASSIVE bug fixes and test updates. (Rebased 61 commits). - Fixed a bug when variants overlap the end of transcripts. - Updated logging in FuncotatorUtils::getAlignedRefAllele. - Fixed a bug in identifying splice sites for intronic regions. - Fixed a bug in FuncotatorUtils::isIndelBetweenCodons that caused issues; on reverse stranded variants. - Added in regression test data input files and expected files.; - Updated regression test sets to include only unique variants.; - Added in a 5' flank and a Start Codon insertion to regression test set. - Fixed a bug in FuncotatorUtils::isIndelBetweenCodons. - Finally fixed a bug with indels and start codons:; Now indels in start codons will not have protein renderings, nor will; they have codon change strings. This brings Funcotator closer to; Oncotator functionality (in Oncotator, start codon insertions/deletions; do not have protein change strings or codon change strings). - Fixed a bug in ordering transcripts by appris ranking. - Fixed a minor bug in how other transcripts are generated:.; With RNA/LINCRNA transcripts, the protein change would be null and was; append ed to the end of each `other transcript`. Now the null is no; longer appended. - Fixed a bug in insertions on the - strand:; All insertions on - strand had incorrectly rendered genome changes,; coding sequence changes, and protein changes.; This is due to how these fields are generated from the reference; sequence. - Fixed a bug (insertions on - strand):; Insertions on the - strand would have incorrect reference; sequences/alleles.; Now they are handled as a special case when computing the aligned; reference allele. - Fixed a bug in transcript selection for GencodeFuncotationFactory:; The LocusLevel / Curation Level was being incorrectly pulled from the; GENE features, rather than the TRANSCRIPT features that contain each; variant. As a result, the order in which representative transcripts; were chosen was wrong. The TRANSCRIPT feature is now being used to; det",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:128,Testability,log,logging,128,"MASSIVE bug fixes and test updates. (Rebased 61 commits). - Fixed a bug when variants overlap the end of transcripts. - Updated logging in FuncotatorUtils::getAlignedRefAllele. - Fixed a bug in identifying splice sites for intronic regions. - Fixed a bug in FuncotatorUtils::isIndelBetweenCodons that caused issues; on reverse stranded variants. - Added in regression test data input files and expected files.; - Updated regression test sets to include only unique variants.; - Added in a 5' flank and a Start Codon insertion to regression test set. - Fixed a bug in FuncotatorUtils::isIndelBetweenCodons. - Finally fixed a bug with indels and start codons:; Now indels in start codons will not have protein renderings, nor will; they have codon change strings. This brings Funcotator closer to; Oncotator functionality (in Oncotator, start codon insertions/deletions; do not have protein change strings or codon change strings). - Fixed a bug in ordering transcripts by appris ranking. - Fixed a minor bug in how other transcripts are generated:.; With RNA/LINCRNA transcripts, the protein change would be null and was; append ed to the end of each `other transcript`. Now the null is no; longer appended. - Fixed a bug in insertions on the - strand:; All insertions on - strand had incorrectly rendered genome changes,; coding sequence changes, and protein changes.; This is due to how these fields are generated from the reference; sequence. - Fixed a bug (insertions on - strand):; Insertions on the - strand would have incorrect reference; sequences/alleles.; Now they are handled as a special case when computing the aligned; reference allele. - Fixed a bug in transcript selection for GencodeFuncotationFactory:; The LocusLevel / Curation Level was being incorrectly pulled from the; GENE features, rather than the TRANSCRIPT features that contain each; variant. As a result, the order in which representative transcripts; were chosen was wrong. The TRANSCRIPT feature is now being used to; det",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:368,Testability,test,test,368,"MASSIVE bug fixes and test updates. (Rebased 61 commits). - Fixed a bug when variants overlap the end of transcripts. - Updated logging in FuncotatorUtils::getAlignedRefAllele. - Fixed a bug in identifying splice sites for intronic regions. - Fixed a bug in FuncotatorUtils::isIndelBetweenCodons that caused issues; on reverse stranded variants. - Added in regression test data input files and expected files.; - Updated regression test sets to include only unique variants.; - Added in a 5' flank and a Start Codon insertion to regression test set. - Fixed a bug in FuncotatorUtils::isIndelBetweenCodons. - Finally fixed a bug with indels and start codons:; Now indels in start codons will not have protein renderings, nor will; they have codon change strings. This brings Funcotator closer to; Oncotator functionality (in Oncotator, start codon insertions/deletions; do not have protein change strings or codon change strings). - Fixed a bug in ordering transcripts by appris ranking. - Fixed a minor bug in how other transcripts are generated:.; With RNA/LINCRNA transcripts, the protein change would be null and was; append ed to the end of each `other transcript`. Now the null is no; longer appended. - Fixed a bug in insertions on the - strand:; All insertions on - strand had incorrectly rendered genome changes,; coding sequence changes, and protein changes.; This is due to how these fields are generated from the reference; sequence. - Fixed a bug (insertions on - strand):; Insertions on the - strand would have incorrect reference; sequences/alleles.; Now they are handled as a special case when computing the aligned; reference allele. - Fixed a bug in transcript selection for GencodeFuncotationFactory:; The LocusLevel / Curation Level was being incorrectly pulled from the; GENE features, rather than the TRANSCRIPT features that contain each; variant. As a result, the order in which representative transcripts; were chosen was wrong. The TRANSCRIPT feature is now being used to; det",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:432,Testability,test,test,432,"MASSIVE bug fixes and test updates. (Rebased 61 commits). - Fixed a bug when variants overlap the end of transcripts. - Updated logging in FuncotatorUtils::getAlignedRefAllele. - Fixed a bug in identifying splice sites for intronic regions. - Fixed a bug in FuncotatorUtils::isIndelBetweenCodons that caused issues; on reverse stranded variants. - Added in regression test data input files and expected files.; - Updated regression test sets to include only unique variants.; - Added in a 5' flank and a Start Codon insertion to regression test set. - Fixed a bug in FuncotatorUtils::isIndelBetweenCodons. - Finally fixed a bug with indels and start codons:; Now indels in start codons will not have protein renderings, nor will; they have codon change strings. This brings Funcotator closer to; Oncotator functionality (in Oncotator, start codon insertions/deletions; do not have protein change strings or codon change strings). - Fixed a bug in ordering transcripts by appris ranking. - Fixed a minor bug in how other transcripts are generated:.; With RNA/LINCRNA transcripts, the protein change would be null and was; append ed to the end of each `other transcript`. Now the null is no; longer appended. - Fixed a bug in insertions on the - strand:; All insertions on - strand had incorrectly rendered genome changes,; coding sequence changes, and protein changes.; This is due to how these fields are generated from the reference; sequence. - Fixed a bug (insertions on - strand):; Insertions on the - strand would have incorrect reference; sequences/alleles.; Now they are handled as a special case when computing the aligned; reference allele. - Fixed a bug in transcript selection for GencodeFuncotationFactory:; The LocusLevel / Curation Level was being incorrectly pulled from the; GENE features, rather than the TRANSCRIPT features that contain each; variant. As a result, the order in which representative transcripts; were chosen was wrong. The TRANSCRIPT feature is now being used to; det",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:540,Testability,test,test,540,"MASSIVE bug fixes and test updates. (Rebased 61 commits). - Fixed a bug when variants overlap the end of transcripts. - Updated logging in FuncotatorUtils::getAlignedRefAllele. - Fixed a bug in identifying splice sites for intronic regions. - Fixed a bug in FuncotatorUtils::isIndelBetweenCodons that caused issues; on reverse stranded variants. - Added in regression test data input files and expected files.; - Updated regression test sets to include only unique variants.; - Added in a 5' flank and a Start Codon insertion to regression test set. - Fixed a bug in FuncotatorUtils::isIndelBetweenCodons. - Finally fixed a bug with indels and start codons:; Now indels in start codons will not have protein renderings, nor will; they have codon change strings. This brings Funcotator closer to; Oncotator functionality (in Oncotator, start codon insertions/deletions; do not have protein change strings or codon change strings). - Fixed a bug in ordering transcripts by appris ranking. - Fixed a minor bug in how other transcripts are generated:.; With RNA/LINCRNA transcripts, the protein change would be null and was; append ed to the end of each `other transcript`. Now the null is no; longer appended. - Fixed a bug in insertions on the - strand:; All insertions on - strand had incorrectly rendered genome changes,; coding sequence changes, and protein changes.; This is due to how these fields are generated from the reference; sequence. - Fixed a bug (insertions on - strand):; Insertions on the - strand would have incorrect reference; sequences/alleles.; Now they are handled as a special case when computing the aligned; reference allele. - Fixed a bug in transcript selection for GencodeFuncotationFactory:; The LocusLevel / Curation Level was being incorrectly pulled from the; GENE features, rather than the TRANSCRIPT features that contain each; variant. As a result, the order in which representative transcripts; were chosen was wrong. The TRANSCRIPT feature is now being used to; det",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:2376,Testability,test,testing,2376,"; This is due to how these fields are generated from the reference; sequence. - Fixed a bug (insertions on - strand):; Insertions on the - strand would have incorrect reference; sequences/alleles.; Now they are handled as a special case when computing the aligned; reference allele. - Fixed a bug in transcript selection for GencodeFuncotationFactory:; The LocusLevel / Curation Level was being incorrectly pulled from the; GENE features, rather than the TRANSCRIPT features that contain each; variant. As a result, the order in which representative transcripts; were chosen was wrong. The TRANSCRIPT feature is now being used to; determine the Locus/Curation Level. - `TranscriptType` now determined by transcript annotation, not gene annotation; - Start/stop codon overlapping now corrected for preceding indel bases (is now correct for more cases).; - Changed algorithm for how 5'UTRs are determined. - Refactored how frameshift indels have codon change strings created. - Added in helper some scripts for testing funcotator. - Fixed how codon change strings are rendered to be consistent and more; correct. - Fixed Protein Change strings to be consistent and more; correct. - Implemented tests for CreateProteinChangeInfo; - Implemented tests for RenderProteinChangeString; - Implemented tests for IsIndelBetweenCodons; - Implemented tests for GetCodonChangeString. - Added a unit test for; testCreateGencodeFuncotationBuilderWithTrivialFieldsPopulated. - Fixed a bug when variant ref allele doesn't match reference genome. - Fixed test cases for - strand indel cdna strings:; There is a bug in oncotator that was fixed in Funcotator involving cdna; strings for - strand indels. In Oncotator the positions reported are off by 1 (they; should be one less) and the base reported is also wrong.; This is now fixed. - Removed some old code that had been taken out of the main codepath. - Fixed a bug in how the gencode reference contexts are created.; - Fixed a bug in how the end points for the genco",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:2559,Testability,test,tests,2559,"e allele. - Fixed a bug in transcript selection for GencodeFuncotationFactory:; The LocusLevel / Curation Level was being incorrectly pulled from the; GENE features, rather than the TRANSCRIPT features that contain each; variant. As a result, the order in which representative transcripts; were chosen was wrong. The TRANSCRIPT feature is now being used to; determine the Locus/Curation Level. - `TranscriptType` now determined by transcript annotation, not gene annotation; - Start/stop codon overlapping now corrected for preceding indel bases (is now correct for more cases).; - Changed algorithm for how 5'UTRs are determined. - Refactored how frameshift indels have codon change strings created. - Added in helper some scripts for testing funcotator. - Fixed how codon change strings are rendered to be consistent and more; correct. - Fixed Protein Change strings to be consistent and more; correct. - Implemented tests for CreateProteinChangeInfo; - Implemented tests for RenderProteinChangeString; - Implemented tests for IsIndelBetweenCodons; - Implemented tests for GetCodonChangeString. - Added a unit test for; testCreateGencodeFuncotationBuilderWithTrivialFieldsPopulated. - Fixed a bug when variant ref allele doesn't match reference genome. - Fixed test cases for - strand indel cdna strings:; There is a bug in oncotator that was fixed in Funcotator involving cdna; strings for - strand indels. In Oncotator the positions reported are off by 1 (they; should be one less) and the base reported is also wrong.; This is now fixed. - Removed some old code that had been taken out of the main codepath. - Fixed a bug in how the gencode reference contexts are created.; - Fixed a bug in how the end points for the gencode annotations are; created. - Ref context field is now consistent for indels.; The reference context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has cha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:2608,Testability,test,tests,2608,"e allele. - Fixed a bug in transcript selection for GencodeFuncotationFactory:; The LocusLevel / Curation Level was being incorrectly pulled from the; GENE features, rather than the TRANSCRIPT features that contain each; variant. As a result, the order in which representative transcripts; were chosen was wrong. The TRANSCRIPT feature is now being used to; determine the Locus/Curation Level. - `TranscriptType` now determined by transcript annotation, not gene annotation; - Start/stop codon overlapping now corrected for preceding indel bases (is now correct for more cases).; - Changed algorithm for how 5'UTRs are determined. - Refactored how frameshift indels have codon change strings created. - Added in helper some scripts for testing funcotator. - Fixed how codon change strings are rendered to be consistent and more; correct. - Fixed Protein Change strings to be consistent and more; correct. - Implemented tests for CreateProteinChangeInfo; - Implemented tests for RenderProteinChangeString; - Implemented tests for IsIndelBetweenCodons; - Implemented tests for GetCodonChangeString. - Added a unit test for; testCreateGencodeFuncotationBuilderWithTrivialFieldsPopulated. - Fixed a bug when variant ref allele doesn't match reference genome. - Fixed test cases for - strand indel cdna strings:; There is a bug in oncotator that was fixed in Funcotator involving cdna; strings for - strand indels. In Oncotator the positions reported are off by 1 (they; should be one less) and the base reported is also wrong.; This is now fixed. - Removed some old code that had been taken out of the main codepath. - Fixed a bug in how the gencode reference contexts are created.; - Fixed a bug in how the end points for the gencode annotations are; created. - Ref context field is now consistent for indels.; The reference context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has cha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:2659,Testability,test,tests,2659,"e allele. - Fixed a bug in transcript selection for GencodeFuncotationFactory:; The LocusLevel / Curation Level was being incorrectly pulled from the; GENE features, rather than the TRANSCRIPT features that contain each; variant. As a result, the order in which representative transcripts; were chosen was wrong. The TRANSCRIPT feature is now being used to; determine the Locus/Curation Level. - `TranscriptType` now determined by transcript annotation, not gene annotation; - Start/stop codon overlapping now corrected for preceding indel bases (is now correct for more cases).; - Changed algorithm for how 5'UTRs are determined. - Refactored how frameshift indels have codon change strings created. - Added in helper some scripts for testing funcotator. - Fixed how codon change strings are rendered to be consistent and more; correct. - Fixed Protein Change strings to be consistent and more; correct. - Implemented tests for CreateProteinChangeInfo; - Implemented tests for RenderProteinChangeString; - Implemented tests for IsIndelBetweenCodons; - Implemented tests for GetCodonChangeString. - Added a unit test for; testCreateGencodeFuncotationBuilderWithTrivialFieldsPopulated. - Fixed a bug when variant ref allele doesn't match reference genome. - Fixed test cases for - strand indel cdna strings:; There is a bug in oncotator that was fixed in Funcotator involving cdna; strings for - strand indels. In Oncotator the positions reported are off by 1 (they; should be one less) and the base reported is also wrong.; This is now fixed. - Removed some old code that had been taken out of the main codepath. - Fixed a bug in how the gencode reference contexts are created.; - Fixed a bug in how the end points for the gencode annotations are; created. - Ref context field is now consistent for indels.; The reference context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has cha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:2705,Testability,test,tests,2705,"e allele. - Fixed a bug in transcript selection for GencodeFuncotationFactory:; The LocusLevel / Curation Level was being incorrectly pulled from the; GENE features, rather than the TRANSCRIPT features that contain each; variant. As a result, the order in which representative transcripts; were chosen was wrong. The TRANSCRIPT feature is now being used to; determine the Locus/Curation Level. - `TranscriptType` now determined by transcript annotation, not gene annotation; - Start/stop codon overlapping now corrected for preceding indel bases (is now correct for more cases).; - Changed algorithm for how 5'UTRs are determined. - Refactored how frameshift indels have codon change strings created. - Added in helper some scripts for testing funcotator. - Fixed how codon change strings are rendered to be consistent and more; correct. - Fixed Protein Change strings to be consistent and more; correct. - Implemented tests for CreateProteinChangeInfo; - Implemented tests for RenderProteinChangeString; - Implemented tests for IsIndelBetweenCodons; - Implemented tests for GetCodonChangeString. - Added a unit test for; testCreateGencodeFuncotationBuilderWithTrivialFieldsPopulated. - Fixed a bug when variant ref allele doesn't match reference genome. - Fixed test cases for - strand indel cdna strings:; There is a bug in oncotator that was fixed in Funcotator involving cdna; strings for - strand indels. In Oncotator the positions reported are off by 1 (they; should be one less) and the base reported is also wrong.; This is now fixed. - Removed some old code that had been taken out of the main codepath. - Fixed a bug in how the gencode reference contexts are created.; - Fixed a bug in how the end points for the gencode annotations are; created. - Ref context field is now consistent for indels.; The reference context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has cha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:2752,Testability,test,test,2752," from the; GENE features, rather than the TRANSCRIPT features that contain each; variant. As a result, the order in which representative transcripts; were chosen was wrong. The TRANSCRIPT feature is now being used to; determine the Locus/Curation Level. - `TranscriptType` now determined by transcript annotation, not gene annotation; - Start/stop codon overlapping now corrected for preceding indel bases (is now correct for more cases).; - Changed algorithm for how 5'UTRs are determined. - Refactored how frameshift indels have codon change strings created. - Added in helper some scripts for testing funcotator. - Fixed how codon change strings are rendered to be consistent and more; correct. - Fixed Protein Change strings to be consistent and more; correct. - Implemented tests for CreateProteinChangeInfo; - Implemented tests for RenderProteinChangeString; - Implemented tests for IsIndelBetweenCodons; - Implemented tests for GetCodonChangeString. - Added a unit test for; testCreateGencodeFuncotationBuilderWithTrivialFieldsPopulated. - Fixed a bug when variant ref allele doesn't match reference genome. - Fixed test cases for - strand indel cdna strings:; There is a bug in oncotator that was fixed in Funcotator involving cdna; strings for - strand indels. In Oncotator the positions reported are off by 1 (they; should be one less) and the base reported is also wrong.; This is now fixed. - Removed some old code that had been taken out of the main codepath. - Fixed a bug in how the gencode reference contexts are created.; - Fixed a bug in how the end points for the gencode annotations are; created. - Ref context field is now consistent for indels.; The reference context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:2762,Testability,test,testCreateGencodeFuncotationBuilderWithTrivialFieldsPopulated,2762," from the; GENE features, rather than the TRANSCRIPT features that contain each; variant. As a result, the order in which representative transcripts; were chosen was wrong. The TRANSCRIPT feature is now being used to; determine the Locus/Curation Level. - `TranscriptType` now determined by transcript annotation, not gene annotation; - Start/stop codon overlapping now corrected for preceding indel bases (is now correct for more cases).; - Changed algorithm for how 5'UTRs are determined. - Refactored how frameshift indels have codon change strings created. - Added in helper some scripts for testing funcotator. - Fixed how codon change strings are rendered to be consistent and more; correct. - Fixed Protein Change strings to be consistent and more; correct. - Implemented tests for CreateProteinChangeInfo; - Implemented tests for RenderProteinChangeString; - Implemented tests for IsIndelBetweenCodons; - Implemented tests for GetCodonChangeString. - Added a unit test for; testCreateGencodeFuncotationBuilderWithTrivialFieldsPopulated. - Fixed a bug when variant ref allele doesn't match reference genome. - Fixed test cases for - strand indel cdna strings:; There is a bug in oncotator that was fixed in Funcotator involving cdna; strings for - strand indels. In Oncotator the positions reported are off by 1 (they; should be one less) and the base reported is also wrong.; This is now fixed. - Removed some old code that had been taken out of the main codepath. - Fixed a bug in how the gencode reference contexts are created.; - Fixed a bug in how the end points for the gencode annotations are; created. - Ref context field is now consistent for indels.; The reference context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:2903,Testability,test,test,2903,"ture is now being used to; determine the Locus/Curation Level. - `TranscriptType` now determined by transcript annotation, not gene annotation; - Start/stop codon overlapping now corrected for preceding indel bases (is now correct for more cases).; - Changed algorithm for how 5'UTRs are determined. - Refactored how frameshift indels have codon change strings created. - Added in helper some scripts for testing funcotator. - Fixed how codon change strings are rendered to be consistent and more; correct. - Fixed Protein Change strings to be consistent and more; correct. - Implemented tests for CreateProteinChangeInfo; - Implemented tests for RenderProteinChangeString; - Implemented tests for IsIndelBetweenCodons; - Implemented tests for GetCodonChangeString. - Added a unit test for; testCreateGencodeFuncotationBuilderWithTrivialFieldsPopulated. - Fixed a bug when variant ref allele doesn't match reference genome. - Fixed test cases for - strand indel cdna strings:; There is a bug in oncotator that was fixed in Funcotator involving cdna; strings for - strand indels. In Oncotator the positions reported are off by 1 (they; should be one less) and the base reported is also wrong.; This is now fixed. - Removed some old code that had been taken out of the main codepath. - Fixed a bug in how the gencode reference contexts are created.; - Fixed a bug in how the end points for the gencode annotations are; created. - Ref context field is now consistent for indels.; The reference context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:3515,Testability,log,logical,3515," strings to be consistent and more; correct. - Implemented tests for CreateProteinChangeInfo; - Implemented tests for RenderProteinChangeString; - Implemented tests for IsIndelBetweenCodons; - Implemented tests for GetCodonChangeString. - Added a unit test for; testCreateGencodeFuncotationBuilderWithTrivialFieldsPopulated. - Fixed a bug when variant ref allele doesn't match reference genome. - Fixed test cases for - strand indel cdna strings:; There is a bug in oncotator that was fixed in Funcotator involving cdna; strings for - strand indels. In Oncotator the positions reported are off by 1 (they; should be one less) and the base reported is also wrong.; This is now fixed. - Removed some old code that had been taken out of the main codepath. - Fixed a bug in how the gencode reference contexts are created.; - Fixed a bug in how the end points for the gencode annotations are; created. - Ref context field is now consistent for indels.; The reference context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:3667,Testability,log,logical,3667,"GetCodonChangeString. - Added a unit test for; testCreateGencodeFuncotationBuilderWithTrivialFieldsPopulated. - Fixed a bug when variant ref allele doesn't match reference genome. - Fixed test cases for - strand indel cdna strings:; There is a bug in oncotator that was fixed in Funcotator involving cdna; strings for - strand indels. In Oncotator the positions reported are off by 1 (they; should be one less) and the base reported is also wrong.; This is now fixed. - Removed some old code that had been taken out of the main codepath. - Fixed a bug in how the gencode reference contexts are created.; - Fixed a bug in how the end points for the gencode annotations are; created. - Ref context field is now consistent for indels.; The reference context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCod",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:3807,Testability,log,logical,3807," bug when variant ref allele doesn't match reference genome. - Fixed test cases for - strand indel cdna strings:; There is a bug in oncotator that was fixed in Funcotator involving cdna; strings for - strand indels. In Oncotator the positions reported are off by 1 (they; should be one less) and the base reported is also wrong.; This is now fixed. - Removed some old code that had been taken out of the main codepath. - Fixed a bug in how the gencode reference contexts are created.; - Fixed a bug in how the end points for the gencode annotations are; created. - Ref context field is now consistent for indels.; The reference context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFunco",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:3897,Testability,log,logical,3897,"ses for - strand indel cdna strings:; There is a bug in oncotator that was fixed in Funcotator involving cdna; strings for - strand indels. In Oncotator the positions reported are off by 1 (they; should be one less) and the base reported is also wrong.; This is now fixed. - Removed some old code that had been taken out of the main codepath. - Fixed a bug in how the gencode reference contexts are created.; - Fixed a bug in how the end points for the gencode annotations are; created. - Ref context field is now consistent for indels.; The reference context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:3952,Testability,test,tests,3952,"oncotator that was fixed in Funcotator involving cdna; strings for - strand indels. In Oncotator the positions reported are off by 1 (they; should be one less) and the base reported is also wrong.; This is now fixed. - Removed some old code that had been taken out of the main codepath. - Fixed a bug in how the gencode reference contexts are created.; - Fixed a bug in how the end points for the gencode annotations are; created. - Ref context field is now consistent for indels.; The reference context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:3962,Testability,test,test,3962,"oncotator that was fixed in Funcotator involving cdna; strings for - strand indels. In Oncotator the positions reported are off by 1 (they; should be one less) and the base reported is also wrong.; This is now fixed. - Removed some old code that had been taken out of the main codepath. - Fixed a bug in how the gencode reference contexts are created.; - Fixed a bug in how the end points for the gencode annotations are; created. - Ref context field is now consistent for indels.; The reference context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:4028,Testability,test,test,4028,"dna; strings for - strand indels. In Oncotator the positions reported are off by 1 (they; should be one less) and the base reported is also wrong.; This is now fixed. - Removed some old code that had been taken out of the main codepath. - Fixed a bug in how the gencode reference contexts are created.; - Fixed a bug in how the end points for the gencode annotations are; created. - Ref context field is now consistent for indels.; The reference context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updated test suite to use full, checked-in references. ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:4431,Testability,test,test,4431,"is now consistent for indels.; The reference context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updated test suite to use full, checked-in references. - Updated many tests to use regression test data sources rather than small data; sources specifically for PIK3CA and MUC16. - Removed some now unused data files (primarily old data sources). - Updated regression tests to work on local data sources and references. - Now `FuncotatorIntegrationTest::regressionTest` works on local, checked-out copies of the data sources so that; they can be run from an",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:4495,Testability,test,test,4495,"nce context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updated test suite to use full, checked-in references. - Updated many tests to use regression test data sources rather than small data; sources specifically for PIK3CA and MUC16. - Removed some now unused data files (primarily old data sources). - Updated regression tests to work on local data sources and references. - Now `FuncotatorIntegrationTest::regressionTest` works on local, checked-out copies of the data sources so that; they can be run from any checkout. - Fixes #4344 ; - Fixes #5295",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:4969,Testability,test,test,4969,"nce context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updated test suite to use full, checked-in references. - Updated many tests to use regression test data sources rather than small data; sources specifically for PIK3CA and MUC16. - Removed some now unused data files (primarily old data sources). - Updated regression tests to work on local data sources and references. - Now `FuncotatorIntegrationTest::regressionTest` works on local, checked-out copies of the data sources so that; they can be run from any checkout. - Fixes #4344 ; - Fixes #5295",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:5031,Testability,test,tests,5031,"nce context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updated test suite to use full, checked-in references. - Updated many tests to use regression test data sources rather than small data; sources specifically for PIK3CA and MUC16. - Removed some now unused data files (primarily old data sources). - Updated regression tests to work on local data sources and references. - Now `FuncotatorIntegrationTest::regressionTest` works on local, checked-out copies of the data sources so that; they can be run from any checkout. - Fixes #4344 ; - Fixes #5295",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:5055,Testability,test,test,5055,"nce context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updated test suite to use full, checked-in references. - Updated many tests to use regression test data sources rather than small data; sources specifically for PIK3CA and MUC16. - Removed some now unused data files (primarily old data sources). - Updated regression tests to work on local data sources and references. - Now `FuncotatorIntegrationTest::regressionTest` works on local, checked-out copies of the data sources so that; they can be run from any checkout. - Fixes #4344 ; - Fixes #5295",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/pull/5302:5228,Testability,test,tests,5228,"nce context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updated test suite to use full, checked-in references. - Updated many tests to use regression test data sources rather than small data; sources specifically for PIK3CA and MUC16. - Removed some now unused data files (primarily old data sources). - Updated regression tests to work on local data sources and references. - Now `FuncotatorIntegrationTest::regressionTest` works on local, checked-out copies of the data sources so that; they can be run from any checkout. - Fixes #4344 ; - Fixes #5295",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302
https://github.com/broadinstitute/gatk/issues/5306:64,Availability,error,errors,64,"The `google-cloud-nio` library should allow us to customize the errors that trigger a retry/reopen. Specifically, it should allow us to add additional http status codes to trigger a retry, as well as additional exception classes to trigger a reopen. It should be possible to either append to the existing defaults, or to replace the defaults completely with your own values.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5306
https://github.com/broadinstitute/gatk/pull/5308:96,Security,authenticat,authenticate,96,* the old version didn't include certain keys in the json that are; necessary for GenomicsDB to authenticate; * closes #5305,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5308
https://github.com/broadinstitute/gatk/issues/5310:232,Availability,avail,available,232,## Feature request. The mitochondria pipeline should have new annotations and filters in Mutect2 and FilterMutectCalls. This is being addressed in #5193. An accompanying best practices WDL should also be developed and eventually be available in Firecloud. I'll update here once the PR is merged and has been released.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5310
https://github.com/broadinstitute/gatk/issues/5310:37,Deployability,pipeline,pipeline,37,## Feature request. The mitochondria pipeline should have new annotations and filters in Mutect2 and FilterMutectCalls. This is being addressed in #5193. An accompanying best practices WDL should also be developed and eventually be available in Firecloud. I'll update here once the PR is merged and has been released.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5310
https://github.com/broadinstitute/gatk/issues/5310:261,Deployability,update,update,261,## Feature request. The mitochondria pipeline should have new annotations and filters in Mutect2 and FilterMutectCalls. This is being addressed in #5193. An accompanying best practices WDL should also be developed and eventually be available in Firecloud. I'll update here once the PR is merged and has been released.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5310
https://github.com/broadinstitute/gatk/issues/5310:308,Deployability,release,released,308,## Feature request. The mitochondria pipeline should have new annotations and filters in Mutect2 and FilterMutectCalls. This is being addressed in #5193. An accompanying best practices WDL should also be developed and eventually be available in Firecloud. I'll update here once the PR is merged and has been released.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5310
https://github.com/broadinstitute/gatk/pull/5312:256,Deployability,integrat,integration,256,"Since we're using Mutect2 for mitochondrial calling, we want some reference confidence representation for joint calling. I did my best, but further refactoring suggestions appreciated. Tests to follow. @davidbenjamin can you take a look at the LODs in the integration test results? I'm not entirely surprised that at the same depth, the variant LOD is higher than the reference LOD. I'm not sure that the NON_REF LOD at variant sites is coming out right though. Is there an effective negative LOD asymptote?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5312
https://github.com/broadinstitute/gatk/pull/5312:256,Integrability,integrat,integration,256,"Since we're using Mutect2 for mitochondrial calling, we want some reference confidence representation for joint calling. I did my best, but further refactoring suggestions appreciated. Tests to follow. @davidbenjamin can you take a look at the LODs in the integration test results? I'm not entirely surprised that at the same depth, the variant LOD is higher than the reference LOD. I'm not sure that the NON_REF LOD at variant sites is coming out right though. Is there an effective negative LOD asymptote?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5312
https://github.com/broadinstitute/gatk/pull/5312:148,Modifiability,refactor,refactoring,148,"Since we're using Mutect2 for mitochondrial calling, we want some reference confidence representation for joint calling. I did my best, but further refactoring suggestions appreciated. Tests to follow. @davidbenjamin can you take a look at the LODs in the integration test results? I'm not entirely surprised that at the same depth, the variant LOD is higher than the reference LOD. I'm not sure that the NON_REF LOD at variant sites is coming out right though. Is there an effective negative LOD asymptote?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5312
https://github.com/broadinstitute/gatk/pull/5312:185,Testability,Test,Tests,185,"Since we're using Mutect2 for mitochondrial calling, we want some reference confidence representation for joint calling. I did my best, but further refactoring suggestions appreciated. Tests to follow. @davidbenjamin can you take a look at the LODs in the integration test results? I'm not entirely surprised that at the same depth, the variant LOD is higher than the reference LOD. I'm not sure that the NON_REF LOD at variant sites is coming out right though. Is there an effective negative LOD asymptote?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5312
https://github.com/broadinstitute/gatk/pull/5312:268,Testability,test,test,268,"Since we're using Mutect2 for mitochondrial calling, we want some reference confidence representation for joint calling. I did my best, but further refactoring suggestions appreciated. Tests to follow. @davidbenjamin can you take a look at the LODs in the integration test results? I'm not entirely surprised that at the same depth, the variant LOD is higher than the reference LOD. I'm not sure that the NON_REF LOD at variant sites is coming out right though. Is there an effective negative LOD asymptote?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5312
https://github.com/broadinstitute/gatk/issues/5313:144,Availability,redundant,redundant,144,"Now that we've added the complete B37 and HG38 references to our test data (https://github.com/broadinstitute/gatk/pull/5309), we should remove redundant snippets of these references to save space, and replace usages of the snippets with usages of the full-sized references.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5313
https://github.com/broadinstitute/gatk/issues/5313:144,Safety,redund,redundant,144,"Now that we've added the complete B37 and HG38 references to our test data (https://github.com/broadinstitute/gatk/pull/5309), we should remove redundant snippets of these references to save space, and replace usages of the snippets with usages of the full-sized references.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5313
https://github.com/broadinstitute/gatk/issues/5313:65,Testability,test,test,65,"Now that we've added the complete B37 and HG38 references to our test data (https://github.com/broadinstitute/gatk/pull/5309), we should remove redundant snippets of these references to save space, and replace usages of the snippets with usages of the full-sized references.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5313
https://github.com/broadinstitute/gatk/issues/5315:113,Deployability,release,release,113,"## Bug Report. ### Affected tool(s) or class(es); AddCommentsToBam. ### Affected version(s); - [x] Latest public release version [version?] GATK-4.0.10.1. ### Description ; I was trying to run AddCommentsToBam and with the -C flag, it was crashing when I had a colon in the string, so I had to delete it. #### Steps to reproduce; `gatk AddCommentsToBam -I=In.bam -O=Out.bam -C=""Bad: comment""`; `gatk AddCommentsToBam -I=In.bam -O=Out.bam -C=""Good comment""`. #### Expected behavior; A new BAM with the comment should be created. #### Actual behavior; I get this output: No value found for tagged argument: C=Bad: comment",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5315
https://github.com/broadinstitute/gatk/issues/5316:1261,Availability,ERROR,ERROR,1261,nd:; `./gatk BaseRecalibratorSpark --tmp-dir /dev/shm/gatktmp/ -I /home/data/WGS/F002/F002.sort.bam -O 1.grp --known-sites /home/data/ref/dbsnp_138.hg19.vcf --known-sites /home/data/ref/1000G_phase1.indels.hg19.sites.vcf --known-sites /home/data/ref/Mills_and_1000G_gold_standard.indels.hg19.sites.vcf -R /home/data/ref/ucsc.hg19.fasta -- --spark-runner SPARK --spark-master local[8] --driver-memory 100G`. Here is the log:. > 19:23:59.384 INFO FeatureManager - Using codec VCFCodec to read file file:///dev/shm/gatktmp/spark-30e238e4-b1b7-41f9-b31e-844f16879051/userFiles-4621c82d-5f86-4b51-9321-ccc84ab49979/Mills_and_1000G_gold_standard.indels.hg19.sites.vcf; 19:23:59.411 INFO BaseRecalibrationEngine - The covariates being used here: ; 19:23:59.411 INFO BaseRecalibrationEngine - 	ReadGroupCovariate; 19:23:59.412 INFO BaseRecalibrationEngine - 	QualityScoreCovariate; 19:23:59.412 INFO BaseRecalibrationEngine - 	ContextCovariate; 19:23:59.412 INFO BaseRecalibrationEngine - 	CycleCovariate; 18/10/17 19:23:59 ERROR Executor: Exception in task 517.0 in stage 0.0 (TID 517); org.broadinstitute.hellbender.exceptions.UserException$NoSuitableCodecs: Cannot read /dev/shm/gatktmp/spark-30e238e4-b1b7-41f9-b31e-844f16879051/userFiles-4621c82d-5f86-4b51-9321-ccc84ab49979/dbsnp_138.hg19.vcf because no suitable codecs found; 	at org.broadinstitute.hellbender.engine.FeatureManager.getCodecForFile(FeatureManager.java:462); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getCodecForFeatureInput(FeatureDataSource.java:320); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:300); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:256); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:230); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:214); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.openFea,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:4767,Availability,ERROR,ERROR,4767,(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 18/10/17 19:23:59 ERROR Executor: Exception in task 518.0 in stage 0.0 (TID 518); java.io.FileNotFoundException: /home/data/WGS/F002/F002.sort.bam (Too many open files); 	at java.io.FileInputStream.open0(Native Method); 	at java.io.FileInputStream.open(FileInputStream.java:195); 	at java.io.FileInputStream.<init>(FileInputStream.java:138); 	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.<init>(RawLocalFileSystem.java:106); 	at org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:202); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:349); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.seqdoop.hadoop_bam.util.WrapSeekable.openPath(WrapSeekable.java:60); 	at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:147); 	at org.seqdoop.hadoop_bam.BAMInputFormat.createRecordReader(BAMInputFormat.java:222); 	at org.seqdoop.hadoop_bam.AnySAMInputFormat.createRecordReader(AnySAMInputFormat.java:190); 	at org.apache.spa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:2743,Energy Efficiency,Reduce,ReduceOps,2743,eatureDataSource.java:320); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:300); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:256); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:230); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:214); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.openFeatureSource(JoinReadsWithVariants.java:63); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$null$0(JoinReadsWithVariants.java:44); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$join$60e5b476$1(JoinReadsWithVariants.java:44); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.compu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:2753,Energy Efficiency,Reduce,ReduceOp,2753,eatureDataSource.java:320); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:300); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:256); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:230); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:214); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.openFeatureSource(JoinReadsWithVariants.java:63); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$null$0(JoinReadsWithVariants.java:44); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$join$60e5b476$1(JoinReadsWithVariants.java:44); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.compu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:2781,Energy Efficiency,Reduce,ReduceOps,2781,0); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:300); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:256); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:230); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:214); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.openFeatureSource(JoinReadsWithVariants.java:63); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$null$0(JoinReadsWithVariants.java:44); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$join$60e5b476$1(JoinReadsWithVariants.java:44); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:4265,Energy Efficiency,schedul,scheduler,4265,rk.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 18/10/17 19:23:59 ERROR Executor: Exception in task 518.0 in stage 0.0 (TID 518); java.io.FileNotFoundException: /home/data/WGS/F002/F002.sort.bam (Too many open files); 	at java.io.FileInputStream.open0(Native Method); 	at java.io.FileInputStream.open(FileInputStream.java:195); 	at java.io.FileInputStream.<init>(FileInputStream.java:138); 	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.<init>(RawLocalFileSystem.java:106); 	at org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:20,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:4345,Energy Efficiency,schedul,scheduler,4345,org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 18/10/17 19:23:59 ERROR Executor: Exception in task 518.0 in stage 0.0 (TID 518); java.io.FileNotFoundException: /home/data/WGS/F002/F002.sort.bam (Too many open files); 	at java.io.FileInputStream.open0(Native Method); 	at java.io.FileInputStream.open(FileInputStream.java:195); 	at java.io.FileInputStream.<init>(FileInputStream.java:138); 	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.<init>(RawLocalFileSystem.java:106); 	at org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:202); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:349,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:4425,Energy Efficiency,schedul,scheduler,4425,ala:800); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 18/10/17 19:23:59 ERROR Executor: Exception in task 518.0 in stage 0.0 (TID 518); java.io.FileNotFoundException: /home/data/WGS/F002/F002.sort.bam (Too many open files); 	at java.io.FileInputStream.open0(Native Method); 	at java.io.FileInputStream.open(FileInputStream.java:195); 	at java.io.FileInputStream.<init>(FileInputStream.java:138); 	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.<init>(RawLocalFileSystem.java:106); 	at org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:202); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:349); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.seqdoo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:7609,Energy Efficiency,schedul,scheduler,7609,"titionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 18/10/17 19:23:59 WARN TaskSetManager: Lost task 517.0 in stage 0.0 (TID 517, localhost, executor driver): org.broadinstitute.hellbender.exceptions.UserException$NoSuitableCodecs: Cannot read /dev/shm/gatktmp/spark-30e238e4-b1b7-41f9-b31e-844f16879051/userFiles-4621c82d-5f86-4b51-9321-ccc84ab49979/dbsnp_138.hg19.vcf because no suitable codecs found; 	at org.broadinstitute.hellbender.engine.FeatureManager.getCodecForFile(FeatureManager.java:462); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getCodecForF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:7689,Energy Efficiency,schedul,scheduler,7689,"uteOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 18/10/17 19:23:59 WARN TaskSetManager: Lost task 517.0 in stage 0.0 (TID 517, localhost, executor driver): org.broadinstitute.hellbender.exceptions.UserException$NoSuitableCodecs: Cannot read /dev/shm/gatktmp/spark-30e238e4-b1b7-41f9-b31e-844f16879051/userFiles-4621c82d-5f86-4b51-9321-ccc84ab49979/dbsnp_138.hg19.vcf because no suitable codecs found; 	at org.broadinstitute.hellbender.engine.FeatureManager.getCodecForFile(FeatureManager.java:462); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getCodecForFeatureInput(FeatureDataSource.java:320); 	at org.broadinstitute.hellbender.engin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:7769,Energy Efficiency,schedul,scheduler,7769,"ala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 18/10/17 19:23:59 WARN TaskSetManager: Lost task 517.0 in stage 0.0 (TID 517, localhost, executor driver): org.broadinstitute.hellbender.exceptions.UserException$NoSuitableCodecs: Cannot read /dev/shm/gatktmp/spark-30e238e4-b1b7-41f9-b31e-844f16879051/userFiles-4621c82d-5f86-4b51-9321-ccc84ab49979/dbsnp_138.hg19.vcf because no suitable codecs found; 	at org.broadinstitute.hellbender.engine.FeatureManager.getCodecForFile(FeatureManager.java:462); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getCodecForFeatureInput(FeatureDataSource.java:320); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:300); 	at org.broadi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:9618,Energy Efficiency,Reduce,ReduceOps,9618,eatureDataSource.java:320); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:300); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:256); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:230); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:214); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.openFeatureSource(JoinReadsWithVariants.java:63); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$null$0(JoinReadsWithVariants.java:44); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$join$60e5b476$1(JoinReadsWithVariants.java:44); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.compu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:9628,Energy Efficiency,Reduce,ReduceOp,9628,eatureDataSource.java:320); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:300); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:256); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:230); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:214); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.openFeatureSource(JoinReadsWithVariants.java:63); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$null$0(JoinReadsWithVariants.java:44); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$join$60e5b476$1(JoinReadsWithVariants.java:44); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.compu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:9656,Energy Efficiency,Reduce,ReduceOps,9656,0); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:300); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:256); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:230); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:214); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.openFeatureSource(JoinReadsWithVariants.java:63); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$null$0(JoinReadsWithVariants.java:44); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$join$60e5b476$1(JoinReadsWithVariants.java:44); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:11140,Energy Efficiency,schedul,scheduler,11140,ceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$join$60e5b476$1(JoinReadsWithVariants.java:44); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:11220,Energy Efficiency,schedul,scheduler,11220,ceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$join$60e5b476$1(JoinReadsWithVariants.java:44); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:11300,Energy Efficiency,schedul,scheduler,11300,ceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$join$60e5b476$1(JoinReadsWithVariants.java:44); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:2678,Integrability,wrap,wrapAndCopyInto,2678,lbender.engine.FeatureDataSource.getCodecForFeatureInput(FeatureDataSource.java:320); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:300); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:256); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:230); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:214); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.openFeatureSource(JoinReadsWithVariants.java:63); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$null$0(JoinReadsWithVariants.java:44); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$join$60e5b476$1(JoinReadsWithVariants.java:44); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapP,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:5448,Integrability,Wrap,WrapSeekable,5448,:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 18/10/17 19:23:59 ERROR Executor: Exception in task 518.0 in stage 0.0 (TID 518); java.io.FileNotFoundException: /home/data/WGS/F002/F002.sort.bam (Too many open files); 	at java.io.FileInputStream.open0(Native Method); 	at java.io.FileInputStream.open(FileInputStream.java:195); 	at java.io.FileInputStream.<init>(FileInputStream.java:138); 	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.<init>(RawLocalFileSystem.java:106); 	at org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:202); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:349); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.seqdoop.hadoop_bam.util.WrapSeekable.openPath(WrapSeekable.java:60); 	at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:147); 	at org.seqdoop.hadoop_bam.BAMInputFormat.createRecordReader(BAMInputFormat.java:222); 	at org.seqdoop.hadoop_bam.AnySAMInputFormat.createRecordReader(AnySAMInputFormat.java:190); 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.liftedTree1$1(NewHadoopRDD.scala:187); 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:186); 	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:141); 	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:70); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:5470,Integrability,Wrap,WrapSeekable,5470,che.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 18/10/17 19:23:59 ERROR Executor: Exception in task 518.0 in stage 0.0 (TID 518); java.io.FileNotFoundException: /home/data/WGS/F002/F002.sort.bam (Too many open files); 	at java.io.FileInputStream.open0(Native Method); 	at java.io.FileInputStream.open(FileInputStream.java:195); 	at java.io.FileInputStream.<init>(FileInputStream.java:138); 	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.<init>(RawLocalFileSystem.java:106); 	at org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:202); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:349); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.seqdoop.hadoop_bam.util.WrapSeekable.openPath(WrapSeekable.java:60); 	at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:147); 	at org.seqdoop.hadoop_bam.BAMInputFormat.createRecordReader(BAMInputFormat.java:222); 	at org.seqdoop.hadoop_bam.AnySAMInputFormat.createRecordReader(AnySAMInputFormat.java:190); 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.liftedTree1$1(NewHadoopRDD.scala:187); 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:186); 	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:141); 	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:70); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apac,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:9553,Integrability,wrap,wrapAndCopyInto,9553,lbender.engine.FeatureDataSource.getCodecForFeatureInput(FeatureDataSource.java:320); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:300); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:256); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:230); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:214); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.openFeatureSource(JoinReadsWithVariants.java:63); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$null$0(JoinReadsWithVariants.java:44); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$join$60e5b476$1(JoinReadsWithVariants.java:44); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapP,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:4550,Performance,concurren,concurrent,4550,rReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 18/10/17 19:23:59 ERROR Executor: Exception in task 518.0 in stage 0.0 (TID 518); java.io.FileNotFoundException: /home/data/WGS/F002/F002.sort.bam (Too many open files); 	at java.io.FileInputStream.open0(Native Method); 	at java.io.FileInputStream.open(FileInputStream.java:195); 	at java.io.FileInputStream.<init>(FileInputStream.java:138); 	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.<init>(RawLocalFileSystem.java:106); 	at org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:202); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:349); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.seqdoop.hadoop_bam.util.WrapSeekable.openPath(WrapSeekable.java:60); 	at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecord,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:4635,Performance,concurren,concurrent,4635, 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 18/10/17 19:23:59 ERROR Executor: Exception in task 518.0 in stage 0.0 (TID 518); java.io.FileNotFoundException: /home/data/WGS/F002/F002.sort.bam (Too many open files); 	at java.io.FileInputStream.open0(Native Method); 	at java.io.FileInputStream.open(FileInputStream.java:195); 	at java.io.FileInputStream.<init>(FileInputStream.java:138); 	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.<init>(RawLocalFileSystem.java:106); 	at org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:202); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:349); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.seqdoop.hadoop_bam.util.WrapSeekable.openPath(WrapSeekable.java:60); 	at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:147); 	at org.seqdoop.hadoop_bam.BAMInputFormat.createRecordReader(BAMInp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:7894,Performance,concurren,concurrent,7894,"rReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 18/10/17 19:23:59 WARN TaskSetManager: Lost task 517.0 in stage 0.0 (TID 517, localhost, executor driver): org.broadinstitute.hellbender.exceptions.UserException$NoSuitableCodecs: Cannot read /dev/shm/gatktmp/spark-30e238e4-b1b7-41f9-b31e-844f16879051/userFiles-4621c82d-5f86-4b51-9321-ccc84ab49979/dbsnp_138.hg19.vcf because no suitable codecs found; 	at org.broadinstitute.hellbender.engine.FeatureManager.getCodecForFile(FeatureManager.java:462); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getCodecForFeatureInput(FeatureDataSource.java:320); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:300); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:256); 	at org.broadinstitute.hellbender.engine.Feat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:7979,Performance,concurren,concurrent,7979," 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 18/10/17 19:23:59 WARN TaskSetManager: Lost task 517.0 in stage 0.0 (TID 517, localhost, executor driver): org.broadinstitute.hellbender.exceptions.UserException$NoSuitableCodecs: Cannot read /dev/shm/gatktmp/spark-30e238e4-b1b7-41f9-b31e-844f16879051/userFiles-4621c82d-5f86-4b51-9321-ccc84ab49979/dbsnp_138.hg19.vcf because no suitable codecs found; 	at org.broadinstitute.hellbender.engine.FeatureManager.getCodecForFile(FeatureManager.java:462); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getCodecForFeatureInput(FeatureDataSource.java:320); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:300); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:256); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:230); 	at org.broadinstitute.hellbender.e",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:11425,Performance,concurren,concurrent,11425,ceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$join$60e5b476$1(JoinReadsWithVariants.java:44); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:11510,Performance,concurren,concurrent,11510,ceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$join$60e5b476$1(JoinReadsWithVariants.java:44); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:5299,Security,Checksum,ChecksumFileSystem,5299,Task.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 18/10/17 19:23:59 ERROR Executor: Exception in task 518.0 in stage 0.0 (TID 518); java.io.FileNotFoundException: /home/data/WGS/F002/F002.sort.bam (Too many open files); 	at java.io.FileInputStream.open0(Native Method); 	at java.io.FileInputStream.open(FileInputStream.java:195); 	at java.io.FileInputStream.<init>(FileInputStream.java:138); 	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.<init>(RawLocalFileSystem.java:106); 	at org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:202); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:349); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.seqdoop.hadoop_bam.util.WrapSeekable.openPath(WrapSeekable.java:60); 	at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:147); 	at org.seqdoop.hadoop_bam.BAMInputFormat.createRecordReader(BAMInputFormat.java:222); 	at org.seqdoop.hadoop_bam.AnySAMInputFormat.createRecordReader(AnySAMInputFormat.java:190); 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.liftedTree1$1(NewHadoopRDD.scala:187); 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:186); 	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:141); 	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:70); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:5323,Security,Checksum,ChecksumFileSystem,5323,g.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 18/10/17 19:23:59 ERROR Executor: Exception in task 518.0 in stage 0.0 (TID 518); java.io.FileNotFoundException: /home/data/WGS/F002/F002.sort.bam (Too many open files); 	at java.io.FileInputStream.open0(Native Method); 	at java.io.FileInputStream.open(FileInputStream.java:195); 	at java.io.FileInputStream.<init>(FileInputStream.java:138); 	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.<init>(RawLocalFileSystem.java:106); 	at org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:202); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:349); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.seqdoop.hadoop_bam.util.WrapSeekable.openPath(WrapSeekable.java:60); 	at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:147); 	at org.seqdoop.hadoop_bam.BAMInputFormat.createRecordReader(BAMInputFormat.java:222); 	at org.seqdoop.hadoop_bam.AnySAMInputFormat.createRecordReader(AnySAMInputFormat.java:190); 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.liftedTree1$1(NewHadoopRDD.scala:187); 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:186); 	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:141); 	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:70); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/issues/5316:664,Testability,log,log,664,I got exception use run BaseRecalibratorSpark:. > java.io.FileNotFoundException: /home/data/WGS/F002/F002.sort.bam (Too many open files). My version is 4393c86 (after 4.0.10.1).; I have set `ulimit -n 4096` but still got this exception. My command:; `./gatk BaseRecalibratorSpark --tmp-dir /dev/shm/gatktmp/ -I /home/data/WGS/F002/F002.sort.bam -O 1.grp --known-sites /home/data/ref/dbsnp_138.hg19.vcf --known-sites /home/data/ref/1000G_phase1.indels.hg19.sites.vcf --known-sites /home/data/ref/Mills_and_1000G_gold_standard.indels.hg19.sites.vcf -R /home/data/ref/ucsc.hg19.fasta -- --spark-runner SPARK --spark-master local[8] --driver-memory 100G`. Here is the log:. > 19:23:59.384 INFO FeatureManager - Using codec VCFCodec to read file file:///dev/shm/gatktmp/spark-30e238e4-b1b7-41f9-b31e-844f16879051/userFiles-4621c82d-5f86-4b51-9321-ccc84ab49979/Mills_and_1000G_gold_standard.indels.hg19.sites.vcf; 19:23:59.411 INFO BaseRecalibrationEngine - The covariates being used here: ; 19:23:59.411 INFO BaseRecalibrationEngine - 	ReadGroupCovariate; 19:23:59.412 INFO BaseRecalibrationEngine - 	QualityScoreCovariate; 19:23:59.412 INFO BaseRecalibrationEngine - 	ContextCovariate; 19:23:59.412 INFO BaseRecalibrationEngine - 	CycleCovariate; 18/10/17 19:23:59 ERROR Executor: Exception in task 517.0 in stage 0.0 (TID 517); org.broadinstitute.hellbender.exceptions.UserException$NoSuitableCodecs: Cannot read /dev/shm/gatktmp/spark-30e238e4-b1b7-41f9-b31e-844f16879051/userFiles-4621c82d-5f86-4b51-9321-ccc84ab49979/dbsnp_138.hg19.vcf because no suitable codecs found; 	at org.broadinstitute.hellbender.engine.FeatureManager.getCodecForFile(FeatureManager.java:462); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getCodecForFeatureInput(FeatureDataSource.java:320); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:300); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:256); 	at org.broadinstitut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316
https://github.com/broadinstitute/gatk/pull/5318:18,Testability,test,tests,18,"I am running unit tests locally, but as you know, they take a long time, so I am starting the PR process anyhow. Background: see #5314",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5318
https://github.com/broadinstitute/gatk/issues/5323:87,Deployability,pipeline,pipeline,87,"Hi,; in the last months for my Master thesis project I've studied your tool, with this pipeline:. ![ngs_pipeline_gatk](https://user-images.githubusercontent.com/10074137/47147968-ae5a0b00-d2cf-11e8-9fd6-15cd23fbcdcf.png); in spark version, yes, I know is still in beta but Ive found these problems when I compared the outputs from Haplotypecaller in spark and in not Spark versions. For comparing these results I've used this tool [https://drive.google.com/file/d/1r2WHyiz5WqOIyY_EZ1VZt92wGlL19SE4/view?usp=sharing](url) and I've obtained these plots for sensitivity and specificity( The sensitivity is defined as the number of sites inwhich both sequencing and microarrays detected a deviation from the reference sequencedivided by the number of sites where a variant was detected by using the microarrays). **Spark**; Sensitivity; ![spark_sensitivity_hg19](https://user-images.githubusercontent.com/10074137/47148261-86b77280-d2d0-11e8-8b5a-9ecfef16d889.png); Specificity; ![sparkspecificityhg19](https://user-images.githubusercontent.com/10074137/47148277-933bcb00-d2d0-11e8-97eb-1adceb4e5ee2.png). **Local non Spark tool with GATK 2.7**; ![hg19local](https://user-images.githubusercontent.com/10074137/47148427-fcbbd980-d2d0-11e8-87d8-04ec20c1005d.png); furthermore I've executed the pipeline until BQSR in Spark version and after, I am focused just on Haplotypecaller because I've used this ""backwards"" approach and I've discovered that the pipeline is deterministic from the phase Variant Discovery, but don't in the phase of Preprocessing because when I've executed this phase more times, I've obtained results completely, this is the test with one single sample:; ![comparisons_pfc32](https://user-images.githubusercontent.com/10074137/47148552-49071980-d2d1-11e8-8b1c-aec468285699.png); furthermore when I've used the output from BQSR (executed in Spark) for execute of Haplotypecaller in local(not in Spark) and adapting this output for Haplotypecaller, I had to use the tool Samtools for s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5323
https://github.com/broadinstitute/gatk/issues/5323:1289,Deployability,pipeline,pipeline,1289," yes, I know is still in beta but Ive found these problems when I compared the outputs from Haplotypecaller in spark and in not Spark versions. For comparing these results I've used this tool [https://drive.google.com/file/d/1r2WHyiz5WqOIyY_EZ1VZt92wGlL19SE4/view?usp=sharing](url) and I've obtained these plots for sensitivity and specificity( The sensitivity is defined as the number of sites inwhich both sequencing and microarrays detected a deviation from the reference sequencedivided by the number of sites where a variant was detected by using the microarrays). **Spark**; Sensitivity; ![spark_sensitivity_hg19](https://user-images.githubusercontent.com/10074137/47148261-86b77280-d2d0-11e8-8b5a-9ecfef16d889.png); Specificity; ![sparkspecificityhg19](https://user-images.githubusercontent.com/10074137/47148277-933bcb00-d2d0-11e8-97eb-1adceb4e5ee2.png). **Local non Spark tool with GATK 2.7**; ![hg19local](https://user-images.githubusercontent.com/10074137/47148427-fcbbd980-d2d0-11e8-87d8-04ec20c1005d.png); furthermore I've executed the pipeline until BQSR in Spark version and after, I am focused just on Haplotypecaller because I've used this ""backwards"" approach and I've discovered that the pipeline is deterministic from the phase Variant Discovery, but don't in the phase of Preprocessing because when I've executed this phase more times, I've obtained results completely, this is the test with one single sample:; ![comparisons_pfc32](https://user-images.githubusercontent.com/10074137/47148552-49071980-d2d1-11e8-8b1c-aec468285699.png); furthermore when I've used the output from BQSR (executed in Spark) for execute of Haplotypecaller in local(not in Spark) and adapting this output for Haplotypecaller, I had to use the tool Samtools for sort the outputs and after this step the outputs are passed from average of 19 gigabytes to 13 gigabytes average for the all samples. I've opened this Issue because I would to help you with my experiments to improvement your tool.; thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5323
https://github.com/broadinstitute/gatk/issues/5323:1447,Deployability,pipeline,pipeline,1447," yes, I know is still in beta but Ive found these problems when I compared the outputs from Haplotypecaller in spark and in not Spark versions. For comparing these results I've used this tool [https://drive.google.com/file/d/1r2WHyiz5WqOIyY_EZ1VZt92wGlL19SE4/view?usp=sharing](url) and I've obtained these plots for sensitivity and specificity( The sensitivity is defined as the number of sites inwhich both sequencing and microarrays detected a deviation from the reference sequencedivided by the number of sites where a variant was detected by using the microarrays). **Spark**; Sensitivity; ![spark_sensitivity_hg19](https://user-images.githubusercontent.com/10074137/47148261-86b77280-d2d0-11e8-8b5a-9ecfef16d889.png); Specificity; ![sparkspecificityhg19](https://user-images.githubusercontent.com/10074137/47148277-933bcb00-d2d0-11e8-97eb-1adceb4e5ee2.png). **Local non Spark tool with GATK 2.7**; ![hg19local](https://user-images.githubusercontent.com/10074137/47148427-fcbbd980-d2d0-11e8-87d8-04ec20c1005d.png); furthermore I've executed the pipeline until BQSR in Spark version and after, I am focused just on Haplotypecaller because I've used this ""backwards"" approach and I've discovered that the pipeline is deterministic from the phase Variant Discovery, but don't in the phase of Preprocessing because when I've executed this phase more times, I've obtained results completely, this is the test with one single sample:; ![comparisons_pfc32](https://user-images.githubusercontent.com/10074137/47148552-49071980-d2d1-11e8-8b1c-aec468285699.png); furthermore when I've used the output from BQSR (executed in Spark) for execute of Haplotypecaller in local(not in Spark) and adapting this output for Haplotypecaller, I had to use the tool Samtools for sort the outputs and after this step the outputs are passed from average of 19 gigabytes to 13 gigabytes average for the all samples. I've opened this Issue because I would to help you with my experiments to improvement your tool.; thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5323
https://github.com/broadinstitute/gatk/issues/5323:1923,Energy Efficiency,adapt,adapting,1923," yes, I know is still in beta but Ive found these problems when I compared the outputs from Haplotypecaller in spark and in not Spark versions. For comparing these results I've used this tool [https://drive.google.com/file/d/1r2WHyiz5WqOIyY_EZ1VZt92wGlL19SE4/view?usp=sharing](url) and I've obtained these plots for sensitivity and specificity( The sensitivity is defined as the number of sites inwhich both sequencing and microarrays detected a deviation from the reference sequencedivided by the number of sites where a variant was detected by using the microarrays). **Spark**; Sensitivity; ![spark_sensitivity_hg19](https://user-images.githubusercontent.com/10074137/47148261-86b77280-d2d0-11e8-8b5a-9ecfef16d889.png); Specificity; ![sparkspecificityhg19](https://user-images.githubusercontent.com/10074137/47148277-933bcb00-d2d0-11e8-97eb-1adceb4e5ee2.png). **Local non Spark tool with GATK 2.7**; ![hg19local](https://user-images.githubusercontent.com/10074137/47148427-fcbbd980-d2d0-11e8-87d8-04ec20c1005d.png); furthermore I've executed the pipeline until BQSR in Spark version and after, I am focused just on Haplotypecaller because I've used this ""backwards"" approach and I've discovered that the pipeline is deterministic from the phase Variant Discovery, but don't in the phase of Preprocessing because when I've executed this phase more times, I've obtained results completely, this is the test with one single sample:; ![comparisons_pfc32](https://user-images.githubusercontent.com/10074137/47148552-49071980-d2d1-11e8-8b1c-aec468285699.png); furthermore when I've used the output from BQSR (executed in Spark) for execute of Haplotypecaller in local(not in Spark) and adapting this output for Haplotypecaller, I had to use the tool Samtools for sort the outputs and after this step the outputs are passed from average of 19 gigabytes to 13 gigabytes average for the all samples. I've opened this Issue because I would to help you with my experiments to improvement your tool.; thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5323
https://github.com/broadinstitute/gatk/issues/5323:1923,Modifiability,adapt,adapting,1923," yes, I know is still in beta but Ive found these problems when I compared the outputs from Haplotypecaller in spark and in not Spark versions. For comparing these results I've used this tool [https://drive.google.com/file/d/1r2WHyiz5WqOIyY_EZ1VZt92wGlL19SE4/view?usp=sharing](url) and I've obtained these plots for sensitivity and specificity( The sensitivity is defined as the number of sites inwhich both sequencing and microarrays detected a deviation from the reference sequencedivided by the number of sites where a variant was detected by using the microarrays). **Spark**; Sensitivity; ![spark_sensitivity_hg19](https://user-images.githubusercontent.com/10074137/47148261-86b77280-d2d0-11e8-8b5a-9ecfef16d889.png); Specificity; ![sparkspecificityhg19](https://user-images.githubusercontent.com/10074137/47148277-933bcb00-d2d0-11e8-97eb-1adceb4e5ee2.png). **Local non Spark tool with GATK 2.7**; ![hg19local](https://user-images.githubusercontent.com/10074137/47148427-fcbbd980-d2d0-11e8-87d8-04ec20c1005d.png); furthermore I've executed the pipeline until BQSR in Spark version and after, I am focused just on Haplotypecaller because I've used this ""backwards"" approach and I've discovered that the pipeline is deterministic from the phase Variant Discovery, but don't in the phase of Preprocessing because when I've executed this phase more times, I've obtained results completely, this is the test with one single sample:; ![comparisons_pfc32](https://user-images.githubusercontent.com/10074137/47148552-49071980-d2d1-11e8-8b1c-aec468285699.png); furthermore when I've used the output from BQSR (executed in Spark) for execute of Haplotypecaller in local(not in Spark) and adapting this output for Haplotypecaller, I had to use the tool Samtools for sort the outputs and after this step the outputs are passed from average of 19 gigabytes to 13 gigabytes average for the all samples. I've opened this Issue because I would to help you with my experiments to improvement your tool.; thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5323
https://github.com/broadinstitute/gatk/issues/5323:675,Safety,detect,detected,675,"Hi,; in the last months for my Master thesis project I've studied your tool, with this pipeline:. ![ngs_pipeline_gatk](https://user-images.githubusercontent.com/10074137/47147968-ae5a0b00-d2cf-11e8-9fd6-15cd23fbcdcf.png); in spark version, yes, I know is still in beta but Ive found these problems when I compared the outputs from Haplotypecaller in spark and in not Spark versions. For comparing these results I've used this tool [https://drive.google.com/file/d/1r2WHyiz5WqOIyY_EZ1VZt92wGlL19SE4/view?usp=sharing](url) and I've obtained these plots for sensitivity and specificity( The sensitivity is defined as the number of sites inwhich both sequencing and microarrays detected a deviation from the reference sequencedivided by the number of sites where a variant was detected by using the microarrays). **Spark**; Sensitivity; ![spark_sensitivity_hg19](https://user-images.githubusercontent.com/10074137/47148261-86b77280-d2d0-11e8-8b5a-9ecfef16d889.png); Specificity; ![sparkspecificityhg19](https://user-images.githubusercontent.com/10074137/47148277-933bcb00-d2d0-11e8-97eb-1adceb4e5ee2.png). **Local non Spark tool with GATK 2.7**; ![hg19local](https://user-images.githubusercontent.com/10074137/47148427-fcbbd980-d2d0-11e8-87d8-04ec20c1005d.png); furthermore I've executed the pipeline until BQSR in Spark version and after, I am focused just on Haplotypecaller because I've used this ""backwards"" approach and I've discovered that the pipeline is deterministic from the phase Variant Discovery, but don't in the phase of Preprocessing because when I've executed this phase more times, I've obtained results completely, this is the test with one single sample:; ![comparisons_pfc32](https://user-images.githubusercontent.com/10074137/47148552-49071980-d2d1-11e8-8b1c-aec468285699.png); furthermore when I've used the output from BQSR (executed in Spark) for execute of Haplotypecaller in local(not in Spark) and adapting this output for Haplotypecaller, I had to use the tool Samtools for s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5323
https://github.com/broadinstitute/gatk/issues/5323:774,Safety,detect,detected,774,"Hi,; in the last months for my Master thesis project I've studied your tool, with this pipeline:. ![ngs_pipeline_gatk](https://user-images.githubusercontent.com/10074137/47147968-ae5a0b00-d2cf-11e8-9fd6-15cd23fbcdcf.png); in spark version, yes, I know is still in beta but Ive found these problems when I compared the outputs from Haplotypecaller in spark and in not Spark versions. For comparing these results I've used this tool [https://drive.google.com/file/d/1r2WHyiz5WqOIyY_EZ1VZt92wGlL19SE4/view?usp=sharing](url) and I've obtained these plots for sensitivity and specificity( The sensitivity is defined as the number of sites inwhich both sequencing and microarrays detected a deviation from the reference sequencedivided by the number of sites where a variant was detected by using the microarrays). **Spark**; Sensitivity; ![spark_sensitivity_hg19](https://user-images.githubusercontent.com/10074137/47148261-86b77280-d2d0-11e8-8b5a-9ecfef16d889.png); Specificity; ![sparkspecificityhg19](https://user-images.githubusercontent.com/10074137/47148277-933bcb00-d2d0-11e8-97eb-1adceb4e5ee2.png). **Local non Spark tool with GATK 2.7**; ![hg19local](https://user-images.githubusercontent.com/10074137/47148427-fcbbd980-d2d0-11e8-87d8-04ec20c1005d.png); furthermore I've executed the pipeline until BQSR in Spark version and after, I am focused just on Haplotypecaller because I've used this ""backwards"" approach and I've discovered that the pipeline is deterministic from the phase Variant Discovery, but don't in the phase of Preprocessing because when I've executed this phase more times, I've obtained results completely, this is the test with one single sample:; ![comparisons_pfc32](https://user-images.githubusercontent.com/10074137/47148552-49071980-d2d1-11e8-8b1c-aec468285699.png); furthermore when I've used the output from BQSR (executed in Spark) for execute of Haplotypecaller in local(not in Spark) and adapting this output for Haplotypecaller, I had to use the tool Samtools for s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5323
https://github.com/broadinstitute/gatk/issues/5323:1643,Testability,test,test,1643," yes, I know is still in beta but Ive found these problems when I compared the outputs from Haplotypecaller in spark and in not Spark versions. For comparing these results I've used this tool [https://drive.google.com/file/d/1r2WHyiz5WqOIyY_EZ1VZt92wGlL19SE4/view?usp=sharing](url) and I've obtained these plots for sensitivity and specificity( The sensitivity is defined as the number of sites inwhich both sequencing and microarrays detected a deviation from the reference sequencedivided by the number of sites where a variant was detected by using the microarrays). **Spark**; Sensitivity; ![spark_sensitivity_hg19](https://user-images.githubusercontent.com/10074137/47148261-86b77280-d2d0-11e8-8b5a-9ecfef16d889.png); Specificity; ![sparkspecificityhg19](https://user-images.githubusercontent.com/10074137/47148277-933bcb00-d2d0-11e8-97eb-1adceb4e5ee2.png). **Local non Spark tool with GATK 2.7**; ![hg19local](https://user-images.githubusercontent.com/10074137/47148427-fcbbd980-d2d0-11e8-87d8-04ec20c1005d.png); furthermore I've executed the pipeline until BQSR in Spark version and after, I am focused just on Haplotypecaller because I've used this ""backwards"" approach and I've discovered that the pipeline is deterministic from the phase Variant Discovery, but don't in the phase of Preprocessing because when I've executed this phase more times, I've obtained results completely, this is the test with one single sample:; ![comparisons_pfc32](https://user-images.githubusercontent.com/10074137/47148552-49071980-d2d1-11e8-8b1c-aec468285699.png); furthermore when I've used the output from BQSR (executed in Spark) for execute of Haplotypecaller in local(not in Spark) and adapting this output for Haplotypecaller, I had to use the tool Samtools for sort the outputs and after this step the outputs are passed from average of 19 gigabytes to 13 gigabytes average for the all samples. I've opened this Issue because I would to help you with my experiments to improvement your tool.; thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5323
https://github.com/broadinstitute/gatk/pull/5327:137,Deployability,Update,Updated,137,Implemented VCF ID for VCF data sources. - Now VCF data sources create an ID field for the ID of the variant; used for the annotation. - Updated the regression test suite with a VCF data source to increase; test coverage. Fixes #5186,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5327
https://github.com/broadinstitute/gatk/pull/5327:160,Testability,test,test,160,Implemented VCF ID for VCF data sources. - Now VCF data sources create an ID field for the ID of the variant; used for the annotation. - Updated the regression test suite with a VCF data source to increase; test coverage. Fixes #5186,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5327
https://github.com/broadinstitute/gatk/pull/5327:207,Testability,test,test,207,Implemented VCF ID for VCF data sources. - Now VCF data sources create an ID field for the ID of the variant; used for the annotation. - Updated the regression test suite with a VCF data source to increase; test coverage. Fixes #5186,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5327
https://github.com/broadinstitute/gatk/pull/5329:116,Testability,log,logging,116,I have been burned one too many times by the fact that this count silently filters reads for you. Since there is no logging output printing the number of filtered reads in spark it makes more sense to remove filtering from the tool altogether.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5329
https://github.com/broadinstitute/gatk/issues/5333:160,Deployability,release,release,160,"## Bug Report. ### Affected tool(s) or class(es); _Funcotator_, _GencodeFuncotationFactory::createUtrFuncotation_. ### Affected version(s); - [x] Latest public release version; - [x] Latest master branch as of [20181019]. ### Description ; When determining whether a variant in a 5' UTR is a `DE_NOVO_START_IN_FRAME` or `DE_NOVO_START_OUT_FRAME`, Funcotator only checks whether the new start codon is in frame with the end of the current UTR (the UTR in which the variant occurs). Funcotator should account for the case that there are multiple 5' UTRs. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; All UTRs should be considered for whether or not the new start codon is in frame. #### Actual behavior; Only the UTR in which the variant occurs is considered for whether the new start codon is in frame.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5333
https://github.com/broadinstitute/gatk/issues/5336:5091,Availability,error,error,5091,"t org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Based on the discussion around #4963 and the [test VCF](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/testGenotypeGivenAllelesMode_givenAlleles.vcf), I gather that this is intended to work without error. I was trying to figure out how these cases differed from the spanning deletion in the aforementioned test VCF. One thing I noticed was that these two problematic cases have the SNP at the very last base of the spanning deletion. I'm just speculating here, but maybe it is related to an off-by-one bug of some sort? . I am testing with v. 4.0.9.0.; I also tried with v. 4.0.5.1 which does not crash, but rather prints the warnings discussed in #4963:; `00:02:10.995 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16137302-16137302, only considering the first record`; `00:03:08.220 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16464051-16464051, only considering the first record`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5336
https://github.com/broadinstitute/gatk/issues/5336:5619,Safety,detect,detected,5619,"t org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Based on the discussion around #4963 and the [test VCF](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/testGenotypeGivenAllelesMode_givenAlleles.vcf), I gather that this is intended to work without error. I was trying to figure out how these cases differed from the spanning deletion in the aforementioned test VCF. One thing I noticed was that these two problematic cases have the SNP at the very last base of the spanning deletion. I'm just speculating here, but maybe it is related to an off-by-one bug of some sort? . I am testing with v. 4.0.9.0.; I also tried with v. 4.0.5.1 which does not crash, but rather prints the warnings discussed in #4963:; `00:02:10.995 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16137302-16137302, only considering the first record`; `00:03:08.220 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16464051-16464051, only considering the first record`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5336
https://github.com/broadinstitute/gatk/issues/5336:5790,Safety,detect,detected,5790,"t org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Based on the discussion around #4963 and the [test VCF](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/testGenotypeGivenAllelesMode_givenAlleles.vcf), I gather that this is intended to work without error. I was trying to figure out how these cases differed from the spanning deletion in the aforementioned test VCF. One thing I noticed was that these two problematic cases have the SNP at the very last base of the spanning deletion. I'm just speculating here, but maybe it is related to an off-by-one bug of some sort? . I am testing with v. 4.0.9.0.; I also tried with v. 4.0.5.1 which does not crash, but rather prints the warnings discussed in #4963:; `00:02:10.995 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16137302-16137302, only considering the first record`; `00:03:08.220 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16464051-16464051, only considering the first record`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5336
https://github.com/broadinstitute/gatk/issues/5336:487,Security,validat,validateGenotypes,487,"Using GENOTYPE_GIVEN_ALLELES (""GGA"") mode with HaplotypeCaller, I've encountered a couple instances of crashes that I've traced to spanning deletions (of the type considered in #4963).; One case involved the following in the `--alleles` input:; ```; 22	16137300	rs567136176	TAG	T; 22	16137302	rs573978809	G	C; ```; and it crashed with:; ```; java.lang.IllegalStateException: Allele in genotype TAG* not in the variant context [G*, *, C]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.makeMergedVariantContext(AssemblyBasedCallerUtils.java:221); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:150); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:599); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hell",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5336
https://github.com/broadinstitute/gatk/issues/5336:581,Security,validat,validate,581,"Using GENOTYPE_GIVEN_ALLELES (""GGA"") mode with HaplotypeCaller, I've encountered a couple instances of crashes that I've traced to spanning deletions (of the type considered in #4963).; One case involved the following in the `--alleles` input:; ```; 22	16137300	rs567136176	TAG	T; 22	16137302	rs573978809	G	C; ```; and it crashed with:; ```; java.lang.IllegalStateException: Allele in genotype TAG* not in the variant context [G*, *, C]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.makeMergedVariantContext(AssemblyBasedCallerUtils.java:221); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:150); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:599); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hell",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5336
https://github.com/broadinstitute/gatk/issues/5336:2804,Security,validat,validateGenotypes,2804,"lbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```; A second case involved `--alleles` input:; ```; 22	16464044	rs571268158	CCAGGTCT	C; 22	16464051	rs569099729	T	C; ```; and crashed similarly, with:; ```; java.lang.IllegalStateException: Allele in genotype CCAGGTCT* not in the variant context [T*, *, C]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.makeMergedVariantContext(AssemblyBasedCallerUtils.java:221); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:150); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5336
https://github.com/broadinstitute/gatk/issues/5336:2898,Security,validat,validate,2898,"institute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```; A second case involved `--alleles` input:; ```; 22	16464044	rs571268158	CCAGGTCT	C; 22	16464051	rs569099729	T	C; ```; and crashed similarly, with:; ```; java.lang.IllegalStateException: Allele in genotype CCAGGTCT* not in the variant context [T*, *, C]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.makeMergedVariantContext(AssemblyBasedCallerUtils.java:221); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:150); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:599); 	at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5336
https://github.com/broadinstitute/gatk/issues/5336:4864,Testability,test,test,4864,"(HaplotypeCallerEngine.java:599); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Based on the discussion around #4963 and the [test VCF](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/testGenotypeGivenAllelesMode_givenAlleles.vcf), I gather that this is intended to work without error. I was trying to figure out how these cases differed from the spanning deletion in the aforementioned test VCF. One thing I noticed was that these two problematic cases have the SNP at the very last base of the spanning deletion. I'm just speculating here, but maybe it is related to an off-by-one bug of some sort? . I am testing with v. 4.0.9.0.; I also tried with v. 4.0.5.1 which does not crash, but rather prints the warnings discussed in #4963:; `00:02:10.995 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16137302-16137302, only considering the first record`; `00:03:08.220 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16464051-16464051",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5336
https://github.com/broadinstitute/gatk/issues/5336:4929,Testability,test,test,4929,"t org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Based on the discussion around #4963 and the [test VCF](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/testGenotypeGivenAllelesMode_givenAlleles.vcf), I gather that this is intended to work without error. I was trying to figure out how these cases differed from the spanning deletion in the aforementioned test VCF. One thing I noticed was that these two problematic cases have the SNP at the very last base of the spanning deletion. I'm just speculating here, but maybe it is related to an off-by-one bug of some sort? . I am testing with v. 4.0.9.0.; I also tried with v. 4.0.5.1 which does not crash, but rather prints the warnings discussed in #4963:; `00:02:10.995 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16137302-16137302, only considering the first record`; `00:03:08.220 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16464051-16464051, only considering the first record`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5336
https://github.com/broadinstitute/gatk/issues/5336:5199,Testability,test,test,5199,"t org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Based on the discussion around #4963 and the [test VCF](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/testGenotypeGivenAllelesMode_givenAlleles.vcf), I gather that this is intended to work without error. I was trying to figure out how these cases differed from the spanning deletion in the aforementioned test VCF. One thing I noticed was that these two problematic cases have the SNP at the very last base of the spanning deletion. I'm just speculating here, but maybe it is related to an off-by-one bug of some sort? . I am testing with v. 4.0.9.0.; I also tried with v. 4.0.5.1 which does not crash, but rather prints the warnings discussed in #4963:; `00:02:10.995 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16137302-16137302, only considering the first record`; `00:03:08.220 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16464051-16464051, only considering the first record`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5336
https://github.com/broadinstitute/gatk/issues/5336:5420,Testability,test,testing,5420,"t org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Based on the discussion around #4963 and the [test VCF](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/testGenotypeGivenAllelesMode_givenAlleles.vcf), I gather that this is intended to work without error. I was trying to figure out how these cases differed from the spanning deletion in the aforementioned test VCF. One thing I noticed was that these two problematic cases have the SNP at the very last base of the spanning deletion. I'm just speculating here, but maybe it is related to an off-by-one bug of some sort? . I am testing with v. 4.0.9.0.; I also tried with v. 4.0.5.1 which does not crash, but rather prints the warnings discussed in #4963:; `00:02:10.995 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16137302-16137302, only considering the first record`; `00:03:08.220 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16464051-16464051, only considering the first record`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5336
https://github.com/broadinstitute/gatk/issues/5336:959,Usability,simpl,simpleMerge,959,"Using GENOTYPE_GIVEN_ALLELES (""GGA"") mode with HaplotypeCaller, I've encountered a couple instances of crashes that I've traced to spanning deletions (of the type considered in #4963).; One case involved the following in the `--alleles` input:; ```; 22	16137300	rs567136176	TAG	T; 22	16137302	rs573978809	G	C; ```; and it crashed with:; ```; java.lang.IllegalStateException: Allele in genotype TAG* not in the variant context [G*, *, C]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.makeMergedVariantContext(AssemblyBasedCallerUtils.java:221); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:150); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:599); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hell",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5336
https://github.com/broadinstitute/gatk/issues/5336:1078,Usability,simpl,simpleMerge,1078,"ces of crashes that I've traced to spanning deletions (of the type considered in #4963).; One case involved the following in the `--alleles` input:; ```; 22	16137300	rs567136176	TAG	T; 22	16137302	rs573978809	G	C; ```; and it crashed with:; ```; java.lang.IllegalStateException: Allele in genotype TAG* not in the variant context [G*, *, C]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.makeMergedVariantContext(AssemblyBasedCallerUtils.java:221); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:150); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:599); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5336
https://github.com/broadinstitute/gatk/issues/5336:3276,Usability,simpl,simpleMerge,3276,"	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```; A second case involved `--alleles` input:; ```; 22	16464044	rs571268158	CCAGGTCT	C; 22	16464051	rs569099729	T	C; ```; and crashed similarly, with:; ```; java.lang.IllegalStateException: Allele in genotype CCAGGTCT* not in the variant context [T*, *, C]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.makeMergedVariantContext(AssemblyBasedCallerUtils.java:221); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:150); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:599); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5336
https://github.com/broadinstitute/gatk/issues/5336:3395,Usability,simpl,simpleMerge,3395,"nEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```; A second case involved `--alleles` input:; ```; 22	16464044	rs571268158	CCAGGTCT	C; 22	16464051	rs569099729	T	C; ```; and crashed similarly, with:; ```; java.lang.IllegalStateException: Allele in genotype CCAGGTCT* not in the variant context [T*, *, C]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.makeMergedVariantContext(AssemblyBasedCallerUtils.java:221); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:150); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:599); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5336
https://github.com/broadinstitute/gatk/issues/5337:5726,Safety,detect,detected,5726,"aplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```; Based on the discussion surrounding #4963 and the [test VCF](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/testGenotypeGivenAllelesMode_givenAlleles.vcf), I gather that cases like these are intended to work, without crashing. I was trying to figure out what was unique in these problematic cases, compared to the spanning deletion in the aforementioned test VCF. I noticed that the problematic cases both have the SNP at the very last base of the spanning deletion. I'm just speculating here, but maybe the issue is related to some sort of ""off-by-one"" bug?. This is based on testing with version 4.0.9.0.; I also tried with 4.0.5.1, and it didn't crash, but rather displayed warnings of the type discussed in #4963:; `00:02:10.995 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16137302-16137302, only considering the first record`; `00:03:08.220 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16464051-16464051, only considering the first record`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5337
https://github.com/broadinstitute/gatk/issues/5337:5897,Safety,detect,detected,5897,"aplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```; Based on the discussion surrounding #4963 and the [test VCF](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/testGenotypeGivenAllelesMode_givenAlleles.vcf), I gather that cases like these are intended to work, without crashing. I was trying to figure out what was unique in these problematic cases, compared to the spanning deletion in the aforementioned test VCF. I noticed that the problematic cases both have the SNP at the very last base of the spanning deletion. I'm just speculating here, but maybe the issue is related to some sort of ""off-by-one"" bug?. This is based on testing with version 4.0.9.0.; I also tried with 4.0.5.1, and it didn't crash, but rather displayed warnings of the type discussed in #4963:; `00:02:10.995 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16137302-16137302, only considering the first record`; `00:03:08.220 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16464051-16464051, only considering the first record`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5337
https://github.com/broadinstitute/gatk/issues/5337:512,Security,validat,validateGenotypes,512,"Using HaplotypeCaller with `GENOTYPE_GIVEN_ALLELES` (""GGA"") mode, I came across a couple of cases that crashed, and I traced them to spanning deletions (of the type considered in #4963). The first case involved the following spanning deletion in the `--alleles` input:; ```; 22	16137300	rs567136176	TAG	T; 22	16137302	rs573978809	G	C; ```; and it crashed with:; ```; java.lang.IllegalStateException: Allele in genotype TAG* not in the variant context [G*, *, C]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.makeMergedVariantContext(AssemblyBasedCallerUtils.java:221); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:150); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:599); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5337
https://github.com/broadinstitute/gatk/issues/5337:606,Security,validat,validate,606,"Using HaplotypeCaller with `GENOTYPE_GIVEN_ALLELES` (""GGA"") mode, I came across a couple of cases that crashed, and I traced them to spanning deletions (of the type considered in #4963). The first case involved the following spanning deletion in the `--alleles` input:; ```; 22	16137300	rs567136176	TAG	T; 22	16137302	rs573978809	G	C; ```; and it crashed with:; ```; java.lang.IllegalStateException: Allele in genotype TAG* not in the variant context [G*, *, C]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.makeMergedVariantContext(AssemblyBasedCallerUtils.java:221); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:150); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:599); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5337
https://github.com/broadinstitute/gatk/issues/5337:2848,Security,validat,validateGenotypes,2848,"mblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```; The second case included the following `--alleles` input:; ```; 22	16464044	rs571268158	CCAGGTCT	C; 22	16464051	rs569099729	T	C; ```; and it crashed similarly, with:; ```; java.lang.IllegalStateException: Allele in genotype CCAGGTCT* not in the variant context [T*, *, C]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.makeMergedVariantContext(AssemblyBasedCallerUtils.java:221); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:150); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5337
https://github.com/broadinstitute/gatk/issues/5337:2942,Security,validat,validate,2942,"r.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```; The second case included the following `--alleles` input:; ```; 22	16464044	rs571268158	CCAGGTCT	C; 22	16464051	rs569099729	T	C; ```; and it crashed similarly, with:; ```; java.lang.IllegalStateException: Allele in genotype CCAGGTCT* not in the variant context [T*, *, C]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.makeMergedVariantContext(AssemblyBasedCallerUtils.java:221); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:150); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:599); 	at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5337
https://github.com/broadinstitute/gatk/issues/5337:4913,Testability,test,test,4913,"egion(HaplotypeCallerEngine.java:599); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```; Based on the discussion surrounding #4963 and the [test VCF](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/testGenotypeGivenAllelesMode_givenAlleles.vcf), I gather that cases like these are intended to work, without crashing. I was trying to figure out what was unique in these problematic cases, compared to the spanning deletion in the aforementioned test VCF. I noticed that the problematic cases both have the SNP at the very last base of the spanning deletion. I'm just speculating here, but maybe the issue is related to some sort of ""off-by-one"" bug?. This is based on testing with version 4.0.9.0.; I also tried with 4.0.5.1, and it didn't crash, but rather displayed warnings of the type discussed in #4963:; `00:02:10.995 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16137302-16137302, only considering the first record`; `00:03:08.220 WARN HaplotypeCallerEngine - Multiple valid VCF rec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5337
https://github.com/broadinstitute/gatk/issues/5337:4978,Testability,test,test,4978,"aplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```; Based on the discussion surrounding #4963 and the [test VCF](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/testGenotypeGivenAllelesMode_givenAlleles.vcf), I gather that cases like these are intended to work, without crashing. I was trying to figure out what was unique in these problematic cases, compared to the spanning deletion in the aforementioned test VCF. I noticed that the problematic cases both have the SNP at the very last base of the spanning deletion. I'm just speculating here, but maybe the issue is related to some sort of ""off-by-one"" bug?. This is based on testing with version 4.0.9.0.; I also tried with 4.0.5.1, and it didn't crash, but rather displayed warnings of the type discussed in #4963:; `00:02:10.995 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16137302-16137302, only considering the first record`; `00:03:08.220 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16464051-16464051, only considering the first record`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5337
https://github.com/broadinstitute/gatk/issues/5337:5291,Testability,test,test,5291,"aplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```; Based on the discussion surrounding #4963 and the [test VCF](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/testGenotypeGivenAllelesMode_givenAlleles.vcf), I gather that cases like these are intended to work, without crashing. I was trying to figure out what was unique in these problematic cases, compared to the spanning deletion in the aforementioned test VCF. I noticed that the problematic cases both have the SNP at the very last base of the spanning deletion. I'm just speculating here, but maybe the issue is related to some sort of ""off-by-one"" bug?. This is based on testing with version 4.0.9.0.; I also tried with 4.0.5.1, and it didn't crash, but rather displayed warnings of the type discussed in #4963:; `00:02:10.995 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16137302-16137302, only considering the first record`; `00:03:08.220 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16464051-16464051, only considering the first record`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5337
https://github.com/broadinstitute/gatk/issues/5337:5514,Testability,test,testing,5514,"aplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```; Based on the discussion surrounding #4963 and the [test VCF](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/testGenotypeGivenAllelesMode_givenAlleles.vcf), I gather that cases like these are intended to work, without crashing. I was trying to figure out what was unique in these problematic cases, compared to the spanning deletion in the aforementioned test VCF. I noticed that the problematic cases both have the SNP at the very last base of the spanning deletion. I'm just speculating here, but maybe the issue is related to some sort of ""off-by-one"" bug?. This is based on testing with version 4.0.9.0.; I also tried with 4.0.5.1, and it didn't crash, but rather displayed warnings of the type discussed in #4963:; `00:02:10.995 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16137302-16137302, only considering the first record`; `00:03:08.220 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16464051-16464051, only considering the first record`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5337
https://github.com/broadinstitute/gatk/issues/5337:984,Usability,simpl,simpleMerge,984,"ing HaplotypeCaller with `GENOTYPE_GIVEN_ALLELES` (""GGA"") mode, I came across a couple of cases that crashed, and I traced them to spanning deletions (of the type considered in #4963). The first case involved the following spanning deletion in the `--alleles` input:; ```; 22	16137300	rs567136176	TAG	T; 22	16137302	rs573978809	G	C; ```; and it crashed with:; ```; java.lang.IllegalStateException: Allele in genotype TAG* not in the variant context [G*, *, C]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.makeMergedVariantContext(AssemblyBasedCallerUtils.java:221); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:150); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:599); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5337
https://github.com/broadinstitute/gatk/issues/5337:1103,Usability,simpl,simpleMerge,1103,"ced them to spanning deletions (of the type considered in #4963). The first case involved the following spanning deletion in the `--alleles` input:; ```; 22	16137300	rs567136176	TAG	T; 22	16137302	rs573978809	G	C; ```; and it crashed with:; ```; java.lang.IllegalStateException: Allele in genotype TAG* not in the variant context [G*, *, C]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.makeMergedVariantContext(AssemblyBasedCallerUtils.java:221); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:150); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:599); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5337
https://github.com/broadinstitute/gatk/issues/5337:3320,Usability,simpl,simpleMerge,3320,"ute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```; The second case included the following `--alleles` input:; ```; 22	16464044	rs571268158	CCAGGTCT	C; 22	16464051	rs569099729	T	C; ```; and it crashed similarly, with:; ```; java.lang.IllegalStateException: Allele in genotype CCAGGTCT* not in the variant context [T*, *, C]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.makeMergedVariantContext(AssemblyBasedCallerUtils.java:221); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:150); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:599); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5337
https://github.com/broadinstitute/gatk/issues/5337:3439,Usability,simpl,simpleMerge,3439,"3); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```; The second case included the following `--alleles` input:; ```; 22	16464044	rs571268158	CCAGGTCT	C; 22	16464051	rs569099729	T	C; ```; and it crashed similarly, with:; ```; java.lang.IllegalStateException: Allele in genotype CCAGGTCT* not in the variant context [T*, *, C]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.makeMergedVariantContext(AssemblyBasedCallerUtils.java:221); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:150); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:599); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5337
https://github.com/broadinstitute/gatk/issues/5338:35,Deployability,upgrade,upgrade,35,"Include codec registry, versioning/upgrade chain framework.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5338
https://github.com/broadinstitute/gatk/issues/5339:34,Testability,test,tests,34,"It looks like we're ignoring some tests by accident, probably due to bugs in their dataproviders. We should fix that. See: https://storage.googleapis.com/hellbender-test-logs/build_reports/master_22886.3/tests/test/index.html",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5339
https://github.com/broadinstitute/gatk/issues/5339:165,Testability,test,test-logs,165,"It looks like we're ignoring some tests by accident, probably due to bugs in their dataproviders. We should fix that. See: https://storage.googleapis.com/hellbender-test-logs/build_reports/master_22886.3/tests/test/index.html",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5339
https://github.com/broadinstitute/gatk/issues/5339:204,Testability,test,tests,204,"It looks like we're ignoring some tests by accident, probably due to bugs in their dataproviders. We should fix that. See: https://storage.googleapis.com/hellbender-test-logs/build_reports/master_22886.3/tests/test/index.html",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5339
https://github.com/broadinstitute/gatk/issues/5339:210,Testability,test,test,210,"It looks like we're ignoring some tests by accident, probably due to bugs in their dataproviders. We should fix that. See: https://storage.googleapis.com/hellbender-test-logs/build_reports/master_22886.3/tests/test/index.html",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5339
https://github.com/broadinstitute/gatk/issues/5340:10,Testability,test,test,10,"We need a test for this case, as https://github.com/disq-bio/disq/issues/47 suggests it might not be working",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5340
https://github.com/broadinstitute/gatk/pull/5341:400,Availability,down,downstream,400,"Fixes https://github.com/broadinstitute/gatk/issues/5336. `HaplotypeCallerGenotypingEngine.replaceWithSpanDelVC` replaces the given alleles `VariantContext` objects with new VCs in which the alleles have been replaced with star alleles to represent spanning deletions. However, if the input variant records had genotypes, the original (non-star) deletion alleles were still being retained, causing a downstream error. This fixes that issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5341
https://github.com/broadinstitute/gatk/pull/5341:411,Availability,error,error,411,"Fixes https://github.com/broadinstitute/gatk/issues/5336. `HaplotypeCallerGenotypingEngine.replaceWithSpanDelVC` replaces the given alleles `VariantContext` objects with new VCs in which the alleles have been replaced with star alleles to represent spanning deletions. However, if the input variant records had genotypes, the original (non-star) deletion alleles were still being retained, causing a downstream error. This fixes that issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5341
https://github.com/broadinstitute/gatk/issues/5342:522,Availability,Error,Error,522,"Command:; `java -Djava.io.tmpdir=/work/TMP \ ; -Xmx40g -jar ~/bin/gatk-4.0.8.1/gatk-package-4.0.8.1-local.jar \ ; GenomicsDBImport \ ; -V /work/Analysis/III_3P_RG_DupMark.raw.snps.indels.g.vcf \ ; -V /work/Analysis/IV_11N_RG_DupMark.raw.snps.indels.g.vcf \; -V /work/Analysis/IV_8N_RG_DupMark.raw.snps.indels.g.vcf \; -V /work/Analysis/IV_10P_RG_DupMark.raw.snps.indels.g.vcf \; -V /work/Analysis/IV_20P_RG_DupMark.raw.snps.indels.g.vcf \; --genomicsdb-workspace-path /work/Analysis/wang_chr19_re \; --intervals chr19`. **Error Log**. 15:00:35.770 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/wang/bin/gatk-4.0.8.1/gatk-package-4.0.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:00:35.944 INFO GenomicsDBImport - ------------------------------------------------------------; 15:00:35.944 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.8.1; 15:00:35.945 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:00:35.945 INFO GenomicsDBImport - Executing as wang@Ubuntu1604 on Linux v3.16.0-43-generic amd64; 15:00:35.945 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-8u171-b11-2~14.04-b11; 15:00:35.945 INFO GenomicsDBImport - Start Date/Time: October 2, 2018 3:00:35 PM JST; 15:00:35.945 INFO GenomicsDBImport - ------------------------------------------------------------; 15:00:35.945 INFO GenomicsDBImport - ------------------------------------------------------------; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Version: 2.16.0; 15:00:35.946 INFO GenomicsDBImport - Picard Version: 2.18.7; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:00:35.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342
https://github.com/broadinstitute/gatk/issues/5342:4336,Availability,Error,Error,4336, /work/Analysis/wgs_chr19/callset.json; 15:00:38.849 INFO GenomicsDBImport - Complete VCF Header will be written to /work/Analysis/wgs_chr19/vcfheader.vcf; 15:00:38.850 INFO GenomicsDBImport - Importing to array - /work/Analysis/wgs_chr19/genomicsdb_array; 15:00:38.850 INFO ProgressMeter - Starting traversal; 15:00:38.850 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 15:00:39.771 INFO GenomicsDBImport - Importing batch 1 with 5 samples; Buffer resized from 28469bytes to 32688; Buffer resized from 28473bytes to 32630; Buffer resized from 28469bytes to 32745; Buffer resized from 28469bytes to 32717; Buffer resized from 28466bytes to 32648; Buffer resized from 32688bytes to 32758; Buffer resized from 32630bytes to 32726; Buffer resized from 32648bytes to 32703; Buffer resized from 32717bytes to 32751; Buffer resized from 32703bytes to 32765; Buffer resized from 32745bytes to 32768; Buffer resized from 32726bytes to 32763; Buffer resized from 32765bytes to 32767; Buffer resized from 32758bytes to 32765; Buffer resized from 32751bytes to 32762; Buffer resized from 32767bytes to 32769; Buffer resized from 32763bytes to 32768; Buffer resized from 32762bytes to 32768; Buffer resized from 32765bytes to 32767; Buffer resized from 32767bytes to 32769; Buffer resized from 32768bytes to 32769; Buffer resized from 32768bytes to 32769; Buffer resized from 32768bytes to 32769; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : Error while syncing array chr19$1$58617616 to disk; TileDB error message : [TileDB::utils] Error: Cannot sync file '/work/Analysis/wgs_chr19/chr19$1$58617616/.__a89fdd44-1241-43ba-9072-6fcf116fbc1d139627949156096_1538460040234'; File syncing error. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/13135/gatk-v4-0-8-1-genomicsdbimport-error-variantstoragemanagerexception-exception/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342
https://github.com/broadinstitute/gatk/issues/5342:4395,Availability,error,error,4395, /work/Analysis/wgs_chr19/callset.json; 15:00:38.849 INFO GenomicsDBImport - Complete VCF Header will be written to /work/Analysis/wgs_chr19/vcfheader.vcf; 15:00:38.850 INFO GenomicsDBImport - Importing to array - /work/Analysis/wgs_chr19/genomicsdb_array; 15:00:38.850 INFO ProgressMeter - Starting traversal; 15:00:38.850 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 15:00:39.771 INFO GenomicsDBImport - Importing batch 1 with 5 samples; Buffer resized from 28469bytes to 32688; Buffer resized from 28473bytes to 32630; Buffer resized from 28469bytes to 32745; Buffer resized from 28469bytes to 32717; Buffer resized from 28466bytes to 32648; Buffer resized from 32688bytes to 32758; Buffer resized from 32630bytes to 32726; Buffer resized from 32648bytes to 32703; Buffer resized from 32717bytes to 32751; Buffer resized from 32703bytes to 32765; Buffer resized from 32745bytes to 32768; Buffer resized from 32726bytes to 32763; Buffer resized from 32765bytes to 32767; Buffer resized from 32758bytes to 32765; Buffer resized from 32751bytes to 32762; Buffer resized from 32767bytes to 32769; Buffer resized from 32763bytes to 32768; Buffer resized from 32762bytes to 32768; Buffer resized from 32765bytes to 32767; Buffer resized from 32767bytes to 32769; Buffer resized from 32768bytes to 32769; Buffer resized from 32768bytes to 32769; Buffer resized from 32768bytes to 32769; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : Error while syncing array chr19$1$58617616 to disk; TileDB error message : [TileDB::utils] Error: Cannot sync file '/work/Analysis/wgs_chr19/chr19$1$58617616/.__a89fdd44-1241-43ba-9072-6fcf116fbc1d139627949156096_1538460040234'; File syncing error. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/13135/gatk-v4-0-8-1-genomicsdbimport-error-variantstoragemanagerexception-exception/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342
https://github.com/broadinstitute/gatk/issues/5342:4427,Availability,Error,Error,4427, /work/Analysis/wgs_chr19/callset.json; 15:00:38.849 INFO GenomicsDBImport - Complete VCF Header will be written to /work/Analysis/wgs_chr19/vcfheader.vcf; 15:00:38.850 INFO GenomicsDBImport - Importing to array - /work/Analysis/wgs_chr19/genomicsdb_array; 15:00:38.850 INFO ProgressMeter - Starting traversal; 15:00:38.850 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 15:00:39.771 INFO GenomicsDBImport - Importing batch 1 with 5 samples; Buffer resized from 28469bytes to 32688; Buffer resized from 28473bytes to 32630; Buffer resized from 28469bytes to 32745; Buffer resized from 28469bytes to 32717; Buffer resized from 28466bytes to 32648; Buffer resized from 32688bytes to 32758; Buffer resized from 32630bytes to 32726; Buffer resized from 32648bytes to 32703; Buffer resized from 32717bytes to 32751; Buffer resized from 32703bytes to 32765; Buffer resized from 32745bytes to 32768; Buffer resized from 32726bytes to 32763; Buffer resized from 32765bytes to 32767; Buffer resized from 32758bytes to 32765; Buffer resized from 32751bytes to 32762; Buffer resized from 32767bytes to 32769; Buffer resized from 32763bytes to 32768; Buffer resized from 32762bytes to 32768; Buffer resized from 32765bytes to 32767; Buffer resized from 32767bytes to 32769; Buffer resized from 32768bytes to 32769; Buffer resized from 32768bytes to 32769; Buffer resized from 32768bytes to 32769; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : Error while syncing array chr19$1$58617616 to disk; TileDB error message : [TileDB::utils] Error: Cannot sync file '/work/Analysis/wgs_chr19/chr19$1$58617616/.__a89fdd44-1241-43ba-9072-6fcf116fbc1d139627949156096_1538460040234'; File syncing error. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/13135/gatk-v4-0-8-1-genomicsdbimport-error-variantstoragemanagerexception-exception/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342
https://github.com/broadinstitute/gatk/issues/5342:4578,Availability,error,error,4578, /work/Analysis/wgs_chr19/callset.json; 15:00:38.849 INFO GenomicsDBImport - Complete VCF Header will be written to /work/Analysis/wgs_chr19/vcfheader.vcf; 15:00:38.850 INFO GenomicsDBImport - Importing to array - /work/Analysis/wgs_chr19/genomicsdb_array; 15:00:38.850 INFO ProgressMeter - Starting traversal; 15:00:38.850 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 15:00:39.771 INFO GenomicsDBImport - Importing batch 1 with 5 samples; Buffer resized from 28469bytes to 32688; Buffer resized from 28473bytes to 32630; Buffer resized from 28469bytes to 32745; Buffer resized from 28469bytes to 32717; Buffer resized from 28466bytes to 32648; Buffer resized from 32688bytes to 32758; Buffer resized from 32630bytes to 32726; Buffer resized from 32648bytes to 32703; Buffer resized from 32717bytes to 32751; Buffer resized from 32703bytes to 32765; Buffer resized from 32745bytes to 32768; Buffer resized from 32726bytes to 32763; Buffer resized from 32765bytes to 32767; Buffer resized from 32758bytes to 32765; Buffer resized from 32751bytes to 32762; Buffer resized from 32767bytes to 32769; Buffer resized from 32763bytes to 32768; Buffer resized from 32762bytes to 32768; Buffer resized from 32765bytes to 32767; Buffer resized from 32767bytes to 32769; Buffer resized from 32768bytes to 32769; Buffer resized from 32768bytes to 32769; Buffer resized from 32768bytes to 32769; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : Error while syncing array chr19$1$58617616 to disk; TileDB error message : [TileDB::utils] Error: Cannot sync file '/work/Analysis/wgs_chr19/chr19$1$58617616/.__a89fdd44-1241-43ba-9072-6fcf116fbc1d139627949156096_1538460040234'; File syncing error. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/13135/gatk-v4-0-8-1-genomicsdbimport-error-variantstoragemanagerexception-exception/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342
https://github.com/broadinstitute/gatk/issues/5342:4732,Availability,error,error-variantstoragemanagerexception-exception,4732, /work/Analysis/wgs_chr19/callset.json; 15:00:38.849 INFO GenomicsDBImport - Complete VCF Header will be written to /work/Analysis/wgs_chr19/vcfheader.vcf; 15:00:38.850 INFO GenomicsDBImport - Importing to array - /work/Analysis/wgs_chr19/genomicsdb_array; 15:00:38.850 INFO ProgressMeter - Starting traversal; 15:00:38.850 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 15:00:39.771 INFO GenomicsDBImport - Importing batch 1 with 5 samples; Buffer resized from 28469bytes to 32688; Buffer resized from 28473bytes to 32630; Buffer resized from 28469bytes to 32745; Buffer resized from 28469bytes to 32717; Buffer resized from 28466bytes to 32648; Buffer resized from 32688bytes to 32758; Buffer resized from 32630bytes to 32726; Buffer resized from 32648bytes to 32703; Buffer resized from 32717bytes to 32751; Buffer resized from 32703bytes to 32765; Buffer resized from 32745bytes to 32768; Buffer resized from 32726bytes to 32763; Buffer resized from 32765bytes to 32767; Buffer resized from 32758bytes to 32765; Buffer resized from 32751bytes to 32762; Buffer resized from 32767bytes to 32769; Buffer resized from 32763bytes to 32768; Buffer resized from 32762bytes to 32768; Buffer resized from 32765bytes to 32767; Buffer resized from 32767bytes to 32769; Buffer resized from 32768bytes to 32769; Buffer resized from 32768bytes to 32769; Buffer resized from 32768bytes to 32769; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : Error while syncing array chr19$1$58617616 to disk; TileDB error message : [TileDB::utils] Error: Cannot sync file '/work/Analysis/wgs_chr19/chr19$1$58617616/.__a89fdd44-1241-43ba-9072-6fcf116fbc1d139627949156096_1538460040234'; File syncing error. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/13135/gatk-v4-0-8-1-genomicsdbimport-error-variantstoragemanagerexception-exception/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342
https://github.com/broadinstitute/gatk/issues/5342:2300,Deployability,release,releases,2300,"ber 2, 2018 3:00:35 PM JST; 15:00:35.945 INFO GenomicsDBImport - ------------------------------------------------------------; 15:00:35.945 INFO GenomicsDBImport - ------------------------------------------------------------; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Version: 2.16.0; 15:00:35.946 INFO GenomicsDBImport - Picard Version: 2.18.7; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:00:35.946 INFO GenomicsDBImport - Deflater: IntelDeflater; 15:00:35.946 INFO GenomicsDBImport - Inflater: IntelInflater; 15:00:35.946 INFO GenomicsDBImport - GCS max retries/reopens: 20; 15:00:35.946 INFO GenomicsDBImport - Using google-cloud-java fork https://github.com/broadinstitute/google-cloud-java/releases/tag/0.20.5-alpha-GCS-RETRY-FIX; 15:00:35.946 INFO GenomicsDBImport - Initializing engine; 15:00:38.360 INFO IntervalArgumentCollection - Processing 58617616 bp from intervals; 15:00:38.366 INFO GenomicsDBImport - Done initializing engine; Created workspace /work/Analysis/wgs_chr19; 15:00:38.849 INFO GenomicsDBImport - Vid Map JSON file will be written to /work/Analysis/wgs_chr19/vidmap.json; 15:00:38.849 INFO GenomicsDBImport - Callset Map JSON file will be written to /work/Analysis/wgs_chr19/callset.json; 15:00:38.849 INFO GenomicsDBImport - Complete VCF Header will be written to /work/Analysis/wgs_chr19/vcfheader.vcf; 15:00:38.850 INFO GenomicsDBImport - Importing to array - /work/Analysis/wgs_chr19/genomicsdb_array; 15:00:38.850 INFO ProgressMeter - Starting traversal; 15:00:38.850 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 15:00:39.771 INFO GenomicsDBImport - Importing batch 1 with 5 samples; Buffer resized from 28469byt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342
https://github.com/broadinstitute/gatk/issues/5342:4401,Integrability,message,message,4401, /work/Analysis/wgs_chr19/callset.json; 15:00:38.849 INFO GenomicsDBImport - Complete VCF Header will be written to /work/Analysis/wgs_chr19/vcfheader.vcf; 15:00:38.850 INFO GenomicsDBImport - Importing to array - /work/Analysis/wgs_chr19/genomicsdb_array; 15:00:38.850 INFO ProgressMeter - Starting traversal; 15:00:38.850 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 15:00:39.771 INFO GenomicsDBImport - Importing batch 1 with 5 samples; Buffer resized from 28469bytes to 32688; Buffer resized from 28473bytes to 32630; Buffer resized from 28469bytes to 32745; Buffer resized from 28469bytes to 32717; Buffer resized from 28466bytes to 32648; Buffer resized from 32688bytes to 32758; Buffer resized from 32630bytes to 32726; Buffer resized from 32648bytes to 32703; Buffer resized from 32717bytes to 32751; Buffer resized from 32703bytes to 32765; Buffer resized from 32745bytes to 32768; Buffer resized from 32726bytes to 32763; Buffer resized from 32765bytes to 32767; Buffer resized from 32758bytes to 32765; Buffer resized from 32751bytes to 32762; Buffer resized from 32767bytes to 32769; Buffer resized from 32763bytes to 32768; Buffer resized from 32762bytes to 32768; Buffer resized from 32765bytes to 32767; Buffer resized from 32767bytes to 32769; Buffer resized from 32768bytes to 32769; Buffer resized from 32768bytes to 32769; Buffer resized from 32768bytes to 32769; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : Error while syncing array chr19$1$58617616 to disk; TileDB error message : [TileDB::utils] Error: Cannot sync file '/work/Analysis/wgs_chr19/chr19$1$58617616/.__a89fdd44-1241-43ba-9072-6fcf116fbc1d139627949156096_1538460040234'; File syncing error. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/13135/gatk-v4-0-8-1-genomicsdbimport-error-variantstoragemanagerexception-exception/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342
https://github.com/broadinstitute/gatk/issues/5342:575,Performance,Load,Loading,575,"Command:; `java -Djava.io.tmpdir=/work/TMP \ ; -Xmx40g -jar ~/bin/gatk-4.0.8.1/gatk-package-4.0.8.1-local.jar \ ; GenomicsDBImport \ ; -V /work/Analysis/III_3P_RG_DupMark.raw.snps.indels.g.vcf \ ; -V /work/Analysis/IV_11N_RG_DupMark.raw.snps.indels.g.vcf \; -V /work/Analysis/IV_8N_RG_DupMark.raw.snps.indels.g.vcf \; -V /work/Analysis/IV_10P_RG_DupMark.raw.snps.indels.g.vcf \; -V /work/Analysis/IV_20P_RG_DupMark.raw.snps.indels.g.vcf \; --genomicsdb-workspace-path /work/Analysis/wang_chr19_re \; --intervals chr19`. **Error Log**. 15:00:35.770 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/wang/bin/gatk-4.0.8.1/gatk-package-4.0.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:00:35.944 INFO GenomicsDBImport - ------------------------------------------------------------; 15:00:35.944 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.8.1; 15:00:35.945 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:00:35.945 INFO GenomicsDBImport - Executing as wang@Ubuntu1604 on Linux v3.16.0-43-generic amd64; 15:00:35.945 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-8u171-b11-2~14.04-b11; 15:00:35.945 INFO GenomicsDBImport - Start Date/Time: October 2, 2018 3:00:35 PM JST; 15:00:35.945 INFO GenomicsDBImport - ------------------------------------------------------------; 15:00:35.945 INFO GenomicsDBImport - ------------------------------------------------------------; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Version: 2.16.0; 15:00:35.946 INFO GenomicsDBImport - Picard Version: 2.18.7; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:00:35.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342
https://github.com/broadinstitute/gatk/issues/5342:528,Testability,Log,Log,528,"Command:; `java -Djava.io.tmpdir=/work/TMP \ ; -Xmx40g -jar ~/bin/gatk-4.0.8.1/gatk-package-4.0.8.1-local.jar \ ; GenomicsDBImport \ ; -V /work/Analysis/III_3P_RG_DupMark.raw.snps.indels.g.vcf \ ; -V /work/Analysis/IV_11N_RG_DupMark.raw.snps.indels.g.vcf \; -V /work/Analysis/IV_8N_RG_DupMark.raw.snps.indels.g.vcf \; -V /work/Analysis/IV_10P_RG_DupMark.raw.snps.indels.g.vcf \; -V /work/Analysis/IV_20P_RG_DupMark.raw.snps.indels.g.vcf \; --genomicsdb-workspace-path /work/Analysis/wang_chr19_re \; --intervals chr19`. **Error Log**. 15:00:35.770 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/wang/bin/gatk-4.0.8.1/gatk-package-4.0.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:00:35.944 INFO GenomicsDBImport - ------------------------------------------------------------; 15:00:35.944 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.8.1; 15:00:35.945 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:00:35.945 INFO GenomicsDBImport - Executing as wang@Ubuntu1604 on Linux v3.16.0-43-generic amd64; 15:00:35.945 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-8u171-b11-2~14.04-b11; 15:00:35.945 INFO GenomicsDBImport - Start Date/Time: October 2, 2018 3:00:35 PM JST; 15:00:35.945 INFO GenomicsDBImport - ------------------------------------------------------------; 15:00:35.945 INFO GenomicsDBImport - ------------------------------------------------------------; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Version: 2.16.0; 15:00:35.946 INFO GenomicsDBImport - Picard Version: 2.18.7; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:00:35.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342
https://github.com/broadinstitute/gatk/issues/5343:306,Safety,avoid,avoid,306,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_, _TranscriptSelectionMode::CanonicalGencodeFuncotationComparator. ### Description; When comparing transcripts by `CanonicalGencodeFuncotationComparator`, the 5' Flank status of each should be compared very high in the comparison chain to avoid `MISSENSE` being trumped by `5_PRIME_FLANK` on another transcript. This comparison can be made just after comparing for `PROTEIN_CODING` status.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5343
https://github.com/broadinstitute/gatk/pull/5344:0,Deployability,upgrade,upgrade,0,upgrade Picard dependency from 2.18.13 -> 2.18.15,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5344
https://github.com/broadinstitute/gatk/pull/5344:15,Integrability,depend,dependency,15,upgrade Picard dependency from 2.18.13 -> 2.18.15,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5344
https://github.com/broadinstitute/gatk/issues/5346:245,Deployability,configurat,configurations,245,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator scripts_. ### Description; The scripts for Funcotator (`src/scripts/funcotator`) should all be refactored, if necessary, to allow for command-line arguments rather than internal configurations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5346
https://github.com/broadinstitute/gatk/issues/5346:162,Modifiability,refactor,refactored,162,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator scripts_. ### Description; The scripts for Funcotator (`src/scripts/funcotator`) should all be refactored, if necessary, to allow for command-line arguments rather than internal configurations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5346
https://github.com/broadinstitute/gatk/issues/5346:245,Modifiability,config,configurations,245,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator scripts_. ### Description; The scripts for Funcotator (`src/scripts/funcotator`) should all be refactored, if necessary, to allow for command-line arguments rather than internal configurations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5346
https://github.com/broadinstitute/gatk/issues/5347:233,Modifiability,refactor,refactor,233,## Feature request. ### Tool(s) or class(es) involved; _Funcotator_. ### Description; Currently Funcotator fuzzy-matches between `b37` and `hg19` to enable reuse of data sources compatible with `hg19`. This was a mistake. We need to refactor the data sources to have a separate set for `b37` and `hg19` and remove the fuzzy matching. Bugs and confusion surrounding this fuzzy matching continue to bite and scratch us and are causing time to be lost.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5347
https://github.com/broadinstitute/gatk/issues/5348:350,Availability,down,downloads,350,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_, _DataSourceFuncotationFactory_. ### Description; Funcotator should support NIO for data sources and data sources backing files.; In addition, the data source readers should be updated to support multiple backing files to support the `gnomAD` case (http://gnomad.broadinstitute.org/downloads).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5348
https://github.com/broadinstitute/gatk/issues/5348:245,Deployability,update,updated,245,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_, _DataSourceFuncotationFactory_. ### Description; Funcotator should support NIO for data sources and data sources backing files.; In addition, the data source readers should be updated to support multiple backing files to support the `gnomAD` case (http://gnomad.broadinstitute.org/downloads).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5348
https://github.com/broadinstitute/gatk/issues/5350:133,Modifiability,refactor,refactored,133,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator tests_. ### Description; Now that the test data sources have been refactored, we need to go through and remove any extraneous data sources that are no longer necessary.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5350
https://github.com/broadinstitute/gatk/issues/5350:105,Testability,test,test,105,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator tests_. ### Description; Now that the test data sources have been refactored, we need to go through and remove any extraneous data sources that are no longer necessary.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5350
https://github.com/broadinstitute/gatk/issues/5351:150,Testability,test,test,150,## Feature request. ### Tool(s) or class(es) involved; _FuncotatorUtilsUnitTest::provideDataForTestGetAlignedRefAllele_. ### Description; Need to add test cases for the following: ; - tests with Strand.NEGATIVE!!!!; - tests with alt allele longer/shorter than ref allele!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5351
https://github.com/broadinstitute/gatk/issues/5351:184,Testability,test,tests,184,## Feature request. ### Tool(s) or class(es) involved; _FuncotatorUtilsUnitTest::provideDataForTestGetAlignedRefAllele_. ### Description; Need to add test cases for the following: ; - tests with Strand.NEGATIVE!!!!; - tests with alt allele longer/shorter than ref allele!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5351
https://github.com/broadinstitute/gatk/issues/5351:218,Testability,test,tests,218,## Feature request. ### Tool(s) or class(es) involved; _FuncotatorUtilsUnitTest::provideDataForTestGetAlignedRefAllele_. ### Description; Need to add test cases for the following: ; - tests with Strand.NEGATIVE!!!!; - tests with alt allele longer/shorter than ref allele!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5351
https://github.com/broadinstitute/gatk/issues/5352:705,Availability,down,down-sampling,705,"## Documentation request. ### Tool(s) or class(es) involved; Mutect2 and FilterMutectCalls. ### Description ; Because both `M2ArgumentCollection` and `M2FiltersArgumentCollection` extend `AssemblyBasedCallerArgumentCollection`, both `Mutect2` and `FilterMutectCalls` display all assembly and caller arguments in the documentation/help even if those arguments don't actually do anything. For example both tools have the argument `--contamination-fraction-to-filter` which has the description:. ```; Fraction of contamination in sequencing data (for all samples) to aggressively remove. If this fraction is greater is than zero, the caller will aggressively attempt to remove contamination ; through biased down-sampling of reads. Basically, it will ignore the contamination fraction of reads for ; each alternate allele. So if the pileup contains N total bases, then we will try to remove ; (N * contamination fraction) bases for each alternate allele.; ```. This argument definitely doesn't do anything in `FilteMutectCalls` but I also don't think it's hooked up to do anything in `Mutect2` either (at least when I tried giving it a high value I still got the same calls). This is by design because Mutect has other ways of handling contamination, but the argument is still displayed in both tools' documentation which is confusing. There are other arguments that have the same issue where it's unclear if they do anything in Mutect or not.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5352
https://github.com/broadinstitute/gatk/issues/5352:180,Modifiability,extend,extend,180,"## Documentation request. ### Tool(s) or class(es) involved; Mutect2 and FilterMutectCalls. ### Description ; Because both `M2ArgumentCollection` and `M2FiltersArgumentCollection` extend `AssemblyBasedCallerArgumentCollection`, both `Mutect2` and `FilterMutectCalls` display all assembly and caller arguments in the documentation/help even if those arguments don't actually do anything. For example both tools have the argument `--contamination-fraction-to-filter` which has the description:. ```; Fraction of contamination in sequencing data (for all samples) to aggressively remove. If this fraction is greater is than zero, the caller will aggressively attempt to remove contamination ; through biased down-sampling of reads. Basically, it will ignore the contamination fraction of reads for ; each alternate allele. So if the pileup contains N total bases, then we will try to remove ; (N * contamination fraction) bases for each alternate allele.; ```. This argument definitely doesn't do anything in `FilteMutectCalls` but I also don't think it's hooked up to do anything in `Mutect2` either (at least when I tried giving it a high value I still got the same calls). This is by design because Mutect has other ways of handling contamination, but the argument is still displayed in both tools' documentation which is confusing. There are other arguments that have the same issue where it's unclear if they do anything in Mutect or not.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5352
https://github.com/broadinstitute/gatk/issues/5353:88,Security,authenticat,authenticated,88,Currently build_docker will run for a long time and then fail at the end if you are not authenticated to gcloud. We should have an upfront test for it.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5353
https://github.com/broadinstitute/gatk/issues/5353:139,Testability,test,test,139,Currently build_docker will run for a long time and then fail at the end if you are not authenticated to gcloud. We should have an upfront test for it.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5353
https://github.com/broadinstitute/gatk/issues/5355:40982,Availability,error,error,40982,"0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|1	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0; ```; Running `$HOME/gatk-4.0.11.0/gatk --java-options ""-Xmx4g"" HaplotypeCaller -R $HOME/GRCh37files/hs37d5.fa -I /mnt/fast/test.bam -O test.out.vcf.gz -L 22 --genotyping-mode GENOTYPE_GIVEN_ALLELES --alleles test.vcf.gz`, the resulting error is:; ```; java.lang.IllegalStateException: Allele in genotype GGTTTGTTT not in the variant context [GGTTTGTTT*, GGTTTGTTTGTTT, GGTTTGTTTGTTTGTTT, G]; at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.makeMergedVariantContext(AssemblyBasedCallerUtils.java:228); at org.broadinst",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5355
https://github.com/broadinstitute/gatk/issues/5355:73921,Availability,error,error,73921,"0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|1	1|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	3|0	0|0	3|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|1	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	1|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|1	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	1|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|1	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	1|0	0|0	0|0	0|0	0|0	1|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	2|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	1|0	0|0	0|0	0|0	0|0	1|0	1|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|3	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	1|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|3	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	1|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	1|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|1	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0; ```; And indeed, with that `--alleles` input with a single condensed record, HaplotypeCaller runs without error.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5355
https://github.com/broadinstitute/gatk/issues/5355:73983,Availability,error,error,73983,"|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	1|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|3	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	1|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	1|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|1	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0; ```; And indeed, with that `--alleles` input with a single condensed record, HaplotypeCaller runs without error. Additionally, omitting the genotypes also runs without error:; ```; ##fileformat=VCFv4.1; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO; 22	27658738	rs145982391	G	GGTTT,GGTTTGTTT	.	PASS	.; 22	27658738	rs374358960	GGTTTGTTT	G	.	PASS	.; ```; So it seems to be the combination of the split-record multiallelic and genotypes in `--alleles` file that is problematic here. Probably an edge case by most definitions (and straightforward to work around by either omitting genotypes or condensing multiallelics into a single record) but I figured it was worth pointing out. I should probably also add that many other split multiallelics seem to be processed fine, without crash, e.g.:; ```; ##fileformat=VCFv4.1; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	HG00096	HG00097	HG00099	HG00100	HG00101	HG00102	HG00103	HG00105	HG00106	HG00107	HG00108	HG00109	HG00110	HG00111	HG00112	HG00113	HG00114	HG00115	HG00116	HG00117	HG00118	HG00119	",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5355
https://github.com/broadinstitute/gatk/issues/5355:41186,Security,validat,validateGenotypes,41186,"|0	0|0	0|0	0|0	0|0	0|1	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0; ```; Running `$HOME/gatk-4.0.11.0/gatk --java-options ""-Xmx4g"" HaplotypeCaller -R $HOME/GRCh37files/hs37d5.fa -I /mnt/fast/test.bam -O test.out.vcf.gz -L 22 --genotyping-mode GENOTYPE_GIVEN_ALLELES --alleles test.vcf.gz`, the resulting error is:; ```; java.lang.IllegalStateException: Allele in genotype GGTTTGTTT not in the variant context [GGTTTGTTT*, GGTTTGTTTGTTT, GGTTTGTTTGTTTGTTT, G]; at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.makeMergedVariantContext(AssemblyBasedCallerUtils.java:228); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:157); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.Haplotype",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5355
https://github.com/broadinstitute/gatk/issues/5355:41279,Security,validat,validate,41279,"0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0; ```; Running `$HOME/gatk-4.0.11.0/gatk --java-options ""-Xmx4g"" HaplotypeCaller -R $HOME/GRCh37files/hs37d5.fa -I /mnt/fast/test.bam -O test.out.vcf.gz -L 22 --genotyping-mode GENOTYPE_GIVEN_ALLELES --alleles test.vcf.gz`, the resulting error is:; ```; java.lang.IllegalStateException: Allele in genotype GGTTTGTTT not in the variant context [GGTTTGTTT*, GGTTTGTTTGTTT, GGTTTGTTTGTTTGTTT, G]; at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.makeMergedVariantContext(AssemblyBasedCallerUtils.java:228); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:157); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:599); at org.broadinstitute.hellbende",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5355
https://github.com/broadinstitute/gatk/issues/5355:40869,Testability,test,test,40869,"0	0|1	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|1	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0; ```; Running `$HOME/gatk-4.0.11.0/gatk --java-options ""-Xmx4g"" HaplotypeCaller -R $HOME/GRCh37files/hs37d5.fa -I /mnt/fast/test.bam -O test.out.vcf.gz -L 22 --genotyping-mode GENOTYPE_GIVEN_ALLELES --alleles test.vcf.gz`, the resulting error is:; ```; java.lang.IllegalStateException: Allele in genotype GGTTTGTTT not in the variant context [GGTTTGTTT*, GGTTTGTTTGTTT, GGTTTGTTTGTTTGTTT, G]; at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); at org.broadinstitute.hellbender.tools.walkers",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5355
https://github.com/broadinstitute/gatk/issues/5355:40881,Testability,test,test,40881,"	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|1	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0; ```; Running `$HOME/gatk-4.0.11.0/gatk --java-options ""-Xmx4g"" HaplotypeCaller -R $HOME/GRCh37files/hs37d5.fa -I /mnt/fast/test.bam -O test.out.vcf.gz -L 22 --genotyping-mode GENOTYPE_GIVEN_ALLELES --alleles test.vcf.gz`, the resulting error is:; ```; java.lang.IllegalStateException: Allele in genotype GGTTTGTTT not in the variant context [GGTTTGTTT*, GGTTTGTTTGTTT, GGTTTGTTTGTTTGTTT, G]; at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5355
https://github.com/broadinstitute/gatk/issues/5355:40954,Testability,test,test,40954,"|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|1	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0; ```; Running `$HOME/gatk-4.0.11.0/gatk --java-options ""-Xmx4g"" HaplotypeCaller -R $HOME/GRCh37files/hs37d5.fa -I /mnt/fast/test.bam -O test.out.vcf.gz -L 22 --genotyping-mode GENOTYPE_GIVEN_ALLELES --alleles test.vcf.gz`, the resulting error is:; ```; java.lang.IllegalStateException: Allele in genotype GGTTTGTTT not in the variant context [GGTTTGTTT*, GGTTTGTTTGTTT, GGTTTGTTTGTTTGTTT, G]; at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.makeMergedVariantCont",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5355
https://github.com/broadinstitute/gatk/issues/5355:41653,Usability,simpl,simpleMerge,41653,"0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0; ```; Running `$HOME/gatk-4.0.11.0/gatk --java-options ""-Xmx4g"" HaplotypeCaller -R $HOME/GRCh37files/hs37d5.fa -I /mnt/fast/test.bam -O test.out.vcf.gz -L 22 --genotyping-mode GENOTYPE_GIVEN_ALLELES --alleles test.vcf.gz`, the resulting error is:; ```; java.lang.IllegalStateException: Allele in genotype GGTTTGTTT not in the variant context [GGTTTGTTT*, GGTTTGTTTGTTT, GGTTTGTTTGTTTGTTT, G]; at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.makeMergedVariantContext(AssemblyBasedCallerUtils.java:228); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:157); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:599); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:240); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); at org.broa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5355
https://github.com/broadinstitute/gatk/issues/5355:41771,Usability,simpl,simpleMerge,41771,"va-options ""-Xmx4g"" HaplotypeCaller -R $HOME/GRCh37files/hs37d5.fa -I /mnt/fast/test.bam -O test.out.vcf.gz -L 22 --genotyping-mode GENOTYPE_GIVEN_ALLELES --alleles test.vcf.gz`, the resulting error is:; ```; java.lang.IllegalStateException: Allele in genotype GGTTTGTTT not in the variant context [GGTTTGTTT*, GGTTTGTTTGTTT, GGTTTGTTTGTTTGTTT, G]; at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.makeMergedVariantContext(AssemblyBasedCallerUtils.java:228); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:157); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:599); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:240); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbende",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5355
https://github.com/broadinstitute/gatk/issues/5356:151,Testability,test,tests,151,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_, _GencodeFuncotationFactoryUnitTest::testCreateFuncotations_. ### Description; The tests all assume `chr` is in contig names right now. This is wrong for the universal b37 reference that was put into the test baseline. However, our datasources are all hg19, so there is an evil mismatch. The horror of b37 vs hg19 raises it's ugly head once more. We need to instrument `GencodeFuncotationFactoryUnitTest::testCreateFuncotations` to allow for the mismatch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5356
https://github.com/broadinstitute/gatk/issues/5356:272,Testability,test,test,272,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_, _GencodeFuncotationFactoryUnitTest::testCreateFuncotations_. ### Description; The tests all assume `chr` is in contig names right now. This is wrong for the universal b37 reference that was put into the test baseline. However, our datasources are all hg19, so there is an evil mismatch. The horror of b37 vs hg19 raises it's ugly head once more. We need to instrument `GencodeFuncotationFactoryUnitTest::testCreateFuncotations` to allow for the mismatch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5356
https://github.com/broadinstitute/gatk/issues/5356:473,Testability,test,testCreateFuncotations,473,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_, _GencodeFuncotationFactoryUnitTest::testCreateFuncotations_. ### Description; The tests all assume `chr` is in contig names right now. This is wrong for the universal b37 reference that was put into the test baseline. However, our datasources are all hg19, so there is an evil mismatch. The horror of b37 vs hg19 raises it's ugly head once more. We need to instrument `GencodeFuncotationFactoryUnitTest::testCreateFuncotations` to allow for the mismatch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5356
https://github.com/broadinstitute/gatk/pull/5357:93,Safety,detect,detected,93,"Fixed an issue with de novo starts in the 5' UTR.; Before, the de novo start itself would be detected just fine, however; the position in the UTR was not correctly calculated (leading to an; incorrect and inconsistent calling of in- vs out-of-frame).; This was due to the code assuming that there was only one 5'UTR. There; is no limit on the number of 5'UTRs a transcript may have. This is now accounted for and the calculations match with Oncotator's; assement for in- vs out-of-frame for de novo starts. Fixes #5333",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5357
https://github.com/broadinstitute/gatk/pull/5359:699,Deployability,integrat,integration,699,"@ldgauthier this finishes what we started in #4858 and is necessary for the pileup-calls-on-bamouts MC3 validation. The cause is the same, in that Pair-HMM has a tiny bias in favor of shorter haplotypes and thus it prefers deletion haplotypes when reads end inside STRs. In #4858 we broke near-ties in favor of the reference; this PR fixes the case where two alt haplotypes share a SNV and one of them has a spurious deletion. One important sanity check was that when I set `cigarTerm` to zero in `AssemblyBasedCallerUtils.java` no tests broke. This means that the refactoring needed to set up the change didn't affect behavior. I looked at most of the sites where `PL`s and/or `DP`s changed in the integration test vcfs and in every case the difference was from a fake deletion that this PR fixed. I also went through the diff of the bamouts in IGV and found the same thing. Finally, the changes to test vcfs in `GenotypeGVCFsIntegrationTest` and `GenomicsDBImporterIntegrationTest` are a consequence of changes to the `HaplotypeCallerIntegrationTest` vcfs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5359
https://github.com/broadinstitute/gatk/pull/5359:699,Integrability,integrat,integration,699,"@ldgauthier this finishes what we started in #4858 and is necessary for the pileup-calls-on-bamouts MC3 validation. The cause is the same, in that Pair-HMM has a tiny bias in favor of shorter haplotypes and thus it prefers deletion haplotypes when reads end inside STRs. In #4858 we broke near-ties in favor of the reference; this PR fixes the case where two alt haplotypes share a SNV and one of them has a spurious deletion. One important sanity check was that when I set `cigarTerm` to zero in `AssemblyBasedCallerUtils.java` no tests broke. This means that the refactoring needed to set up the change didn't affect behavior. I looked at most of the sites where `PL`s and/or `DP`s changed in the integration test vcfs and in every case the difference was from a fake deletion that this PR fixed. I also went through the diff of the bamouts in IGV and found the same thing. Finally, the changes to test vcfs in `GenotypeGVCFsIntegrationTest` and `GenomicsDBImporterIntegrationTest` are a consequence of changes to the `HaplotypeCallerIntegrationTest` vcfs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5359
https://github.com/broadinstitute/gatk/pull/5359:565,Modifiability,refactor,refactoring,565,"@ldgauthier this finishes what we started in #4858 and is necessary for the pileup-calls-on-bamouts MC3 validation. The cause is the same, in that Pair-HMM has a tiny bias in favor of shorter haplotypes and thus it prefers deletion haplotypes when reads end inside STRs. In #4858 we broke near-ties in favor of the reference; this PR fixes the case where two alt haplotypes share a SNV and one of them has a spurious deletion. One important sanity check was that when I set `cigarTerm` to zero in `AssemblyBasedCallerUtils.java` no tests broke. This means that the refactoring needed to set up the change didn't affect behavior. I looked at most of the sites where `PL`s and/or `DP`s changed in the integration test vcfs and in every case the difference was from a fake deletion that this PR fixed. I also went through the diff of the bamouts in IGV and found the same thing. Finally, the changes to test vcfs in `GenotypeGVCFsIntegrationTest` and `GenomicsDBImporterIntegrationTest` are a consequence of changes to the `HaplotypeCallerIntegrationTest` vcfs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5359
https://github.com/broadinstitute/gatk/pull/5359:441,Safety,sanity check,sanity check,441,"@ldgauthier this finishes what we started in #4858 and is necessary for the pileup-calls-on-bamouts MC3 validation. The cause is the same, in that Pair-HMM has a tiny bias in favor of shorter haplotypes and thus it prefers deletion haplotypes when reads end inside STRs. In #4858 we broke near-ties in favor of the reference; this PR fixes the case where two alt haplotypes share a SNV and one of them has a spurious deletion. One important sanity check was that when I set `cigarTerm` to zero in `AssemblyBasedCallerUtils.java` no tests broke. This means that the refactoring needed to set up the change didn't affect behavior. I looked at most of the sites where `PL`s and/or `DP`s changed in the integration test vcfs and in every case the difference was from a fake deletion that this PR fixed. I also went through the diff of the bamouts in IGV and found the same thing. Finally, the changes to test vcfs in `GenotypeGVCFsIntegrationTest` and `GenomicsDBImporterIntegrationTest` are a consequence of changes to the `HaplotypeCallerIntegrationTest` vcfs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5359
https://github.com/broadinstitute/gatk/pull/5359:104,Security,validat,validation,104,"@ldgauthier this finishes what we started in #4858 and is necessary for the pileup-calls-on-bamouts MC3 validation. The cause is the same, in that Pair-HMM has a tiny bias in favor of shorter haplotypes and thus it prefers deletion haplotypes when reads end inside STRs. In #4858 we broke near-ties in favor of the reference; this PR fixes the case where two alt haplotypes share a SNV and one of them has a spurious deletion. One important sanity check was that when I set `cigarTerm` to zero in `AssemblyBasedCallerUtils.java` no tests broke. This means that the refactoring needed to set up the change didn't affect behavior. I looked at most of the sites where `PL`s and/or `DP`s changed in the integration test vcfs and in every case the difference was from a fake deletion that this PR fixed. I also went through the diff of the bamouts in IGV and found the same thing. Finally, the changes to test vcfs in `GenotypeGVCFsIntegrationTest` and `GenomicsDBImporterIntegrationTest` are a consequence of changes to the `HaplotypeCallerIntegrationTest` vcfs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5359
https://github.com/broadinstitute/gatk/pull/5359:532,Testability,test,tests,532,"@ldgauthier this finishes what we started in #4858 and is necessary for the pileup-calls-on-bamouts MC3 validation. The cause is the same, in that Pair-HMM has a tiny bias in favor of shorter haplotypes and thus it prefers deletion haplotypes when reads end inside STRs. In #4858 we broke near-ties in favor of the reference; this PR fixes the case where two alt haplotypes share a SNV and one of them has a spurious deletion. One important sanity check was that when I set `cigarTerm` to zero in `AssemblyBasedCallerUtils.java` no tests broke. This means that the refactoring needed to set up the change didn't affect behavior. I looked at most of the sites where `PL`s and/or `DP`s changed in the integration test vcfs and in every case the difference was from a fake deletion that this PR fixed. I also went through the diff of the bamouts in IGV and found the same thing. Finally, the changes to test vcfs in `GenotypeGVCFsIntegrationTest` and `GenomicsDBImporterIntegrationTest` are a consequence of changes to the `HaplotypeCallerIntegrationTest` vcfs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5359
https://github.com/broadinstitute/gatk/pull/5359:711,Testability,test,test,711,"@ldgauthier this finishes what we started in #4858 and is necessary for the pileup-calls-on-bamouts MC3 validation. The cause is the same, in that Pair-HMM has a tiny bias in favor of shorter haplotypes and thus it prefers deletion haplotypes when reads end inside STRs. In #4858 we broke near-ties in favor of the reference; this PR fixes the case where two alt haplotypes share a SNV and one of them has a spurious deletion. One important sanity check was that when I set `cigarTerm` to zero in `AssemblyBasedCallerUtils.java` no tests broke. This means that the refactoring needed to set up the change didn't affect behavior. I looked at most of the sites where `PL`s and/or `DP`s changed in the integration test vcfs and in every case the difference was from a fake deletion that this PR fixed. I also went through the diff of the bamouts in IGV and found the same thing. Finally, the changes to test vcfs in `GenotypeGVCFsIntegrationTest` and `GenomicsDBImporterIntegrationTest` are a consequence of changes to the `HaplotypeCallerIntegrationTest` vcfs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5359
https://github.com/broadinstitute/gatk/pull/5359:900,Testability,test,test,900,"@ldgauthier this finishes what we started in #4858 and is necessary for the pileup-calls-on-bamouts MC3 validation. The cause is the same, in that Pair-HMM has a tiny bias in favor of shorter haplotypes and thus it prefers deletion haplotypes when reads end inside STRs. In #4858 we broke near-ties in favor of the reference; this PR fixes the case where two alt haplotypes share a SNV and one of them has a spurious deletion. One important sanity check was that when I set `cigarTerm` to zero in `AssemblyBasedCallerUtils.java` no tests broke. This means that the refactoring needed to set up the change didn't affect behavior. I looked at most of the sites where `PL`s and/or `DP`s changed in the integration test vcfs and in every case the difference was from a fake deletion that this PR fixed. I also went through the diff of the bamouts in IGV and found the same thing. Finally, the changes to test vcfs in `GenotypeGVCFsIntegrationTest` and `GenomicsDBImporterIntegrationTest` are a consequence of changes to the `HaplotypeCallerIntegrationTest` vcfs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5359
https://github.com/broadinstitute/gatk/issues/5360:19,Availability,error,error,19,I'm getting a type error here because L278 expects a string for `POSSIBLE_GERMLINE` but `pandas.read_csv` returns a `float64` for this column.; https://github.com/broadinstitute/gatk/blob/d4db277dfa1a9c13188644fd28616249061f8704/scripts/unsupported/combine_tracks_postprocessing_cnv/combine_tracks.wdl#L277-L278,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5360
https://github.com/broadinstitute/gatk/pull/5361:412,Deployability,Update,Updated,412,"- Now will detect variants on mitochondrial contigs and will use the; correct, alternate coding sequence to create protein change strings for; such variants.; - Added MT sequences to Gencode data source.; - Added tests for MT protein change strings.; - Now `FuncotatorUtils::getMitochondrialAminoAcidByCodon` has more; complete tests and handles special cases for known initiation site; differences by genus.; - Updated scripts to detect the directory in which the scripts are run.; - Added MT variants to integration tests.; - Added MT genes to gencode testing data source. Fixes #4863",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5361
https://github.com/broadinstitute/gatk/pull/5361:506,Deployability,integrat,integration,506,"- Now will detect variants on mitochondrial contigs and will use the; correct, alternate coding sequence to create protein change strings for; such variants.; - Added MT sequences to Gencode data source.; - Added tests for MT protein change strings.; - Now `FuncotatorUtils::getMitochondrialAminoAcidByCodon` has more; complete tests and handles special cases for known initiation site; differences by genus.; - Updated scripts to detect the directory in which the scripts are run.; - Added MT variants to integration tests.; - Added MT genes to gencode testing data source. Fixes #4863",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5361
https://github.com/broadinstitute/gatk/pull/5361:506,Integrability,integrat,integration,506,"- Now will detect variants on mitochondrial contigs and will use the; correct, alternate coding sequence to create protein change strings for; such variants.; - Added MT sequences to Gencode data source.; - Added tests for MT protein change strings.; - Now `FuncotatorUtils::getMitochondrialAminoAcidByCodon` has more; complete tests and handles special cases for known initiation site; differences by genus.; - Updated scripts to detect the directory in which the scripts are run.; - Added MT variants to integration tests.; - Added MT genes to gencode testing data source. Fixes #4863",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5361
https://github.com/broadinstitute/gatk/pull/5361:11,Safety,detect,detect,11,"- Now will detect variants on mitochondrial contigs and will use the; correct, alternate coding sequence to create protein change strings for; such variants.; - Added MT sequences to Gencode data source.; - Added tests for MT protein change strings.; - Now `FuncotatorUtils::getMitochondrialAminoAcidByCodon` has more; complete tests and handles special cases for known initiation site; differences by genus.; - Updated scripts to detect the directory in which the scripts are run.; - Added MT variants to integration tests.; - Added MT genes to gencode testing data source. Fixes #4863",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5361
https://github.com/broadinstitute/gatk/pull/5361:431,Safety,detect,detect,431,"- Now will detect variants on mitochondrial contigs and will use the; correct, alternate coding sequence to create protein change strings for; such variants.; - Added MT sequences to Gencode data source.; - Added tests for MT protein change strings.; - Now `FuncotatorUtils::getMitochondrialAminoAcidByCodon` has more; complete tests and handles special cases for known initiation site; differences by genus.; - Updated scripts to detect the directory in which the scripts are run.; - Added MT variants to integration tests.; - Added MT genes to gencode testing data source. Fixes #4863",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5361
https://github.com/broadinstitute/gatk/pull/5361:213,Testability,test,tests,213,"- Now will detect variants on mitochondrial contigs and will use the; correct, alternate coding sequence to create protein change strings for; such variants.; - Added MT sequences to Gencode data source.; - Added tests for MT protein change strings.; - Now `FuncotatorUtils::getMitochondrialAminoAcidByCodon` has more; complete tests and handles special cases for known initiation site; differences by genus.; - Updated scripts to detect the directory in which the scripts are run.; - Added MT variants to integration tests.; - Added MT genes to gencode testing data source. Fixes #4863",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5361
https://github.com/broadinstitute/gatk/pull/5361:328,Testability,test,tests,328,"- Now will detect variants on mitochondrial contigs and will use the; correct, alternate coding sequence to create protein change strings for; such variants.; - Added MT sequences to Gencode data source.; - Added tests for MT protein change strings.; - Now `FuncotatorUtils::getMitochondrialAminoAcidByCodon` has more; complete tests and handles special cases for known initiation site; differences by genus.; - Updated scripts to detect the directory in which the scripts are run.; - Added MT variants to integration tests.; - Added MT genes to gencode testing data source. Fixes #4863",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5361
https://github.com/broadinstitute/gatk/pull/5361:518,Testability,test,tests,518,"- Now will detect variants on mitochondrial contigs and will use the; correct, alternate coding sequence to create protein change strings for; such variants.; - Added MT sequences to Gencode data source.; - Added tests for MT protein change strings.; - Now `FuncotatorUtils::getMitochondrialAminoAcidByCodon` has more; complete tests and handles special cases for known initiation site; differences by genus.; - Updated scripts to detect the directory in which the scripts are run.; - Added MT variants to integration tests.; - Added MT genes to gencode testing data source. Fixes #4863",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5361
https://github.com/broadinstitute/gatk/pull/5361:554,Testability,test,testing,554,"- Now will detect variants on mitochondrial contigs and will use the; correct, alternate coding sequence to create protein change strings for; such variants.; - Added MT sequences to Gencode data source.; - Added tests for MT protein change strings.; - Now `FuncotatorUtils::getMitochondrialAminoAcidByCodon` has more; complete tests and handles special cases for known initiation site; differences by genus.; - Updated scripts to detect the directory in which the scripts are run.; - Added MT variants to integration tests.; - Added MT genes to gencode testing data source. Fixes #4863",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5361
https://github.com/broadinstitute/gatk/issues/5362:105,Safety,avoid,avoid,105,"We need to help users help themselves either with better checks in tools or with better documentation to avoid the discrepancies observed in this thread, whose answer is recapitulated below. ---; Hi @obigriffith,. I am using GATK v4.0.11.0 and I also see what you are seeing. I've been taking an Android App development course since January (in my free time of course), and I've learned that with multiple expressions, sometimes the Java programming language needs help in parsing expressions. That is, we need to help the tool demarcate where an expression begins and ends. **1. no filtering expected works as expected (but this is misleading)**; ```; --filter-expression ""QD < 2.0 || FS > 60.0 || MQ < 40.0 || MQRandSum < -12.5 || ReadPosRankSum < -8.0 || SOR > 3.0""; ```; ![](https://us.v-cdn.net/5019796/uploads/editor/a8/q0yjdx55d0fz.png """"). **2. should be filtered based on SOR (at 0.608) but is not**; ```; --filter-expression ""QD < 2.0 || FS > 60.0 || MQ < 40.0 || MQRandSum < -12.5 || ReadPosRankSum < -8.0 || SOR > 0.5"" ; ```; ![](https://us.v-cdn.net/5019796/uploads/editor/2s/tft38knytdib.png """"). **3. Using parentheses around each expression allows SOR (and presumably other expressions) to be read correctly**; ```; --filter-expression ""(QD < 2.0) || (FS > 60.0) || (MQ < 40.0) || (MQRankSum < -12.5) || (ReadPosRankSum < -8.0) || (SOR > 0.5)""; ```; This will allow the tool to read the SOR expression unambiguously. Here are results from my testing:; ![](https://us.v-cdn.net/5019796/uploads/editor/o4/5939fiysxmr4.png """"). **4. Providing each expression as a separate parameter also allows SOR (and others) to be read correctly and also provides additional insight**; Separate out each condition into individual filter expressions:; ```; --filter-expression ""QD < 2.0"" --filter-name ""QDlessthan2"" --filter-expression ""FS > 60.0"" --filter-name ""FSgreaterthan60"" --filter-expression ""MQ < 90.0"" --filter-name ""MQlessthan90"" --filter-expression ""MQRankSum < -12.5"" --filter-name ""MQRank",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5362
https://github.com/broadinstitute/gatk/issues/5362:1458,Testability,test,testing,1458,"e needs help in parsing expressions. That is, we need to help the tool demarcate where an expression begins and ends. **1. no filtering expected works as expected (but this is misleading)**; ```; --filter-expression ""QD < 2.0 || FS > 60.0 || MQ < 40.0 || MQRandSum < -12.5 || ReadPosRankSum < -8.0 || SOR > 3.0""; ```; ![](https://us.v-cdn.net/5019796/uploads/editor/a8/q0yjdx55d0fz.png """"). **2. should be filtered based on SOR (at 0.608) but is not**; ```; --filter-expression ""QD < 2.0 || FS > 60.0 || MQ < 40.0 || MQRandSum < -12.5 || ReadPosRankSum < -8.0 || SOR > 0.5"" ; ```; ![](https://us.v-cdn.net/5019796/uploads/editor/2s/tft38knytdib.png """"). **3. Using parentheses around each expression allows SOR (and presumably other expressions) to be read correctly**; ```; --filter-expression ""(QD < 2.0) || (FS > 60.0) || (MQ < 40.0) || (MQRankSum < -12.5) || (ReadPosRankSum < -8.0) || (SOR > 0.5)""; ```; This will allow the tool to read the SOR expression unambiguously. Here are results from my testing:; ![](https://us.v-cdn.net/5019796/uploads/editor/o4/5939fiysxmr4.png """"). **4. Providing each expression as a separate parameter also allows SOR (and others) to be read correctly and also provides additional insight**; Separate out each condition into individual filter expressions:; ```; --filter-expression ""QD < 2.0"" --filter-name ""QDlessthan2"" --filter-expression ""FS > 60.0"" --filter-name ""FSgreaterthan60"" --filter-expression ""MQ < 90.0"" --filter-name ""MQlessthan90"" --filter-expression ""MQRankSum < -12.5"" --filter-name ""MQRankSumlessthannegative12.5"" --filter-expression ""ReadPosRankSum < -8.0"" --filter-name ""ReadPosRankSumlessthannegative8"" --filter-expression ""SOR > 0.5"" --filter-name ""SORgreaterthan0.5""; ```; This gives you additional resolution into what was the condition that triggered the filtering. Here are the results from my testing:; ![](https://us.v-cdn.net/5019796/uploads/editor/w7/ftad81b6bcq0.png """"). ## So be sure to either use parentheses around each expressio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5362
https://github.com/broadinstitute/gatk/issues/5362:2314,Testability,test,testing,2314,"ding)**; ```; --filter-expression ""QD < 2.0 || FS > 60.0 || MQ < 40.0 || MQRandSum < -12.5 || ReadPosRankSum < -8.0 || SOR > 3.0""; ```; ![](https://us.v-cdn.net/5019796/uploads/editor/a8/q0yjdx55d0fz.png """"). **2. should be filtered based on SOR (at 0.608) but is not**; ```; --filter-expression ""QD < 2.0 || FS > 60.0 || MQ < 40.0 || MQRandSum < -12.5 || ReadPosRankSum < -8.0 || SOR > 0.5"" ; ```; ![](https://us.v-cdn.net/5019796/uploads/editor/2s/tft38knytdib.png """"). **3. Using parentheses around each expression allows SOR (and presumably other expressions) to be read correctly**; ```; --filter-expression ""(QD < 2.0) || (FS > 60.0) || (MQ < 40.0) || (MQRankSum < -12.5) || (ReadPosRankSum < -8.0) || (SOR > 0.5)""; ```; This will allow the tool to read the SOR expression unambiguously. Here are results from my testing:; ![](https://us.v-cdn.net/5019796/uploads/editor/o4/5939fiysxmr4.png """"). **4. Providing each expression as a separate parameter also allows SOR (and others) to be read correctly and also provides additional insight**; Separate out each condition into individual filter expressions:; ```; --filter-expression ""QD < 2.0"" --filter-name ""QDlessthan2"" --filter-expression ""FS > 60.0"" --filter-name ""FSgreaterthan60"" --filter-expression ""MQ < 90.0"" --filter-name ""MQlessthan90"" --filter-expression ""MQRankSum < -12.5"" --filter-name ""MQRankSumlessthannegative12.5"" --filter-expression ""ReadPosRankSum < -8.0"" --filter-name ""ReadPosRankSumlessthannegative8"" --filter-expression ""SOR > 0.5"" --filter-name ""SORgreaterthan0.5""; ```; This gives you additional resolution into what was the condition that triggered the filtering. Here are the results from my testing:; ![](https://us.v-cdn.net/5019796/uploads/editor/w7/ftad81b6bcq0.png """"). ## So be sure to either use parentheses around each expression or to express conditions independently. . This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/53310#Comment_53310",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5362
https://github.com/broadinstitute/gatk/issues/5362:379,Usability,learn,learned,379,"We need to help users help themselves either with better checks in tools or with better documentation to avoid the discrepancies observed in this thread, whose answer is recapitulated below. ---; Hi @obigriffith,. I am using GATK v4.0.11.0 and I also see what you are seeing. I've been taking an Android App development course since January (in my free time of course), and I've learned that with multiple expressions, sometimes the Java programming language needs help in parsing expressions. That is, we need to help the tool demarcate where an expression begins and ends. **1. no filtering expected works as expected (but this is misleading)**; ```; --filter-expression ""QD < 2.0 || FS > 60.0 || MQ < 40.0 || MQRandSum < -12.5 || ReadPosRankSum < -8.0 || SOR > 3.0""; ```; ![](https://us.v-cdn.net/5019796/uploads/editor/a8/q0yjdx55d0fz.png """"). **2. should be filtered based on SOR (at 0.608) but is not**; ```; --filter-expression ""QD < 2.0 || FS > 60.0 || MQ < 40.0 || MQRandSum < -12.5 || ReadPosRankSum < -8.0 || SOR > 0.5"" ; ```; ![](https://us.v-cdn.net/5019796/uploads/editor/2s/tft38knytdib.png """"). **3. Using parentheses around each expression allows SOR (and presumably other expressions) to be read correctly**; ```; --filter-expression ""(QD < 2.0) || (FS > 60.0) || (MQ < 40.0) || (MQRankSum < -12.5) || (ReadPosRankSum < -8.0) || (SOR > 0.5)""; ```; This will allow the tool to read the SOR expression unambiguously. Here are results from my testing:; ![](https://us.v-cdn.net/5019796/uploads/editor/o4/5939fiysxmr4.png """"). **4. Providing each expression as a separate parameter also allows SOR (and others) to be read correctly and also provides additional insight**; Separate out each condition into individual filter expressions:; ```; --filter-expression ""QD < 2.0"" --filter-name ""QDlessthan2"" --filter-expression ""FS > 60.0"" --filter-name ""FSgreaterthan60"" --filter-expression ""MQ < 90.0"" --filter-name ""MQlessthan90"" --filter-expression ""MQRankSum < -12.5"" --filter-name ""MQRank",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5362
https://github.com/broadinstitute/gatk/issues/5364:358,Availability,robust,robust,358,## Feature request. ### Tool(s) or class(es) involved; _GencodeFuncotationFactory_. ### Description; Currently the mitochondrial contig is determined using a simple string comparison by contig name. ; This determination is then used to decode the mitochondrial protein sequence (which gets decoded differently than the normal gene sequences). Make this more robust by detecting the mito contig based on the reference used.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5364
https://github.com/broadinstitute/gatk/issues/5364:368,Safety,detect,detecting,368,## Feature request. ### Tool(s) or class(es) involved; _GencodeFuncotationFactory_. ### Description; Currently the mitochondrial contig is determined using a simple string comparison by contig name. ; This determination is then used to decode the mitochondrial protein sequence (which gets decoded differently than the normal gene sequences). Make this more robust by detecting the mito contig based on the reference used.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5364
https://github.com/broadinstitute/gatk/issues/5364:158,Usability,simpl,simple,158,## Feature request. ### Tool(s) or class(es) involved; _GencodeFuncotationFactory_. ### Description; Currently the mitochondrial contig is determined using a simple string comparison by contig name. ; This determination is then used to decode the mitochondrial protein sequence (which gets decoded differently than the normal gene sequences). Make this more robust by detecting the mito contig based on the reference used.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5364
https://github.com/broadinstitute/gatk/issues/5366:124,Deployability,release,release,124,"## Bug Report. ### Affected tool(s) or class(es); _GencodeFuncotationFactory_. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; When a trouble transcript comes up for an allele pairl, `GencodeFuncotationFactory::createFuncotationsHelper` does not create a default annotation for the allele pair. This will cause the parsing of funcotations to fail because not all the alleles are represented in the funcotation list. See the `todo` in `GencodeFuncotationFactory::createFuncotationsHelper`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5366
https://github.com/broadinstitute/gatk/issues/5366:194,Testability,test,test,194,"## Bug Report. ### Affected tool(s) or class(es); _GencodeFuncotationFactory_. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; When a trouble transcript comes up for an allele pairl, `GencodeFuncotationFactory::createFuncotationsHelper` does not create a default annotation for the allele pair. This will cause the parsing of funcotations to fail because not all the alleles are represented in the funcotation list. See the `todo` in `GencodeFuncotationFactory::createFuncotationsHelper`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5366
https://github.com/broadinstitute/gatk/issues/5368:109,Deployability,release,release,109,"## Bug Report. ### Affected tool(s) or class(es); _Funcotator_. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; When annotating a VCF, if the VCF already contains funcotations Funcotator will add a new funcotator line to the header. This will cause the parser to fail because it will not be able to get the correct line from the header. This is a bit of a pathological case (I can't currently see a good reason to funcotate a VCF twice), but since this behavior is valid it should be accounted for. The primary issue is how to resolve the two funcotation sets. Ideally we would leave them both in and somehow version them (to preserve all the information). Alternatively we can append to the existing funcotation list. This second method will likely involve a lot of work and probably isn't worth it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5368
https://github.com/broadinstitute/gatk/issues/5368:179,Testability,test,test,179,"## Bug Report. ### Affected tool(s) or class(es); _Funcotator_. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; When annotating a VCF, if the VCF already contains funcotations Funcotator will add a new funcotator line to the header. This will cause the parser to fail because it will not be able to get the correct line from the header. This is a bit of a pathological case (I can't currently see a good reason to funcotate a VCF twice), but since this behavior is valid it should be accounted for. The primary issue is how to resolve the two funcotation sets. Ideally we would leave them both in and somehow version them (to preserve all the information). Alternatively we can append to the existing funcotation list. This second method will likely involve a lot of work and probably isn't worth it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5368
https://github.com/broadinstitute/gatk/issues/5369:181,Availability,error,error,181,"build_docker.sh creates unzippedJar and testsJar, but it does not remove them and it fails as a result in subsequent runs. . I ran ./build_socker.sh -e <GIT LOG HASH> and I got the error message ; mv: rename ./build/bundle-files-collected to ./unzippedJar/bundle-files-collected: Directory not empty. Only after removing unzippedJar and testJar could I build the image again successfully.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5369
https://github.com/broadinstitute/gatk/issues/5369:187,Integrability,message,message,187,"build_docker.sh creates unzippedJar and testsJar, but it does not remove them and it fails as a result in subsequent runs. . I ran ./build_socker.sh -e <GIT LOG HASH> and I got the error message ; mv: rename ./build/bundle-files-collected to ./unzippedJar/bundle-files-collected: Directory not empty. Only after removing unzippedJar and testJar could I build the image again successfully.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5369
https://github.com/broadinstitute/gatk/issues/5369:161,Security,HASH,HASH,161,"build_docker.sh creates unzippedJar and testsJar, but it does not remove them and it fails as a result in subsequent runs. . I ran ./build_socker.sh -e <GIT LOG HASH> and I got the error message ; mv: rename ./build/bundle-files-collected to ./unzippedJar/bundle-files-collected: Directory not empty. Only after removing unzippedJar and testJar could I build the image again successfully.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5369
https://github.com/broadinstitute/gatk/issues/5369:40,Testability,test,testsJar,40,"build_docker.sh creates unzippedJar and testsJar, but it does not remove them and it fails as a result in subsequent runs. . I ran ./build_socker.sh -e <GIT LOG HASH> and I got the error message ; mv: rename ./build/bundle-files-collected to ./unzippedJar/bundle-files-collected: Directory not empty. Only after removing unzippedJar and testJar could I build the image again successfully.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5369
https://github.com/broadinstitute/gatk/issues/5369:157,Testability,LOG,LOG,157,"build_docker.sh creates unzippedJar and testsJar, but it does not remove them and it fails as a result in subsequent runs. . I ran ./build_socker.sh -e <GIT LOG HASH> and I got the error message ; mv: rename ./build/bundle-files-collected to ./unzippedJar/bundle-files-collected: Directory not empty. Only after removing unzippedJar and testJar could I build the image again successfully.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5369
https://github.com/broadinstitute/gatk/issues/5369:337,Testability,test,testJar,337,"build_docker.sh creates unzippedJar and testsJar, but it does not remove them and it fails as a result in subsequent runs. . I ran ./build_socker.sh -e <GIT LOG HASH> and I got the error message ; mv: rename ./build/bundle-files-collected to ./unzippedJar/bundle-files-collected: Directory not empty. Only after removing unzippedJar and testJar could I build the image again successfully.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5369
https://github.com/broadinstitute/gatk/pull/5371:250,Safety,avoid,avoids,250,"Closes #4290. Since the Mann-Whitney U statistic is always integer or half-integer, we don't need to store a histogram of `Double`s, which causes issues on some JVMs. Instead we can multiply U by two and store an integer key for the histogram, which avoids the issue. @droazen I'll assign you and also @meganshand to sign off on the statistics.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5371
https://github.com/broadinstitute/gatk/issues/5372:270,Availability,error,error,270,"Hi,. I am using GATK version 4.0.3.0 using a shell script. I am working with a haploid organism. I created a single sample BAM file by first aligning PE reads using HISAT2; samtools sort .SAM to .BAM files; then marked duplicate reads using picard. . I keep getting the error message:. A USER ERROR has occurred: Argument --emitRefConfidence has a bad value: Can only be used in single sample mode currently. Use the sample_name argument to run on a single sample out of a multi-sample BAM file. ```; time gatk --java-options ""-Xmx4g"" HaplotypeCaller \ ; -R reference.fa \ ; -I sample1.md.bam \ ; -O sample1.raw.g.vcf \; -ERC GVCF. ```. What am I doing wrong? . ```; Using GATK jar /usr/local/apps/eb/GATK/4.0.3.0-Java-1.8.0_144/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /usr/local/apps/eb/GATK/4.0.3.0-Java-1.8.0_144/gatk-package-4.0.3.0-local.jar HaplotypeCaller -R Af293.41.fa -I eAF01_md.bam -O eAF01.raw.g.vcf.gz -ERC GVCF; 22:25:20.396 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/apps/eb/GATK/4.0.3.0-Java-1.8.0_144/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 22:25:20.633 INFO HaplotypeCaller - ------------------------------------------------------------; 22:25:20.634 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.3.0; 22:25:20.634 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:25:20.635 INFO HaplotypeCaller - Executing as sek53827@n583.ecompute on Linux v3.10.0-229.20.1.el7.x86_64 amd64; 22:25:20.635 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_144-b01; 22:25:20.635 INFO HaplotypeCaller - Start Date/Time: October 29, 2018 10:25:20 PM EDT; 22:25:20.635 INFO HaplotypeCaller - ------------------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5372
https://github.com/broadinstitute/gatk/issues/5372:293,Availability,ERROR,ERROR,293,"Hi,. I am using GATK version 4.0.3.0 using a shell script. I am working with a haploid organism. I created a single sample BAM file by first aligning PE reads using HISAT2; samtools sort .SAM to .BAM files; then marked duplicate reads using picard. . I keep getting the error message:. A USER ERROR has occurred: Argument --emitRefConfidence has a bad value: Can only be used in single sample mode currently. Use the sample_name argument to run on a single sample out of a multi-sample BAM file. ```; time gatk --java-options ""-Xmx4g"" HaplotypeCaller \ ; -R reference.fa \ ; -I sample1.md.bam \ ; -O sample1.raw.g.vcf \; -ERC GVCF. ```. What am I doing wrong? . ```; Using GATK jar /usr/local/apps/eb/GATK/4.0.3.0-Java-1.8.0_144/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /usr/local/apps/eb/GATK/4.0.3.0-Java-1.8.0_144/gatk-package-4.0.3.0-local.jar HaplotypeCaller -R Af293.41.fa -I eAF01_md.bam -O eAF01.raw.g.vcf.gz -ERC GVCF; 22:25:20.396 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/apps/eb/GATK/4.0.3.0-Java-1.8.0_144/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 22:25:20.633 INFO HaplotypeCaller - ------------------------------------------------------------; 22:25:20.634 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.3.0; 22:25:20.634 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:25:20.635 INFO HaplotypeCaller - Executing as sek53827@n583.ecompute on Linux v3.10.0-229.20.1.el7.x86_64 amd64; 22:25:20.635 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_144-b01; 22:25:20.635 INFO HaplotypeCaller - Start Date/Time: October 29, 2018 10:25:20 PM EDT; 22:25:20.635 INFO HaplotypeCaller - ------------------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5372
https://github.com/broadinstitute/gatk/issues/5372:3361,Availability,down,down,3361,"ler - ------------------------------------------------------------; 22:25:20.635 INFO HaplotypeCaller - HTSJDK Version: 2.14.3; 22:25:20.635 INFO HaplotypeCaller - Picard Version: 2.17.2; 22:25:20.635 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:25:20.635 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:25:20.636 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:25:20.636 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:25:20.636 INFO HaplotypeCaller - Deflater: IntelDeflater; 22:25:20.636 INFO HaplotypeCaller - Inflater: IntelInflater; 22:25:20.636 INFO HaplotypeCaller - GCS max retries/reopens: 20; 22:25:20.636 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 22:25:20.636 INFO HaplotypeCaller - Initializing engine; 22:25:21.061 INFO HaplotypeCaller - Done initializing engine; 22:25:21.069 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 22:25:21.069 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 22:25:21.070 INFO HaplotypeCaller - Shutting down engine; [October 29, 2018 10:25:21 PM EDT] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=1506344960; USAGE: HaplotypeCaller [arguments]. Call germline SNPs and indels via local re-assembly of haplotypes; Version:4.0.3.0. ***********************************************************************. A USER ERROR has occurred: Argument --emitRefConfidence has a bad value: Can only be used in single sample mode currently. Use the sample_name argument to run on a single sample out of a multi-sample BAM file. ***********************************************************************. ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5372
https://github.com/broadinstitute/gatk/issues/5372:3754,Availability,ERROR,ERROR,3754,"ler - ------------------------------------------------------------; 22:25:20.635 INFO HaplotypeCaller - HTSJDK Version: 2.14.3; 22:25:20.635 INFO HaplotypeCaller - Picard Version: 2.17.2; 22:25:20.635 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:25:20.635 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:25:20.636 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:25:20.636 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:25:20.636 INFO HaplotypeCaller - Deflater: IntelDeflater; 22:25:20.636 INFO HaplotypeCaller - Inflater: IntelInflater; 22:25:20.636 INFO HaplotypeCaller - GCS max retries/reopens: 20; 22:25:20.636 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 22:25:20.636 INFO HaplotypeCaller - Initializing engine; 22:25:21.061 INFO HaplotypeCaller - Done initializing engine; 22:25:21.069 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 22:25:21.069 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 22:25:21.070 INFO HaplotypeCaller - Shutting down engine; [October 29, 2018 10:25:21 PM EDT] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=1506344960; USAGE: HaplotypeCaller [arguments]. Call germline SNPs and indels via local re-assembly of haplotypes; Version:4.0.3.0. ***********************************************************************. A USER ERROR has occurred: Argument --emitRefConfidence has a bad value: Can only be used in single sample mode currently. Use the sample_name argument to run on a single sample out of a multi-sample BAM file. ***********************************************************************. ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5372
https://github.com/broadinstitute/gatk/issues/5372:2819,Deployability,patch,patch,2819,"635 INFO HaplotypeCaller - Start Date/Time: October 29, 2018 10:25:20 PM EDT; 22:25:20.635 INFO HaplotypeCaller - ------------------------------------------------------------; 22:25:20.635 INFO HaplotypeCaller - ------------------------------------------------------------; 22:25:20.635 INFO HaplotypeCaller - HTSJDK Version: 2.14.3; 22:25:20.635 INFO HaplotypeCaller - Picard Version: 2.17.2; 22:25:20.635 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:25:20.635 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:25:20.636 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:25:20.636 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:25:20.636 INFO HaplotypeCaller - Deflater: IntelDeflater; 22:25:20.636 INFO HaplotypeCaller - Inflater: IntelInflater; 22:25:20.636 INFO HaplotypeCaller - GCS max retries/reopens: 20; 22:25:20.636 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 22:25:20.636 INFO HaplotypeCaller - Initializing engine; 22:25:21.061 INFO HaplotypeCaller - Done initializing engine; 22:25:21.069 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 22:25:21.069 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 22:25:21.070 INFO HaplotypeCaller - Shutting down engine; [October 29, 2018 10:25:21 PM EDT] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=1506344960; USAGE: HaplotypeCaller [arguments]. Call germline SNPs and indels via local re-assembly of haplotypes; Version:4.0.3.0. ***********************************************************************. A USER ERROR has occurred: Argument --emitRefConfidence has a bad value: Can onl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5372
https://github.com/broadinstitute/gatk/issues/5372:276,Integrability,message,message,276,"Hi,. I am using GATK version 4.0.3.0 using a shell script. I am working with a haploid organism. I created a single sample BAM file by first aligning PE reads using HISAT2; samtools sort .SAM to .BAM files; then marked duplicate reads using picard. . I keep getting the error message:. A USER ERROR has occurred: Argument --emitRefConfidence has a bad value: Can only be used in single sample mode currently. Use the sample_name argument to run on a single sample out of a multi-sample BAM file. ```; time gatk --java-options ""-Xmx4g"" HaplotypeCaller \ ; -R reference.fa \ ; -I sample1.md.bam \ ; -O sample1.raw.g.vcf \; -ERC GVCF. ```. What am I doing wrong? . ```; Using GATK jar /usr/local/apps/eb/GATK/4.0.3.0-Java-1.8.0_144/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /usr/local/apps/eb/GATK/4.0.3.0-Java-1.8.0_144/gatk-package-4.0.3.0-local.jar HaplotypeCaller -R Af293.41.fa -I eAF01_md.bam -O eAF01.raw.g.vcf.gz -ERC GVCF; 22:25:20.396 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/apps/eb/GATK/4.0.3.0-Java-1.8.0_144/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 22:25:20.633 INFO HaplotypeCaller - ------------------------------------------------------------; 22:25:20.634 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.3.0; 22:25:20.634 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:25:20.635 INFO HaplotypeCaller - Executing as sek53827@n583.ecompute on Linux v3.10.0-229.20.1.el7.x86_64 amd64; 22:25:20.635 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_144-b01; 22:25:20.635 INFO HaplotypeCaller - Start Date/Time: October 29, 2018 10:25:20 PM EDT; 22:25:20.635 INFO HaplotypeCaller - ------------------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5372
https://github.com/broadinstitute/gatk/issues/5372:1141,Performance,Load,Loading,1141,"t aligning PE reads using HISAT2; samtools sort .SAM to .BAM files; then marked duplicate reads using picard. . I keep getting the error message:. A USER ERROR has occurred: Argument --emitRefConfidence has a bad value: Can only be used in single sample mode currently. Use the sample_name argument to run on a single sample out of a multi-sample BAM file. ```; time gatk --java-options ""-Xmx4g"" HaplotypeCaller \ ; -R reference.fa \ ; -I sample1.md.bam \ ; -O sample1.raw.g.vcf \; -ERC GVCF. ```. What am I doing wrong? . ```; Using GATK jar /usr/local/apps/eb/GATK/4.0.3.0-Java-1.8.0_144/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /usr/local/apps/eb/GATK/4.0.3.0-Java-1.8.0_144/gatk-package-4.0.3.0-local.jar HaplotypeCaller -R Af293.41.fa -I eAF01_md.bam -O eAF01.raw.g.vcf.gz -ERC GVCF; 22:25:20.396 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/apps/eb/GATK/4.0.3.0-Java-1.8.0_144/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 22:25:20.633 INFO HaplotypeCaller - ------------------------------------------------------------; 22:25:20.634 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.3.0; 22:25:20.634 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:25:20.635 INFO HaplotypeCaller - Executing as sek53827@n583.ecompute on Linux v3.10.0-229.20.1.el7.x86_64 amd64; 22:25:20.635 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_144-b01; 22:25:20.635 INFO HaplotypeCaller - Start Date/Time: October 29, 2018 10:25:20 PM EDT; 22:25:20.635 INFO HaplotypeCaller - ------------------------------------------------------------; 22:25:20.635 INFO HaplotypeCaller - ------------------------------------------------------------; 22:25:20.635 INFO HaplotypeCaller - HT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5372
https://github.com/broadinstitute/gatk/issues/5373:48,Performance,perform,perform,48,"## Feature request. I used gatk4 docker file to perform the germline CNV cohort analysis. I got the individual vcf file for each sample. But I cannot find the tool to combine the CNV, just like combining gvcf in SNV analysis. Is there any such tool that I missed? And, is there any visualization scripts I can use? Thank you so much.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5373
https://github.com/broadinstitute/gatk/issues/5375:138,Deployability,release,release,138,"## Bug Report. ### Affected tool(s) or class(es); _Funcotator_, _GencodeFuncotationFactory_. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; The transcript position in Funcotator is always being populated as a single integer value. While this is correct for SNPs, it should be populated as a range - `<START_POS>_<END_POS>` for events spanning more than 1 base.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5375
https://github.com/broadinstitute/gatk/issues/5375:208,Testability,test,test,208,"## Bug Report. ### Affected tool(s) or class(es); _Funcotator_, _GencodeFuncotationFactory_. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; The transcript position in Funcotator is always being populated as a single integer value. While this is correct for SNPs, it should be populated as a range - `<START_POS>_<END_POS>` for events spanning more than 1 base.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5375
https://github.com/broadinstitute/gatk/issues/5376:32,Testability,log,log,32,"The ELBO values reported in the log file are supposed to be a smoothed version of the raw ELBO across sliding window average. However, in practice, If you look at these smoothed values rather than the tracked tar file one, one would get a distort impression of how the ELBO converges. They consistently underestimate the raw value which suggests that this is indeed a bug. The example below is kind of mild ... I have seem far worse smoothing with bigger depressed parabolas. (as the one seen between iteration = 1000-1500). The black line is the raw ELBO and the blue lines is the ""smoothed"" ELBO reported in the log. ![elbosmooth](https://user-images.githubusercontent.com/791104/47748444-38578b80-dc61-11e8-8e7e-1ee21e2d22e7.png). **The biggest worry here is whether the bug in calculating that average ""smoothed"" ELBO means that the stats used for convergence (i.e. sigma and SNR) are also miscalculated.** In that is the case, It would probably result in postponing convergence thus it might have gone unnoticed as it would not have a negative effect in the results (unless there is some overfitting as a consequence).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5376
https://github.com/broadinstitute/gatk/issues/5376:614,Testability,log,log,614,"The ELBO values reported in the log file are supposed to be a smoothed version of the raw ELBO across sliding window average. However, in practice, If you look at these smoothed values rather than the tracked tar file one, one would get a distort impression of how the ELBO converges. They consistently underestimate the raw value which suggests that this is indeed a bug. The example below is kind of mild ... I have seem far worse smoothing with bigger depressed parabolas. (as the one seen between iteration = 1000-1500). The black line is the raw ELBO and the blue lines is the ""smoothed"" ELBO reported in the log. ![elbosmooth](https://user-images.githubusercontent.com/791104/47748444-38578b80-dc61-11e8-8e7e-1ee21e2d22e7.png). **The biggest worry here is whether the bug in calculating that average ""smoothed"" ELBO means that the stats used for convergence (i.e. sigma and SNR) are also miscalculated.** In that is the case, It would probably result in postponing convergence thus it might have gone unnoticed as it would not have a negative effect in the results (unless there is some overfitting as a consequence).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5376
https://github.com/broadinstitute/gatk/pull/5377:211,Deployability,release,released,211,"Note: This branch is still blocked on several changes in Picard (https://github.com/broadinstitute/picard/pull/1236, and possibly https://github.com/broadinstitute/picard/pull/1245), once those are resolved and released then this branch should hopefully get the stamp of approval from @takutosato. * Added optical/library duplicate marking of reads (note: this does not include library tagging); * Added the ability to remove reads from the output bam based on their duplicate marking status. Resolves #4675 ; Resolves #5377",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5377
https://github.com/broadinstitute/gatk/pull/5378:372,Deployability,update,updates,372,"NIO output support for SelectVariants. Tested like so:. ```; $ ./gatk SelectVariants \; --variant dbsnp_138.b37.excluding_sites_after_129.vcf \; --select-random-fraction 0.01 \; --output gs://mybucket/variants.vcf; $ gsutil ls -lh gs://mybucket/*.vcf; 23.38 MiB 2018-10-30T23:58:12Z gs://mybucket/variants.vcf; ```. Includes the required changes under the hood, plus test updates. This change also gives NIO support to **HaplotypeCaller**, so it can write its VCF to cloud storage. Also, fixes #2128.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5378
https://github.com/broadinstitute/gatk/pull/5378:39,Testability,Test,Tested,39,"NIO output support for SelectVariants. Tested like so:. ```; $ ./gatk SelectVariants \; --variant dbsnp_138.b37.excluding_sites_after_129.vcf \; --select-random-fraction 0.01 \; --output gs://mybucket/variants.vcf; $ gsutil ls -lh gs://mybucket/*.vcf; 23.38 MiB 2018-10-30T23:58:12Z gs://mybucket/variants.vcf; ```. Includes the required changes under the hood, plus test updates. This change also gives NIO support to **HaplotypeCaller**, so it can write its VCF to cloud storage. Also, fixes #2128.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5378
https://github.com/broadinstitute/gatk/pull/5378:367,Testability,test,test,367,"NIO output support for SelectVariants. Tested like so:. ```; $ ./gatk SelectVariants \; --variant dbsnp_138.b37.excluding_sites_after_129.vcf \; --select-random-fraction 0.01 \; --output gs://mybucket/variants.vcf; $ gsutil ls -lh gs://mybucket/*.vcf; 23.38 MiB 2018-10-30T23:58:12Z gs://mybucket/variants.vcf; ```. Includes the required changes under the hood, plus test updates. This change also gives NIO support to **HaplotypeCaller**, so it can write its VCF to cloud storage. Also, fixes #2128.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5378
https://github.com/broadinstitute/gatk/pull/5381:8,Security,validat,validation,8,Now the validation test data sets are in the normal git file repository.; This allows them to be visually inspected for differences when they have; changed (during a code review). Fixes #5379,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5381
https://github.com/broadinstitute/gatk/pull/5381:19,Testability,test,test,19,Now the validation test data sets are in the normal git file repository.; This allows them to be visually inspected for differences when they have; changed (during a code review). Fixes #5379,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5381
https://github.com/broadinstitute/gatk/issues/5383:250,Deployability,update,update,250,"From a researcher in the field. Their data processing would be much simpler if GenomicsDB accepted non-diploid and mixed-ploidy cases. Currently, researcher is encountering challenges to a workaround that uses CombineGVCFs (a GATK3 tool). ---. As an update, looks like `GenomicsDBImport` only supports diploid data, so we cannot use it. Would really appreciate your help on this. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/53201#Comment_53201",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5383
https://github.com/broadinstitute/gatk/issues/5383:68,Usability,simpl,simpler,68,"From a researcher in the field. Their data processing would be much simpler if GenomicsDB accepted non-diploid and mixed-ploidy cases. Currently, researcher is encountering challenges to a workaround that uses CombineGVCFs (a GATK3 tool). ---. As an update, looks like `GenomicsDBImport` only supports diploid data, so we cannot use it. Would really appreciate your help on this. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/53201#Comment_53201",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5383
https://github.com/broadinstitute/gatk/pull/5384:18,Testability,test,test,18,"- Fixed automated test for that was being skipped. When re-enabled, it passed without code changes.; - Added automated test. Passed without any other code changes.; - Added documentation for blacklisting when running the PoN and case sample/pair workflows (as opposed to using the postprocessing combine_tracks.wdl).; - Added hard num het filter. This helps guard against oversegmentation skewing MAF estimates. For WGS, `10` is a reasonable value.; - Changed default maf threshold for calling balanced segments. The previous value was too sensitive.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5384
https://github.com/broadinstitute/gatk/pull/5384:119,Testability,test,test,119,"- Fixed automated test for that was being skipped. When re-enabled, it passed without code changes.; - Added automated test. Passed without any other code changes.; - Added documentation for blacklisting when running the PoN and case sample/pair workflows (as opposed to using the postprocessing combine_tracks.wdl).; - Added hard num het filter. This helps guard against oversegmentation skewing MAF estimates. For WGS, `10` is a reasonable value.; - Changed default maf threshold for calling balanced segments. The previous value was too sensitive.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5384
https://github.com/broadinstitute/gatk/issues/5385:6149,Deployability,release,release,6149,"gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5385
https://github.com/broadinstitute/gatk/issues/5385:6219,Testability,test,test,6219,"gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5385
https://github.com/broadinstitute/gatk/issues/5385:6319,Testability,log,logs,6319,"gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5385
https://github.com/broadinstitute/gatk/issues/5387:221,Security,expose,expose,221,"We noticed today that there's no way in GATK4 to change the sigma of the band pass filter Gaussian kernel in `AssemblyRegionWalker`, even though `maxProbPropagationDistance` is settable. For consistency's sake, we should expose the band pass sigma via an arg as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5387
https://github.com/broadinstitute/gatk/issues/5388:1978,Modifiability,variab,variable,1978,"## Bug Report. ### Affected tool(s) or class(es); GermlineCNVCaller . ### Affected version(s); v4.0.4.0 and v4.0.11.0 tested with same result. ### Description ; ![screenshot 2018-11-02 14 50 17](https://user-images.githubusercontent.com/11543866/47934764-a8a71c80-deae-11e8-9f8f-c8a8b563d77a.png). ```; java.lang.IllegalArgumentException: Intervals for read-count file /home/shlee/gcnv/cvg/HG00096_chr20XY.hdf5 do not contain all specified intervals.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.writeIntervalSubsetReadCountFiles(GermlineCNVCaller.java:390); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:285); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Command runs fine sans `-XL` parameter. The contents of `-XL` are simply:. ![screenshot 2018-11-02 14 51 58](https://user-images.githubusercontent.com/11543866/47934827-e0ae5f80-deae-11e8-891e-473ec8420433.png). #### Expected behavior; It would be great to be able to iterate GermlineCNVCaller on coverage data while excluding various regions, e.g. centromeric regions, to test the impact of such regions on the denoising. Currently, the hypothetical workaround would be to collect coverage while excluding regions or to manually remove such intervals from the coverage data. Having to collect coverage once over all of the data is preferable to collecting coverage again and again over slightly variable regions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5388
https://github.com/broadinstitute/gatk/issues/5388:497,Security,validat,validateArg,497,"## Bug Report. ### Affected tool(s) or class(es); GermlineCNVCaller . ### Affected version(s); v4.0.4.0 and v4.0.11.0 tested with same result. ### Description ; ![screenshot 2018-11-02 14 50 17](https://user-images.githubusercontent.com/11543866/47934764-a8a71c80-deae-11e8-9f8f-c8a8b563d77a.png). ```; java.lang.IllegalArgumentException: Intervals for read-count file /home/shlee/gcnv/cvg/HG00096_chr20XY.hdf5 do not contain all specified intervals.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.writeIntervalSubsetReadCountFiles(GermlineCNVCaller.java:390); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:285); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Command runs fine sans `-XL` parameter. The contents of `-XL` are simply:. ![screenshot 2018-11-02 14 51 58](https://user-images.githubusercontent.com/11543866/47934827-e0ae5f80-deae-11e8-891e-473ec8420433.png). #### Expected behavior; It would be great to be able to iterate GermlineCNVCaller on coverage data while excluding various regions, e.g. centromeric regions, to test the impact of such regions on the denoising. Currently, the hypothetical workaround would be to collect coverage while excluding regions or to manually remove such intervals from the coverage data. Having to collect coverage once over all of the data is preferable to collecting coverage again and again over slightly variable regions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5388
https://github.com/broadinstitute/gatk/issues/5388:118,Testability,test,tested,118,"## Bug Report. ### Affected tool(s) or class(es); GermlineCNVCaller . ### Affected version(s); v4.0.4.0 and v4.0.11.0 tested with same result. ### Description ; ![screenshot 2018-11-02 14 50 17](https://user-images.githubusercontent.com/11543866/47934764-a8a71c80-deae-11e8-9f8f-c8a8b563d77a.png). ```; java.lang.IllegalArgumentException: Intervals for read-count file /home/shlee/gcnv/cvg/HG00096_chr20XY.hdf5 do not contain all specified intervals.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.writeIntervalSubsetReadCountFiles(GermlineCNVCaller.java:390); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:285); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Command runs fine sans `-XL` parameter. The contents of `-XL` are simply:. ![screenshot 2018-11-02 14 51 58](https://user-images.githubusercontent.com/11543866/47934827-e0ae5f80-deae-11e8-891e-473ec8420433.png). #### Expected behavior; It would be great to be able to iterate GermlineCNVCaller on coverage data while excluding various regions, e.g. centromeric regions, to test the impact of such regions on the denoising. Currently, the hypothetical workaround would be to collect coverage while excluding regions or to manually remove such intervals from the coverage data. Having to collect coverage once over all of the data is preferable to collecting coverage again and again over slightly variable regions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5388
https://github.com/broadinstitute/gatk/issues/5388:1655,Testability,test,test,1655,"## Bug Report. ### Affected tool(s) or class(es); GermlineCNVCaller . ### Affected version(s); v4.0.4.0 and v4.0.11.0 tested with same result. ### Description ; ![screenshot 2018-11-02 14 50 17](https://user-images.githubusercontent.com/11543866/47934764-a8a71c80-deae-11e8-9f8f-c8a8b563d77a.png). ```; java.lang.IllegalArgumentException: Intervals for read-count file /home/shlee/gcnv/cvg/HG00096_chr20XY.hdf5 do not contain all specified intervals.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.writeIntervalSubsetReadCountFiles(GermlineCNVCaller.java:390); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:285); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Command runs fine sans `-XL` parameter. The contents of `-XL` are simply:. ![screenshot 2018-11-02 14 51 58](https://user-images.githubusercontent.com/11543866/47934827-e0ae5f80-deae-11e8-891e-473ec8420433.png). #### Expected behavior; It would be great to be able to iterate GermlineCNVCaller on coverage data while excluding various regions, e.g. centromeric regions, to test the impact of such regions on the denoising. Currently, the hypothetical workaround would be to collect coverage while excluding regions or to manually remove such intervals from the coverage data. Having to collect coverage once over all of the data is preferable to collecting coverage again and again over slightly variable regions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5388
https://github.com/broadinstitute/gatk/issues/5388:1348,Usability,simpl,simply,1348,"## Bug Report. ### Affected tool(s) or class(es); GermlineCNVCaller . ### Affected version(s); v4.0.4.0 and v4.0.11.0 tested with same result. ### Description ; ![screenshot 2018-11-02 14 50 17](https://user-images.githubusercontent.com/11543866/47934764-a8a71c80-deae-11e8-9f8f-c8a8b563d77a.png). ```; java.lang.IllegalArgumentException: Intervals for read-count file /home/shlee/gcnv/cvg/HG00096_chr20XY.hdf5 do not contain all specified intervals.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.writeIntervalSubsetReadCountFiles(GermlineCNVCaller.java:390); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:285); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Command runs fine sans `-XL` parameter. The contents of `-XL` are simply:. ![screenshot 2018-11-02 14 51 58](https://user-images.githubusercontent.com/11543866/47934827-e0ae5f80-deae-11e8-891e-473ec8420433.png). #### Expected behavior; It would be great to be able to iterate GermlineCNVCaller on coverage data while excluding various regions, e.g. centromeric regions, to test the impact of such regions on the denoising. Currently, the hypothetical workaround would be to collect coverage while excluding regions or to manually remove such intervals from the coverage data. Having to collect coverage once over all of the data is preferable to collecting coverage again and again over slightly variable regions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5388
https://github.com/broadinstitute/gatk/issues/5389:490,Availability,down,downloaded,490,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:743,Availability,Failure,Failure,743,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:787,Availability,error,error,787,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:806,Availability,error,error,806,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:121,Deployability,integrat,integration,121,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:302,Deployability,install,installation,302,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:339,Deployability,install,install,339,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:356,Deployability,install,install,356,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:420,Deployability,install,install,420,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:431,Deployability,install,installed,431,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:592,Deployability,install,installation,592,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:681,Deployability,install,install,681,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:869,Deployability,update,update,869,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:926,Deployability,install,installed,926,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:979,Deployability,install,install,979,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:1010,Deployability,install,install,1010,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:1041,Deployability,install,install,1041,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:1071,Deployability,install,install,1071,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:1104,Deployability,install,install,1104,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:1134,Deployability,install,install,1134,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:1220,Deployability,integrat,integration,1220,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:121,Integrability,integrat,integration,121,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:1220,Integrability,integrat,integration,1220,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:133,Testability,test,tests,133,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:1200,Testability,test,test,1200,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5389:1252,Testability,test,test,1252,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389
https://github.com/broadinstitute/gatk/issues/5396:409,Availability,down,down,409,We need to produce a script that will make it easy to evaluate what changes to the HalpotypeCallerSpark will result in the biggest performance impact. To that end we want to write wdls and associated scripts that will make it easier for us to evaluate what each incremental change to the tool will change about accuracy and runtime for the machine configurations we care about. We should probably also hammer down what the machine types we consider important are as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5396
https://github.com/broadinstitute/gatk/issues/5396:348,Deployability,configurat,configurations,348,We need to produce a script that will make it easy to evaluate what changes to the HalpotypeCallerSpark will result in the biggest performance impact. To that end we want to write wdls and associated scripts that will make it easier for us to evaluate what each incremental change to the tool will change about accuracy and runtime for the machine configurations we care about. We should probably also hammer down what the machine types we consider important are as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5396
https://github.com/broadinstitute/gatk/issues/5396:348,Modifiability,config,configurations,348,We need to produce a script that will make it easy to evaluate what changes to the HalpotypeCallerSpark will result in the biggest performance impact. To that end we want to write wdls and associated scripts that will make it easier for us to evaluate what each incremental change to the tool will change about accuracy and runtime for the machine configurations we care about. We should probably also hammer down what the machine types we consider important are as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5396
https://github.com/broadinstitute/gatk/issues/5396:131,Performance,perform,performance,131,We need to produce a script that will make it easy to evaluate what changes to the HalpotypeCallerSpark will result in the biggest performance impact. To that end we want to write wdls and associated scripts that will make it easier for us to evaluate what each incremental change to the tool will change about accuracy and runtime for the machine configurations we care about. We should probably also hammer down what the machine types we consider important are as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5396
https://github.com/broadinstitute/gatk/pull/5397:426,Availability,error,error,426,The newest release of GenomicsDB treats spanning deletions (spanning; from earlier positions) as deletions in the min PL value computation.; This behavior now matches the behavior of CombineGVCFs. A more detailed description of the issue is provided in; https://github.com/broadinstitute/gatk/pull/4963. * Deleted a couple of files which are no longer necessary.; * Fixed the index of newMQcalc.combined.g.vcf; * Fixes #5045 (error out with a helpful error message); * Fixes #5300,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5397
https://github.com/broadinstitute/gatk/pull/5397:451,Availability,error,error,451,The newest release of GenomicsDB treats spanning deletions (spanning; from earlier positions) as deletions in the min PL value computation.; This behavior now matches the behavior of CombineGVCFs. A more detailed description of the issue is provided in; https://github.com/broadinstitute/gatk/pull/4963. * Deleted a couple of files which are no longer necessary.; * Fixed the index of newMQcalc.combined.g.vcf; * Fixes #5045 (error out with a helpful error message); * Fixes #5300,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5397
https://github.com/broadinstitute/gatk/pull/5397:11,Deployability,release,release,11,The newest release of GenomicsDB treats spanning deletions (spanning; from earlier positions) as deletions in the min PL value computation.; This behavior now matches the behavior of CombineGVCFs. A more detailed description of the issue is provided in; https://github.com/broadinstitute/gatk/pull/4963. * Deleted a couple of files which are no longer necessary.; * Fixed the index of newMQcalc.combined.g.vcf; * Fixes #5045 (error out with a helpful error message); * Fixes #5300,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5397
https://github.com/broadinstitute/gatk/pull/5397:457,Integrability,message,message,457,The newest release of GenomicsDB treats spanning deletions (spanning; from earlier positions) as deletions in the min PL value computation.; This behavior now matches the behavior of CombineGVCFs. A more detailed description of the issue is provided in; https://github.com/broadinstitute/gatk/pull/4963. * Deleted a couple of files which are no longer necessary.; * Fixed the index of newMQcalc.combined.g.vcf; * Fixes #5045 (error out with a helpful error message); * Fixes #5300,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5397
https://github.com/broadinstitute/gatk/issues/5398:104,Availability,error,error,104,"I tried running MarkDuplicatesSpark with multiple inputs like it is run in production and got this user error. ```; A USER ERROR has occurred: Sorry, we only support a single reads input for spark tools for now.; ```. For this to go into production it would need to have the ability to take in multiple inputs (I'm currently trying to make a ""fast"" version of the production germline pipeline and it would be great to have this tool included in that pipeline). @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5398
https://github.com/broadinstitute/gatk/issues/5398:123,Availability,ERROR,ERROR,123,"I tried running MarkDuplicatesSpark with multiple inputs like it is run in production and got this user error. ```; A USER ERROR has occurred: Sorry, we only support a single reads input for spark tools for now.; ```. For this to go into production it would need to have the ability to take in multiple inputs (I'm currently trying to make a ""fast"" version of the production germline pipeline and it would be great to have this tool included in that pipeline). @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5398
https://github.com/broadinstitute/gatk/issues/5398:384,Deployability,pipeline,pipeline,384,"I tried running MarkDuplicatesSpark with multiple inputs like it is run in production and got this user error. ```; A USER ERROR has occurred: Sorry, we only support a single reads input for spark tools for now.; ```. For this to go into production it would need to have the ability to take in multiple inputs (I'm currently trying to make a ""fast"" version of the production germline pipeline and it would be great to have this tool included in that pipeline). @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5398
https://github.com/broadinstitute/gatk/issues/5398:450,Deployability,pipeline,pipeline,450,"I tried running MarkDuplicatesSpark with multiple inputs like it is run in production and got this user error. ```; A USER ERROR has occurred: Sorry, we only support a single reads input for spark tools for now.; ```. For this to go into production it would need to have the ability to take in multiple inputs (I'm currently trying to make a ""fast"" version of the production germline pipeline and it would be great to have this tool included in that pipeline). @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5398
https://github.com/broadinstitute/gatk/pull/5401:47,Deployability,release,release,47,* Adding the dataproc-cluster-ui script to the release bundle so users can access it.; * Fixes #5400,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5401
https://github.com/broadinstitute/gatk/pull/5401:75,Security,access,access,75,* Adding the dataproc-cluster-ui script to the release bundle so users can access it.; * Fixes #5400,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5401
https://github.com/broadinstitute/gatk/issues/5402:103,Safety,detect,detect,103,"`GencodeFuncotationFactory.createFuncotationsOnVariant()` (and possibly other places as well) needs to detect symbolic alleles and handle them appropriately. Should add a good test with, eg., the output of GCNV.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5402
https://github.com/broadinstitute/gatk/issues/5402:176,Testability,test,test,176,"`GencodeFuncotationFactory.createFuncotationsOnVariant()` (and possibly other places as well) needs to detect symbolic alleles and handle them appropriately. Should add a good test with, eg., the output of GCNV.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5402
https://github.com/broadinstitute/gatk/pull/5403:542,Deployability,update,updated,542,"* Added support for annotating 5'/3' flanks via new FIVE_PRIME_FLANK and THREE_PRIME_FLANK funcotations. * Added --five-prime-flank-size and --three-prime-flank-size arguments to control the size of each flanking region. * Refactored datasource classes to allow for padded/custom queries to make this feature possible. * We now emit IGR funcotations in more cases (in particular, when a gene has no basic transcripts, and when the basic transcripts do not fully span a gene and the flank size is small). * Added comprehensive unit tests, and updated integration test data. Resolves #4771",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5403
https://github.com/broadinstitute/gatk/pull/5403:550,Deployability,integrat,integration,550,"* Added support for annotating 5'/3' flanks via new FIVE_PRIME_FLANK and THREE_PRIME_FLANK funcotations. * Added --five-prime-flank-size and --three-prime-flank-size arguments to control the size of each flanking region. * Refactored datasource classes to allow for padded/custom queries to make this feature possible. * We now emit IGR funcotations in more cases (in particular, when a gene has no basic transcripts, and when the basic transcripts do not fully span a gene and the flank size is small). * Added comprehensive unit tests, and updated integration test data. Resolves #4771",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5403
https://github.com/broadinstitute/gatk/pull/5403:550,Integrability,integrat,integration,550,"* Added support for annotating 5'/3' flanks via new FIVE_PRIME_FLANK and THREE_PRIME_FLANK funcotations. * Added --five-prime-flank-size and --three-prime-flank-size arguments to control the size of each flanking region. * Refactored datasource classes to allow for padded/custom queries to make this feature possible. * We now emit IGR funcotations in more cases (in particular, when a gene has no basic transcripts, and when the basic transcripts do not fully span a gene and the flank size is small). * Added comprehensive unit tests, and updated integration test data. Resolves #4771",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5403
https://github.com/broadinstitute/gatk/pull/5403:223,Modifiability,Refactor,Refactored,223,"* Added support for annotating 5'/3' flanks via new FIVE_PRIME_FLANK and THREE_PRIME_FLANK funcotations. * Added --five-prime-flank-size and --three-prime-flank-size arguments to control the size of each flanking region. * Refactored datasource classes to allow for padded/custom queries to make this feature possible. * We now emit IGR funcotations in more cases (in particular, when a gene has no basic transcripts, and when the basic transcripts do not fully span a gene and the flank size is small). * Added comprehensive unit tests, and updated integration test data. Resolves #4771",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5403
https://github.com/broadinstitute/gatk/pull/5403:531,Testability,test,tests,531,"* Added support for annotating 5'/3' flanks via new FIVE_PRIME_FLANK and THREE_PRIME_FLANK funcotations. * Added --five-prime-flank-size and --three-prime-flank-size arguments to control the size of each flanking region. * Refactored datasource classes to allow for padded/custom queries to make this feature possible. * We now emit IGR funcotations in more cases (in particular, when a gene has no basic transcripts, and when the basic transcripts do not fully span a gene and the flank size is small). * Added comprehensive unit tests, and updated integration test data. Resolves #4771",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5403
https://github.com/broadinstitute/gatk/pull/5403:562,Testability,test,test,562,"* Added support for annotating 5'/3' flanks via new FIVE_PRIME_FLANK and THREE_PRIME_FLANK funcotations. * Added --five-prime-flank-size and --three-prime-flank-size arguments to control the size of each flanking region. * Refactored datasource classes to allow for padded/custom queries to make this feature possible. * We now emit IGR funcotations in more cases (in particular, when a gene has no basic transcripts, and when the basic transcripts do not fully span a gene and the flank size is small). * Added comprehensive unit tests, and updated integration test data. Resolves #4771",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5403
https://github.com/broadinstitute/gatk/pull/5405:126,Availability,error,error,126,The [Google Java Style guide](http://google-styleguide.googlecode.com/svn/trunk/javaguide.html) links are dead and give a 404 error. References were changed to [Google Java Style guide](https://google.github.io/styleguide/javaguide.html).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5405
https://github.com/broadinstitute/gatk/pull/5405:23,Usability,guid,guide,23,The [Google Java Style guide](http://google-styleguide.googlecode.com/svn/trunk/javaguide.html) links are dead and give a 404 error. References were changed to [Google Java Style guide](https://google.github.io/styleguide/javaguide.html).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5405
https://github.com/broadinstitute/gatk/pull/5405:179,Usability,guid,guide,179,The [Google Java Style guide](http://google-styleguide.googlecode.com/svn/trunk/javaguide.html) links are dead and give a 404 error. References were changed to [Google Java Style guide](https://google.github.io/styleguide/javaguide.html).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5405
https://github.com/broadinstitute/gatk/pull/5406:167,Availability,mask,masked,167,- Updated data sources to include variant sites for symbolic alleles.; - Fixed tests to be correct for new logic.; - Now has tests for symbollic alternate alleles and masked alleles. Fixes #5402,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5406
https://github.com/broadinstitute/gatk/pull/5406:2,Deployability,Update,Updated,2,- Updated data sources to include variant sites for symbolic alleles.; - Fixed tests to be correct for new logic.; - Now has tests for symbollic alternate alleles and masked alleles. Fixes #5402,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5406
https://github.com/broadinstitute/gatk/pull/5406:79,Testability,test,tests,79,- Updated data sources to include variant sites for symbolic alleles.; - Fixed tests to be correct for new logic.; - Now has tests for symbollic alternate alleles and masked alleles. Fixes #5402,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5406
https://github.com/broadinstitute/gatk/pull/5406:107,Testability,log,logic,107,- Updated data sources to include variant sites for symbolic alleles.; - Fixed tests to be correct for new logic.; - Now has tests for symbollic alternate alleles and masked alleles. Fixes #5402,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5406
https://github.com/broadinstitute/gatk/pull/5406:125,Testability,test,tests,125,- Updated data sources to include variant sites for symbolic alleles.; - Fixed tests to be correct for new logic.; - Now has tests for symbollic alternate alleles and masked alleles. Fixes #5402,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5406
https://github.com/broadinstitute/gatk/issues/5409:738,Availability,robust,robustly,738,"It is not clear to me from the docs whether parent/child pairs are intended to be supported by `CalculateGenotypePosteriors`, but a quick glance at the [mention](https://github.com/broadinstitute/gatk/blob/67f0f0f2e59185b721398b17c24eba487a2ac76c/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/FamilyLikelihoods.java#L210)[s](https://github.com/broadinstitute/gatk/blob/67f0f0f2e59185b721398b17c24eba487a2ac76c/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/FamilyLikelihoods.java#L231) in comments in `FamilyLikelihoods.java` makes me suspect that they are intended to be supported. (In any case, from my perspective, it would be a very nice feature as I have yet to find a tool that will robustly handle this use case.). Here are the main issues that I'm encountering when trying to use `CalculateGenotypePosteriors` for a parent-child pair:; 1) If I supply a ped file with two individuals like the following, [this check](https://github.com/broadinstitute/gatk/blob/1e98c6d02cefefbaa1a15db0aea64ea7518025fa/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/CalculateGenotypePosteriors.java#L260) gets triggered, resulting in printing of the warning and skipping family priors.; ```; FAM	MOM	0	0	2	0; FAM	CHILD	0	MOM	2	0; ```; 2) If I add a father to the ped file to form a trio, like below, `CalculateGenotypePosteriors` proceeds without the warning that occurs in first approach, but the output doesn't appear to make any adjustments to genotypes, posteriors, etc. Note that there is no entry for ""DAD"" in the input VCF.; ```; FAM	MOM	0	0	2	0; FAM	DAD	0	0	1	0; FAM	CHILD	DAD	MOM	2	0; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5409
https://github.com/broadinstitute/gatk/issues/5409:10,Usability,clear,clear,10,"It is not clear to me from the docs whether parent/child pairs are intended to be supported by `CalculateGenotypePosteriors`, but a quick glance at the [mention](https://github.com/broadinstitute/gatk/blob/67f0f0f2e59185b721398b17c24eba487a2ac76c/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/FamilyLikelihoods.java#L210)[s](https://github.com/broadinstitute/gatk/blob/67f0f0f2e59185b721398b17c24eba487a2ac76c/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/FamilyLikelihoods.java#L231) in comments in `FamilyLikelihoods.java` makes me suspect that they are intended to be supported. (In any case, from my perspective, it would be a very nice feature as I have yet to find a tool that will robustly handle this use case.). Here are the main issues that I'm encountering when trying to use `CalculateGenotypePosteriors` for a parent-child pair:; 1) If I supply a ped file with two individuals like the following, [this check](https://github.com/broadinstitute/gatk/blob/1e98c6d02cefefbaa1a15db0aea64ea7518025fa/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/CalculateGenotypePosteriors.java#L260) gets triggered, resulting in printing of the warning and skipping family priors.; ```; FAM	MOM	0	0	2	0; FAM	CHILD	0	MOM	2	0; ```; 2) If I add a father to the ped file to form a trio, like below, `CalculateGenotypePosteriors` proceeds without the warning that occurs in first approach, but the output doesn't appear to make any adjustments to genotypes, posteriors, etc. Note that there is no entry for ""DAD"" in the input VCF.; ```; FAM	MOM	0	0	2	0; FAM	DAD	0	0	1	0; FAM	CHILD	DAD	MOM	2	0; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5409
https://github.com/broadinstitute/gatk/issues/5410:254,Availability,error,error,254,"In IntervalUtils, when Picard intervals are parsed and checked for validity, (line 359 `glParser.isValidGenomeLoc(interval.getContig(), interval.getStart(), interval.getEnd(), true)`), if the contig doesn't match the supplied reference (via -R) then the error produced is `has an invalid interval`. The interval is perfectly valid, especially since the Picard interval_list has a corresponding sequence dictionary. I'm not sure if the preferred behavior here is to validate against the interval_list seqdict and then note that the -R reference doesn't match or to error because the -R ref doesn't match. Maybe if the tool requiresReference() and the -R doesn't match throw an error?. I encountered this in the context of a tool similar to SplitIntervals, which requires a reference even if a Picard interval_list is provided. I see that this is a TODO in GATKTool::getBestAvailableSequenceDictionary.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5410
https://github.com/broadinstitute/gatk/issues/5410:564,Availability,error,error,564,"In IntervalUtils, when Picard intervals are parsed and checked for validity, (line 359 `glParser.isValidGenomeLoc(interval.getContig(), interval.getStart(), interval.getEnd(), true)`), if the contig doesn't match the supplied reference (via -R) then the error produced is `has an invalid interval`. The interval is perfectly valid, especially since the Picard interval_list has a corresponding sequence dictionary. I'm not sure if the preferred behavior here is to validate against the interval_list seqdict and then note that the -R reference doesn't match or to error because the -R ref doesn't match. Maybe if the tool requiresReference() and the -R doesn't match throw an error?. I encountered this in the context of a tool similar to SplitIntervals, which requires a reference even if a Picard interval_list is provided. I see that this is a TODO in GATKTool::getBestAvailableSequenceDictionary.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5410
https://github.com/broadinstitute/gatk/issues/5410:676,Availability,error,error,676,"In IntervalUtils, when Picard intervals are parsed and checked for validity, (line 359 `glParser.isValidGenomeLoc(interval.getContig(), interval.getStart(), interval.getEnd(), true)`), if the contig doesn't match the supplied reference (via -R) then the error produced is `has an invalid interval`. The interval is perfectly valid, especially since the Picard interval_list has a corresponding sequence dictionary. I'm not sure if the preferred behavior here is to validate against the interval_list seqdict and then note that the -R reference doesn't match or to error because the -R ref doesn't match. Maybe if the tool requiresReference() and the -R doesn't match throw an error?. I encountered this in the context of a tool similar to SplitIntervals, which requires a reference even if a Picard interval_list is provided. I see that this is a TODO in GATKTool::getBestAvailableSequenceDictionary.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5410
https://github.com/broadinstitute/gatk/issues/5410:465,Security,validat,validate,465,"In IntervalUtils, when Picard intervals are parsed and checked for validity, (line 359 `glParser.isValidGenomeLoc(interval.getContig(), interval.getStart(), interval.getEnd(), true)`), if the contig doesn't match the supplied reference (via -R) then the error produced is `has an invalid interval`. The interval is perfectly valid, especially since the Picard interval_list has a corresponding sequence dictionary. I'm not sure if the preferred behavior here is to validate against the interval_list seqdict and then note that the -R reference doesn't match or to error because the -R ref doesn't match. Maybe if the tool requiresReference() and the -R doesn't match throw an error?. I encountered this in the context of a tool similar to SplitIntervals, which requires a reference even if a Picard interval_list is provided. I see that this is a TODO in GATKTool::getBestAvailableSequenceDictionary.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5410
https://github.com/broadinstitute/gatk/pull/5413:551,Deployability,update,update,551,"@takutosato This dramatically improves `CalculateContamination` by giving more care to distinguishing hom alts from hets. It makes an especially big difference in our tumor-only HCC1143 validations, where the accuracy is now very good (and BTW, ContEst gets these all completely wrong even *with* a matched normal). It also makes the tool work better in targeted panels where there might not be enough hom alt sites by adding a backup hom ref mode that gets triggered automatically. This is based on a Broadie request. I will file a separate issue to update the docs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5413
https://github.com/broadinstitute/gatk/pull/5413:186,Security,validat,validations,186,"@takutosato This dramatically improves `CalculateContamination` by giving more care to distinguishing hom alts from hets. It makes an especially big difference in our tumor-only HCC1143 validations, where the accuracy is now very good (and BTW, ContEst gets these all completely wrong even *with* a matched normal). It also makes the tool work better in targeted panels where there might not be enough hom alt sites by adding a backup hom ref mode that gets triggered automatically. This is based on a Broadie request. I will file a separate issue to update the docs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5413
https://github.com/broadinstitute/gatk/pull/5416:10,Testability,test,tests,10,"There are tests for both the strict and non-strict modes for HaplotypeCallerSpark and for ExampleAssemblyRegionWalkerSpark. The strict modes produce identical results to the walker versions. Strict mode is off by default and still needs work to scale to exome-sized data (another PR). This is an improvement over the current HaplotypeCallerSpark implementation since it fixes two bugs (reads overlapping more than two intervals; and editIntervals not being picked up) that caused the output to differ significantly from the walker version. While the output does still differ in non-strict mode, the difference is a lot less (compare expected.testVCFMode.gatk4.vcf and expected.testVCFMode.gatk4.nonstrict.vcf).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5416
https://github.com/broadinstitute/gatk/pull/5416:642,Testability,test,testVCFMode,642,"There are tests for both the strict and non-strict modes for HaplotypeCallerSpark and for ExampleAssemblyRegionWalkerSpark. The strict modes produce identical results to the walker versions. Strict mode is off by default and still needs work to scale to exome-sized data (another PR). This is an improvement over the current HaplotypeCallerSpark implementation since it fixes two bugs (reads overlapping more than two intervals; and editIntervals not being picked up) that caused the output to differ significantly from the walker version. While the output does still differ in non-strict mode, the difference is a lot less (compare expected.testVCFMode.gatk4.vcf and expected.testVCFMode.gatk4.nonstrict.vcf).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5416
https://github.com/broadinstitute/gatk/pull/5416:677,Testability,test,testVCFMode,677,"There are tests for both the strict and non-strict modes for HaplotypeCallerSpark and for ExampleAssemblyRegionWalkerSpark. The strict modes produce identical results to the walker versions. Strict mode is off by default and still needs work to scale to exome-sized data (another PR). This is an improvement over the current HaplotypeCallerSpark implementation since it fixes two bugs (reads overlapping more than two intervals; and editIntervals not being picked up) that caused the output to differ significantly from the walker version. While the output does still differ in non-strict mode, the difference is a lot less (compare expected.testVCFMode.gatk4.vcf and expected.testVCFMode.gatk4.nonstrict.vcf).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5416
https://github.com/broadinstitute/gatk/issues/5417:105,Testability,test,test,105,The Funcotator flanks feature introduced in https://github.com/broadinstitute/gatk/pull/5403 still needs test cases for:. * multiple genes overlapping a variant; * all transcripts mode,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5417
https://github.com/broadinstitute/gatk/issues/5421:85,Integrability,wrap,wraps,85,"Some tools (ex: GenotypeConcordance) have a description line that is so long that it wraps onto multiple lines in the tool list. This looks bad. . We should either enforce a shorter line length, or make our help output multiline aware so it looks better. <img width=""1440"" alt=""screen shot 2018-11-15 at 3 58 57 pm"" src=""https://user-images.githubusercontent.com/4700332/48581533-19f3c000-e8f0-11e8-8823-bd3f20675975.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5421
https://github.com/broadinstitute/gatk/pull/5422:117,Integrability,message,messages,117,"Fixes https://github.com/broadinstitute/gatk/issues/5362 and https://github.com/broadinstitute/gatk/issues/4637. Log messages from JEXL were being dropped because of https://github.com/broadinstitute/gatk/issues/4637, making the behavior described by https://github.com/broadinstitute/gatk/issues/5362 even more subtle to discern. Now, instead of seeing:. > log4j:WARN No appenders could be found for logger (org.apache.commons.jexl2.JexlEngine).; > log4j:WARN Please initialize the log4j system properly.; > log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. the user will see:. > 11:51:06.321 WARN JexlEngine - ![51,60]: 'QD < 2.0 || MQ < 40.0 || FS > 60.0 || SOR > 3.0 || MQRankSum < -12.5 || ReadPosRankSum < -8.0 || ExcessHet > 54.69;' undefined variable MQRankSum",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5422
https://github.com/broadinstitute/gatk/pull/5422:784,Modifiability,variab,variable,784,"Fixes https://github.com/broadinstitute/gatk/issues/5362 and https://github.com/broadinstitute/gatk/issues/4637. Log messages from JEXL were being dropped because of https://github.com/broadinstitute/gatk/issues/4637, making the behavior described by https://github.com/broadinstitute/gatk/issues/5362 even more subtle to discern. Now, instead of seeing:. > log4j:WARN No appenders could be found for logger (org.apache.commons.jexl2.JexlEngine).; > log4j:WARN Please initialize the log4j system properly.; > log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. the user will see:. > 11:51:06.321 WARN JexlEngine - ![51,60]: 'QD < 2.0 || MQ < 40.0 || FS > 60.0 || SOR > 3.0 || MQRankSum < -12.5 || ReadPosRankSum < -8.0 || ExcessHet > 54.69;' undefined variable MQRankSum",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5422
https://github.com/broadinstitute/gatk/pull/5422:113,Testability,Log,Log,113,"Fixes https://github.com/broadinstitute/gatk/issues/5362 and https://github.com/broadinstitute/gatk/issues/4637. Log messages from JEXL were being dropped because of https://github.com/broadinstitute/gatk/issues/4637, making the behavior described by https://github.com/broadinstitute/gatk/issues/5362 even more subtle to discern. Now, instead of seeing:. > log4j:WARN No appenders could be found for logger (org.apache.commons.jexl2.JexlEngine).; > log4j:WARN Please initialize the log4j system properly.; > log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. the user will see:. > 11:51:06.321 WARN JexlEngine - ![51,60]: 'QD < 2.0 || MQ < 40.0 || FS > 60.0 || SOR > 3.0 || MQRankSum < -12.5 || ReadPosRankSum < -8.0 || ExcessHet > 54.69;' undefined variable MQRankSum",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5422
https://github.com/broadinstitute/gatk/pull/5422:401,Testability,log,logger,401,"Fixes https://github.com/broadinstitute/gatk/issues/5362 and https://github.com/broadinstitute/gatk/issues/4637. Log messages from JEXL were being dropped because of https://github.com/broadinstitute/gatk/issues/4637, making the behavior described by https://github.com/broadinstitute/gatk/issues/5362 even more subtle to discern. Now, instead of seeing:. > log4j:WARN No appenders could be found for logger (org.apache.commons.jexl2.JexlEngine).; > log4j:WARN Please initialize the log4j system properly.; > log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. the user will see:. > 11:51:06.321 WARN JexlEngine - ![51,60]: 'QD < 2.0 || MQ < 40.0 || FS > 60.0 || SOR > 3.0 || MQRankSum < -12.5 || ReadPosRankSum < -8.0 || ExcessHet > 54.69;' undefined variable MQRankSum",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5422
https://github.com/broadinstitute/gatk/pull/5422:531,Testability,log,logging,531,"Fixes https://github.com/broadinstitute/gatk/issues/5362 and https://github.com/broadinstitute/gatk/issues/4637. Log messages from JEXL were being dropped because of https://github.com/broadinstitute/gatk/issues/4637, making the behavior described by https://github.com/broadinstitute/gatk/issues/5362 even more subtle to discern. Now, instead of seeing:. > log4j:WARN No appenders could be found for logger (org.apache.commons.jexl2.JexlEngine).; > log4j:WARN Please initialize the log4j system properly.; > log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. the user will see:. > 11:51:06.321 WARN JexlEngine - ![51,60]: 'QD < 2.0 || MQ < 40.0 || FS > 60.0 || SOR > 3.0 || MQRankSum < -12.5 || ReadPosRankSum < -8.0 || ExcessHet > 54.69;' undefined variable MQRankSum",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5422
https://github.com/broadinstitute/gatk/pull/5423:104,Deployability,update,update,104,"The test Gencode data sources did not include any genes in flanking distance to the test variants. This update creates a new Gencode data source that includes such regions. Additionally, the Gencode excision script was updated to include a flanking range defaulting to the funcotator 5' flanking defualt (5000 bases). . Fixes #5419",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5423
https://github.com/broadinstitute/gatk/pull/5423:219,Deployability,update,updated,219,"The test Gencode data sources did not include any genes in flanking distance to the test variants. This update creates a new Gencode data source that includes such regions. Additionally, the Gencode excision script was updated to include a flanking range defaulting to the funcotator 5' flanking defualt (5000 bases). . Fixes #5419",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5423
https://github.com/broadinstitute/gatk/pull/5423:4,Testability,test,test,4,"The test Gencode data sources did not include any genes in flanking distance to the test variants. This update creates a new Gencode data source that includes such regions. Additionally, the Gencode excision script was updated to include a flanking range defaulting to the funcotator 5' flanking defualt (5000 bases). . Fixes #5419",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5423
https://github.com/broadinstitute/gatk/pull/5423:84,Testability,test,test,84,"The test Gencode data sources did not include any genes in flanking distance to the test variants. This update creates a new Gencode data source that includes such regions. Additionally, the Gencode excision script was updated to include a flanking range defaulting to the funcotator 5' flanking defualt (5000 bases). . Fixes #5419",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5423
https://github.com/broadinstitute/gatk/pull/5424:135,Deployability,update,updated,135,* updating htsjdk 2.16.1 -> 2.18.0; * the most noticable change is that we will now produce bam 1.6 instead; of 1.5; * some test files updated to have the new version since they were being; compared with exact match tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5424
https://github.com/broadinstitute/gatk/pull/5424:124,Testability,test,test,124,* updating htsjdk 2.16.1 -> 2.18.0; * the most noticable change is that we will now produce bam 1.6 instead; of 1.5; * some test files updated to have the new version since they were being; compared with exact match tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5424
https://github.com/broadinstitute/gatk/pull/5424:216,Testability,test,tests,216,* updating htsjdk 2.16.1 -> 2.18.0; * the most noticable change is that we will now produce bam 1.6 instead; of 1.5; * some test files updated to have the new version since they were being; compared with exact match tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5424
https://github.com/broadinstitute/gatk/issues/5426:151,Performance,scalab,scalable,151,Many times the question comes up of whether variants are lost in HaplotypeCaller and Mutect2 because they are not assembled. It seems like an easy and scalable way to answer this would be to emit an optional sites-only vcf of all variants in the `EventMap` before genotyping. That way we could do internal validations about assembly much faster than currently. and user questions in this vein would not require the IDE or looking at bamouts in IGV. I envision something like this:; ```; gatk Mutect2 -I tumor.bam -O out.vcf --assembled-variants assembled.vcf; gatk SelectVariants truth.vcf --discordance assembled.vcf -O assembly-false-negatives.vcf; ```. @ldgauthier @yfarjoun what do you think about this?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5426
https://github.com/broadinstitute/gatk/issues/5426:306,Security,validat,validations,306,Many times the question comes up of whether variants are lost in HaplotypeCaller and Mutect2 because they are not assembled. It seems like an easy and scalable way to answer this would be to emit an optional sites-only vcf of all variants in the `EventMap` before genotyping. That way we could do internal validations about assembly much faster than currently. and user questions in this vein would not require the IDE or looking at bamouts in IGV. I envision something like this:; ```; gatk Mutect2 -I tumor.bam -O out.vcf --assembled-variants assembled.vcf; gatk SelectVariants truth.vcf --discordance assembled.vcf -O assembly-false-negatives.vcf; ```. @ldgauthier @yfarjoun what do you think about this?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5426
https://github.com/broadinstitute/gatk/issues/5427:132,Availability,mask,masks,132,"We have the basic DREAM somatic challenge, but there's also an RNA challenge, and perhaps others. If it's a similar format of bams, masks, and truth vcfs it would be really easy to set up a validation like the one we currently have on Firecloud.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5427
https://github.com/broadinstitute/gatk/issues/5427:190,Security,validat,validation,190,"We have the basic DREAM somatic challenge, but there's also an RNA challenge, and perhaps others. If it's a similar format of bams, masks, and truth vcfs it would be really easy to set up a validation like the one we currently have on Firecloud.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5427
https://github.com/broadinstitute/gatk/issues/5428:117,Availability,down,downloader,117,"The somatic are labeled as germline and visa-versa. The files themselves must be changed, as well as the data source downloader (to point to the new paths).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5428
https://github.com/broadinstitute/gatk/issues/5431:494,Availability,down,downstream,494,"Hi; I'm using HaplotypeCaller to do mutation calling in a large single-cell RNA-seq dataset. Im wondering if there is a way to get HaplotypeCaller to spit out _amino acid_ coordinates instead of _genome_? Can we maybe include a column such as 'Mutation.AA' that would contain entries like this? ; <img width=""151"" alt=""screen shot 2018-11-16 at 5 51 52 pm"" src=""https://user-images.githubusercontent.com/33501625/48655137-54309080-e9c8-11e8-9431-6e83b5afd9d7.png"">; It would be very useful for downstream applications like searching for clinically relevant SNPs/indels. ; Thanks; Lincoln",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5431
https://github.com/broadinstitute/gatk/issues/5433:279,Availability,error,error,279,"This issue affect version of GATK4 from version 4.0.10.1 and onwards, and is related to github feature/pull request [#4969: ""Improve MQ calculation accuracy""](https://github.com/broadinstitute/gatk/pull/4969). GVCFs with large `RAW_MQ` sum of sumSquaredMQs values end up with an error like:. ```; A USER ERROR has occurred: Bad input: malformed RAW_MQ annotation: 3415207168,1749038; ```. The error happens when the sumSquaredMQs element value is greater than Java's. `Integer.MAX_VALUE`. See the following [github pull request comment for more details](https://github.com/broadinstitute/gatk/pull/4969#issuecomment-439642813). . Should a `java.lang.Long`, `java.math.BigInteger`, `java.lang.Double`, or `java.math.BigDecimal` be used here instead of a `java.lang.Integer` for method: [`parseRawDataString` inside `org.broadinstitute.hellbender.tools.walkers.annotator.RMSMappingQuality`](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/RMSMappingQuality.java#L251) ?. I've noticed this error when using GATK4 version 4.0.11.0 on the [GATK4 Germline SNPs-INDELs WDL workflow](https://github.com/gatk-workflows/gatk4-germline-snps-indels). It happens on the [`JointGenotyping.GenotypeGVCFs` task](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl#L127).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5433
https://github.com/broadinstitute/gatk/issues/5433:304,Availability,ERROR,ERROR,304,"This issue affect version of GATK4 from version 4.0.10.1 and onwards, and is related to github feature/pull request [#4969: ""Improve MQ calculation accuracy""](https://github.com/broadinstitute/gatk/pull/4969). GVCFs with large `RAW_MQ` sum of sumSquaredMQs values end up with an error like:. ```; A USER ERROR has occurred: Bad input: malformed RAW_MQ annotation: 3415207168,1749038; ```. The error happens when the sumSquaredMQs element value is greater than Java's. `Integer.MAX_VALUE`. See the following [github pull request comment for more details](https://github.com/broadinstitute/gatk/pull/4969#issuecomment-439642813). . Should a `java.lang.Long`, `java.math.BigInteger`, `java.lang.Double`, or `java.math.BigDecimal` be used here instead of a `java.lang.Integer` for method: [`parseRawDataString` inside `org.broadinstitute.hellbender.tools.walkers.annotator.RMSMappingQuality`](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/RMSMappingQuality.java#L251) ?. I've noticed this error when using GATK4 version 4.0.11.0 on the [GATK4 Germline SNPs-INDELs WDL workflow](https://github.com/gatk-workflows/gatk4-germline-snps-indels). It happens on the [`JointGenotyping.GenotypeGVCFs` task](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl#L127).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5433
https://github.com/broadinstitute/gatk/issues/5433:393,Availability,error,error,393,"This issue affect version of GATK4 from version 4.0.10.1 and onwards, and is related to github feature/pull request [#4969: ""Improve MQ calculation accuracy""](https://github.com/broadinstitute/gatk/pull/4969). GVCFs with large `RAW_MQ` sum of sumSquaredMQs values end up with an error like:. ```; A USER ERROR has occurred: Bad input: malformed RAW_MQ annotation: 3415207168,1749038; ```. The error happens when the sumSquaredMQs element value is greater than Java's. `Integer.MAX_VALUE`. See the following [github pull request comment for more details](https://github.com/broadinstitute/gatk/pull/4969#issuecomment-439642813). . Should a `java.lang.Long`, `java.math.BigInteger`, `java.lang.Double`, or `java.math.BigDecimal` be used here instead of a `java.lang.Integer` for method: [`parseRawDataString` inside `org.broadinstitute.hellbender.tools.walkers.annotator.RMSMappingQuality`](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/RMSMappingQuality.java#L251) ?. I've noticed this error when using GATK4 version 4.0.11.0 on the [GATK4 Germline SNPs-INDELs WDL workflow](https://github.com/gatk-workflows/gatk4-germline-snps-indels). It happens on the [`JointGenotyping.GenotypeGVCFs` task](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl#L127).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5433
https://github.com/broadinstitute/gatk/issues/5433:1058,Availability,error,error,1058,"This issue affect version of GATK4 from version 4.0.10.1 and onwards, and is related to github feature/pull request [#4969: ""Improve MQ calculation accuracy""](https://github.com/broadinstitute/gatk/pull/4969). GVCFs with large `RAW_MQ` sum of sumSquaredMQs values end up with an error like:. ```; A USER ERROR has occurred: Bad input: malformed RAW_MQ annotation: 3415207168,1749038; ```. The error happens when the sumSquaredMQs element value is greater than Java's. `Integer.MAX_VALUE`. See the following [github pull request comment for more details](https://github.com/broadinstitute/gatk/pull/4969#issuecomment-439642813). . Should a `java.lang.Long`, `java.math.BigInteger`, `java.lang.Double`, or `java.math.BigDecimal` be used here instead of a `java.lang.Integer` for method: [`parseRawDataString` inside `org.broadinstitute.hellbender.tools.walkers.annotator.RMSMappingQuality`](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/RMSMappingQuality.java#L251) ?. I've noticed this error when using GATK4 version 4.0.11.0 on the [GATK4 Germline SNPs-INDELs WDL workflow](https://github.com/gatk-workflows/gatk4-germline-snps-indels). It happens on the [`JointGenotyping.GenotypeGVCFs` task](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl#L127).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5433
https://github.com/broadinstitute/gatk/issues/5434:955,Deployability,configurat,configuration,955,"### Instructions. Initially reported by a user on the [forum](https://gatkforums.broadinstitute.org/gatk/discussion/13680/variants-with-ad-0-0-and-dp-0#latest)... Aparently some variants with non-zero quals have 0 AD and DPs. Other annotations are also missing from the INFO columns. . After some debugging it turns out that the criteria to determine whether a read should be considered for a variant in terms of alignment overlap are different for taking part of PL calculation and AD/DP calculation. . Where is not totally clear what is the best way to go in practice. It seems to me that we should be consistent here and both PL and AD/DP should use the same criterion. The offending code lines:. **HaplotypeCallerGenotypingEngine.java ln171**:. ```java; ReadLikelihoods<Allele> readAlleleLikelihoods = readLikelihoods.marginalize(alleleMapper, ; new SimpleInterval(mergedVC).expandWithinContig(ALLELE_EXTENSION, header.getSequenceDictionary()));; if (configuration.isSampleContaminationPresent()) {; readAlleleLikelihoods.contaminationDownsampling(configuration.getSampleContamination());; }. ```; The code above decides the involvement in PL calculations. Notice that ```ALLELE_EXTENSION``` is set to ```2```. . For the AD/DP and so on the code responsible is in **AssemblyBasedCallerGenotypingEngine.java ln366**:. ```; // Otherwise (else part) we need to do it again.; if (configuration.useFilteredReadMapForAnnotations || !configuration.isSampleContaminationPresent()) {; readAlleleLikelihoodsForAnnotations = readAlleleLikelihoodsForGenotyping;; readAlleleLikelihoodsForAnnotations.filterToOnlyOverlappingReads(loc);; } else {; readAlleleLikelihoodsForAnnotations = readHaplotypeLikelihoods.marginalize(alleleMapper, loc);; if (emitReferenceConfidence) {; readAlleleLikelihoodsForAnnotations.addNonReferenceAllele(Allele.NON_REF_ALLELE);; }; }. ```. The ```filterToOnlyOverlappingReads(loc)``` is called then the overlap criterion is strict. (e.g. 0bp padding). This is also the case for the ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5434
https://github.com/broadinstitute/gatk/issues/5434:1052,Deployability,configurat,configuration,1052," on the [forum](https://gatkforums.broadinstitute.org/gatk/discussion/13680/variants-with-ad-0-0-and-dp-0#latest)... Aparently some variants with non-zero quals have 0 AD and DPs. Other annotations are also missing from the INFO columns. . After some debugging it turns out that the criteria to determine whether a read should be considered for a variant in terms of alignment overlap are different for taking part of PL calculation and AD/DP calculation. . Where is not totally clear what is the best way to go in practice. It seems to me that we should be consistent here and both PL and AD/DP should use the same criterion. The offending code lines:. **HaplotypeCallerGenotypingEngine.java ln171**:. ```java; ReadLikelihoods<Allele> readAlleleLikelihoods = readLikelihoods.marginalize(alleleMapper, ; new SimpleInterval(mergedVC).expandWithinContig(ALLELE_EXTENSION, header.getSequenceDictionary()));; if (configuration.isSampleContaminationPresent()) {; readAlleleLikelihoods.contaminationDownsampling(configuration.getSampleContamination());; }. ```; The code above decides the involvement in PL calculations. Notice that ```ALLELE_EXTENSION``` is set to ```2```. . For the AD/DP and so on the code responsible is in **AssemblyBasedCallerGenotypingEngine.java ln366**:. ```; // Otherwise (else part) we need to do it again.; if (configuration.useFilteredReadMapForAnnotations || !configuration.isSampleContaminationPresent()) {; readAlleleLikelihoodsForAnnotations = readAlleleLikelihoodsForGenotyping;; readAlleleLikelihoodsForAnnotations.filterToOnlyOverlappingReads(loc);; } else {; readAlleleLikelihoodsForAnnotations = readHaplotypeLikelihoods.marginalize(alleleMapper, loc);; if (emitReferenceConfidence) {; readAlleleLikelihoodsForAnnotations.addNonReferenceAllele(Allele.NON_REF_ALLELE);; }; }. ```. The ```filterToOnlyOverlappingReads(loc)``` is called then the overlap criterion is strict. (e.g. 0bp padding). This is also the case for the ```marginalize``` call if the conditional is ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5434
https://github.com/broadinstitute/gatk/issues/5434:1380,Deployability,configurat,configuration,1380,"ed for a variant in terms of alignment overlap are different for taking part of PL calculation and AD/DP calculation. . Where is not totally clear what is the best way to go in practice. It seems to me that we should be consistent here and both PL and AD/DP should use the same criterion. The offending code lines:. **HaplotypeCallerGenotypingEngine.java ln171**:. ```java; ReadLikelihoods<Allele> readAlleleLikelihoods = readLikelihoods.marginalize(alleleMapper, ; new SimpleInterval(mergedVC).expandWithinContig(ALLELE_EXTENSION, header.getSequenceDictionary()));; if (configuration.isSampleContaminationPresent()) {; readAlleleLikelihoods.contaminationDownsampling(configuration.getSampleContamination());; }. ```; The code above decides the involvement in PL calculations. Notice that ```ALLELE_EXTENSION``` is set to ```2```. . For the AD/DP and so on the code responsible is in **AssemblyBasedCallerGenotypingEngine.java ln366**:. ```; // Otherwise (else part) we need to do it again.; if (configuration.useFilteredReadMapForAnnotations || !configuration.isSampleContaminationPresent()) {; readAlleleLikelihoodsForAnnotations = readAlleleLikelihoodsForGenotyping;; readAlleleLikelihoodsForAnnotations.filterToOnlyOverlappingReads(loc);; } else {; readAlleleLikelihoodsForAnnotations = readHaplotypeLikelihoods.marginalize(alleleMapper, loc);; if (emitReferenceConfidence) {; readAlleleLikelihoodsForAnnotations.addNonReferenceAllele(Allele.NON_REF_ALLELE);; }; }. ```. The ```filterToOnlyOverlappingReads(loc)``` is called then the overlap criterion is strict. (e.g. 0bp padding). This is also the case for the ```marginalize``` call if the conditional is false as the loc passed has not been padded. It seems to me that setting the ```ALLELE_EXTENSION == 2``` is a very deliberative action (so it was done for a reason) and perhaps this is the way to go... but in deed if the read really does not overlap the variant should be considered at all. . This come from a more complex discussion whet",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5434
https://github.com/broadinstitute/gatk/issues/5434:1431,Deployability,configurat,configuration,1431,"ent overlap are different for taking part of PL calculation and AD/DP calculation. . Where is not totally clear what is the best way to go in practice. It seems to me that we should be consistent here and both PL and AD/DP should use the same criterion. The offending code lines:. **HaplotypeCallerGenotypingEngine.java ln171**:. ```java; ReadLikelihoods<Allele> readAlleleLikelihoods = readLikelihoods.marginalize(alleleMapper, ; new SimpleInterval(mergedVC).expandWithinContig(ALLELE_EXTENSION, header.getSequenceDictionary()));; if (configuration.isSampleContaminationPresent()) {; readAlleleLikelihoods.contaminationDownsampling(configuration.getSampleContamination());; }. ```; The code above decides the involvement in PL calculations. Notice that ```ALLELE_EXTENSION``` is set to ```2```. . For the AD/DP and so on the code responsible is in **AssemblyBasedCallerGenotypingEngine.java ln366**:. ```; // Otherwise (else part) we need to do it again.; if (configuration.useFilteredReadMapForAnnotations || !configuration.isSampleContaminationPresent()) {; readAlleleLikelihoodsForAnnotations = readAlleleLikelihoodsForGenotyping;; readAlleleLikelihoodsForAnnotations.filterToOnlyOverlappingReads(loc);; } else {; readAlleleLikelihoodsForAnnotations = readHaplotypeLikelihoods.marginalize(alleleMapper, loc);; if (emitReferenceConfidence) {; readAlleleLikelihoodsForAnnotations.addNonReferenceAllele(Allele.NON_REF_ALLELE);; }; }. ```. The ```filterToOnlyOverlappingReads(loc)``` is called then the overlap criterion is strict. (e.g. 0bp padding). This is also the case for the ```marginalize``` call if the conditional is false as the loc passed has not been padded. It seems to me that setting the ```ALLELE_EXTENSION == 2``` is a very deliberative action (so it was done for a reason) and perhaps this is the way to go... but in deed if the read really does not overlap the variant should be considered at all. . This come from a more complex discussion whether the in cases whether variants ar",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5434
https://github.com/broadinstitute/gatk/issues/5434:955,Modifiability,config,configuration,955,"### Instructions. Initially reported by a user on the [forum](https://gatkforums.broadinstitute.org/gatk/discussion/13680/variants-with-ad-0-0-and-dp-0#latest)... Aparently some variants with non-zero quals have 0 AD and DPs. Other annotations are also missing from the INFO columns. . After some debugging it turns out that the criteria to determine whether a read should be considered for a variant in terms of alignment overlap are different for taking part of PL calculation and AD/DP calculation. . Where is not totally clear what is the best way to go in practice. It seems to me that we should be consistent here and both PL and AD/DP should use the same criterion. The offending code lines:. **HaplotypeCallerGenotypingEngine.java ln171**:. ```java; ReadLikelihoods<Allele> readAlleleLikelihoods = readLikelihoods.marginalize(alleleMapper, ; new SimpleInterval(mergedVC).expandWithinContig(ALLELE_EXTENSION, header.getSequenceDictionary()));; if (configuration.isSampleContaminationPresent()) {; readAlleleLikelihoods.contaminationDownsampling(configuration.getSampleContamination());; }. ```; The code above decides the involvement in PL calculations. Notice that ```ALLELE_EXTENSION``` is set to ```2```. . For the AD/DP and so on the code responsible is in **AssemblyBasedCallerGenotypingEngine.java ln366**:. ```; // Otherwise (else part) we need to do it again.; if (configuration.useFilteredReadMapForAnnotations || !configuration.isSampleContaminationPresent()) {; readAlleleLikelihoodsForAnnotations = readAlleleLikelihoodsForGenotyping;; readAlleleLikelihoodsForAnnotations.filterToOnlyOverlappingReads(loc);; } else {; readAlleleLikelihoodsForAnnotations = readHaplotypeLikelihoods.marginalize(alleleMapper, loc);; if (emitReferenceConfidence) {; readAlleleLikelihoodsForAnnotations.addNonReferenceAllele(Allele.NON_REF_ALLELE);; }; }. ```. The ```filterToOnlyOverlappingReads(loc)``` is called then the overlap criterion is strict. (e.g. 0bp padding). This is also the case for the ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5434
https://github.com/broadinstitute/gatk/issues/5434:1052,Modifiability,config,configuration,1052," on the [forum](https://gatkforums.broadinstitute.org/gatk/discussion/13680/variants-with-ad-0-0-and-dp-0#latest)... Aparently some variants with non-zero quals have 0 AD and DPs. Other annotations are also missing from the INFO columns. . After some debugging it turns out that the criteria to determine whether a read should be considered for a variant in terms of alignment overlap are different for taking part of PL calculation and AD/DP calculation. . Where is not totally clear what is the best way to go in practice. It seems to me that we should be consistent here and both PL and AD/DP should use the same criterion. The offending code lines:. **HaplotypeCallerGenotypingEngine.java ln171**:. ```java; ReadLikelihoods<Allele> readAlleleLikelihoods = readLikelihoods.marginalize(alleleMapper, ; new SimpleInterval(mergedVC).expandWithinContig(ALLELE_EXTENSION, header.getSequenceDictionary()));; if (configuration.isSampleContaminationPresent()) {; readAlleleLikelihoods.contaminationDownsampling(configuration.getSampleContamination());; }. ```; The code above decides the involvement in PL calculations. Notice that ```ALLELE_EXTENSION``` is set to ```2```. . For the AD/DP and so on the code responsible is in **AssemblyBasedCallerGenotypingEngine.java ln366**:. ```; // Otherwise (else part) we need to do it again.; if (configuration.useFilteredReadMapForAnnotations || !configuration.isSampleContaminationPresent()) {; readAlleleLikelihoodsForAnnotations = readAlleleLikelihoodsForGenotyping;; readAlleleLikelihoodsForAnnotations.filterToOnlyOverlappingReads(loc);; } else {; readAlleleLikelihoodsForAnnotations = readHaplotypeLikelihoods.marginalize(alleleMapper, loc);; if (emitReferenceConfidence) {; readAlleleLikelihoodsForAnnotations.addNonReferenceAllele(Allele.NON_REF_ALLELE);; }; }. ```. The ```filterToOnlyOverlappingReads(loc)``` is called then the overlap criterion is strict. (e.g. 0bp padding). This is also the case for the ```marginalize``` call if the conditional is ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5434
https://github.com/broadinstitute/gatk/issues/5434:1380,Modifiability,config,configuration,1380,"ed for a variant in terms of alignment overlap are different for taking part of PL calculation and AD/DP calculation. . Where is not totally clear what is the best way to go in practice. It seems to me that we should be consistent here and both PL and AD/DP should use the same criterion. The offending code lines:. **HaplotypeCallerGenotypingEngine.java ln171**:. ```java; ReadLikelihoods<Allele> readAlleleLikelihoods = readLikelihoods.marginalize(alleleMapper, ; new SimpleInterval(mergedVC).expandWithinContig(ALLELE_EXTENSION, header.getSequenceDictionary()));; if (configuration.isSampleContaminationPresent()) {; readAlleleLikelihoods.contaminationDownsampling(configuration.getSampleContamination());; }. ```; The code above decides the involvement in PL calculations. Notice that ```ALLELE_EXTENSION``` is set to ```2```. . For the AD/DP and so on the code responsible is in **AssemblyBasedCallerGenotypingEngine.java ln366**:. ```; // Otherwise (else part) we need to do it again.; if (configuration.useFilteredReadMapForAnnotations || !configuration.isSampleContaminationPresent()) {; readAlleleLikelihoodsForAnnotations = readAlleleLikelihoodsForGenotyping;; readAlleleLikelihoodsForAnnotations.filterToOnlyOverlappingReads(loc);; } else {; readAlleleLikelihoodsForAnnotations = readHaplotypeLikelihoods.marginalize(alleleMapper, loc);; if (emitReferenceConfidence) {; readAlleleLikelihoodsForAnnotations.addNonReferenceAllele(Allele.NON_REF_ALLELE);; }; }. ```. The ```filterToOnlyOverlappingReads(loc)``` is called then the overlap criterion is strict. (e.g. 0bp padding). This is also the case for the ```marginalize``` call if the conditional is false as the loc passed has not been padded. It seems to me that setting the ```ALLELE_EXTENSION == 2``` is a very deliberative action (so it was done for a reason) and perhaps this is the way to go... but in deed if the read really does not overlap the variant should be considered at all. . This come from a more complex discussion whet",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5434
https://github.com/broadinstitute/gatk/issues/5434:1431,Modifiability,config,configuration,1431,"ent overlap are different for taking part of PL calculation and AD/DP calculation. . Where is not totally clear what is the best way to go in practice. It seems to me that we should be consistent here and both PL and AD/DP should use the same criterion. The offending code lines:. **HaplotypeCallerGenotypingEngine.java ln171**:. ```java; ReadLikelihoods<Allele> readAlleleLikelihoods = readLikelihoods.marginalize(alleleMapper, ; new SimpleInterval(mergedVC).expandWithinContig(ALLELE_EXTENSION, header.getSequenceDictionary()));; if (configuration.isSampleContaminationPresent()) {; readAlleleLikelihoods.contaminationDownsampling(configuration.getSampleContamination());; }. ```; The code above decides the involvement in PL calculations. Notice that ```ALLELE_EXTENSION``` is set to ```2```. . For the AD/DP and so on the code responsible is in **AssemblyBasedCallerGenotypingEngine.java ln366**:. ```; // Otherwise (else part) we need to do it again.; if (configuration.useFilteredReadMapForAnnotations || !configuration.isSampleContaminationPresent()) {; readAlleleLikelihoodsForAnnotations = readAlleleLikelihoodsForGenotyping;; readAlleleLikelihoodsForAnnotations.filterToOnlyOverlappingReads(loc);; } else {; readAlleleLikelihoodsForAnnotations = readHaplotypeLikelihoods.marginalize(alleleMapper, loc);; if (emitReferenceConfidence) {; readAlleleLikelihoodsForAnnotations.addNonReferenceAllele(Allele.NON_REF_ALLELE);; }; }. ```. The ```filterToOnlyOverlappingReads(loc)``` is called then the overlap criterion is strict. (e.g. 0bp padding). This is also the case for the ```marginalize``` call if the conditional is false as the loc passed has not been padded. It seems to me that setting the ```ALLELE_EXTENSION == 2``` is a very deliberative action (so it was done for a reason) and perhaps this is the way to go... but in deed if the read really does not overlap the variant should be considered at all. . This come from a more complex discussion whether the in cases whether variants ar",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5434
https://github.com/broadinstitute/gatk/issues/5434:525,Usability,clear,clear,525,"### Instructions. Initially reported by a user on the [forum](https://gatkforums.broadinstitute.org/gatk/discussion/13680/variants-with-ad-0-0-and-dp-0#latest)... Aparently some variants with non-zero quals have 0 AD and DPs. Other annotations are also missing from the INFO columns. . After some debugging it turns out that the criteria to determine whether a read should be considered for a variant in terms of alignment overlap are different for taking part of PL calculation and AD/DP calculation. . Where is not totally clear what is the best way to go in practice. It seems to me that we should be consistent here and both PL and AD/DP should use the same criterion. The offending code lines:. **HaplotypeCallerGenotypingEngine.java ln171**:. ```java; ReadLikelihoods<Allele> readAlleleLikelihoods = readLikelihoods.marginalize(alleleMapper, ; new SimpleInterval(mergedVC).expandWithinContig(ALLELE_EXTENSION, header.getSequenceDictionary()));; if (configuration.isSampleContaminationPresent()) {; readAlleleLikelihoods.contaminationDownsampling(configuration.getSampleContamination());; }. ```; The code above decides the involvement in PL calculations. Notice that ```ALLELE_EXTENSION``` is set to ```2```. . For the AD/DP and so on the code responsible is in **AssemblyBasedCallerGenotypingEngine.java ln366**:. ```; // Otherwise (else part) we need to do it again.; if (configuration.useFilteredReadMapForAnnotations || !configuration.isSampleContaminationPresent()) {; readAlleleLikelihoodsForAnnotations = readAlleleLikelihoodsForGenotyping;; readAlleleLikelihoodsForAnnotations.filterToOnlyOverlappingReads(loc);; } else {; readAlleleLikelihoodsForAnnotations = readHaplotypeLikelihoods.marginalize(alleleMapper, loc);; if (emitReferenceConfidence) {; readAlleleLikelihoodsForAnnotations.addNonReferenceAllele(Allele.NON_REF_ALLELE);; }; }. ```. The ```filterToOnlyOverlappingReads(loc)``` is called then the overlap criterion is strict. (e.g. 0bp padding). This is also the case for the ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5434
https://github.com/broadinstitute/gatk/issues/5434:854,Usability,Simpl,SimpleInterval,854,"### Instructions. Initially reported by a user on the [forum](https://gatkforums.broadinstitute.org/gatk/discussion/13680/variants-with-ad-0-0-and-dp-0#latest)... Aparently some variants with non-zero quals have 0 AD and DPs. Other annotations are also missing from the INFO columns. . After some debugging it turns out that the criteria to determine whether a read should be considered for a variant in terms of alignment overlap are different for taking part of PL calculation and AD/DP calculation. . Where is not totally clear what is the best way to go in practice. It seems to me that we should be consistent here and both PL and AD/DP should use the same criterion. The offending code lines:. **HaplotypeCallerGenotypingEngine.java ln171**:. ```java; ReadLikelihoods<Allele> readAlleleLikelihoods = readLikelihoods.marginalize(alleleMapper, ; new SimpleInterval(mergedVC).expandWithinContig(ALLELE_EXTENSION, header.getSequenceDictionary()));; if (configuration.isSampleContaminationPresent()) {; readAlleleLikelihoods.contaminationDownsampling(configuration.getSampleContamination());; }. ```; The code above decides the involvement in PL calculations. Notice that ```ALLELE_EXTENSION``` is set to ```2```. . For the AD/DP and so on the code responsible is in **AssemblyBasedCallerGenotypingEngine.java ln366**:. ```; // Otherwise (else part) we need to do it again.; if (configuration.useFilteredReadMapForAnnotations || !configuration.isSampleContaminationPresent()) {; readAlleleLikelihoodsForAnnotations = readAlleleLikelihoodsForGenotyping;; readAlleleLikelihoodsForAnnotations.filterToOnlyOverlappingReads(loc);; } else {; readAlleleLikelihoodsForAnnotations = readHaplotypeLikelihoods.marginalize(alleleMapper, loc);; if (emitReferenceConfidence) {; readAlleleLikelihoodsForAnnotations.addNonReferenceAllele(Allele.NON_REF_ALLELE);; }; }. ```. The ```filterToOnlyOverlappingReads(loc)``` is called then the overlap criterion is strict. (e.g. 0bp padding). This is also the case for the ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5434
https://github.com/broadinstitute/gatk/pull/5435:0,Deployability,Update,Updated,0,"Updated RMS Mapping quality annotations to be of type long/Long instead; of int/Integer. With int types, a large cohort could overflow; Integer.MAX_VALUE when handling sum of squared mapping qualities. With; long types this should not be a problem until we have off-world; colonies. This resolves issue 5433.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5435
https://github.com/broadinstitute/gatk/issues/5437:105,Availability,down,downsampler,105,The work that @tomwhite has done in the HalpotypeCallerSpark has illuminated the fact that currently the downsampler is statefully dependent on the random generator underneath in terms of how it selects reads to be downsampled. This has become an issue since we would like to separate the process of assembly region construction and genotype calling across stages of the spark task. In order to do this successfully there needs to be some way to reproduce the same downsampled results for a given site based solely on the reads present at that site.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5437
https://github.com/broadinstitute/gatk/issues/5437:215,Availability,down,downsampled,215,The work that @tomwhite has done in the HalpotypeCallerSpark has illuminated the fact that currently the downsampler is statefully dependent on the random generator underneath in terms of how it selects reads to be downsampled. This has become an issue since we would like to separate the process of assembly region construction and genotype calling across stages of the spark task. In order to do this successfully there needs to be some way to reproduce the same downsampled results for a given site based solely on the reads present at that site.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5437
https://github.com/broadinstitute/gatk/issues/5437:465,Availability,down,downsampled,465,The work that @tomwhite has done in the HalpotypeCallerSpark has illuminated the fact that currently the downsampler is statefully dependent on the random generator underneath in terms of how it selects reads to be downsampled. This has become an issue since we would like to separate the process of assembly region construction and genotype calling across stages of the spark task. In order to do this successfully there needs to be some way to reproduce the same downsampled results for a given site based solely on the reads present at that site.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5437
https://github.com/broadinstitute/gatk/issues/5437:131,Integrability,depend,dependent,131,The work that @tomwhite has done in the HalpotypeCallerSpark has illuminated the fact that currently the downsampler is statefully dependent on the random generator underneath in terms of how it selects reads to be downsampled. This has become an issue since we would like to separate the process of assembly region construction and genotype calling across stages of the spark task. In order to do this successfully there needs to be some way to reproduce the same downsampled results for a given site based solely on the reads present at that site.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5437
https://github.com/broadinstitute/gatk/issues/5439:1359,Availability,redundant,redundant,1359,"The GATK4 port of GATK3 VariantEval uses a MultiVariantWalker traversal, along with individual `FeatureInput` arguments for evals, knowns, comps, etc., which are all manually merged together as the walker's driving variants. The resulting variants are then manually processed in groups, by start position. Since the tool needs to know the origin of each variant (eval, comp, dbsnp, known, etc.), and since this isn't preserved by the engine, it re-queries the `FeatureContext` for each input to get the same set of variants grouped by source. Since the inputs are typed as `FeatureInput`, this results in all inputs being both consumed and cached twice; once by `MultiVariantDataSource` and once by `FeatureManager`. Once alternative would be to use a LocusWalker, but that would still require index queries (though the features would be cached), and it would still require manual filtering/aggregation on start position. Proposed fix is to switch the base class to use `MultiVariantWalkerGroupedOnStart` (this would allow removal of `PositionAggregator` class); change the engine to preserve the input source of each variant as proposed in https://github.com/broadinstitute/gatk/pull/4571; and change the input arguments for VariantEval from individual named arguments to tagged feature inputs. This would greatly simplify the initialization code, eliminate redundant reading and caching, and allow the tool to do the input source grouping by just looking at each variant's source field.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5439
https://github.com/broadinstitute/gatk/issues/5439:640,Performance,cache,cached,640,"The GATK4 port of GATK3 VariantEval uses a MultiVariantWalker traversal, along with individual `FeatureInput` arguments for evals, knowns, comps, etc., which are all manually merged together as the walker's driving variants. The resulting variants are then manually processed in groups, by start position. Since the tool needs to know the origin of each variant (eval, comp, dbsnp, known, etc.), and since this isn't preserved by the engine, it re-queries the `FeatureContext` for each input to get the same set of variants grouped by source. Since the inputs are typed as `FeatureInput`, this results in all inputs being both consumed and cached twice; once by `MultiVariantDataSource` and once by `FeatureManager`. Once alternative would be to use a LocusWalker, but that would still require index queries (though the features would be cached), and it would still require manual filtering/aggregation on start position. Proposed fix is to switch the base class to use `MultiVariantWalkerGroupedOnStart` (this would allow removal of `PositionAggregator` class); change the engine to preserve the input source of each variant as proposed in https://github.com/broadinstitute/gatk/pull/4571; and change the input arguments for VariantEval from individual named arguments to tagged feature inputs. This would greatly simplify the initialization code, eliminate redundant reading and caching, and allow the tool to do the input source grouping by just looking at each variant's source field.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5439
https://github.com/broadinstitute/gatk/issues/5439:838,Performance,cache,cached,838,"The GATK4 port of GATK3 VariantEval uses a MultiVariantWalker traversal, along with individual `FeatureInput` arguments for evals, knowns, comps, etc., which are all manually merged together as the walker's driving variants. The resulting variants are then manually processed in groups, by start position. Since the tool needs to know the origin of each variant (eval, comp, dbsnp, known, etc.), and since this isn't preserved by the engine, it re-queries the `FeatureContext` for each input to get the same set of variants grouped by source. Since the inputs are typed as `FeatureInput`, this results in all inputs being both consumed and cached twice; once by `MultiVariantDataSource` and once by `FeatureManager`. Once alternative would be to use a LocusWalker, but that would still require index queries (though the features would be cached), and it would still require manual filtering/aggregation on start position. Proposed fix is to switch the base class to use `MultiVariantWalkerGroupedOnStart` (this would allow removal of `PositionAggregator` class); change the engine to preserve the input source of each variant as proposed in https://github.com/broadinstitute/gatk/pull/4571; and change the input arguments for VariantEval from individual named arguments to tagged feature inputs. This would greatly simplify the initialization code, eliminate redundant reading and caching, and allow the tool to do the input source grouping by just looking at each variant's source field.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5439
https://github.com/broadinstitute/gatk/issues/5439:1359,Safety,redund,redundant,1359,"The GATK4 port of GATK3 VariantEval uses a MultiVariantWalker traversal, along with individual `FeatureInput` arguments for evals, knowns, comps, etc., which are all manually merged together as the walker's driving variants. The resulting variants are then manually processed in groups, by start position. Since the tool needs to know the origin of each variant (eval, comp, dbsnp, known, etc.), and since this isn't preserved by the engine, it re-queries the `FeatureContext` for each input to get the same set of variants grouped by source. Since the inputs are typed as `FeatureInput`, this results in all inputs being both consumed and cached twice; once by `MultiVariantDataSource` and once by `FeatureManager`. Once alternative would be to use a LocusWalker, but that would still require index queries (though the features would be cached), and it would still require manual filtering/aggregation on start position. Proposed fix is to switch the base class to use `MultiVariantWalkerGroupedOnStart` (this would allow removal of `PositionAggregator` class); change the engine to preserve the input source of each variant as proposed in https://github.com/broadinstitute/gatk/pull/4571; and change the input arguments for VariantEval from individual named arguments to tagged feature inputs. This would greatly simplify the initialization code, eliminate redundant reading and caching, and allow the tool to do the input source grouping by just looking at each variant's source field.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5439
https://github.com/broadinstitute/gatk/issues/5439:1315,Usability,simpl,simplify,1315,"The GATK4 port of GATK3 VariantEval uses a MultiVariantWalker traversal, along with individual `FeatureInput` arguments for evals, knowns, comps, etc., which are all manually merged together as the walker's driving variants. The resulting variants are then manually processed in groups, by start position. Since the tool needs to know the origin of each variant (eval, comp, dbsnp, known, etc.), and since this isn't preserved by the engine, it re-queries the `FeatureContext` for each input to get the same set of variants grouped by source. Since the inputs are typed as `FeatureInput`, this results in all inputs being both consumed and cached twice; once by `MultiVariantDataSource` and once by `FeatureManager`. Once alternative would be to use a LocusWalker, but that would still require index queries (though the features would be cached), and it would still require manual filtering/aggregation on start position. Proposed fix is to switch the base class to use `MultiVariantWalkerGroupedOnStart` (this would allow removal of `PositionAggregator` class); change the engine to preserve the input source of each variant as proposed in https://github.com/broadinstitute/gatk/pull/4571; and change the input arguments for VariantEval from individual named arguments to tagged feature inputs. This would greatly simplify the initialization code, eliminate redundant reading and caching, and allow the tool to do the input source grouping by just looking at each variant's source field.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5439
https://github.com/broadinstitute/gatk/issues/5440:240,Availability,redundant,redundant,240,"The varianteval package (VariantEval, evaluators/stratifiers and stratification manager) was ported directly from GATK3 to minimize diffs for review, and needs a code-style cleanup pass:. - rename variables with names in ALL_CAPS; - remove redundant type instantiation params in favor of <> operator; - add finals; - revisit the use of generic type params and required casts, etc in StratificationManager and stratifier classes",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5440
https://github.com/broadinstitute/gatk/issues/5440:197,Modifiability,variab,variables,197,"The varianteval package (VariantEval, evaluators/stratifiers and stratification manager) was ported directly from GATK3 to minimize diffs for review, and needs a code-style cleanup pass:. - rename variables with names in ALL_CAPS; - remove redundant type instantiation params in favor of <> operator; - add finals; - revisit the use of generic type params and required casts, etc in StratificationManager and stratifier classes",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5440
https://github.com/broadinstitute/gatk/issues/5440:240,Safety,redund,redundant,240,"The varianteval package (VariantEval, evaluators/stratifiers and stratification manager) was ported directly from GATK3 to minimize diffs for review, and needs a code-style cleanup pass:. - rename variables with names in ALL_CAPS; - remove redundant type instantiation params in favor of <> operator; - add finals; - revisit the use of generic type params and required casts, etc in StratificationManager and stratifier classes",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5440
https://github.com/broadinstitute/gatk/issues/5441:55,Integrability,depend,depends,55,"`VariantEvalUtils` was ported directly from GATK3, and depends on `VariantEval` to discover various input argument values for stratifiers, etc. An argument collection class should be factored out of VariantEval, with `@Argument`values for these values, and methods for retrieving them (each method in `VariantEvalUtils` that calls back to the walker should be moved into the arg collection class). The dependency should be inverted so that `VariantEvalUtils` and `VariantEval` both depend on the argument collection class. Currently the various classes in the varianteval project make assumptions about things like VariantEval command line argument names.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5441
https://github.com/broadinstitute/gatk/issues/5441:402,Integrability,depend,dependency,402,"`VariantEvalUtils` was ported directly from GATK3, and depends on `VariantEval` to discover various input argument values for stratifiers, etc. An argument collection class should be factored out of VariantEval, with `@Argument`values for these values, and methods for retrieving them (each method in `VariantEvalUtils` that calls back to the walker should be moved into the arg collection class). The dependency should be inverted so that `VariantEvalUtils` and `VariantEval` both depend on the argument collection class. Currently the various classes in the varianteval project make assumptions about things like VariantEval command line argument names.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5441
https://github.com/broadinstitute/gatk/issues/5441:482,Integrability,depend,depend,482,"`VariantEvalUtils` was ported directly from GATK3, and depends on `VariantEval` to discover various input argument values for stratifiers, etc. An argument collection class should be factored out of VariantEval, with `@Argument`values for these values, and methods for retrieving them (each method in `VariantEvalUtils` that calls back to the walker should be moved into the arg collection class). The dependency should be inverted so that `VariantEvalUtils` and `VariantEval` both depend on the argument collection class. Currently the various classes in the varianteval project make assumptions about things like VariantEval command line argument names.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5441
https://github.com/broadinstitute/gatk/pull/5442:265,Availability,error,error-java-lang-numberformatexception-for-input-string,265,"Closes #5391. @takutosato this will make several users happy: https://gatkforums.broadinstitute.org/gatk/discussion/13467/mutect2-pipeline-fails-for-some-inputs#latest, https://gatkforums.broadinstitute.org/gatk/discussion/12618/run-mutect2-in-gatk-4-0-4-0-with-an-error-java-lang-numberformatexception-for-input-string. Looping in @bhanugandham",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5442
https://github.com/broadinstitute/gatk/pull/5442:130,Deployability,pipeline,pipeline-fails-for-some-inputs,130,"Closes #5391. @takutosato this will make several users happy: https://gatkforums.broadinstitute.org/gatk/discussion/13467/mutect2-pipeline-fails-for-some-inputs#latest, https://gatkforums.broadinstitute.org/gatk/discussion/12618/run-mutect2-in-gatk-4-0-4-0-with-an-error-java-lang-numberformatexception-for-input-string. Looping in @bhanugandham",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5442
https://github.com/broadinstitute/gatk/pull/5443:13,Usability,simpl,simple,13,"This PR is a simple addition to add GATKReportTable.getDescription(), to allow other classes to get the value of description.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5443
https://github.com/broadinstitute/gatk/pull/5444:20,Availability,error,error,20,* Fixing an obscure error in an error message in GATKAnnotationPluginDescription.; * entrySet was called instead of values as a result everything was always filtered from the error message. Noticed this one completely by chance.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5444
https://github.com/broadinstitute/gatk/pull/5444:32,Availability,error,error,32,* Fixing an obscure error in an error message in GATKAnnotationPluginDescription.; * entrySet was called instead of values as a result everything was always filtered from the error message. Noticed this one completely by chance.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5444
https://github.com/broadinstitute/gatk/pull/5444:175,Availability,error,error,175,* Fixing an obscure error in an error message in GATKAnnotationPluginDescription.; * entrySet was called instead of values as a result everything was always filtered from the error message. Noticed this one completely by chance.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5444
https://github.com/broadinstitute/gatk/pull/5444:38,Integrability,message,message,38,* Fixing an obscure error in an error message in GATKAnnotationPluginDescription.; * entrySet was called instead of values as a result everything was always filtered from the error message. Noticed this one completely by chance.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5444
https://github.com/broadinstitute/gatk/pull/5444:181,Integrability,message,message,181,* Fixing an obscure error in an error message in GATKAnnotationPluginDescription.; * entrySet was called instead of values as a result everything was always filtered from the error message. Noticed this one completely by chance.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5444
https://github.com/broadinstitute/gatk/issues/5445:143,Deployability,Pipeline,Pipeline,143,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); GATK 3.7 and GATK 4.0.11.0; ### Description . GATK Pipeline (HaplotypeCaller->gvcf-->importdb->GenotypeGVCF)is calling sites with a GQ=0. But these sites often have plenty of coverage and no obvious reason for such a low GQ score. Often the GQ should be 99 as the DP >40. This seems to be primarily an issue with homozygous reference calls. . The GT is accurate for the high DP sites but the inaccurate GQ is problematic for any genotype level qc on the pVCF. If the site is recoded from 0/0 to './.' for GQ <20, the result is higher missing rate due to the inaccurate GQ=0. . Directly calling the VCF with HaplotypeCaller without the gVCF intermediate gVCF file calculates the correct GQ score. Freebayes also calculates a correct GQ on these samples.; [rs429358_gq_dp.pdf](https://github.com/broadinstitute/gatk/files/2612419/rs429358_gq_dp.pdf). #### Steps to reproduce. I am seeing this bug for 57 samples of 5000 crams at snp rs429358 but I would expect it is not unique to this site. . Select two crams with a Passed site with:; cram 1. Call with GT='0/0, GQ=0 and DP >40.; cram 2. Call with GT='0/1' or '1/1' and DP>20. . Create vcf with two approaches:. Pipeline 1. HaplotypeCaller-->vcf. module load gatk/4.0.11.0; gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I gq0_cram.list\; -L chr19:44907684-44909822\; --use-new-qual-calculator\; -O good.vcf.gz. Good GQ scores were also estimated with Freebayes on these samples also. Pipeline 2 HaplotypeCaller --> bvcf--->ImportVCF-->GenotypeVCF-->VCF with 2 samples. gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I $sample.cram\; --use-new-qual-calculator\; -L chr19:44907684-44909822\; -ERC GVCF\; -O bad.g.vcf.gz. Followed by import and GenotypeVCF. . #### Expected behavior; Pipeline 2 should generate accurate GQ scores that match the GQ in th",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5445
https://github.com/broadinstitute/gatk/issues/5445:1254,Deployability,Pipeline,Pipeline,1254,"enty of coverage and no obvious reason for such a low GQ score. Often the GQ should be 99 as the DP >40. This seems to be primarily an issue with homozygous reference calls. . The GT is accurate for the high DP sites but the inaccurate GQ is problematic for any genotype level qc on the pVCF. If the site is recoded from 0/0 to './.' for GQ <20, the result is higher missing rate due to the inaccurate GQ=0. . Directly calling the VCF with HaplotypeCaller without the gVCF intermediate gVCF file calculates the correct GQ score. Freebayes also calculates a correct GQ on these samples.; [rs429358_gq_dp.pdf](https://github.com/broadinstitute/gatk/files/2612419/rs429358_gq_dp.pdf). #### Steps to reproduce. I am seeing this bug for 57 samples of 5000 crams at snp rs429358 but I would expect it is not unique to this site. . Select two crams with a Passed site with:; cram 1. Call with GT='0/0, GQ=0 and DP >40.; cram 2. Call with GT='0/1' or '1/1' and DP>20. . Create vcf with two approaches:. Pipeline 1. HaplotypeCaller-->vcf. module load gatk/4.0.11.0; gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I gq0_cram.list\; -L chr19:44907684-44909822\; --use-new-qual-calculator\; -O good.vcf.gz. Good GQ scores were also estimated with Freebayes on these samples also. Pipeline 2 HaplotypeCaller --> bvcf--->ImportVCF-->GenotypeVCF-->VCF with 2 samples. gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I $sample.cram\; --use-new-qual-calculator\; -L chr19:44907684-44909822\; -ERC GVCF\; -O bad.g.vcf.gz. Followed by import and GenotypeVCF. . #### Expected behavior; Pipeline 2 should generate accurate GQ scores that match the GQ in the HaplotypeCaller vcf output of pipeline 1. Instead GQ=0. . This is the output for the 57 GQ=0 samples with pipeline 1 which is accurate. AC=7;AF=0.061;AN=114;BaseQRankSum=-6.147;DP=1846;ExcessHet=3.8592;FS=0.000;InbreedingCoeff=-0.0640;MLEAC=6;MLEAF=0.053;M",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5445
https://github.com/broadinstitute/gatk/issues/5445:1581,Deployability,Pipeline,Pipeline,1581,"her missing rate due to the inaccurate GQ=0. . Directly calling the VCF with HaplotypeCaller without the gVCF intermediate gVCF file calculates the correct GQ score. Freebayes also calculates a correct GQ on these samples.; [rs429358_gq_dp.pdf](https://github.com/broadinstitute/gatk/files/2612419/rs429358_gq_dp.pdf). #### Steps to reproduce. I am seeing this bug for 57 samples of 5000 crams at snp rs429358 but I would expect it is not unique to this site. . Select two crams with a Passed site with:; cram 1. Call with GT='0/0, GQ=0 and DP >40.; cram 2. Call with GT='0/1' or '1/1' and DP>20. . Create vcf with two approaches:. Pipeline 1. HaplotypeCaller-->vcf. module load gatk/4.0.11.0; gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I gq0_cram.list\; -L chr19:44907684-44909822\; --use-new-qual-calculator\; -O good.vcf.gz. Good GQ scores were also estimated with Freebayes on these samples also. Pipeline 2 HaplotypeCaller --> bvcf--->ImportVCF-->GenotypeVCF-->VCF with 2 samples. gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I $sample.cram\; --use-new-qual-calculator\; -L chr19:44907684-44909822\; -ERC GVCF\; -O bad.g.vcf.gz. Followed by import and GenotypeVCF. . #### Expected behavior; Pipeline 2 should generate accurate GQ scores that match the GQ in the HaplotypeCaller vcf output of pipeline 1. Instead GQ=0. . This is the output for the 57 GQ=0 samples with pipeline 1 which is accurate. AC=7;AF=0.061;AN=114;BaseQRankSum=-6.147;DP=1846;ExcessHet=3.8592;FS=0.000;InbreedingCoeff=-0.0640;MLEAC=6;MLEAF=0.053;MQ=60.00;MQRankSum=0.000;QD=4.52;ReadPosRankSum=-0.781;SOR=2.833; GT:AD:DP:GQ:PL; 0/0:37,0:37:99:0,111,1236; 0/0:40,0:40:99:0,120,1357; 0/0:34,0:34:99:0,102,1161; 0/0:49,0:49:99:0,147,1673; 0/0:33,0:33:99:0,99,1036; 0/0:48,0:48:99:0,144,1728; 0/0:42,0:42:99:0,126,1410; 0/0:37,0:37:99:0,111,1215; 0/0:39,0:39:99:0,117,1311; 0/0:42,0:42:99:0,126,1419; 0/0:53,0:53:99",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5445
https://github.com/broadinstitute/gatk/issues/5445:1932,Deployability,Pipeline,Pipeline,1932,"ng this bug for 57 samples of 5000 crams at snp rs429358 but I would expect it is not unique to this site. . Select two crams with a Passed site with:; cram 1. Call with GT='0/0, GQ=0 and DP >40.; cram 2. Call with GT='0/1' or '1/1' and DP>20. . Create vcf with two approaches:. Pipeline 1. HaplotypeCaller-->vcf. module load gatk/4.0.11.0; gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I gq0_cram.list\; -L chr19:44907684-44909822\; --use-new-qual-calculator\; -O good.vcf.gz. Good GQ scores were also estimated with Freebayes on these samples also. Pipeline 2 HaplotypeCaller --> bvcf--->ImportVCF-->GenotypeVCF-->VCF with 2 samples. gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I $sample.cram\; --use-new-qual-calculator\; -L chr19:44907684-44909822\; -ERC GVCF\; -O bad.g.vcf.gz. Followed by import and GenotypeVCF. . #### Expected behavior; Pipeline 2 should generate accurate GQ scores that match the GQ in the HaplotypeCaller vcf output of pipeline 1. Instead GQ=0. . This is the output for the 57 GQ=0 samples with pipeline 1 which is accurate. AC=7;AF=0.061;AN=114;BaseQRankSum=-6.147;DP=1846;ExcessHet=3.8592;FS=0.000;InbreedingCoeff=-0.0640;MLEAC=6;MLEAF=0.053;MQ=60.00;MQRankSum=0.000;QD=4.52;ReadPosRankSum=-0.781;SOR=2.833; GT:AD:DP:GQ:PL; 0/0:37,0:37:99:0,111,1236; 0/0:40,0:40:99:0,120,1357; 0/0:34,0:34:99:0,102,1161; 0/0:49,0:49:99:0,147,1673; 0/0:33,0:33:99:0,99,1036; 0/0:48,0:48:99:0,144,1728; 0/0:42,0:42:99:0,126,1410; 0/0:37,0:37:99:0,111,1215; 0/0:39,0:39:99:0,117,1311; 0/0:42,0:42:99:0,126,1419; 0/0:53,0:53:99:0,159,1744; 0/0:45,0:45:99:0,135,1529; 0/0:44,0:44:99:0,132,1419; 0/0:38,0:38:99:0,114,1299; 0/0:37,0:37:99:0,111,1205; 0/0:34,0:34:99:0,102,1151; 0/0:57,0:57:99:0,171,1826; 0/0:27,1:28:49:0,49,904; 0/0:41,0:41:99:0,123,1364; 0/0:28,0:28:84:0,84,933; 0/0:36,0:36:99:0,108,1171; 0/0:29,0:29:87:0,87,987; 0/0:31,0:31:93:0,93,997; 0/0:37,0:37:99:0,111,126",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5445
https://github.com/broadinstitute/gatk/issues/5445:2033,Deployability,pipeline,pipeline,2033,"ng this bug for 57 samples of 5000 crams at snp rs429358 but I would expect it is not unique to this site. . Select two crams with a Passed site with:; cram 1. Call with GT='0/0, GQ=0 and DP >40.; cram 2. Call with GT='0/1' or '1/1' and DP>20. . Create vcf with two approaches:. Pipeline 1. HaplotypeCaller-->vcf. module load gatk/4.0.11.0; gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I gq0_cram.list\; -L chr19:44907684-44909822\; --use-new-qual-calculator\; -O good.vcf.gz. Good GQ scores were also estimated with Freebayes on these samples also. Pipeline 2 HaplotypeCaller --> bvcf--->ImportVCF-->GenotypeVCF-->VCF with 2 samples. gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I $sample.cram\; --use-new-qual-calculator\; -L chr19:44907684-44909822\; -ERC GVCF\; -O bad.g.vcf.gz. Followed by import and GenotypeVCF. . #### Expected behavior; Pipeline 2 should generate accurate GQ scores that match the GQ in the HaplotypeCaller vcf output of pipeline 1. Instead GQ=0. . This is the output for the 57 GQ=0 samples with pipeline 1 which is accurate. AC=7;AF=0.061;AN=114;BaseQRankSum=-6.147;DP=1846;ExcessHet=3.8592;FS=0.000;InbreedingCoeff=-0.0640;MLEAC=6;MLEAF=0.053;MQ=60.00;MQRankSum=0.000;QD=4.52;ReadPosRankSum=-0.781;SOR=2.833; GT:AD:DP:GQ:PL; 0/0:37,0:37:99:0,111,1236; 0/0:40,0:40:99:0,120,1357; 0/0:34,0:34:99:0,102,1161; 0/0:49,0:49:99:0,147,1673; 0/0:33,0:33:99:0,99,1036; 0/0:48,0:48:99:0,144,1728; 0/0:42,0:42:99:0,126,1410; 0/0:37,0:37:99:0,111,1215; 0/0:39,0:39:99:0,117,1311; 0/0:42,0:42:99:0,126,1419; 0/0:53,0:53:99:0,159,1744; 0/0:45,0:45:99:0,135,1529; 0/0:44,0:44:99:0,132,1419; 0/0:38,0:38:99:0,114,1299; 0/0:37,0:37:99:0,111,1205; 0/0:34,0:34:99:0,102,1151; 0/0:57,0:57:99:0,171,1826; 0/0:27,1:28:49:0,49,904; 0/0:41,0:41:99:0,123,1364; 0/0:28,0:28:84:0,84,933; 0/0:36,0:36:99:0,108,1171; 0/0:29,0:29:87:0,87,987; 0/0:31,0:31:93:0,93,997; 0/0:37,0:37:99:0,111,126",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5445
https://github.com/broadinstitute/gatk/issues/5445:2109,Deployability,pipeline,pipeline,2109,"s with a Passed site with:; cram 1. Call with GT='0/0, GQ=0 and DP >40.; cram 2. Call with GT='0/1' or '1/1' and DP>20. . Create vcf with two approaches:. Pipeline 1. HaplotypeCaller-->vcf. module load gatk/4.0.11.0; gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I gq0_cram.list\; -L chr19:44907684-44909822\; --use-new-qual-calculator\; -O good.vcf.gz. Good GQ scores were also estimated with Freebayes on these samples also. Pipeline 2 HaplotypeCaller --> bvcf--->ImportVCF-->GenotypeVCF-->VCF with 2 samples. gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I $sample.cram\; --use-new-qual-calculator\; -L chr19:44907684-44909822\; -ERC GVCF\; -O bad.g.vcf.gz. Followed by import and GenotypeVCF. . #### Expected behavior; Pipeline 2 should generate accurate GQ scores that match the GQ in the HaplotypeCaller vcf output of pipeline 1. Instead GQ=0. . This is the output for the 57 GQ=0 samples with pipeline 1 which is accurate. AC=7;AF=0.061;AN=114;BaseQRankSum=-6.147;DP=1846;ExcessHet=3.8592;FS=0.000;InbreedingCoeff=-0.0640;MLEAC=6;MLEAF=0.053;MQ=60.00;MQRankSum=0.000;QD=4.52;ReadPosRankSum=-0.781;SOR=2.833; GT:AD:DP:GQ:PL; 0/0:37,0:37:99:0,111,1236; 0/0:40,0:40:99:0,120,1357; 0/0:34,0:34:99:0,102,1161; 0/0:49,0:49:99:0,147,1673; 0/0:33,0:33:99:0,99,1036; 0/0:48,0:48:99:0,144,1728; 0/0:42,0:42:99:0,126,1410; 0/0:37,0:37:99:0,111,1215; 0/0:39,0:39:99:0,117,1311; 0/0:42,0:42:99:0,126,1419; 0/0:53,0:53:99:0,159,1744; 0/0:45,0:45:99:0,135,1529; 0/0:44,0:44:99:0,132,1419; 0/0:38,0:38:99:0,114,1299; 0/0:37,0:37:99:0,111,1205; 0/0:34,0:34:99:0,102,1151; 0/0:57,0:57:99:0,171,1826; 0/0:27,1:28:49:0,49,904; 0/0:41,0:41:99:0,123,1364; 0/0:28,0:28:84:0,84,933; 0/0:36,0:36:99:0,108,1171; 0/0:29,0:29:87:0,87,987; 0/0:31,0:31:93:0,93,997; 0/0:37,0:37:99:0,111,1266; 0/0:28,2:30:70:0,70,914; 0/0:36,0:36:99:0,108,1230; 0/0:49,0:49:99:0,147,1613; 0/0:38,1:39:82:0,82,1231; 0/0:26,0:26:78:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5445
https://github.com/broadinstitute/gatk/issues/5445:4039,Deployability,Pipeline,Pipeline,4039,"0/0:38,1:39:82:0,82,1231; 0/0:26,0:26:78:0,78,869; 0/0:22,0:22:66:0,66,734; 0/0:28,1:29:52:0,52,878; 0/0:36,0:36:99:0,108,1221; 0/0:39,0:39:99:0,117,1315; 0/0:31,1:32:86:0,86,1008; 0/0:38,0:38:99:0,114,1286; 0/0:37,0:37:99:0,111,1266; 0/0:35,0:35:99:0,105,1131; 0/0:32,0:32:96:0,96,1071; 0/0:46,0:46:99:0,138,1508; 0/0:27,0:27:81:0,81,918; 0/0:19,0:19:57:0,57,620; 0/0:45,0:45:99:0,135,1474; 0/0:50,0:50:99:0,150,1687; 0/0:29,0:29:87:0,87,968; 0/0:30,0:30:90:0,90,992; 0/0:55,1:56:99:0,158,1830; 0/1:8,2:10:19:19,0,248; 0/1:5,1:6:17:17,0,145; 0/1:4,1:5:10:10,0,124; 0/1:5,1:6:17:17,0,155; 0/1:3,2:5:34:34,0,79; 0/1:5,3:8:45:45,0,150; 0/1:2,1:3:26:26,0,61; 0/0:13,0:13:39:0,39,431; 0/0:17,0:17:51:0,51,571; 0/0:28,0:28:84:0,84,993; 0/0:10,0:10:30:0,30,328 . Freebayes has been run on these 57 samples and also get '0/0' but with GQ in the 140-160 range for most samples and are in line with the results with the HaplotypeCaller VCF direct results (see below). #### Actual behavior. Pipeline 2 is incorrectly setting GQ=0. This is the output for the 57 GQ=0 samples+ 1 extra sample GQ=99 with pipeline 2. The extra sample is needed to produce a variant record otherwise all the records would be homoz refer with GQ=0. . AC=1;AF=8.621e-03;AN=116;BaseQRankSum=-8.310e-01;DP=2213;ExcessHet=41.0061;FS=1.957;InbreedingCoeff=-0.3410;MLEAC=10;MLEAF=0.086;MQ=60.00;MQRankSum=0.00;QD=12.17;ReadPosRankSum=0.616;SOR=1.080; GT:AD:DP:GQ:PL; 0/1:9,8:17:99:227,0,272; 0/0:41,0:41:0:0,0,1097; 0/0:51,0:51:0:0,0,1216; 0/0:61,0:61:0:0,0,1373; 0/0:54,0:54:0:0,0,962; 0/0:49,0:49:0:0,0,1156; 0/0:53,0:53:0:0,0,729; 0/0:44,0:44:0:0,0,1161; 0/0:38,0:38:0:0,0,963; 0/0:68,0:68:0:0,0,1518; 0/0:33,0:33:0:0,0,841; 0/0:54,0:54:0:0,0,687; 0/0:44,0:44:0:0,0,1003; 0/0:33,0:33:0:0,0,709; 0/0:54,0:54:0:0,0,580; 0/0:31,0:31:0:0,0,790; 0/0:36,0:36:0:0,0,843; 0/0:49,0:49:0:0,0,978; 0/0:34,0:34:0:0,0,669; 0/0:39,0:39:0:0,0,898; 0/0:60,0:60:0:0,0,1270; 0/0:48,0:48:0:0,0,908; 0/0:30,0:30:0:0,0,780; 0/0:44,0:44:0:0,0,778; 0/0:24,0:2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5445
https://github.com/broadinstitute/gatk/issues/5445:4149,Deployability,pipeline,pipeline,4149,"2:66:0,66,734; 0/0:28,1:29:52:0,52,878; 0/0:36,0:36:99:0,108,1221; 0/0:39,0:39:99:0,117,1315; 0/0:31,1:32:86:0,86,1008; 0/0:38,0:38:99:0,114,1286; 0/0:37,0:37:99:0,111,1266; 0/0:35,0:35:99:0,105,1131; 0/0:32,0:32:96:0,96,1071; 0/0:46,0:46:99:0,138,1508; 0/0:27,0:27:81:0,81,918; 0/0:19,0:19:57:0,57,620; 0/0:45,0:45:99:0,135,1474; 0/0:50,0:50:99:0,150,1687; 0/0:29,0:29:87:0,87,968; 0/0:30,0:30:90:0,90,992; 0/0:55,1:56:99:0,158,1830; 0/1:8,2:10:19:19,0,248; 0/1:5,1:6:17:17,0,145; 0/1:4,1:5:10:10,0,124; 0/1:5,1:6:17:17,0,155; 0/1:3,2:5:34:34,0,79; 0/1:5,3:8:45:45,0,150; 0/1:2,1:3:26:26,0,61; 0/0:13,0:13:39:0,39,431; 0/0:17,0:17:51:0,51,571; 0/0:28,0:28:84:0,84,993; 0/0:10,0:10:30:0,30,328 . Freebayes has been run on these 57 samples and also get '0/0' but with GQ in the 140-160 range for most samples and are in line with the results with the HaplotypeCaller VCF direct results (see below). #### Actual behavior. Pipeline 2 is incorrectly setting GQ=0. This is the output for the 57 GQ=0 samples+ 1 extra sample GQ=99 with pipeline 2. The extra sample is needed to produce a variant record otherwise all the records would be homoz refer with GQ=0. . AC=1;AF=8.621e-03;AN=116;BaseQRankSum=-8.310e-01;DP=2213;ExcessHet=41.0061;FS=1.957;InbreedingCoeff=-0.3410;MLEAC=10;MLEAF=0.086;MQ=60.00;MQRankSum=0.00;QD=12.17;ReadPosRankSum=0.616;SOR=1.080; GT:AD:DP:GQ:PL; 0/1:9,8:17:99:227,0,272; 0/0:41,0:41:0:0,0,1097; 0/0:51,0:51:0:0,0,1216; 0/0:61,0:61:0:0,0,1373; 0/0:54,0:54:0:0,0,962; 0/0:49,0:49:0:0,0,1156; 0/0:53,0:53:0:0,0,729; 0/0:44,0:44:0:0,0,1161; 0/0:38,0:38:0:0,0,963; 0/0:68,0:68:0:0,0,1518; 0/0:33,0:33:0:0,0,841; 0/0:54,0:54:0:0,0,687; 0/0:44,0:44:0:0,0,1003; 0/0:33,0:33:0:0,0,709; 0/0:54,0:54:0:0,0,580; 0/0:31,0:31:0:0,0,790; 0/0:36,0:36:0:0,0,843; 0/0:49,0:49:0:0,0,978; 0/0:34,0:34:0:0,0,669; 0/0:39,0:39:0:0,0,898; 0/0:60,0:60:0:0,0,1270; 0/0:48,0:48:0:0,0,908; 0/0:30,0:30:0:0,0,780; 0/0:44,0:44:0:0,0,778; 0/0:24,0:24:0:0,0,531; 0/0:40,0:40:0:0,0,714; 0/0:50,0:50:0:0,0,794; 0/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5445
https://github.com/broadinstitute/gatk/issues/5445:1296,Performance,load,load,1296,"for such a low GQ score. Often the GQ should be 99 as the DP >40. This seems to be primarily an issue with homozygous reference calls. . The GT is accurate for the high DP sites but the inaccurate GQ is problematic for any genotype level qc on the pVCF. If the site is recoded from 0/0 to './.' for GQ <20, the result is higher missing rate due to the inaccurate GQ=0. . Directly calling the VCF with HaplotypeCaller without the gVCF intermediate gVCF file calculates the correct GQ score. Freebayes also calculates a correct GQ on these samples.; [rs429358_gq_dp.pdf](https://github.com/broadinstitute/gatk/files/2612419/rs429358_gq_dp.pdf). #### Steps to reproduce. I am seeing this bug for 57 samples of 5000 crams at snp rs429358 but I would expect it is not unique to this site. . Select two crams with a Passed site with:; cram 1. Call with GT='0/0, GQ=0 and DP >40.; cram 2. Call with GT='0/1' or '1/1' and DP>20. . Create vcf with two approaches:. Pipeline 1. HaplotypeCaller-->vcf. module load gatk/4.0.11.0; gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I gq0_cram.list\; -L chr19:44907684-44909822\; --use-new-qual-calculator\; -O good.vcf.gz. Good GQ scores were also estimated with Freebayes on these samples also. Pipeline 2 HaplotypeCaller --> bvcf--->ImportVCF-->GenotypeVCF-->VCF with 2 samples. gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I $sample.cram\; --use-new-qual-calculator\; -L chr19:44907684-44909822\; -ERC GVCF\; -O bad.g.vcf.gz. Followed by import and GenotypeVCF. . #### Expected behavior; Pipeline 2 should generate accurate GQ scores that match the GQ in the HaplotypeCaller vcf output of pipeline 1. Instead GQ=0. . This is the output for the 57 GQ=0 samples with pipeline 1 which is accurate. AC=7;AF=0.061;AN=114;BaseQRankSum=-6.147;DP=1846;ExcessHet=3.8592;FS=0.000;InbreedingCoeff=-0.0640;MLEAC=6;MLEAF=0.053;MQ=60.00;MQRankSum=0.000;QD=4.52;ReadPos",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5445
https://github.com/broadinstitute/gatk/issues/5446:75,Deployability,integrat,integration,75,"I didnt see an example, but I could have easily missed it: I am writing an integration test for a walker where the user supplies an output prefix, and the tool created three output files based on that prefix. I want to write an integration test, ideally using IntegrationTestSpec in an intended form. Are there any existing integration tests of tools that use this style argument, and/or is there an intended way to make that work using IntegrationTestSpec? . Most examples I see require the test to supply ""-O %s"", and the test creates a temp file for the output. It then expects to compare that path to the expected file you supply. I see how to work around it (dont use %s and manually check outputs), but I thought I'd check to see if there was an official solution. Thanks in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5446
https://github.com/broadinstitute/gatk/issues/5446:228,Deployability,integrat,integration,228,"I didnt see an example, but I could have easily missed it: I am writing an integration test for a walker where the user supplies an output prefix, and the tool created three output files based on that prefix. I want to write an integration test, ideally using IntegrationTestSpec in an intended form. Are there any existing integration tests of tools that use this style argument, and/or is there an intended way to make that work using IntegrationTestSpec? . Most examples I see require the test to supply ""-O %s"", and the test creates a temp file for the output. It then expects to compare that path to the expected file you supply. I see how to work around it (dont use %s and manually check outputs), but I thought I'd check to see if there was an official solution. Thanks in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5446
https://github.com/broadinstitute/gatk/issues/5446:260,Deployability,Integrat,IntegrationTestSpec,260,"I didnt see an example, but I could have easily missed it: I am writing an integration test for a walker where the user supplies an output prefix, and the tool created three output files based on that prefix. I want to write an integration test, ideally using IntegrationTestSpec in an intended form. Are there any existing integration tests of tools that use this style argument, and/or is there an intended way to make that work using IntegrationTestSpec? . Most examples I see require the test to supply ""-O %s"", and the test creates a temp file for the output. It then expects to compare that path to the expected file you supply. I see how to work around it (dont use %s and manually check outputs), but I thought I'd check to see if there was an official solution. Thanks in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5446
https://github.com/broadinstitute/gatk/issues/5446:324,Deployability,integrat,integration,324,"I didnt see an example, but I could have easily missed it: I am writing an integration test for a walker where the user supplies an output prefix, and the tool created three output files based on that prefix. I want to write an integration test, ideally using IntegrationTestSpec in an intended form. Are there any existing integration tests of tools that use this style argument, and/or is there an intended way to make that work using IntegrationTestSpec? . Most examples I see require the test to supply ""-O %s"", and the test creates a temp file for the output. It then expects to compare that path to the expected file you supply. I see how to work around it (dont use %s and manually check outputs), but I thought I'd check to see if there was an official solution. Thanks in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5446
https://github.com/broadinstitute/gatk/issues/5446:437,Deployability,Integrat,IntegrationTestSpec,437,"I didnt see an example, but I could have easily missed it: I am writing an integration test for a walker where the user supplies an output prefix, and the tool created three output files based on that prefix. I want to write an integration test, ideally using IntegrationTestSpec in an intended form. Are there any existing integration tests of tools that use this style argument, and/or is there an intended way to make that work using IntegrationTestSpec? . Most examples I see require the test to supply ""-O %s"", and the test creates a temp file for the output. It then expects to compare that path to the expected file you supply. I see how to work around it (dont use %s and manually check outputs), but I thought I'd check to see if there was an official solution. Thanks in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5446
https://github.com/broadinstitute/gatk/issues/5446:75,Integrability,integrat,integration,75,"I didnt see an example, but I could have easily missed it: I am writing an integration test for a walker where the user supplies an output prefix, and the tool created three output files based on that prefix. I want to write an integration test, ideally using IntegrationTestSpec in an intended form. Are there any existing integration tests of tools that use this style argument, and/or is there an intended way to make that work using IntegrationTestSpec? . Most examples I see require the test to supply ""-O %s"", and the test creates a temp file for the output. It then expects to compare that path to the expected file you supply. I see how to work around it (dont use %s and manually check outputs), but I thought I'd check to see if there was an official solution. Thanks in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5446
https://github.com/broadinstitute/gatk/issues/5446:228,Integrability,integrat,integration,228,"I didnt see an example, but I could have easily missed it: I am writing an integration test for a walker where the user supplies an output prefix, and the tool created three output files based on that prefix. I want to write an integration test, ideally using IntegrationTestSpec in an intended form. Are there any existing integration tests of tools that use this style argument, and/or is there an intended way to make that work using IntegrationTestSpec? . Most examples I see require the test to supply ""-O %s"", and the test creates a temp file for the output. It then expects to compare that path to the expected file you supply. I see how to work around it (dont use %s and manually check outputs), but I thought I'd check to see if there was an official solution. Thanks in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5446
https://github.com/broadinstitute/gatk/issues/5446:260,Integrability,Integrat,IntegrationTestSpec,260,"I didnt see an example, but I could have easily missed it: I am writing an integration test for a walker where the user supplies an output prefix, and the tool created three output files based on that prefix. I want to write an integration test, ideally using IntegrationTestSpec in an intended form. Are there any existing integration tests of tools that use this style argument, and/or is there an intended way to make that work using IntegrationTestSpec? . Most examples I see require the test to supply ""-O %s"", and the test creates a temp file for the output. It then expects to compare that path to the expected file you supply. I see how to work around it (dont use %s and manually check outputs), but I thought I'd check to see if there was an official solution. Thanks in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5446
https://github.com/broadinstitute/gatk/issues/5446:324,Integrability,integrat,integration,324,"I didnt see an example, but I could have easily missed it: I am writing an integration test for a walker where the user supplies an output prefix, and the tool created three output files based on that prefix. I want to write an integration test, ideally using IntegrationTestSpec in an intended form. Are there any existing integration tests of tools that use this style argument, and/or is there an intended way to make that work using IntegrationTestSpec? . Most examples I see require the test to supply ""-O %s"", and the test creates a temp file for the output. It then expects to compare that path to the expected file you supply. I see how to work around it (dont use %s and manually check outputs), but I thought I'd check to see if there was an official solution. Thanks in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5446
https://github.com/broadinstitute/gatk/issues/5446:437,Integrability,Integrat,IntegrationTestSpec,437,"I didnt see an example, but I could have easily missed it: I am writing an integration test for a walker where the user supplies an output prefix, and the tool created three output files based on that prefix. I want to write an integration test, ideally using IntegrationTestSpec in an intended form. Are there any existing integration tests of tools that use this style argument, and/or is there an intended way to make that work using IntegrationTestSpec? . Most examples I see require the test to supply ""-O %s"", and the test creates a temp file for the output. It then expects to compare that path to the expected file you supply. I see how to work around it (dont use %s and manually check outputs), but I thought I'd check to see if there was an official solution. Thanks in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5446
https://github.com/broadinstitute/gatk/issues/5446:87,Testability,test,test,87,"I didnt see an example, but I could have easily missed it: I am writing an integration test for a walker where the user supplies an output prefix, and the tool created three output files based on that prefix. I want to write an integration test, ideally using IntegrationTestSpec in an intended form. Are there any existing integration tests of tools that use this style argument, and/or is there an intended way to make that work using IntegrationTestSpec? . Most examples I see require the test to supply ""-O %s"", and the test creates a temp file for the output. It then expects to compare that path to the expected file you supply. I see how to work around it (dont use %s and manually check outputs), but I thought I'd check to see if there was an official solution. Thanks in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5446
https://github.com/broadinstitute/gatk/issues/5446:240,Testability,test,test,240,"I didnt see an example, but I could have easily missed it: I am writing an integration test for a walker where the user supplies an output prefix, and the tool created three output files based on that prefix. I want to write an integration test, ideally using IntegrationTestSpec in an intended form. Are there any existing integration tests of tools that use this style argument, and/or is there an intended way to make that work using IntegrationTestSpec? . Most examples I see require the test to supply ""-O %s"", and the test creates a temp file for the output. It then expects to compare that path to the expected file you supply. I see how to work around it (dont use %s and manually check outputs), but I thought I'd check to see if there was an official solution. Thanks in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5446
https://github.com/broadinstitute/gatk/issues/5446:336,Testability,test,tests,336,"I didnt see an example, but I could have easily missed it: I am writing an integration test for a walker where the user supplies an output prefix, and the tool created three output files based on that prefix. I want to write an integration test, ideally using IntegrationTestSpec in an intended form. Are there any existing integration tests of tools that use this style argument, and/or is there an intended way to make that work using IntegrationTestSpec? . Most examples I see require the test to supply ""-O %s"", and the test creates a temp file for the output. It then expects to compare that path to the expected file you supply. I see how to work around it (dont use %s and manually check outputs), but I thought I'd check to see if there was an official solution. Thanks in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5446
https://github.com/broadinstitute/gatk/issues/5446:492,Testability,test,test,492,"I didnt see an example, but I could have easily missed it: I am writing an integration test for a walker where the user supplies an output prefix, and the tool created three output files based on that prefix. I want to write an integration test, ideally using IntegrationTestSpec in an intended form. Are there any existing integration tests of tools that use this style argument, and/or is there an intended way to make that work using IntegrationTestSpec? . Most examples I see require the test to supply ""-O %s"", and the test creates a temp file for the output. It then expects to compare that path to the expected file you supply. I see how to work around it (dont use %s and manually check outputs), but I thought I'd check to see if there was an official solution. Thanks in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5446
https://github.com/broadinstitute/gatk/issues/5446:524,Testability,test,test,524,"I didnt see an example, but I could have easily missed it: I am writing an integration test for a walker where the user supplies an output prefix, and the tool created three output files based on that prefix. I want to write an integration test, ideally using IntegrationTestSpec in an intended form. Are there any existing integration tests of tools that use this style argument, and/or is there an intended way to make that work using IntegrationTestSpec? . Most examples I see require the test to supply ""-O %s"", and the test creates a temp file for the output. It then expects to compare that path to the expected file you supply. I see how to work around it (dont use %s and manually check outputs), but I thought I'd check to see if there was an official solution. Thanks in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5446
https://github.com/broadinstitute/gatk/issues/5447:10359,Availability,down,down,10359,"arnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3741 alt examples, EM converged in 14 steps; 16:21:14.667 INFO LearnReadOrientationModel - Context CTA: with 173673 ref and 1472 alt examples, EM converged in 13 steps; 16:21:15.238 INFO LearnReadOrientationModel - Context CTC: with 439622 ref and 3855 alt examples, EM converged in 12 steps; 16:21:15.709 INFO LearnReadOrientationModel - Context GAA: with 333082 ref and 2101 alt examples, EM converged in 11 steps; 16:21:16.222 INFO LearnReadOrientationModel - Context GAC: with 227508 ref and 2090 alt examples, EM converged in 12 steps; 16:21:17.015 INFO LearnReadOrientationModel - Context GCA: with 340865 ref and 6507 alt examples, EM converged in 14 steps; 16:21:17.906 INFO LearnReadOrientationModel - Context GCC: with 404793 ref and 8210 alt examples, EM converged in 14 steps; 16:21:18.677 INFO LearnReadOrientationModel - Context GGA: with 409382 ref and 6784 alt examples, EM converged in 13 steps; 16:21:19.182 INFO LearnReadOrientationModel - Context GTA: with 153202 ref and 1568 alt examples, EM converged in 12 steps; 16:21:19.680 INFO LearnReadOrientationModel - Context TAA: with 208518 ref and 1131 alt examples, EM converged in 12 steps; 16:21:20.459 INFO LearnReadOrientationModel - Context TCA: with 325628 ref and 6437 alt examples, EM converged in 13 steps; 16:21:21.008 INFO LearnReadOrientationModel - Context AAA: with 426193 ref and 2055 alt examples, EM converged in 12 steps; 16:21:21.044 INFO LearnReadOrientationModel - Shutting down engine; [November 26, 2018 4:21:21 PM UTC] org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel done. Elapsed time: 0.39 minutes.; Runtime.totalMemory()=780140544; Tool returned:; SUCCESS; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:289,Integrability,message,message,289,"Posting issue on @cmnbroad's request. . I see this stacktrace of a WARN for some GATK tools. The tools proceed to run successfully. For example, LearnReadOrientationModel gives this. I've been preparing for the GATK workshop and have been running a variety of tools. . For this particular message, I am running GATK v4.0.11.0 locally on my Mac laptop, in the 4.0.11.0 Docker. How can I deal with this WARN?. ```; (gatk) root@3231a24c7afb:/gatk/my_data/3-somatic# gatk LearnReadOrientationModel -alt-table 13_tumor-alt.tsv -ref-hist 13_tumor-ref.metrics -alt-hist 13_tumor-alt-depth1.metrics -O tumor-artifact-prior-table.tsv ; Using GATK jar /gatk/gatk-package-4.0.11.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.0.11.0-local.jar LearnReadOrientationModel -alt-table 13_tumor-alt.tsv -ref-hist 13_tumor-ref.metrics -alt-hist 13_tumor-alt-depth1.metrics -O tumor-artifact-prior-table.tsv; 16:20:57.885 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.0.11.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 26, 2018 4:20:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused (Connection refused); 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(H",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:2280,Integrability,protocol,protocol,2280,omputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused (Connection refused); 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); 	at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); 	at sun.net.www.http.HttpClient.New(HttpClient.java:339); 	at sun.net.www.http.HttpClient.New(HttpClient.java:357); 	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1050); 	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:984); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:104); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.runningOnComputeEngine(ComputeEngineCredentials.java:210); 	at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.tryGetComputeCredentials(DefaultCredentialsProvider.java:290); 	at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentialsUnsynchronized(DefaultCredentialsProvider.java:207); 	at shaded.cloud_nio.com.google.auth.oauth2.DefaultCred,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:2375,Integrability,protocol,protocol,2375,g on Google Compute Engine.; java.net.ConnectException: Connection refused (Connection refused); 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); 	at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); 	at sun.net.www.http.HttpClient.New(HttpClient.java:339); 	at sun.net.www.http.HttpClient.New(HttpClient.java:357); 	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1050); 	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:984); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:104); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.runningOnComputeEngine(ComputeEngineCredentials.java:210); 	at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.tryGetComputeCredentials(DefaultCredentialsProvider.java:290); 	at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentialsUnsynchronized(DefaultCredentialsProvider.java:207); 	at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentials(DefaultCredentialsProvider.java:124); 	at shaded.cloud_ni,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:2467,Integrability,protocol,protocol,2467,ed); 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); 	at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); 	at sun.net.www.http.HttpClient.New(HttpClient.java:339); 	at sun.net.www.http.HttpClient.New(HttpClient.java:357); 	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1050); 	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:984); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:104); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.runningOnComputeEngine(ComputeEngineCredentials.java:210); 	at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.tryGetComputeCredentials(DefaultCredentialsProvider.java:290); 	at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentialsUnsynchronized(DefaultCredentialsProvider.java:207); 	at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentials(DefaultCredentialsProvider.java:124); 	at shaded.cloud_nio.com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:127),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:2558,Integrability,protocol,protocol,2558,ocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); 	at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); 	at sun.net.www.http.HttpClient.New(HttpClient.java:339); 	at sun.net.www.http.HttpClient.New(HttpClient.java:357); 	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1050); 	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:984); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:104); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.runningOnComputeEngine(ComputeEngineCredentials.java:210); 	at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.tryGetComputeCredentials(DefaultCredentialsProvider.java:290); 	at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentialsUnsynchronized(DefaultCredentialsProvider.java:207); 	at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentials(DefaultCredentialsProvider.java:124); 	at shaded.cloud_nio.com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:127); 	at shaded.cloud_nio.com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(Googl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:1092,Performance,Load,Loading,1092,"s. The tools proceed to run successfully. For example, LearnReadOrientationModel gives this. I've been preparing for the GATK workshop and have been running a variety of tools. . For this particular message, I am running GATK v4.0.11.0 locally on my Mac laptop, in the 4.0.11.0 Docker. How can I deal with this WARN?. ```; (gatk) root@3231a24c7afb:/gatk/my_data/3-somatic# gatk LearnReadOrientationModel -alt-table 13_tumor-alt.tsv -ref-hist 13_tumor-ref.metrics -alt-hist 13_tumor-alt-depth1.metrics -O tumor-artifact-prior-table.tsv ; Using GATK jar /gatk/gatk-package-4.0.11.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.0.11.0-local.jar LearnReadOrientationModel -alt-table 13_tumor-alt.tsv -ref-hist 13_tumor-ref.metrics -alt-hist 13_tumor-alt-depth1.metrics -O tumor-artifact-prior-table.tsv; 16:20:57.885 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.0.11.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 26, 2018 4:20:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused (Connection refused); 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); 	at",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:1351,Safety,detect,detect,1351,"laptop, in the 4.0.11.0 Docker. How can I deal with this WARN?. ```; (gatk) root@3231a24c7afb:/gatk/my_data/3-somatic# gatk LearnReadOrientationModel -alt-table 13_tumor-alt.tsv -ref-hist 13_tumor-ref.metrics -alt-hist 13_tumor-alt-depth1.metrics -O tumor-artifact-prior-table.tsv ; Using GATK jar /gatk/gatk-package-4.0.11.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.0.11.0-local.jar LearnReadOrientationModel -alt-table 13_tumor-alt.tsv -ref-hist 13_tumor-ref.metrics -alt-hist 13_tumor-alt-depth1.metrics -O tumor-artifact-prior-table.tsv; 16:20:57.885 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.0.11.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 26, 2018 4:20:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused (Connection refused); 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); 	at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); 	at sun.net.www.http.HttpClient.New(HttpClient.java:339); 	at sun.net.www.http.HttpClient.New(HttpClient.java:357); 	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnectio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:145,Usability,Learn,LearnReadOrientationModel,145,"Posting issue on @cmnbroad's request. . I see this stacktrace of a WARN for some GATK tools. The tools proceed to run successfully. For example, LearnReadOrientationModel gives this. I've been preparing for the GATK workshop and have been running a variety of tools. . For this particular message, I am running GATK v4.0.11.0 locally on my Mac laptop, in the 4.0.11.0 Docker. How can I deal with this WARN?. ```; (gatk) root@3231a24c7afb:/gatk/my_data/3-somatic# gatk LearnReadOrientationModel -alt-table 13_tumor-alt.tsv -ref-hist 13_tumor-ref.metrics -alt-hist 13_tumor-alt-depth1.metrics -O tumor-artifact-prior-table.tsv ; Using GATK jar /gatk/gatk-package-4.0.11.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.0.11.0-local.jar LearnReadOrientationModel -alt-table 13_tumor-alt.tsv -ref-hist 13_tumor-ref.metrics -alt-hist 13_tumor-alt-depth1.metrics -O tumor-artifact-prior-table.tsv; 16:20:57.885 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.0.11.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 26, 2018 4:20:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused (Connection refused); 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(H",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:468,Usability,Learn,LearnReadOrientationModel,468,"Posting issue on @cmnbroad's request. . I see this stacktrace of a WARN for some GATK tools. The tools proceed to run successfully. For example, LearnReadOrientationModel gives this. I've been preparing for the GATK workshop and have been running a variety of tools. . For this particular message, I am running GATK v4.0.11.0 locally on my Mac laptop, in the 4.0.11.0 Docker. How can I deal with this WARN?. ```; (gatk) root@3231a24c7afb:/gatk/my_data/3-somatic# gatk LearnReadOrientationModel -alt-table 13_tumor-alt.tsv -ref-hist 13_tumor-ref.metrics -alt-hist 13_tumor-alt-depth1.metrics -O tumor-artifact-prior-table.tsv ; Using GATK jar /gatk/gatk-package-4.0.11.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.0.11.0-local.jar LearnReadOrientationModel -alt-table 13_tumor-alt.tsv -ref-hist 13_tumor-ref.metrics -alt-hist 13_tumor-alt-depth1.metrics -O tumor-artifact-prior-table.tsv; 16:20:57.885 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.0.11.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 26, 2018 4:20:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused (Connection refused); 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(H",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:894,Usability,Learn,LearnReadOrientationModel,894,"Posting issue on @cmnbroad's request. . I see this stacktrace of a WARN for some GATK tools. The tools proceed to run successfully. For example, LearnReadOrientationModel gives this. I've been preparing for the GATK workshop and have been running a variety of tools. . For this particular message, I am running GATK v4.0.11.0 locally on my Mac laptop, in the 4.0.11.0 Docker. How can I deal with this WARN?. ```; (gatk) root@3231a24c7afb:/gatk/my_data/3-somatic# gatk LearnReadOrientationModel -alt-table 13_tumor-alt.tsv -ref-hist 13_tumor-ref.metrics -alt-hist 13_tumor-alt-depth1.metrics -O tumor-artifact-prior-table.tsv ; Using GATK jar /gatk/gatk-package-4.0.11.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.0.11.0-local.jar LearnReadOrientationModel -alt-table 13_tumor-alt.tsv -ref-hist 13_tumor-ref.metrics -alt-hist 13_tumor-alt-depth1.metrics -O tumor-artifact-prior-table.tsv; 16:20:57.885 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.0.11.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 26, 2018 4:20:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused (Connection refused); 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(H",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:4519,Usability,Learn,LearnReadOrientationModel,4519,"Credentials.java:100); 	at com.google.cloud.ServiceOptions.defaultCredentials(ServiceOptions.java:304); 	at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:278); 	at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:83); 	at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:31); 	at com.google.cloud.storage.StorageOptions$Builder.build(StorageOptions.java:78); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtils.setGlobalNIODefaultOptions(BucketUtils.java:382); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:183); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). 16:20:59.204 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.205 INFO LearnReadOrientationModel - The Genome Analysis Toolkit (GATK) v4.0.11.0; 16:20:59.205 INFO LearnReadOrientationModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:20:59.206 INFO LearnReadOrientationModel - Executing as root@3231a24c7afb on Linux v4.9.125-linuxkit amd64; 16:20:59.206 INFO LearnReadOrientationModel - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_181-8u181-b13-0ubuntu0.16.04.1-b13; 16:20:59.207 INFO LearnReadOrientationModel - Start Date/Time: November 26, 2018 4:20:57 PM UTC; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Version: 2.16.1; 16:20:59.208 INFO LearnReadOrientationModel - Picard Version: 2.18.13; 16:20:59.208 INFO LearnReadOrientationModel -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:4627,Usability,Learn,LearnReadOrientationModel,4627,"s.java:304); 	at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:278); 	at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:83); 	at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:31); 	at com.google.cloud.storage.StorageOptions$Builder.build(StorageOptions.java:78); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtils.setGlobalNIODefaultOptions(BucketUtils.java:382); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:183); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). 16:20:59.204 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.205 INFO LearnReadOrientationModel - The Genome Analysis Toolkit (GATK) v4.0.11.0; 16:20:59.205 INFO LearnReadOrientationModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:20:59.206 INFO LearnReadOrientationModel - Executing as root@3231a24c7afb on Linux v4.9.125-linuxkit amd64; 16:20:59.206 INFO LearnReadOrientationModel - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_181-8u181-b13-0ubuntu0.16.04.1-b13; 16:20:59.207 INFO LearnReadOrientationModel - Start Date/Time: November 26, 2018 4:20:57 PM UTC; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Version: 2.16.1; 16:20:59.208 INFO LearnReadOrientationModel - Picard Version: 2.18.13; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:4719,Usability,Learn,LearnReadOrientationModel,4719,"ud.storage.StorageOptions.<init>(StorageOptions.java:83); 	at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:31); 	at com.google.cloud.storage.StorageOptions$Builder.build(StorageOptions.java:78); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtils.setGlobalNIODefaultOptions(BucketUtils.java:382); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:183); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). 16:20:59.204 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.205 INFO LearnReadOrientationModel - The Genome Analysis Toolkit (GATK) v4.0.11.0; 16:20:59.205 INFO LearnReadOrientationModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:20:59.206 INFO LearnReadOrientationModel - Executing as root@3231a24c7afb on Linux v4.9.125-linuxkit amd64; 16:20:59.206 INFO LearnReadOrientationModel - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_181-8u181-b13-0ubuntu0.16.04.1-b13; 16:20:59.207 INFO LearnReadOrientationModel - Start Date/Time: November 26, 2018 4:20:57 PM UTC; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Version: 2.16.1; 16:20:59.208 INFO LearnReadOrientationModel - Picard Version: 2.18.13; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:20:59.209 INFO LearnReadOrientationModel - HTS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:4844,Usability,Learn,LearnReadOrientationModel,4844,"ons.java:31); 	at com.google.cloud.storage.StorageOptions$Builder.build(StorageOptions.java:78); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtils.setGlobalNIODefaultOptions(BucketUtils.java:382); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:183); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). 16:20:59.204 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.205 INFO LearnReadOrientationModel - The Genome Analysis Toolkit (GATK) v4.0.11.0; 16:20:59.205 INFO LearnReadOrientationModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:20:59.206 INFO LearnReadOrientationModel - Executing as root@3231a24c7afb on Linux v4.9.125-linuxkit amd64; 16:20:59.206 INFO LearnReadOrientationModel - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_181-8u181-b13-0ubuntu0.16.04.1-b13; 16:20:59.207 INFO LearnReadOrientationModel - Start Date/Time: November 26, 2018 4:20:57 PM UTC; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Version: 2.16.1; 16:20:59.208 INFO LearnReadOrientationModel - Picard Version: 2.18.13; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_A",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:4955,Usability,Learn,LearnReadOrientationModel,4955,"institute.hellbender.utils.gcs.BucketUtils.setGlobalNIODefaultOptions(BucketUtils.java:382); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:183); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). 16:20:59.204 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.205 INFO LearnReadOrientationModel - The Genome Analysis Toolkit (GATK) v4.0.11.0; 16:20:59.205 INFO LearnReadOrientationModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:20:59.206 INFO LearnReadOrientationModel - Executing as root@3231a24c7afb on Linux v4.9.125-linuxkit amd64; 16:20:59.206 INFO LearnReadOrientationModel - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_181-8u181-b13-0ubuntu0.16.04.1-b13; 16:20:59.207 INFO LearnReadOrientationModel - Start Date/Time: November 26, 2018 4:20:57 PM UTC; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Version: 2.16.1; 16:20:59.208 INFO LearnReadOrientationModel - Picard Version: 2.18.13; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:20:59.209 INFO LearnReadOrientationModel - Deflater: IntelDeflater; 16:20",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:5083,Usability,Learn,LearnReadOrientationModel,5083,"mmandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:183); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). 16:20:59.204 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.205 INFO LearnReadOrientationModel - The Genome Analysis Toolkit (GATK) v4.0.11.0; 16:20:59.205 INFO LearnReadOrientationModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:20:59.206 INFO LearnReadOrientationModel - Executing as root@3231a24c7afb on Linux v4.9.125-linuxkit amd64; 16:20:59.206 INFO LearnReadOrientationModel - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_181-8u181-b13-0ubuntu0.16.04.1-b13; 16:20:59.207 INFO LearnReadOrientationModel - Start Date/Time: November 26, 2018 4:20:57 PM UTC; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Version: 2.16.1; 16:20:59.208 INFO LearnReadOrientationModel - Picard Version: 2.18.13; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:20:59.209 INFO LearnReadOrientationModel - Deflater: IntelDeflater; 16:20:59.209 INFO LearnReadOrientationModel - Inflater: IntelInflater; 16:20:59.209 INFO LearnReadOrientationModel - GCS max retries/reopens: ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:5180,Usability,Learn,LearnReadOrientationModel,5180,"der.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). 16:20:59.204 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.205 INFO LearnReadOrientationModel - The Genome Analysis Toolkit (GATK) v4.0.11.0; 16:20:59.205 INFO LearnReadOrientationModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:20:59.206 INFO LearnReadOrientationModel - Executing as root@3231a24c7afb on Linux v4.9.125-linuxkit amd64; 16:20:59.206 INFO LearnReadOrientationModel - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_181-8u181-b13-0ubuntu0.16.04.1-b13; 16:20:59.207 INFO LearnReadOrientationModel - Start Date/Time: November 26, 2018 4:20:57 PM UTC; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Version: 2.16.1; 16:20:59.208 INFO LearnReadOrientationModel - Picard Version: 2.18.13; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:20:59.209 INFO LearnReadOrientationModel - Deflater: IntelDeflater; 16:20:59.209 INFO LearnReadOrientationModel - Inflater: IntelInflater; 16:20:59.209 INFO LearnReadOrientationModel - GCS max retries/reopens: 20; 16:20:59.209 INFO LearnReadOrientationModel - Requester pays: disabled; 16:20:59.209 INFO LearnRea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:5288,Usability,Learn,LearnReadOrientationModel,5288,"Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). 16:20:59.204 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.205 INFO LearnReadOrientationModel - The Genome Analysis Toolkit (GATK) v4.0.11.0; 16:20:59.205 INFO LearnReadOrientationModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:20:59.206 INFO LearnReadOrientationModel - Executing as root@3231a24c7afb on Linux v4.9.125-linuxkit amd64; 16:20:59.206 INFO LearnReadOrientationModel - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_181-8u181-b13-0ubuntu0.16.04.1-b13; 16:20:59.207 INFO LearnReadOrientationModel - Start Date/Time: November 26, 2018 4:20:57 PM UTC; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Version: 2.16.1; 16:20:59.208 INFO LearnReadOrientationModel - Picard Version: 2.18.13; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:20:59.209 INFO LearnReadOrientationModel - Deflater: IntelDeflater; 16:20:59.209 INFO LearnReadOrientationModel - Inflater: IntelInflater; 16:20:59.209 INFO LearnReadOrientationModel - GCS max retries/reopens: 20; 16:20:59.209 INFO LearnReadOrientationModel - Requester pays: disabled; 16:20:59.209 INFO LearnReadOrientationModel - Initializing engine; 16:20:59.352 INFO LearnReadOrientationModel - Done initializing eng",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:5396,Usability,Learn,LearnReadOrientationModel,5396,".mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). 16:20:59.204 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.205 INFO LearnReadOrientationModel - The Genome Analysis Toolkit (GATK) v4.0.11.0; 16:20:59.205 INFO LearnReadOrientationModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:20:59.206 INFO LearnReadOrientationModel - Executing as root@3231a24c7afb on Linux v4.9.125-linuxkit amd64; 16:20:59.206 INFO LearnReadOrientationModel - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_181-8u181-b13-0ubuntu0.16.04.1-b13; 16:20:59.207 INFO LearnReadOrientationModel - Start Date/Time: November 26, 2018 4:20:57 PM UTC; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Version: 2.16.1; 16:20:59.208 INFO LearnReadOrientationModel - Picard Version: 2.18.13; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:20:59.209 INFO LearnReadOrientationModel - Deflater: IntelDeflater; 16:20:59.209 INFO LearnReadOrientationModel - Inflater: IntelInflater; 16:20:59.209 INFO LearnReadOrientationModel - GCS max retries/reopens: 20; 16:20:59.209 INFO LearnReadOrientationModel - Requester pays: disabled; 16:20:59.209 INFO LearnReadOrientationModel - Initializing engine; 16:20:59.352 INFO LearnReadOrientationModel - Done initializing engine; 16:21:00.402 INFO LearnReadOrientationModel - Context AAC: with 217930 ref an",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:5466,Usability,Learn,LearnReadOrientationModel,5466,"(Main.java:289). 16:20:59.204 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.205 INFO LearnReadOrientationModel - The Genome Analysis Toolkit (GATK) v4.0.11.0; 16:20:59.205 INFO LearnReadOrientationModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:20:59.206 INFO LearnReadOrientationModel - Executing as root@3231a24c7afb on Linux v4.9.125-linuxkit amd64; 16:20:59.206 INFO LearnReadOrientationModel - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_181-8u181-b13-0ubuntu0.16.04.1-b13; 16:20:59.207 INFO LearnReadOrientationModel - Start Date/Time: November 26, 2018 4:20:57 PM UTC; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Version: 2.16.1; 16:20:59.208 INFO LearnReadOrientationModel - Picard Version: 2.18.13; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:20:59.209 INFO LearnReadOrientationModel - Deflater: IntelDeflater; 16:20:59.209 INFO LearnReadOrientationModel - Inflater: IntelInflater; 16:20:59.209 INFO LearnReadOrientationModel - GCS max retries/reopens: 20; 16:20:59.209 INFO LearnReadOrientationModel - Requester pays: disabled; 16:20:59.209 INFO LearnReadOrientationModel - Initializing engine; 16:20:59.352 INFO LearnReadOrientationModel - Done initializing engine; 16:21:00.402 INFO LearnReadOrientationModel - Context AAC: with 217930 ref and 1616 alt examples, EM converged in 12 steps; 16:21:01.086 INFO Learn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:5537,Usability,Learn,LearnReadOrientationModel,5537,"-----------------------------------------------------; 16:20:59.205 INFO LearnReadOrientationModel - The Genome Analysis Toolkit (GATK) v4.0.11.0; 16:20:59.205 INFO LearnReadOrientationModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:20:59.206 INFO LearnReadOrientationModel - Executing as root@3231a24c7afb on Linux v4.9.125-linuxkit amd64; 16:20:59.206 INFO LearnReadOrientationModel - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_181-8u181-b13-0ubuntu0.16.04.1-b13; 16:20:59.207 INFO LearnReadOrientationModel - Start Date/Time: November 26, 2018 4:20:57 PM UTC; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Version: 2.16.1; 16:20:59.208 INFO LearnReadOrientationModel - Picard Version: 2.18.13; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:20:59.209 INFO LearnReadOrientationModel - Deflater: IntelDeflater; 16:20:59.209 INFO LearnReadOrientationModel - Inflater: IntelInflater; 16:20:59.209 INFO LearnReadOrientationModel - GCS max retries/reopens: 20; 16:20:59.209 INFO LearnReadOrientationModel - Requester pays: disabled; 16:20:59.209 INFO LearnReadOrientationModel - Initializing engine; 16:20:59.352 INFO LearnReadOrientationModel - Done initializing engine; 16:21:00.402 INFO LearnReadOrientationModel - Context AAC: with 217930 ref and 1616 alt examples, EM converged in 12 steps; 16:21:01.086 INFO LearnReadOrientationModel - Context AAG: with 354938 ref and 3337 alt examp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:5622,Usability,Learn,LearnReadOrientationModel,5622,"entationModel - The Genome Analysis Toolkit (GATK) v4.0.11.0; 16:20:59.205 INFO LearnReadOrientationModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:20:59.206 INFO LearnReadOrientationModel - Executing as root@3231a24c7afb on Linux v4.9.125-linuxkit amd64; 16:20:59.206 INFO LearnReadOrientationModel - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_181-8u181-b13-0ubuntu0.16.04.1-b13; 16:20:59.207 INFO LearnReadOrientationModel - Start Date/Time: November 26, 2018 4:20:57 PM UTC; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Version: 2.16.1; 16:20:59.208 INFO LearnReadOrientationModel - Picard Version: 2.18.13; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:20:59.209 INFO LearnReadOrientationModel - Deflater: IntelDeflater; 16:20:59.209 INFO LearnReadOrientationModel - Inflater: IntelInflater; 16:20:59.209 INFO LearnReadOrientationModel - GCS max retries/reopens: 20; 16:20:59.209 INFO LearnReadOrientationModel - Requester pays: disabled; 16:20:59.209 INFO LearnReadOrientationModel - Initializing engine; 16:20:59.352 INFO LearnReadOrientationModel - Done initializing engine; 16:21:00.402 INFO LearnReadOrientationModel - Context AAC: with 217930 ref and 1616 alt examples, EM converged in 12 steps; 16:21:01.086 INFO LearnReadOrientationModel - Context AAG: with 354938 ref and 3337 alt examples, EM converged in 12 steps; 16:21:01.611 INFO LearnReadOrientationModel - Context ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:5724,Usability,Learn,LearnReadOrientationModel,5724,"del - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:20:59.206 INFO LearnReadOrientationModel - Executing as root@3231a24c7afb on Linux v4.9.125-linuxkit amd64; 16:20:59.206 INFO LearnReadOrientationModel - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_181-8u181-b13-0ubuntu0.16.04.1-b13; 16:20:59.207 INFO LearnReadOrientationModel - Start Date/Time: November 26, 2018 4:20:57 PM UTC; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Version: 2.16.1; 16:20:59.208 INFO LearnReadOrientationModel - Picard Version: 2.18.13; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:20:59.209 INFO LearnReadOrientationModel - Deflater: IntelDeflater; 16:20:59.209 INFO LearnReadOrientationModel - Inflater: IntelInflater; 16:20:59.209 INFO LearnReadOrientationModel - GCS max retries/reopens: 20; 16:20:59.209 INFO LearnReadOrientationModel - Requester pays: disabled; 16:20:59.209 INFO LearnReadOrientationModel - Initializing engine; 16:20:59.352 INFO LearnReadOrientationModel - Done initializing engine; 16:21:00.402 INFO LearnReadOrientationModel - Context AAC: with 217930 ref and 1616 alt examples, EM converged in 12 steps; 16:21:01.086 INFO LearnReadOrientationModel - Context AAG: with 354938 ref and 3337 alt examples, EM converged in 12 steps; 16:21:01.611 INFO LearnReadOrientationModel - Context AAT: with 252561 ref and 1308 alt examples, EM converged in 11 steps; 16:21:02.365 INFO LearnReadOrien",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:5826,Usability,Learn,LearnReadOrientationModel,5826," LearnReadOrientationModel - Executing as root@3231a24c7afb on Linux v4.9.125-linuxkit amd64; 16:20:59.206 INFO LearnReadOrientationModel - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_181-8u181-b13-0ubuntu0.16.04.1-b13; 16:20:59.207 INFO LearnReadOrientationModel - Start Date/Time: November 26, 2018 4:20:57 PM UTC; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Version: 2.16.1; 16:20:59.208 INFO LearnReadOrientationModel - Picard Version: 2.18.13; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:20:59.209 INFO LearnReadOrientationModel - Deflater: IntelDeflater; 16:20:59.209 INFO LearnReadOrientationModel - Inflater: IntelInflater; 16:20:59.209 INFO LearnReadOrientationModel - GCS max retries/reopens: 20; 16:20:59.209 INFO LearnReadOrientationModel - Requester pays: disabled; 16:20:59.209 INFO LearnReadOrientationModel - Initializing engine; 16:20:59.352 INFO LearnReadOrientationModel - Done initializing engine; 16:21:00.402 INFO LearnReadOrientationModel - Context AAC: with 217930 ref and 1616 alt examples, EM converged in 12 steps; 16:21:01.086 INFO LearnReadOrientationModel - Context AAG: with 354938 ref and 3337 alt examples, EM converged in 12 steps; 16:21:01.611 INFO LearnReadOrientationModel - Context AAT: with 252561 ref and 1308 alt examples, EM converged in 11 steps; 16:21:02.365 INFO LearnReadOrientationModel - Context ACA: with 323210 ref and 5248 alt examples, EM converged in 13 steps; 16:21:03.0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:5928,Usability,Learn,LearnReadOrientationModel,5928," LearnReadOrientationModel - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_181-8u181-b13-0ubuntu0.16.04.1-b13; 16:20:59.207 INFO LearnReadOrientationModel - Start Date/Time: November 26, 2018 4:20:57 PM UTC; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Version: 2.16.1; 16:20:59.208 INFO LearnReadOrientationModel - Picard Version: 2.18.13; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:20:59.209 INFO LearnReadOrientationModel - Deflater: IntelDeflater; 16:20:59.209 INFO LearnReadOrientationModel - Inflater: IntelInflater; 16:20:59.209 INFO LearnReadOrientationModel - GCS max retries/reopens: 20; 16:20:59.209 INFO LearnReadOrientationModel - Requester pays: disabled; 16:20:59.209 INFO LearnReadOrientationModel - Initializing engine; 16:20:59.352 INFO LearnReadOrientationModel - Done initializing engine; 16:21:00.402 INFO LearnReadOrientationModel - Context AAC: with 217930 ref and 1616 alt examples, EM converged in 12 steps; 16:21:01.086 INFO LearnReadOrientationModel - Context AAG: with 354938 ref and 3337 alt examples, EM converged in 12 steps; 16:21:01.611 INFO LearnReadOrientationModel - Context AAT: with 252561 ref and 1308 alt examples, EM converged in 11 steps; 16:21:02.365 INFO LearnReadOrientationModel - Context ACA: with 323210 ref and 5248 alt examples, EM converged in 13 steps; 16:21:03.088 INFO LearnReadOrientationModel - Context ACC: with 284921 ref and 4351 alt examples, EM converged in 13 step",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:5999,Usability,Learn,LearnReadOrientationModel,5999,"8.0_181-8u181-b13-0ubuntu0.16.04.1-b13; 16:20:59.207 INFO LearnReadOrientationModel - Start Date/Time: November 26, 2018 4:20:57 PM UTC; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Version: 2.16.1; 16:20:59.208 INFO LearnReadOrientationModel - Picard Version: 2.18.13; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:20:59.209 INFO LearnReadOrientationModel - Deflater: IntelDeflater; 16:20:59.209 INFO LearnReadOrientationModel - Inflater: IntelInflater; 16:20:59.209 INFO LearnReadOrientationModel - GCS max retries/reopens: 20; 16:20:59.209 INFO LearnReadOrientationModel - Requester pays: disabled; 16:20:59.209 INFO LearnReadOrientationModel - Initializing engine; 16:20:59.352 INFO LearnReadOrientationModel - Done initializing engine; 16:21:00.402 INFO LearnReadOrientationModel - Context AAC: with 217930 ref and 1616 alt examples, EM converged in 12 steps; 16:21:01.086 INFO LearnReadOrientationModel - Context AAG: with 354938 ref and 3337 alt examples, EM converged in 12 steps; 16:21:01.611 INFO LearnReadOrientationModel - Context AAT: with 252561 ref and 1308 alt examples, EM converged in 11 steps; 16:21:02.365 INFO LearnReadOrientationModel - Context ACA: with 323210 ref and 5248 alt examples, EM converged in 13 steps; 16:21:03.088 INFO LearnReadOrientationModel - Context ACC: with 284921 ref and 4351 alt examples, EM converged in 13 steps; 16:21:03.785 INFO LearnReadOrientationModel - Context ACG: with 8318",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:6070,Usability,Learn,LearnReadOrientationModel,6070,"ationModel - Start Date/Time: November 26, 2018 4:20:57 PM UTC; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Version: 2.16.1; 16:20:59.208 INFO LearnReadOrientationModel - Picard Version: 2.18.13; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:20:59.209 INFO LearnReadOrientationModel - Deflater: IntelDeflater; 16:20:59.209 INFO LearnReadOrientationModel - Inflater: IntelInflater; 16:20:59.209 INFO LearnReadOrientationModel - GCS max retries/reopens: 20; 16:20:59.209 INFO LearnReadOrientationModel - Requester pays: disabled; 16:20:59.209 INFO LearnReadOrientationModel - Initializing engine; 16:20:59.352 INFO LearnReadOrientationModel - Done initializing engine; 16:21:00.402 INFO LearnReadOrientationModel - Context AAC: with 217930 ref and 1616 alt examples, EM converged in 12 steps; 16:21:01.086 INFO LearnReadOrientationModel - Context AAG: with 354938 ref and 3337 alt examples, EM converged in 12 steps; 16:21:01.611 INFO LearnReadOrientationModel - Context AAT: with 252561 ref and 1308 alt examples, EM converged in 11 steps; 16:21:02.365 INFO LearnReadOrientationModel - Context ACA: with 323210 ref and 5248 alt examples, EM converged in 13 steps; 16:21:03.088 INFO LearnReadOrientationModel - Context ACC: with 284921 ref and 4351 alt examples, EM converged in 13 steps; 16:21:03.785 INFO LearnReadOrientationModel - Context ACG: with 83185 ref and 2548 alt examples, EM converged in 13 steps; 16:21:04.441 INFO ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:6145,Usability,Learn,LearnReadOrientationModel,6145,"07 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Version: 2.16.1; 16:20:59.208 INFO LearnReadOrientationModel - Picard Version: 2.18.13; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:20:59.209 INFO LearnReadOrientationModel - Deflater: IntelDeflater; 16:20:59.209 INFO LearnReadOrientationModel - Inflater: IntelInflater; 16:20:59.209 INFO LearnReadOrientationModel - GCS max retries/reopens: 20; 16:20:59.209 INFO LearnReadOrientationModel - Requester pays: disabled; 16:20:59.209 INFO LearnReadOrientationModel - Initializing engine; 16:20:59.352 INFO LearnReadOrientationModel - Done initializing engine; 16:21:00.402 INFO LearnReadOrientationModel - Context AAC: with 217930 ref and 1616 alt examples, EM converged in 12 steps; 16:21:01.086 INFO LearnReadOrientationModel - Context AAG: with 354938 ref and 3337 alt examples, EM converged in 12 steps; 16:21:01.611 INFO LearnReadOrientationModel - Context AAT: with 252561 ref and 1308 alt examples, EM converged in 11 steps; 16:21:02.365 INFO LearnReadOrientationModel - Context ACA: with 323210 ref and 5248 alt examples, EM converged in 13 steps; 16:21:03.088 INFO LearnReadOrientationModel - Context ACC: with 284921 ref and 4351 alt examples, EM converged in 13 steps; 16:21:03.785 INFO LearnReadOrientationModel - Context ACG: with 83185 ref and 2548 alt examples, EM converged in 13 steps; 16:21:04.441 INFO LearnReadOrientationModel - Context ACT: with 257420 ref and 3512 alt exa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:6217,Usability,Learn,LearnReadOrientationModel,6217,"---------------------------; 16:20:59.207 INFO LearnReadOrientationModel - ------------------------------------------------------------; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Version: 2.16.1; 16:20:59.208 INFO LearnReadOrientationModel - Picard Version: 2.18.13; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:20:59.209 INFO LearnReadOrientationModel - Deflater: IntelDeflater; 16:20:59.209 INFO LearnReadOrientationModel - Inflater: IntelInflater; 16:20:59.209 INFO LearnReadOrientationModel - GCS max retries/reopens: 20; 16:20:59.209 INFO LearnReadOrientationModel - Requester pays: disabled; 16:20:59.209 INFO LearnReadOrientationModel - Initializing engine; 16:20:59.352 INFO LearnReadOrientationModel - Done initializing engine; 16:21:00.402 INFO LearnReadOrientationModel - Context AAC: with 217930 ref and 1616 alt examples, EM converged in 12 steps; 16:21:01.086 INFO LearnReadOrientationModel - Context AAG: with 354938 ref and 3337 alt examples, EM converged in 12 steps; 16:21:01.611 INFO LearnReadOrientationModel - Context AAT: with 252561 ref and 1308 alt examples, EM converged in 11 steps; 16:21:02.365 INFO LearnReadOrientationModel - Context ACA: with 323210 ref and 5248 alt examples, EM converged in 13 steps; 16:21:03.088 INFO LearnReadOrientationModel - Context ACC: with 284921 ref and 4351 alt examples, EM converged in 13 steps; 16:21:03.785 INFO LearnReadOrientationModel - Context ACG: with 83185 ref and 2548 alt examples, EM converged in 13 steps; 16:21:04.441 INFO LearnReadOrientationModel - Context ACT: with 257420 ref and 3512 alt examples, EM converged in 13 steps; 16:21:05.269 INFO LearnReadOrientatio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:6284,Usability,Learn,LearnReadOrientationModel,6284,"el - ------------------------------------------------------------; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Version: 2.16.1; 16:20:59.208 INFO LearnReadOrientationModel - Picard Version: 2.18.13; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:20:59.209 INFO LearnReadOrientationModel - Deflater: IntelDeflater; 16:20:59.209 INFO LearnReadOrientationModel - Inflater: IntelInflater; 16:20:59.209 INFO LearnReadOrientationModel - GCS max retries/reopens: 20; 16:20:59.209 INFO LearnReadOrientationModel - Requester pays: disabled; 16:20:59.209 INFO LearnReadOrientationModel - Initializing engine; 16:20:59.352 INFO LearnReadOrientationModel - Done initializing engine; 16:21:00.402 INFO LearnReadOrientationModel - Context AAC: with 217930 ref and 1616 alt examples, EM converged in 12 steps; 16:21:01.086 INFO LearnReadOrientationModel - Context AAG: with 354938 ref and 3337 alt examples, EM converged in 12 steps; 16:21:01.611 INFO LearnReadOrientationModel - Context AAT: with 252561 ref and 1308 alt examples, EM converged in 11 steps; 16:21:02.365 INFO LearnReadOrientationModel - Context ACA: with 323210 ref and 5248 alt examples, EM converged in 13 steps; 16:21:03.088 INFO LearnReadOrientationModel - Context ACC: with 284921 ref and 4351 alt examples, EM converged in 13 steps; 16:21:03.785 INFO LearnReadOrientationModel - Context ACG: with 83185 ref and 2548 alt examples, EM converged in 13 steps; 16:21:04.441 INFO LearnReadOrientationModel - Context ACT: with 257420 ref and 3512 alt examples, EM converged in 13 steps; 16:21:05.269 INFO LearnReadOrientationModel - Context AGA: with 393111 ref and 6728 alt examples, EM conve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:6356,Usability,Learn,LearnReadOrientationModel,6356,"ntationModel - HTSJDK Version: 2.16.1; 16:20:59.208 INFO LearnReadOrientationModel - Picard Version: 2.18.13; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:20:59.209 INFO LearnReadOrientationModel - Deflater: IntelDeflater; 16:20:59.209 INFO LearnReadOrientationModel - Inflater: IntelInflater; 16:20:59.209 INFO LearnReadOrientationModel - GCS max retries/reopens: 20; 16:20:59.209 INFO LearnReadOrientationModel - Requester pays: disabled; 16:20:59.209 INFO LearnReadOrientationModel - Initializing engine; 16:20:59.352 INFO LearnReadOrientationModel - Done initializing engine; 16:21:00.402 INFO LearnReadOrientationModel - Context AAC: with 217930 ref and 1616 alt examples, EM converged in 12 steps; 16:21:01.086 INFO LearnReadOrientationModel - Context AAG: with 354938 ref and 3337 alt examples, EM converged in 12 steps; 16:21:01.611 INFO LearnReadOrientationModel - Context AAT: with 252561 ref and 1308 alt examples, EM converged in 11 steps; 16:21:02.365 INFO LearnReadOrientationModel - Context ACA: with 323210 ref and 5248 alt examples, EM converged in 13 steps; 16:21:03.088 INFO LearnReadOrientationModel - Context ACC: with 284921 ref and 4351 alt examples, EM converged in 13 steps; 16:21:03.785 INFO LearnReadOrientationModel - Context ACG: with 83185 ref and 2548 alt examples, EM converged in 13 steps; 16:21:04.441 INFO LearnReadOrientationModel - Context ACT: with 257420 ref and 3512 alt examples, EM converged in 13 steps; 16:21:05.269 INFO LearnReadOrientationModel - Context AGA: with 393111 ref and 6728 alt examples, EM converged in 13 steps; 16:21:06.090 INFO LearnReadOrientationModel - Context AGC: with 358648 ref and 6",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:6480,Usability,Learn,LearnReadOrientationModel,6480,"NFO LearnReadOrientationModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:20:59.208 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:20:59.209 INFO LearnReadOrientationModel - Deflater: IntelDeflater; 16:20:59.209 INFO LearnReadOrientationModel - Inflater: IntelInflater; 16:20:59.209 INFO LearnReadOrientationModel - GCS max retries/reopens: 20; 16:20:59.209 INFO LearnReadOrientationModel - Requester pays: disabled; 16:20:59.209 INFO LearnReadOrientationModel - Initializing engine; 16:20:59.352 INFO LearnReadOrientationModel - Done initializing engine; 16:21:00.402 INFO LearnReadOrientationModel - Context AAC: with 217930 ref and 1616 alt examples, EM converged in 12 steps; 16:21:01.086 INFO LearnReadOrientationModel - Context AAG: with 354938 ref and 3337 alt examples, EM converged in 12 steps; 16:21:01.611 INFO LearnReadOrientationModel - Context AAT: with 252561 ref and 1308 alt examples, EM converged in 11 steps; 16:21:02.365 INFO LearnReadOrientationModel - Context ACA: with 323210 ref and 5248 alt examples, EM converged in 13 steps; 16:21:03.088 INFO LearnReadOrientationModel - Context ACC: with 284921 ref and 4351 alt examples, EM converged in 13 steps; 16:21:03.785 INFO LearnReadOrientationModel - Context ACG: with 83185 ref and 2548 alt examples, EM converged in 13 steps; 16:21:04.441 INFO LearnReadOrientationModel - Context ACT: with 257420 ref and 3512 alt examples, EM converged in 13 steps; 16:21:05.269 INFO LearnReadOrientationModel - Context AGA: with 393111 ref and 6728 alt examples, EM converged in 13 steps; 16:21:06.090 INFO LearnReadOrientationModel - Context AGC: with 358648 ref and 6527 alt examples, EM converged in 13 steps; 16:21:06.995 INFO LearnReadOrientationModel - Context AGG: with 494947 ref and 7",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:6604,Usability,Learn,LearnReadOrientationModel,6604,"Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:20:59.209 INFO LearnReadOrientationModel - Deflater: IntelDeflater; 16:20:59.209 INFO LearnReadOrientationModel - Inflater: IntelInflater; 16:20:59.209 INFO LearnReadOrientationModel - GCS max retries/reopens: 20; 16:20:59.209 INFO LearnReadOrientationModel - Requester pays: disabled; 16:20:59.209 INFO LearnReadOrientationModel - Initializing engine; 16:20:59.352 INFO LearnReadOrientationModel - Done initializing engine; 16:21:00.402 INFO LearnReadOrientationModel - Context AAC: with 217930 ref and 1616 alt examples, EM converged in 12 steps; 16:21:01.086 INFO LearnReadOrientationModel - Context AAG: with 354938 ref and 3337 alt examples, EM converged in 12 steps; 16:21:01.611 INFO LearnReadOrientationModel - Context AAT: with 252561 ref and 1308 alt examples, EM converged in 11 steps; 16:21:02.365 INFO LearnReadOrientationModel - Context ACA: with 323210 ref and 5248 alt examples, EM converged in 13 steps; 16:21:03.088 INFO LearnReadOrientationModel - Context ACC: with 284921 ref and 4351 alt examples, EM converged in 13 steps; 16:21:03.785 INFO LearnReadOrientationModel - Context ACG: with 83185 ref and 2548 alt examples, EM converged in 13 steps; 16:21:04.441 INFO LearnReadOrientationModel - Context ACT: with 257420 ref and 3512 alt examples, EM converged in 13 steps; 16:21:05.269 INFO LearnReadOrientationModel - Context AGA: with 393111 ref and 6728 alt examples, EM converged in 13 steps; 16:21:06.090 INFO LearnReadOrientationModel - Context AGC: with 358648 ref and 6527 alt examples, EM converged in 13 steps; 16:21:06.995 INFO LearnReadOrientationModel - Context AGG: with 494947 ref and 7682 alt examples, EM converged in 14 steps; 16:21:07.497 INFO LearnReadOrientationModel - Context ATA: with 180560 ref and 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:6728,Usability,Learn,LearnReadOrientationModel,6728,"WRITE_FOR_SAMTOOLS : true; 16:20:59.209 INFO LearnReadOrientationModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:20:59.209 INFO LearnReadOrientationModel - Deflater: IntelDeflater; 16:20:59.209 INFO LearnReadOrientationModel - Inflater: IntelInflater; 16:20:59.209 INFO LearnReadOrientationModel - GCS max retries/reopens: 20; 16:20:59.209 INFO LearnReadOrientationModel - Requester pays: disabled; 16:20:59.209 INFO LearnReadOrientationModel - Initializing engine; 16:20:59.352 INFO LearnReadOrientationModel - Done initializing engine; 16:21:00.402 INFO LearnReadOrientationModel - Context AAC: with 217930 ref and 1616 alt examples, EM converged in 12 steps; 16:21:01.086 INFO LearnReadOrientationModel - Context AAG: with 354938 ref and 3337 alt examples, EM converged in 12 steps; 16:21:01.611 INFO LearnReadOrientationModel - Context AAT: with 252561 ref and 1308 alt examples, EM converged in 11 steps; 16:21:02.365 INFO LearnReadOrientationModel - Context ACA: with 323210 ref and 5248 alt examples, EM converged in 13 steps; 16:21:03.088 INFO LearnReadOrientationModel - Context ACC: with 284921 ref and 4351 alt examples, EM converged in 13 steps; 16:21:03.785 INFO LearnReadOrientationModel - Context ACG: with 83185 ref and 2548 alt examples, EM converged in 13 steps; 16:21:04.441 INFO LearnReadOrientationModel - Context ACT: with 257420 ref and 3512 alt examples, EM converged in 13 steps; 16:21:05.269 INFO LearnReadOrientationModel - Context AGA: with 393111 ref and 6728 alt examples, EM converged in 13 steps; 16:21:06.090 INFO LearnReadOrientationModel - Context AGC: with 358648 ref and 6527 alt examples, EM converged in 13 steps; 16:21:06.995 INFO LearnReadOrientationModel - Context AGG: with 494947 ref and 7682 alt examples, EM converged in 14 steps; 16:21:07.497 INFO LearnReadOrientationModel - Context ATA: with 180560 ref and 1078 alt examples, EM converged in 12 steps; 16:21:07.964 INFO LearnReadOrientationModel - Context ATC: with 213079 ref and 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:6852,Usability,Learn,LearnReadOrientationModel,6852,"lse; 16:20:59.209 INFO LearnReadOrientationModel - Deflater: IntelDeflater; 16:20:59.209 INFO LearnReadOrientationModel - Inflater: IntelInflater; 16:20:59.209 INFO LearnReadOrientationModel - GCS max retries/reopens: 20; 16:20:59.209 INFO LearnReadOrientationModel - Requester pays: disabled; 16:20:59.209 INFO LearnReadOrientationModel - Initializing engine; 16:20:59.352 INFO LearnReadOrientationModel - Done initializing engine; 16:21:00.402 INFO LearnReadOrientationModel - Context AAC: with 217930 ref and 1616 alt examples, EM converged in 12 steps; 16:21:01.086 INFO LearnReadOrientationModel - Context AAG: with 354938 ref and 3337 alt examples, EM converged in 12 steps; 16:21:01.611 INFO LearnReadOrientationModel - Context AAT: with 252561 ref and 1308 alt examples, EM converged in 11 steps; 16:21:02.365 INFO LearnReadOrientationModel - Context ACA: with 323210 ref and 5248 alt examples, EM converged in 13 steps; 16:21:03.088 INFO LearnReadOrientationModel - Context ACC: with 284921 ref and 4351 alt examples, EM converged in 13 steps; 16:21:03.785 INFO LearnReadOrientationModel - Context ACG: with 83185 ref and 2548 alt examples, EM converged in 13 steps; 16:21:04.441 INFO LearnReadOrientationModel - Context ACT: with 257420 ref and 3512 alt examples, EM converged in 13 steps; 16:21:05.269 INFO LearnReadOrientationModel - Context AGA: with 393111 ref and 6728 alt examples, EM converged in 13 steps; 16:21:06.090 INFO LearnReadOrientationModel - Context AGC: with 358648 ref and 6527 alt examples, EM converged in 13 steps; 16:21:06.995 INFO LearnReadOrientationModel - Context AGG: with 494947 ref and 7682 alt examples, EM converged in 14 steps; 16:21:07.497 INFO LearnReadOrientationModel - Context ATA: with 180560 ref and 1078 alt examples, EM converged in 12 steps; 16:21:07.964 INFO LearnReadOrientationModel - Context ATC: with 213079 ref and 1522 alt examples, EM converged in 11 steps; 16:21:08.549 INFO LearnReadOrientationModel - Context ATG: with 282561 ref and 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:6976,Usability,Learn,LearnReadOrientationModel,6976,"nflater: IntelInflater; 16:20:59.209 INFO LearnReadOrientationModel - GCS max retries/reopens: 20; 16:20:59.209 INFO LearnReadOrientationModel - Requester pays: disabled; 16:20:59.209 INFO LearnReadOrientationModel - Initializing engine; 16:20:59.352 INFO LearnReadOrientationModel - Done initializing engine; 16:21:00.402 INFO LearnReadOrientationModel - Context AAC: with 217930 ref and 1616 alt examples, EM converged in 12 steps; 16:21:01.086 INFO LearnReadOrientationModel - Context AAG: with 354938 ref and 3337 alt examples, EM converged in 12 steps; 16:21:01.611 INFO LearnReadOrientationModel - Context AAT: with 252561 ref and 1308 alt examples, EM converged in 11 steps; 16:21:02.365 INFO LearnReadOrientationModel - Context ACA: with 323210 ref and 5248 alt examples, EM converged in 13 steps; 16:21:03.088 INFO LearnReadOrientationModel - Context ACC: with 284921 ref and 4351 alt examples, EM converged in 13 steps; 16:21:03.785 INFO LearnReadOrientationModel - Context ACG: with 83185 ref and 2548 alt examples, EM converged in 13 steps; 16:21:04.441 INFO LearnReadOrientationModel - Context ACT: with 257420 ref and 3512 alt examples, EM converged in 13 steps; 16:21:05.269 INFO LearnReadOrientationModel - Context AGA: with 393111 ref and 6728 alt examples, EM converged in 13 steps; 16:21:06.090 INFO LearnReadOrientationModel - Context AGC: with 358648 ref and 6527 alt examples, EM converged in 13 steps; 16:21:06.995 INFO LearnReadOrientationModel - Context AGG: with 494947 ref and 7682 alt examples, EM converged in 14 steps; 16:21:07.497 INFO LearnReadOrientationModel - Context ATA: with 180560 ref and 1078 alt examples, EM converged in 12 steps; 16:21:07.964 INFO LearnReadOrientationModel - Context ATC: with 213079 ref and 1522 alt examples, EM converged in 11 steps; 16:21:08.549 INFO LearnReadOrientationModel - Context ATG: with 282561 ref and 2106 alt examples, EM converged in 12 steps; 16:21:09.030 INFO LearnReadOrientationModel - Context CAA: with 284942 ref and 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:7099,Usability,Learn,LearnReadOrientationModel,7099,"adOrientationModel - Requester pays: disabled; 16:20:59.209 INFO LearnReadOrientationModel - Initializing engine; 16:20:59.352 INFO LearnReadOrientationModel - Done initializing engine; 16:21:00.402 INFO LearnReadOrientationModel - Context AAC: with 217930 ref and 1616 alt examples, EM converged in 12 steps; 16:21:01.086 INFO LearnReadOrientationModel - Context AAG: with 354938 ref and 3337 alt examples, EM converged in 12 steps; 16:21:01.611 INFO LearnReadOrientationModel - Context AAT: with 252561 ref and 1308 alt examples, EM converged in 11 steps; 16:21:02.365 INFO LearnReadOrientationModel - Context ACA: with 323210 ref and 5248 alt examples, EM converged in 13 steps; 16:21:03.088 INFO LearnReadOrientationModel - Context ACC: with 284921 ref and 4351 alt examples, EM converged in 13 steps; 16:21:03.785 INFO LearnReadOrientationModel - Context ACG: with 83185 ref and 2548 alt examples, EM converged in 13 steps; 16:21:04.441 INFO LearnReadOrientationModel - Context ACT: with 257420 ref and 3512 alt examples, EM converged in 13 steps; 16:21:05.269 INFO LearnReadOrientationModel - Context AGA: with 393111 ref and 6728 alt examples, EM converged in 13 steps; 16:21:06.090 INFO LearnReadOrientationModel - Context AGC: with 358648 ref and 6527 alt examples, EM converged in 13 steps; 16:21:06.995 INFO LearnReadOrientationModel - Context AGG: with 494947 ref and 7682 alt examples, EM converged in 14 steps; 16:21:07.497 INFO LearnReadOrientationModel - Context ATA: with 180560 ref and 1078 alt examples, EM converged in 12 steps; 16:21:07.964 INFO LearnReadOrientationModel - Context ATC: with 213079 ref and 1522 alt examples, EM converged in 11 steps; 16:21:08.549 INFO LearnReadOrientationModel - Context ATG: with 282561 ref and 2106 alt examples, EM converged in 12 steps; 16:21:09.030 INFO LearnReadOrientationModel - Context CAA: with 284942 ref and 1909 alt examples, EM converged in 11 steps; 16:21:09.635 INFO LearnReadOrientationModel - Context CAC: with 351720 ref and ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:7223,Usability,Learn,LearnReadOrientationModel,7223,"52 INFO LearnReadOrientationModel - Done initializing engine; 16:21:00.402 INFO LearnReadOrientationModel - Context AAC: with 217930 ref and 1616 alt examples, EM converged in 12 steps; 16:21:01.086 INFO LearnReadOrientationModel - Context AAG: with 354938 ref and 3337 alt examples, EM converged in 12 steps; 16:21:01.611 INFO LearnReadOrientationModel - Context AAT: with 252561 ref and 1308 alt examples, EM converged in 11 steps; 16:21:02.365 INFO LearnReadOrientationModel - Context ACA: with 323210 ref and 5248 alt examples, EM converged in 13 steps; 16:21:03.088 INFO LearnReadOrientationModel - Context ACC: with 284921 ref and 4351 alt examples, EM converged in 13 steps; 16:21:03.785 INFO LearnReadOrientationModel - Context ACG: with 83185 ref and 2548 alt examples, EM converged in 13 steps; 16:21:04.441 INFO LearnReadOrientationModel - Context ACT: with 257420 ref and 3512 alt examples, EM converged in 13 steps; 16:21:05.269 INFO LearnReadOrientationModel - Context AGA: with 393111 ref and 6728 alt examples, EM converged in 13 steps; 16:21:06.090 INFO LearnReadOrientationModel - Context AGC: with 358648 ref and 6527 alt examples, EM converged in 13 steps; 16:21:06.995 INFO LearnReadOrientationModel - Context AGG: with 494947 ref and 7682 alt examples, EM converged in 14 steps; 16:21:07.497 INFO LearnReadOrientationModel - Context ATA: with 180560 ref and 1078 alt examples, EM converged in 12 steps; 16:21:07.964 INFO LearnReadOrientationModel - Context ATC: with 213079 ref and 1522 alt examples, EM converged in 11 steps; 16:21:08.549 INFO LearnReadOrientationModel - Context ATG: with 282561 ref and 2106 alt examples, EM converged in 12 steps; 16:21:09.030 INFO LearnReadOrientationModel - Context CAA: with 284942 ref and 1909 alt examples, EM converged in 11 steps; 16:21:09.635 INFO LearnReadOrientationModel - Context CAC: with 351720 ref and 3718 alt examples, EM converged in 12 steps; 16:21:10.322 INFO LearnReadOrientationModel - Context CAG: with 547234 ref and ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:7347,Usability,Learn,LearnReadOrientationModel,7347,"h 217930 ref and 1616 alt examples, EM converged in 12 steps; 16:21:01.086 INFO LearnReadOrientationModel - Context AAG: with 354938 ref and 3337 alt examples, EM converged in 12 steps; 16:21:01.611 INFO LearnReadOrientationModel - Context AAT: with 252561 ref and 1308 alt examples, EM converged in 11 steps; 16:21:02.365 INFO LearnReadOrientationModel - Context ACA: with 323210 ref and 5248 alt examples, EM converged in 13 steps; 16:21:03.088 INFO LearnReadOrientationModel - Context ACC: with 284921 ref and 4351 alt examples, EM converged in 13 steps; 16:21:03.785 INFO LearnReadOrientationModel - Context ACG: with 83185 ref and 2548 alt examples, EM converged in 13 steps; 16:21:04.441 INFO LearnReadOrientationModel - Context ACT: with 257420 ref and 3512 alt examples, EM converged in 13 steps; 16:21:05.269 INFO LearnReadOrientationModel - Context AGA: with 393111 ref and 6728 alt examples, EM converged in 13 steps; 16:21:06.090 INFO LearnReadOrientationModel - Context AGC: with 358648 ref and 6527 alt examples, EM converged in 13 steps; 16:21:06.995 INFO LearnReadOrientationModel - Context AGG: with 494947 ref and 7682 alt examples, EM converged in 14 steps; 16:21:07.497 INFO LearnReadOrientationModel - Context ATA: with 180560 ref and 1078 alt examples, EM converged in 12 steps; 16:21:07.964 INFO LearnReadOrientationModel - Context ATC: with 213079 ref and 1522 alt examples, EM converged in 11 steps; 16:21:08.549 INFO LearnReadOrientationModel - Context ATG: with 282561 ref and 2106 alt examples, EM converged in 12 steps; 16:21:09.030 INFO LearnReadOrientationModel - Context CAA: with 284942 ref and 1909 alt examples, EM converged in 11 steps; 16:21:09.635 INFO LearnReadOrientationModel - Context CAC: with 351720 ref and 3718 alt examples, EM converged in 12 steps; 16:21:10.322 INFO LearnReadOrientationModel - Context CAG: with 547234 ref and 5940 alt examples, EM converged in 12 steps; 16:21:11.103 INFO LearnReadOrientationModel - Context CCA: with 464164 ref and ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:7471,Usability,Learn,LearnReadOrientationModel,7471,"h 354938 ref and 3337 alt examples, EM converged in 12 steps; 16:21:01.611 INFO LearnReadOrientationModel - Context AAT: with 252561 ref and 1308 alt examples, EM converged in 11 steps; 16:21:02.365 INFO LearnReadOrientationModel - Context ACA: with 323210 ref and 5248 alt examples, EM converged in 13 steps; 16:21:03.088 INFO LearnReadOrientationModel - Context ACC: with 284921 ref and 4351 alt examples, EM converged in 13 steps; 16:21:03.785 INFO LearnReadOrientationModel - Context ACG: with 83185 ref and 2548 alt examples, EM converged in 13 steps; 16:21:04.441 INFO LearnReadOrientationModel - Context ACT: with 257420 ref and 3512 alt examples, EM converged in 13 steps; 16:21:05.269 INFO LearnReadOrientationModel - Context AGA: with 393111 ref and 6728 alt examples, EM converged in 13 steps; 16:21:06.090 INFO LearnReadOrientationModel - Context AGC: with 358648 ref and 6527 alt examples, EM converged in 13 steps; 16:21:06.995 INFO LearnReadOrientationModel - Context AGG: with 494947 ref and 7682 alt examples, EM converged in 14 steps; 16:21:07.497 INFO LearnReadOrientationModel - Context ATA: with 180560 ref and 1078 alt examples, EM converged in 12 steps; 16:21:07.964 INFO LearnReadOrientationModel - Context ATC: with 213079 ref and 1522 alt examples, EM converged in 11 steps; 16:21:08.549 INFO LearnReadOrientationModel - Context ATG: with 282561 ref and 2106 alt examples, EM converged in 12 steps; 16:21:09.030 INFO LearnReadOrientationModel - Context CAA: with 284942 ref and 1909 alt examples, EM converged in 11 steps; 16:21:09.635 INFO LearnReadOrientationModel - Context CAC: with 351720 ref and 3718 alt examples, EM converged in 12 steps; 16:21:10.322 INFO LearnReadOrientationModel - Context CAG: with 547234 ref and 5940 alt examples, EM converged in 12 steps; 16:21:11.103 INFO LearnReadOrientationModel - Context CCA: with 464164 ref and 8212 alt examples, EM converged in 13 steps; 16:21:11.899 INFO LearnReadOrientationModel - Context CCC: with 512128 ref and ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:7595,Usability,Learn,LearnReadOrientationModel,7595,"h 252561 ref and 1308 alt examples, EM converged in 11 steps; 16:21:02.365 INFO LearnReadOrientationModel - Context ACA: with 323210 ref and 5248 alt examples, EM converged in 13 steps; 16:21:03.088 INFO LearnReadOrientationModel - Context ACC: with 284921 ref and 4351 alt examples, EM converged in 13 steps; 16:21:03.785 INFO LearnReadOrientationModel - Context ACG: with 83185 ref and 2548 alt examples, EM converged in 13 steps; 16:21:04.441 INFO LearnReadOrientationModel - Context ACT: with 257420 ref and 3512 alt examples, EM converged in 13 steps; 16:21:05.269 INFO LearnReadOrientationModel - Context AGA: with 393111 ref and 6728 alt examples, EM converged in 13 steps; 16:21:06.090 INFO LearnReadOrientationModel - Context AGC: with 358648 ref and 6527 alt examples, EM converged in 13 steps; 16:21:06.995 INFO LearnReadOrientationModel - Context AGG: with 494947 ref and 7682 alt examples, EM converged in 14 steps; 16:21:07.497 INFO LearnReadOrientationModel - Context ATA: with 180560 ref and 1078 alt examples, EM converged in 12 steps; 16:21:07.964 INFO LearnReadOrientationModel - Context ATC: with 213079 ref and 1522 alt examples, EM converged in 11 steps; 16:21:08.549 INFO LearnReadOrientationModel - Context ATG: with 282561 ref and 2106 alt examples, EM converged in 12 steps; 16:21:09.030 INFO LearnReadOrientationModel - Context CAA: with 284942 ref and 1909 alt examples, EM converged in 11 steps; 16:21:09.635 INFO LearnReadOrientationModel - Context CAC: with 351720 ref and 3718 alt examples, EM converged in 12 steps; 16:21:10.322 INFO LearnReadOrientationModel - Context CAG: with 547234 ref and 5940 alt examples, EM converged in 12 steps; 16:21:11.103 INFO LearnReadOrientationModel - Context CCA: with 464164 ref and 8212 alt examples, EM converged in 13 steps; 16:21:11.899 INFO LearnReadOrientationModel - Context CCC: with 512128 ref and 7580 alt examples, EM converged in 13 steps; 16:21:12.719 INFO LearnReadOrientationModel - Context CCG: with 143118 ref and ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:7719,Usability,Learn,LearnReadOrientationModel,7719,"h 323210 ref and 5248 alt examples, EM converged in 13 steps; 16:21:03.088 INFO LearnReadOrientationModel - Context ACC: with 284921 ref and 4351 alt examples, EM converged in 13 steps; 16:21:03.785 INFO LearnReadOrientationModel - Context ACG: with 83185 ref and 2548 alt examples, EM converged in 13 steps; 16:21:04.441 INFO LearnReadOrientationModel - Context ACT: with 257420 ref and 3512 alt examples, EM converged in 13 steps; 16:21:05.269 INFO LearnReadOrientationModel - Context AGA: with 393111 ref and 6728 alt examples, EM converged in 13 steps; 16:21:06.090 INFO LearnReadOrientationModel - Context AGC: with 358648 ref and 6527 alt examples, EM converged in 13 steps; 16:21:06.995 INFO LearnReadOrientationModel - Context AGG: with 494947 ref and 7682 alt examples, EM converged in 14 steps; 16:21:07.497 INFO LearnReadOrientationModel - Context ATA: with 180560 ref and 1078 alt examples, EM converged in 12 steps; 16:21:07.964 INFO LearnReadOrientationModel - Context ATC: with 213079 ref and 1522 alt examples, EM converged in 11 steps; 16:21:08.549 INFO LearnReadOrientationModel - Context ATG: with 282561 ref and 2106 alt examples, EM converged in 12 steps; 16:21:09.030 INFO LearnReadOrientationModel - Context CAA: with 284942 ref and 1909 alt examples, EM converged in 11 steps; 16:21:09.635 INFO LearnReadOrientationModel - Context CAC: with 351720 ref and 3718 alt examples, EM converged in 12 steps; 16:21:10.322 INFO LearnReadOrientationModel - Context CAG: with 547234 ref and 5940 alt examples, EM converged in 12 steps; 16:21:11.103 INFO LearnReadOrientationModel - Context CCA: with 464164 ref and 8212 alt examples, EM converged in 13 steps; 16:21:11.899 INFO LearnReadOrientationModel - Context CCC: with 512128 ref and 7580 alt examples, EM converged in 13 steps; 16:21:12.719 INFO LearnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:7843,Usability,Learn,LearnReadOrientationModel,7843,"h 284921 ref and 4351 alt examples, EM converged in 13 steps; 16:21:03.785 INFO LearnReadOrientationModel - Context ACG: with 83185 ref and 2548 alt examples, EM converged in 13 steps; 16:21:04.441 INFO LearnReadOrientationModel - Context ACT: with 257420 ref and 3512 alt examples, EM converged in 13 steps; 16:21:05.269 INFO LearnReadOrientationModel - Context AGA: with 393111 ref and 6728 alt examples, EM converged in 13 steps; 16:21:06.090 INFO LearnReadOrientationModel - Context AGC: with 358648 ref and 6527 alt examples, EM converged in 13 steps; 16:21:06.995 INFO LearnReadOrientationModel - Context AGG: with 494947 ref and 7682 alt examples, EM converged in 14 steps; 16:21:07.497 INFO LearnReadOrientationModel - Context ATA: with 180560 ref and 1078 alt examples, EM converged in 12 steps; 16:21:07.964 INFO LearnReadOrientationModel - Context ATC: with 213079 ref and 1522 alt examples, EM converged in 11 steps; 16:21:08.549 INFO LearnReadOrientationModel - Context ATG: with 282561 ref and 2106 alt examples, EM converged in 12 steps; 16:21:09.030 INFO LearnReadOrientationModel - Context CAA: with 284942 ref and 1909 alt examples, EM converged in 11 steps; 16:21:09.635 INFO LearnReadOrientationModel - Context CAC: with 351720 ref and 3718 alt examples, EM converged in 12 steps; 16:21:10.322 INFO LearnReadOrientationModel - Context CAG: with 547234 ref and 5940 alt examples, EM converged in 12 steps; 16:21:11.103 INFO LearnReadOrientationModel - Context CCA: with 464164 ref and 8212 alt examples, EM converged in 13 steps; 16:21:11.899 INFO LearnReadOrientationModel - Context CCC: with 512128 ref and 7580 alt examples, EM converged in 13 steps; 16:21:12.719 INFO LearnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:7967,Usability,Learn,LearnReadOrientationModel,7967,"h 83185 ref and 2548 alt examples, EM converged in 13 steps; 16:21:04.441 INFO LearnReadOrientationModel - Context ACT: with 257420 ref and 3512 alt examples, EM converged in 13 steps; 16:21:05.269 INFO LearnReadOrientationModel - Context AGA: with 393111 ref and 6728 alt examples, EM converged in 13 steps; 16:21:06.090 INFO LearnReadOrientationModel - Context AGC: with 358648 ref and 6527 alt examples, EM converged in 13 steps; 16:21:06.995 INFO LearnReadOrientationModel - Context AGG: with 494947 ref and 7682 alt examples, EM converged in 14 steps; 16:21:07.497 INFO LearnReadOrientationModel - Context ATA: with 180560 ref and 1078 alt examples, EM converged in 12 steps; 16:21:07.964 INFO LearnReadOrientationModel - Context ATC: with 213079 ref and 1522 alt examples, EM converged in 11 steps; 16:21:08.549 INFO LearnReadOrientationModel - Context ATG: with 282561 ref and 2106 alt examples, EM converged in 12 steps; 16:21:09.030 INFO LearnReadOrientationModel - Context CAA: with 284942 ref and 1909 alt examples, EM converged in 11 steps; 16:21:09.635 INFO LearnReadOrientationModel - Context CAC: with 351720 ref and 3718 alt examples, EM converged in 12 steps; 16:21:10.322 INFO LearnReadOrientationModel - Context CAG: with 547234 ref and 5940 alt examples, EM converged in 12 steps; 16:21:11.103 INFO LearnReadOrientationModel - Context CCA: with 464164 ref and 8212 alt examples, EM converged in 13 steps; 16:21:11.899 INFO LearnReadOrientationModel - Context CCC: with 512128 ref and 7580 alt examples, EM converged in 13 steps; 16:21:12.719 INFO LearnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3741 alt examples, EM converged in 14 steps; 16:21:14.667 INFO LearnReadOrientationModel - Context CTA: with 173673 ref and 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:8091,Usability,Learn,LearnReadOrientationModel,8091," 257420 ref and 3512 alt examples, EM converged in 13 steps; 16:21:05.269 INFO LearnReadOrientationModel - Context AGA: with 393111 ref and 6728 alt examples, EM converged in 13 steps; 16:21:06.090 INFO LearnReadOrientationModel - Context AGC: with 358648 ref and 6527 alt examples, EM converged in 13 steps; 16:21:06.995 INFO LearnReadOrientationModel - Context AGG: with 494947 ref and 7682 alt examples, EM converged in 14 steps; 16:21:07.497 INFO LearnReadOrientationModel - Context ATA: with 180560 ref and 1078 alt examples, EM converged in 12 steps; 16:21:07.964 INFO LearnReadOrientationModel - Context ATC: with 213079 ref and 1522 alt examples, EM converged in 11 steps; 16:21:08.549 INFO LearnReadOrientationModel - Context ATG: with 282561 ref and 2106 alt examples, EM converged in 12 steps; 16:21:09.030 INFO LearnReadOrientationModel - Context CAA: with 284942 ref and 1909 alt examples, EM converged in 11 steps; 16:21:09.635 INFO LearnReadOrientationModel - Context CAC: with 351720 ref and 3718 alt examples, EM converged in 12 steps; 16:21:10.322 INFO LearnReadOrientationModel - Context CAG: with 547234 ref and 5940 alt examples, EM converged in 12 steps; 16:21:11.103 INFO LearnReadOrientationModel - Context CCA: with 464164 ref and 8212 alt examples, EM converged in 13 steps; 16:21:11.899 INFO LearnReadOrientationModel - Context CCC: with 512128 ref and 7580 alt examples, EM converged in 13 steps; 16:21:12.719 INFO LearnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3741 alt examples, EM converged in 14 steps; 16:21:14.667 INFO LearnReadOrientationModel - Context CTA: with 173673 ref and 1472 alt examples, EM converged in 13 steps; 16:21:15.238 INFO LearnReadOrientationModel - Context CTC: with 439622 ref and 3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:8215,Usability,Learn,LearnReadOrientationModel,8215," 393111 ref and 6728 alt examples, EM converged in 13 steps; 16:21:06.090 INFO LearnReadOrientationModel - Context AGC: with 358648 ref and 6527 alt examples, EM converged in 13 steps; 16:21:06.995 INFO LearnReadOrientationModel - Context AGG: with 494947 ref and 7682 alt examples, EM converged in 14 steps; 16:21:07.497 INFO LearnReadOrientationModel - Context ATA: with 180560 ref and 1078 alt examples, EM converged in 12 steps; 16:21:07.964 INFO LearnReadOrientationModel - Context ATC: with 213079 ref and 1522 alt examples, EM converged in 11 steps; 16:21:08.549 INFO LearnReadOrientationModel - Context ATG: with 282561 ref and 2106 alt examples, EM converged in 12 steps; 16:21:09.030 INFO LearnReadOrientationModel - Context CAA: with 284942 ref and 1909 alt examples, EM converged in 11 steps; 16:21:09.635 INFO LearnReadOrientationModel - Context CAC: with 351720 ref and 3718 alt examples, EM converged in 12 steps; 16:21:10.322 INFO LearnReadOrientationModel - Context CAG: with 547234 ref and 5940 alt examples, EM converged in 12 steps; 16:21:11.103 INFO LearnReadOrientationModel - Context CCA: with 464164 ref and 8212 alt examples, EM converged in 13 steps; 16:21:11.899 INFO LearnReadOrientationModel - Context CCC: with 512128 ref and 7580 alt examples, EM converged in 13 steps; 16:21:12.719 INFO LearnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3741 alt examples, EM converged in 14 steps; 16:21:14.667 INFO LearnReadOrientationModel - Context CTA: with 173673 ref and 1472 alt examples, EM converged in 13 steps; 16:21:15.238 INFO LearnReadOrientationModel - Context CTC: with 439622 ref and 3855 alt examples, EM converged in 12 steps; 16:21:15.709 INFO LearnReadOrientationModel - Context GAA: with 333082 ref and 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:8339,Usability,Learn,LearnReadOrientationModel,8339," 358648 ref and 6527 alt examples, EM converged in 13 steps; 16:21:06.995 INFO LearnReadOrientationModel - Context AGG: with 494947 ref and 7682 alt examples, EM converged in 14 steps; 16:21:07.497 INFO LearnReadOrientationModel - Context ATA: with 180560 ref and 1078 alt examples, EM converged in 12 steps; 16:21:07.964 INFO LearnReadOrientationModel - Context ATC: with 213079 ref and 1522 alt examples, EM converged in 11 steps; 16:21:08.549 INFO LearnReadOrientationModel - Context ATG: with 282561 ref and 2106 alt examples, EM converged in 12 steps; 16:21:09.030 INFO LearnReadOrientationModel - Context CAA: with 284942 ref and 1909 alt examples, EM converged in 11 steps; 16:21:09.635 INFO LearnReadOrientationModel - Context CAC: with 351720 ref and 3718 alt examples, EM converged in 12 steps; 16:21:10.322 INFO LearnReadOrientationModel - Context CAG: with 547234 ref and 5940 alt examples, EM converged in 12 steps; 16:21:11.103 INFO LearnReadOrientationModel - Context CCA: with 464164 ref and 8212 alt examples, EM converged in 13 steps; 16:21:11.899 INFO LearnReadOrientationModel - Context CCC: with 512128 ref and 7580 alt examples, EM converged in 13 steps; 16:21:12.719 INFO LearnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3741 alt examples, EM converged in 14 steps; 16:21:14.667 INFO LearnReadOrientationModel - Context CTA: with 173673 ref and 1472 alt examples, EM converged in 13 steps; 16:21:15.238 INFO LearnReadOrientationModel - Context CTC: with 439622 ref and 3855 alt examples, EM converged in 12 steps; 16:21:15.709 INFO LearnReadOrientationModel - Context GAA: with 333082 ref and 2101 alt examples, EM converged in 11 steps; 16:21:16.222 INFO LearnReadOrientationModel - Context GAC: with 227508 ref and 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:8463,Usability,Learn,LearnReadOrientationModel,8463," 494947 ref and 7682 alt examples, EM converged in 14 steps; 16:21:07.497 INFO LearnReadOrientationModel - Context ATA: with 180560 ref and 1078 alt examples, EM converged in 12 steps; 16:21:07.964 INFO LearnReadOrientationModel - Context ATC: with 213079 ref and 1522 alt examples, EM converged in 11 steps; 16:21:08.549 INFO LearnReadOrientationModel - Context ATG: with 282561 ref and 2106 alt examples, EM converged in 12 steps; 16:21:09.030 INFO LearnReadOrientationModel - Context CAA: with 284942 ref and 1909 alt examples, EM converged in 11 steps; 16:21:09.635 INFO LearnReadOrientationModel - Context CAC: with 351720 ref and 3718 alt examples, EM converged in 12 steps; 16:21:10.322 INFO LearnReadOrientationModel - Context CAG: with 547234 ref and 5940 alt examples, EM converged in 12 steps; 16:21:11.103 INFO LearnReadOrientationModel - Context CCA: with 464164 ref and 8212 alt examples, EM converged in 13 steps; 16:21:11.899 INFO LearnReadOrientationModel - Context CCC: with 512128 ref and 7580 alt examples, EM converged in 13 steps; 16:21:12.719 INFO LearnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3741 alt examples, EM converged in 14 steps; 16:21:14.667 INFO LearnReadOrientationModel - Context CTA: with 173673 ref and 1472 alt examples, EM converged in 13 steps; 16:21:15.238 INFO LearnReadOrientationModel - Context CTC: with 439622 ref and 3855 alt examples, EM converged in 12 steps; 16:21:15.709 INFO LearnReadOrientationModel - Context GAA: with 333082 ref and 2101 alt examples, EM converged in 11 steps; 16:21:16.222 INFO LearnReadOrientationModel - Context GAC: with 227508 ref and 2090 alt examples, EM converged in 12 steps; 16:21:17.015 INFO LearnReadOrientationModel - Context GCA: with 340865 ref and 6",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:8587,Usability,Learn,LearnReadOrientationModel,8587," 180560 ref and 1078 alt examples, EM converged in 12 steps; 16:21:07.964 INFO LearnReadOrientationModel - Context ATC: with 213079 ref and 1522 alt examples, EM converged in 11 steps; 16:21:08.549 INFO LearnReadOrientationModel - Context ATG: with 282561 ref and 2106 alt examples, EM converged in 12 steps; 16:21:09.030 INFO LearnReadOrientationModel - Context CAA: with 284942 ref and 1909 alt examples, EM converged in 11 steps; 16:21:09.635 INFO LearnReadOrientationModel - Context CAC: with 351720 ref and 3718 alt examples, EM converged in 12 steps; 16:21:10.322 INFO LearnReadOrientationModel - Context CAG: with 547234 ref and 5940 alt examples, EM converged in 12 steps; 16:21:11.103 INFO LearnReadOrientationModel - Context CCA: with 464164 ref and 8212 alt examples, EM converged in 13 steps; 16:21:11.899 INFO LearnReadOrientationModel - Context CCC: with 512128 ref and 7580 alt examples, EM converged in 13 steps; 16:21:12.719 INFO LearnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3741 alt examples, EM converged in 14 steps; 16:21:14.667 INFO LearnReadOrientationModel - Context CTA: with 173673 ref and 1472 alt examples, EM converged in 13 steps; 16:21:15.238 INFO LearnReadOrientationModel - Context CTC: with 439622 ref and 3855 alt examples, EM converged in 12 steps; 16:21:15.709 INFO LearnReadOrientationModel - Context GAA: with 333082 ref and 2101 alt examples, EM converged in 11 steps; 16:21:16.222 INFO LearnReadOrientationModel - Context GAC: with 227508 ref and 2090 alt examples, EM converged in 12 steps; 16:21:17.015 INFO LearnReadOrientationModel - Context GCA: with 340865 ref and 6507 alt examples, EM converged in 14 steps; 16:21:17.906 INFO LearnReadOrientationModel - Context GCC: with 404793 ref and 8",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:8711,Usability,Learn,LearnReadOrientationModel,8711,"h 213079 ref and 1522 alt examples, EM converged in 11 steps; 16:21:08.549 INFO LearnReadOrientationModel - Context ATG: with 282561 ref and 2106 alt examples, EM converged in 12 steps; 16:21:09.030 INFO LearnReadOrientationModel - Context CAA: with 284942 ref and 1909 alt examples, EM converged in 11 steps; 16:21:09.635 INFO LearnReadOrientationModel - Context CAC: with 351720 ref and 3718 alt examples, EM converged in 12 steps; 16:21:10.322 INFO LearnReadOrientationModel - Context CAG: with 547234 ref and 5940 alt examples, EM converged in 12 steps; 16:21:11.103 INFO LearnReadOrientationModel - Context CCA: with 464164 ref and 8212 alt examples, EM converged in 13 steps; 16:21:11.899 INFO LearnReadOrientationModel - Context CCC: with 512128 ref and 7580 alt examples, EM converged in 13 steps; 16:21:12.719 INFO LearnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3741 alt examples, EM converged in 14 steps; 16:21:14.667 INFO LearnReadOrientationModel - Context CTA: with 173673 ref and 1472 alt examples, EM converged in 13 steps; 16:21:15.238 INFO LearnReadOrientationModel - Context CTC: with 439622 ref and 3855 alt examples, EM converged in 12 steps; 16:21:15.709 INFO LearnReadOrientationModel - Context GAA: with 333082 ref and 2101 alt examples, EM converged in 11 steps; 16:21:16.222 INFO LearnReadOrientationModel - Context GAC: with 227508 ref and 2090 alt examples, EM converged in 12 steps; 16:21:17.015 INFO LearnReadOrientationModel - Context GCA: with 340865 ref and 6507 alt examples, EM converged in 14 steps; 16:21:17.906 INFO LearnReadOrientationModel - Context GCC: with 404793 ref and 8210 alt examples, EM converged in 14 steps; 16:21:18.677 INFO LearnReadOrientationModel - Context GGA: with 409382 ref and 6",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:8834,Usability,Learn,LearnReadOrientationModel,8834,"h 282561 ref and 2106 alt examples, EM converged in 12 steps; 16:21:09.030 INFO LearnReadOrientationModel - Context CAA: with 284942 ref and 1909 alt examples, EM converged in 11 steps; 16:21:09.635 INFO LearnReadOrientationModel - Context CAC: with 351720 ref and 3718 alt examples, EM converged in 12 steps; 16:21:10.322 INFO LearnReadOrientationModel - Context CAG: with 547234 ref and 5940 alt examples, EM converged in 12 steps; 16:21:11.103 INFO LearnReadOrientationModel - Context CCA: with 464164 ref and 8212 alt examples, EM converged in 13 steps; 16:21:11.899 INFO LearnReadOrientationModel - Context CCC: with 512128 ref and 7580 alt examples, EM converged in 13 steps; 16:21:12.719 INFO LearnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3741 alt examples, EM converged in 14 steps; 16:21:14.667 INFO LearnReadOrientationModel - Context CTA: with 173673 ref and 1472 alt examples, EM converged in 13 steps; 16:21:15.238 INFO LearnReadOrientationModel - Context CTC: with 439622 ref and 3855 alt examples, EM converged in 12 steps; 16:21:15.709 INFO LearnReadOrientationModel - Context GAA: with 333082 ref and 2101 alt examples, EM converged in 11 steps; 16:21:16.222 INFO LearnReadOrientationModel - Context GAC: with 227508 ref and 2090 alt examples, EM converged in 12 steps; 16:21:17.015 INFO LearnReadOrientationModel - Context GCA: with 340865 ref and 6507 alt examples, EM converged in 14 steps; 16:21:17.906 INFO LearnReadOrientationModel - Context GCC: with 404793 ref and 8210 alt examples, EM converged in 14 steps; 16:21:18.677 INFO LearnReadOrientationModel - Context GGA: with 409382 ref and 6784 alt examples, EM converged in 13 steps; 16:21:19.182 INFO LearnReadOrientationModel - Context GTA: with 153202 ref and ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:8958,Usability,Learn,LearnReadOrientationModel,8958,"h 284942 ref and 1909 alt examples, EM converged in 11 steps; 16:21:09.635 INFO LearnReadOrientationModel - Context CAC: with 351720 ref and 3718 alt examples, EM converged in 12 steps; 16:21:10.322 INFO LearnReadOrientationModel - Context CAG: with 547234 ref and 5940 alt examples, EM converged in 12 steps; 16:21:11.103 INFO LearnReadOrientationModel - Context CCA: with 464164 ref and 8212 alt examples, EM converged in 13 steps; 16:21:11.899 INFO LearnReadOrientationModel - Context CCC: with 512128 ref and 7580 alt examples, EM converged in 13 steps; 16:21:12.719 INFO LearnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3741 alt examples, EM converged in 14 steps; 16:21:14.667 INFO LearnReadOrientationModel - Context CTA: with 173673 ref and 1472 alt examples, EM converged in 13 steps; 16:21:15.238 INFO LearnReadOrientationModel - Context CTC: with 439622 ref and 3855 alt examples, EM converged in 12 steps; 16:21:15.709 INFO LearnReadOrientationModel - Context GAA: with 333082 ref and 2101 alt examples, EM converged in 11 steps; 16:21:16.222 INFO LearnReadOrientationModel - Context GAC: with 227508 ref and 2090 alt examples, EM converged in 12 steps; 16:21:17.015 INFO LearnReadOrientationModel - Context GCA: with 340865 ref and 6507 alt examples, EM converged in 14 steps; 16:21:17.906 INFO LearnReadOrientationModel - Context GCC: with 404793 ref and 8210 alt examples, EM converged in 14 steps; 16:21:18.677 INFO LearnReadOrientationModel - Context GGA: with 409382 ref and 6784 alt examples, EM converged in 13 steps; 16:21:19.182 INFO LearnReadOrientationModel - Context GTA: with 153202 ref and 1568 alt examples, EM converged in 12 steps; 16:21:19.680 INFO LearnReadOrientationModel - Context TAA: with 208518 ref and ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:9082,Usability,Learn,LearnReadOrientationModel,9082,"h 351720 ref and 3718 alt examples, EM converged in 12 steps; 16:21:10.322 INFO LearnReadOrientationModel - Context CAG: with 547234 ref and 5940 alt examples, EM converged in 12 steps; 16:21:11.103 INFO LearnReadOrientationModel - Context CCA: with 464164 ref and 8212 alt examples, EM converged in 13 steps; 16:21:11.899 INFO LearnReadOrientationModel - Context CCC: with 512128 ref and 7580 alt examples, EM converged in 13 steps; 16:21:12.719 INFO LearnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3741 alt examples, EM converged in 14 steps; 16:21:14.667 INFO LearnReadOrientationModel - Context CTA: with 173673 ref and 1472 alt examples, EM converged in 13 steps; 16:21:15.238 INFO LearnReadOrientationModel - Context CTC: with 439622 ref and 3855 alt examples, EM converged in 12 steps; 16:21:15.709 INFO LearnReadOrientationModel - Context GAA: with 333082 ref and 2101 alt examples, EM converged in 11 steps; 16:21:16.222 INFO LearnReadOrientationModel - Context GAC: with 227508 ref and 2090 alt examples, EM converged in 12 steps; 16:21:17.015 INFO LearnReadOrientationModel - Context GCA: with 340865 ref and 6507 alt examples, EM converged in 14 steps; 16:21:17.906 INFO LearnReadOrientationModel - Context GCC: with 404793 ref and 8210 alt examples, EM converged in 14 steps; 16:21:18.677 INFO LearnReadOrientationModel - Context GGA: with 409382 ref and 6784 alt examples, EM converged in 13 steps; 16:21:19.182 INFO LearnReadOrientationModel - Context GTA: with 153202 ref and 1568 alt examples, EM converged in 12 steps; 16:21:19.680 INFO LearnReadOrientationModel - Context TAA: with 208518 ref and 1131 alt examples, EM converged in 12 steps; 16:21:20.459 INFO LearnReadOrientationModel - Context TCA: with 325628 ref and ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:9206,Usability,Learn,LearnReadOrientationModel,9206,"h 547234 ref and 5940 alt examples, EM converged in 12 steps; 16:21:11.103 INFO LearnReadOrientationModel - Context CCA: with 464164 ref and 8212 alt examples, EM converged in 13 steps; 16:21:11.899 INFO LearnReadOrientationModel - Context CCC: with 512128 ref and 7580 alt examples, EM converged in 13 steps; 16:21:12.719 INFO LearnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3741 alt examples, EM converged in 14 steps; 16:21:14.667 INFO LearnReadOrientationModel - Context CTA: with 173673 ref and 1472 alt examples, EM converged in 13 steps; 16:21:15.238 INFO LearnReadOrientationModel - Context CTC: with 439622 ref and 3855 alt examples, EM converged in 12 steps; 16:21:15.709 INFO LearnReadOrientationModel - Context GAA: with 333082 ref and 2101 alt examples, EM converged in 11 steps; 16:21:16.222 INFO LearnReadOrientationModel - Context GAC: with 227508 ref and 2090 alt examples, EM converged in 12 steps; 16:21:17.015 INFO LearnReadOrientationModel - Context GCA: with 340865 ref and 6507 alt examples, EM converged in 14 steps; 16:21:17.906 INFO LearnReadOrientationModel - Context GCC: with 404793 ref and 8210 alt examples, EM converged in 14 steps; 16:21:18.677 INFO LearnReadOrientationModel - Context GGA: with 409382 ref and 6784 alt examples, EM converged in 13 steps; 16:21:19.182 INFO LearnReadOrientationModel - Context GTA: with 153202 ref and 1568 alt examples, EM converged in 12 steps; 16:21:19.680 INFO LearnReadOrientationModel - Context TAA: with 208518 ref and 1131 alt examples, EM converged in 12 steps; 16:21:20.459 INFO LearnReadOrientationModel - Context TCA: with 325628 ref and 6437 alt examples, EM converged in 13 steps; 16:21:21.008 INFO LearnReadOrientationModel - Context AAA: with 426193 ref and ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:9330,Usability,Learn,LearnReadOrientationModel,9330,"h 464164 ref and 8212 alt examples, EM converged in 13 steps; 16:21:11.899 INFO LearnReadOrientationModel - Context CCC: with 512128 ref and 7580 alt examples, EM converged in 13 steps; 16:21:12.719 INFO LearnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3741 alt examples, EM converged in 14 steps; 16:21:14.667 INFO LearnReadOrientationModel - Context CTA: with 173673 ref and 1472 alt examples, EM converged in 13 steps; 16:21:15.238 INFO LearnReadOrientationModel - Context CTC: with 439622 ref and 3855 alt examples, EM converged in 12 steps; 16:21:15.709 INFO LearnReadOrientationModel - Context GAA: with 333082 ref and 2101 alt examples, EM converged in 11 steps; 16:21:16.222 INFO LearnReadOrientationModel - Context GAC: with 227508 ref and 2090 alt examples, EM converged in 12 steps; 16:21:17.015 INFO LearnReadOrientationModel - Context GCA: with 340865 ref and 6507 alt examples, EM converged in 14 steps; 16:21:17.906 INFO LearnReadOrientationModel - Context GCC: with 404793 ref and 8210 alt examples, EM converged in 14 steps; 16:21:18.677 INFO LearnReadOrientationModel - Context GGA: with 409382 ref and 6784 alt examples, EM converged in 13 steps; 16:21:19.182 INFO LearnReadOrientationModel - Context GTA: with 153202 ref and 1568 alt examples, EM converged in 12 steps; 16:21:19.680 INFO LearnReadOrientationModel - Context TAA: with 208518 ref and 1131 alt examples, EM converged in 12 steps; 16:21:20.459 INFO LearnReadOrientationModel - Context TCA: with 325628 ref and 6437 alt examples, EM converged in 13 steps; 16:21:21.008 INFO LearnReadOrientationModel - Context AAA: with 426193 ref and 2055 alt examples, EM converged in 12 steps; 16:21:21.044 INFO LearnReadOrientationModel - Shutting down engine; [November 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:9454,Usability,Learn,LearnReadOrientationModel,9454,"h 512128 ref and 7580 alt examples, EM converged in 13 steps; 16:21:12.719 INFO LearnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3741 alt examples, EM converged in 14 steps; 16:21:14.667 INFO LearnReadOrientationModel - Context CTA: with 173673 ref and 1472 alt examples, EM converged in 13 steps; 16:21:15.238 INFO LearnReadOrientationModel - Context CTC: with 439622 ref and 3855 alt examples, EM converged in 12 steps; 16:21:15.709 INFO LearnReadOrientationModel - Context GAA: with 333082 ref and 2101 alt examples, EM converged in 11 steps; 16:21:16.222 INFO LearnReadOrientationModel - Context GAC: with 227508 ref and 2090 alt examples, EM converged in 12 steps; 16:21:17.015 INFO LearnReadOrientationModel - Context GCA: with 340865 ref and 6507 alt examples, EM converged in 14 steps; 16:21:17.906 INFO LearnReadOrientationModel - Context GCC: with 404793 ref and 8210 alt examples, EM converged in 14 steps; 16:21:18.677 INFO LearnReadOrientationModel - Context GGA: with 409382 ref and 6784 alt examples, EM converged in 13 steps; 16:21:19.182 INFO LearnReadOrientationModel - Context GTA: with 153202 ref and 1568 alt examples, EM converged in 12 steps; 16:21:19.680 INFO LearnReadOrientationModel - Context TAA: with 208518 ref and 1131 alt examples, EM converged in 12 steps; 16:21:20.459 INFO LearnReadOrientationModel - Context TCA: with 325628 ref and 6437 alt examples, EM converged in 13 steps; 16:21:21.008 INFO LearnReadOrientationModel - Context AAA: with 426193 ref and 2055 alt examples, EM converged in 12 steps; 16:21:21.044 INFO LearnReadOrientationModel - Shutting down engine; [November 26, 2018 4:21:21 PM UTC] org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel done. Elapsed ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:9578,Usability,Learn,LearnReadOrientationModel,9578,"arnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3741 alt examples, EM converged in 14 steps; 16:21:14.667 INFO LearnReadOrientationModel - Context CTA: with 173673 ref and 1472 alt examples, EM converged in 13 steps; 16:21:15.238 INFO LearnReadOrientationModel - Context CTC: with 439622 ref and 3855 alt examples, EM converged in 12 steps; 16:21:15.709 INFO LearnReadOrientationModel - Context GAA: with 333082 ref and 2101 alt examples, EM converged in 11 steps; 16:21:16.222 INFO LearnReadOrientationModel - Context GAC: with 227508 ref and 2090 alt examples, EM converged in 12 steps; 16:21:17.015 INFO LearnReadOrientationModel - Context GCA: with 340865 ref and 6507 alt examples, EM converged in 14 steps; 16:21:17.906 INFO LearnReadOrientationModel - Context GCC: with 404793 ref and 8210 alt examples, EM converged in 14 steps; 16:21:18.677 INFO LearnReadOrientationModel - Context GGA: with 409382 ref and 6784 alt examples, EM converged in 13 steps; 16:21:19.182 INFO LearnReadOrientationModel - Context GTA: with 153202 ref and 1568 alt examples, EM converged in 12 steps; 16:21:19.680 INFO LearnReadOrientationModel - Context TAA: with 208518 ref and 1131 alt examples, EM converged in 12 steps; 16:21:20.459 INFO LearnReadOrientationModel - Context TCA: with 325628 ref and 6437 alt examples, EM converged in 13 steps; 16:21:21.008 INFO LearnReadOrientationModel - Context AAA: with 426193 ref and 2055 alt examples, EM converged in 12 steps; 16:21:21.044 INFO LearnReadOrientationModel - Shutting down engine; [November 26, 2018 4:21:21 PM UTC] org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel done. Elapsed time: 0.39 minutes.; Runtime.totalMemory()=780140544; Tool returned:; SUCCESS; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:9702,Usability,Learn,LearnReadOrientationModel,9702,"arnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3741 alt examples, EM converged in 14 steps; 16:21:14.667 INFO LearnReadOrientationModel - Context CTA: with 173673 ref and 1472 alt examples, EM converged in 13 steps; 16:21:15.238 INFO LearnReadOrientationModel - Context CTC: with 439622 ref and 3855 alt examples, EM converged in 12 steps; 16:21:15.709 INFO LearnReadOrientationModel - Context GAA: with 333082 ref and 2101 alt examples, EM converged in 11 steps; 16:21:16.222 INFO LearnReadOrientationModel - Context GAC: with 227508 ref and 2090 alt examples, EM converged in 12 steps; 16:21:17.015 INFO LearnReadOrientationModel - Context GCA: with 340865 ref and 6507 alt examples, EM converged in 14 steps; 16:21:17.906 INFO LearnReadOrientationModel - Context GCC: with 404793 ref and 8210 alt examples, EM converged in 14 steps; 16:21:18.677 INFO LearnReadOrientationModel - Context GGA: with 409382 ref and 6784 alt examples, EM converged in 13 steps; 16:21:19.182 INFO LearnReadOrientationModel - Context GTA: with 153202 ref and 1568 alt examples, EM converged in 12 steps; 16:21:19.680 INFO LearnReadOrientationModel - Context TAA: with 208518 ref and 1131 alt examples, EM converged in 12 steps; 16:21:20.459 INFO LearnReadOrientationModel - Context TCA: with 325628 ref and 6437 alt examples, EM converged in 13 steps; 16:21:21.008 INFO LearnReadOrientationModel - Context AAA: with 426193 ref and 2055 alt examples, EM converged in 12 steps; 16:21:21.044 INFO LearnReadOrientationModel - Shutting down engine; [November 26, 2018 4:21:21 PM UTC] org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel done. Elapsed time: 0.39 minutes.; Runtime.totalMemory()=780140544; Tool returned:; SUCCESS; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:9826,Usability,Learn,LearnReadOrientationModel,9826,"arnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3741 alt examples, EM converged in 14 steps; 16:21:14.667 INFO LearnReadOrientationModel - Context CTA: with 173673 ref and 1472 alt examples, EM converged in 13 steps; 16:21:15.238 INFO LearnReadOrientationModel - Context CTC: with 439622 ref and 3855 alt examples, EM converged in 12 steps; 16:21:15.709 INFO LearnReadOrientationModel - Context GAA: with 333082 ref and 2101 alt examples, EM converged in 11 steps; 16:21:16.222 INFO LearnReadOrientationModel - Context GAC: with 227508 ref and 2090 alt examples, EM converged in 12 steps; 16:21:17.015 INFO LearnReadOrientationModel - Context GCA: with 340865 ref and 6507 alt examples, EM converged in 14 steps; 16:21:17.906 INFO LearnReadOrientationModel - Context GCC: with 404793 ref and 8210 alt examples, EM converged in 14 steps; 16:21:18.677 INFO LearnReadOrientationModel - Context GGA: with 409382 ref and 6784 alt examples, EM converged in 13 steps; 16:21:19.182 INFO LearnReadOrientationModel - Context GTA: with 153202 ref and 1568 alt examples, EM converged in 12 steps; 16:21:19.680 INFO LearnReadOrientationModel - Context TAA: with 208518 ref and 1131 alt examples, EM converged in 12 steps; 16:21:20.459 INFO LearnReadOrientationModel - Context TCA: with 325628 ref and 6437 alt examples, EM converged in 13 steps; 16:21:21.008 INFO LearnReadOrientationModel - Context AAA: with 426193 ref and 2055 alt examples, EM converged in 12 steps; 16:21:21.044 INFO LearnReadOrientationModel - Shutting down engine; [November 26, 2018 4:21:21 PM UTC] org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel done. Elapsed time: 0.39 minutes.; Runtime.totalMemory()=780140544; Tool returned:; SUCCESS; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:9950,Usability,Learn,LearnReadOrientationModel,9950,"arnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3741 alt examples, EM converged in 14 steps; 16:21:14.667 INFO LearnReadOrientationModel - Context CTA: with 173673 ref and 1472 alt examples, EM converged in 13 steps; 16:21:15.238 INFO LearnReadOrientationModel - Context CTC: with 439622 ref and 3855 alt examples, EM converged in 12 steps; 16:21:15.709 INFO LearnReadOrientationModel - Context GAA: with 333082 ref and 2101 alt examples, EM converged in 11 steps; 16:21:16.222 INFO LearnReadOrientationModel - Context GAC: with 227508 ref and 2090 alt examples, EM converged in 12 steps; 16:21:17.015 INFO LearnReadOrientationModel - Context GCA: with 340865 ref and 6507 alt examples, EM converged in 14 steps; 16:21:17.906 INFO LearnReadOrientationModel - Context GCC: with 404793 ref and 8210 alt examples, EM converged in 14 steps; 16:21:18.677 INFO LearnReadOrientationModel - Context GGA: with 409382 ref and 6784 alt examples, EM converged in 13 steps; 16:21:19.182 INFO LearnReadOrientationModel - Context GTA: with 153202 ref and 1568 alt examples, EM converged in 12 steps; 16:21:19.680 INFO LearnReadOrientationModel - Context TAA: with 208518 ref and 1131 alt examples, EM converged in 12 steps; 16:21:20.459 INFO LearnReadOrientationModel - Context TCA: with 325628 ref and 6437 alt examples, EM converged in 13 steps; 16:21:21.008 INFO LearnReadOrientationModel - Context AAA: with 426193 ref and 2055 alt examples, EM converged in 12 steps; 16:21:21.044 INFO LearnReadOrientationModel - Shutting down engine; [November 26, 2018 4:21:21 PM UTC] org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel done. Elapsed time: 0.39 minutes.; Runtime.totalMemory()=780140544; Tool returned:; SUCCESS; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:10074,Usability,Learn,LearnReadOrientationModel,10074,"arnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3741 alt examples, EM converged in 14 steps; 16:21:14.667 INFO LearnReadOrientationModel - Context CTA: with 173673 ref and 1472 alt examples, EM converged in 13 steps; 16:21:15.238 INFO LearnReadOrientationModel - Context CTC: with 439622 ref and 3855 alt examples, EM converged in 12 steps; 16:21:15.709 INFO LearnReadOrientationModel - Context GAA: with 333082 ref and 2101 alt examples, EM converged in 11 steps; 16:21:16.222 INFO LearnReadOrientationModel - Context GAC: with 227508 ref and 2090 alt examples, EM converged in 12 steps; 16:21:17.015 INFO LearnReadOrientationModel - Context GCA: with 340865 ref and 6507 alt examples, EM converged in 14 steps; 16:21:17.906 INFO LearnReadOrientationModel - Context GCC: with 404793 ref and 8210 alt examples, EM converged in 14 steps; 16:21:18.677 INFO LearnReadOrientationModel - Context GGA: with 409382 ref and 6784 alt examples, EM converged in 13 steps; 16:21:19.182 INFO LearnReadOrientationModel - Context GTA: with 153202 ref and 1568 alt examples, EM converged in 12 steps; 16:21:19.680 INFO LearnReadOrientationModel - Context TAA: with 208518 ref and 1131 alt examples, EM converged in 12 steps; 16:21:20.459 INFO LearnReadOrientationModel - Context TCA: with 325628 ref and 6437 alt examples, EM converged in 13 steps; 16:21:21.008 INFO LearnReadOrientationModel - Context AAA: with 426193 ref and 2055 alt examples, EM converged in 12 steps; 16:21:21.044 INFO LearnReadOrientationModel - Shutting down engine; [November 26, 2018 4:21:21 PM UTC] org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel done. Elapsed time: 0.39 minutes.; Runtime.totalMemory()=780140544; Tool returned:; SUCCESS; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:10198,Usability,Learn,LearnReadOrientationModel,10198,"arnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3741 alt examples, EM converged in 14 steps; 16:21:14.667 INFO LearnReadOrientationModel - Context CTA: with 173673 ref and 1472 alt examples, EM converged in 13 steps; 16:21:15.238 INFO LearnReadOrientationModel - Context CTC: with 439622 ref and 3855 alt examples, EM converged in 12 steps; 16:21:15.709 INFO LearnReadOrientationModel - Context GAA: with 333082 ref and 2101 alt examples, EM converged in 11 steps; 16:21:16.222 INFO LearnReadOrientationModel - Context GAC: with 227508 ref and 2090 alt examples, EM converged in 12 steps; 16:21:17.015 INFO LearnReadOrientationModel - Context GCA: with 340865 ref and 6507 alt examples, EM converged in 14 steps; 16:21:17.906 INFO LearnReadOrientationModel - Context GCC: with 404793 ref and 8210 alt examples, EM converged in 14 steps; 16:21:18.677 INFO LearnReadOrientationModel - Context GGA: with 409382 ref and 6784 alt examples, EM converged in 13 steps; 16:21:19.182 INFO LearnReadOrientationModel - Context GTA: with 153202 ref and 1568 alt examples, EM converged in 12 steps; 16:21:19.680 INFO LearnReadOrientationModel - Context TAA: with 208518 ref and 1131 alt examples, EM converged in 12 steps; 16:21:20.459 INFO LearnReadOrientationModel - Context TCA: with 325628 ref and 6437 alt examples, EM converged in 13 steps; 16:21:21.008 INFO LearnReadOrientationModel - Context AAA: with 426193 ref and 2055 alt examples, EM converged in 12 steps; 16:21:21.044 INFO LearnReadOrientationModel - Shutting down engine; [November 26, 2018 4:21:21 PM UTC] org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel done. Elapsed time: 0.39 minutes.; Runtime.totalMemory()=780140544; Tool returned:; SUCCESS; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:10322,Usability,Learn,LearnReadOrientationModel,10322,"arnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3741 alt examples, EM converged in 14 steps; 16:21:14.667 INFO LearnReadOrientationModel - Context CTA: with 173673 ref and 1472 alt examples, EM converged in 13 steps; 16:21:15.238 INFO LearnReadOrientationModel - Context CTC: with 439622 ref and 3855 alt examples, EM converged in 12 steps; 16:21:15.709 INFO LearnReadOrientationModel - Context GAA: with 333082 ref and 2101 alt examples, EM converged in 11 steps; 16:21:16.222 INFO LearnReadOrientationModel - Context GAC: with 227508 ref and 2090 alt examples, EM converged in 12 steps; 16:21:17.015 INFO LearnReadOrientationModel - Context GCA: with 340865 ref and 6507 alt examples, EM converged in 14 steps; 16:21:17.906 INFO LearnReadOrientationModel - Context GCC: with 404793 ref and 8210 alt examples, EM converged in 14 steps; 16:21:18.677 INFO LearnReadOrientationModel - Context GGA: with 409382 ref and 6784 alt examples, EM converged in 13 steps; 16:21:19.182 INFO LearnReadOrientationModel - Context GTA: with 153202 ref and 1568 alt examples, EM converged in 12 steps; 16:21:19.680 INFO LearnReadOrientationModel - Context TAA: with 208518 ref and 1131 alt examples, EM converged in 12 steps; 16:21:20.459 INFO LearnReadOrientationModel - Context TCA: with 325628 ref and 6437 alt examples, EM converged in 13 steps; 16:21:21.008 INFO LearnReadOrientationModel - Context AAA: with 426193 ref and 2055 alt examples, EM converged in 12 steps; 16:21:21.044 INFO LearnReadOrientationModel - Shutting down engine; [November 26, 2018 4:21:21 PM UTC] org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel done. Elapsed time: 0.39 minutes.; Runtime.totalMemory()=780140544; Tool returned:; SUCCESS; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/issues/5447:10467,Usability,Learn,LearnReadOrientationModel,10467,"arnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3741 alt examples, EM converged in 14 steps; 16:21:14.667 INFO LearnReadOrientationModel - Context CTA: with 173673 ref and 1472 alt examples, EM converged in 13 steps; 16:21:15.238 INFO LearnReadOrientationModel - Context CTC: with 439622 ref and 3855 alt examples, EM converged in 12 steps; 16:21:15.709 INFO LearnReadOrientationModel - Context GAA: with 333082 ref and 2101 alt examples, EM converged in 11 steps; 16:21:16.222 INFO LearnReadOrientationModel - Context GAC: with 227508 ref and 2090 alt examples, EM converged in 12 steps; 16:21:17.015 INFO LearnReadOrientationModel - Context GCA: with 340865 ref and 6507 alt examples, EM converged in 14 steps; 16:21:17.906 INFO LearnReadOrientationModel - Context GCC: with 404793 ref and 8210 alt examples, EM converged in 14 steps; 16:21:18.677 INFO LearnReadOrientationModel - Context GGA: with 409382 ref and 6784 alt examples, EM converged in 13 steps; 16:21:19.182 INFO LearnReadOrientationModel - Context GTA: with 153202 ref and 1568 alt examples, EM converged in 12 steps; 16:21:19.680 INFO LearnReadOrientationModel - Context TAA: with 208518 ref and 1131 alt examples, EM converged in 12 steps; 16:21:20.459 INFO LearnReadOrientationModel - Context TCA: with 325628 ref and 6437 alt examples, EM converged in 13 steps; 16:21:21.008 INFO LearnReadOrientationModel - Context AAA: with 426193 ref and 2055 alt examples, EM converged in 12 steps; 16:21:21.044 INFO LearnReadOrientationModel - Shutting down engine; [November 26, 2018 4:21:21 PM UTC] org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel done. Elapsed time: 0.39 minutes.; Runtime.totalMemory()=780140544; Tool returned:; SUCCESS; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447
https://github.com/broadinstitute/gatk/pull/5448:85,Availability,down,downsampling,85,"I suspect this may be a somewhat controversial change. One of the issues we face for downsampling in spark is that we need to be able to reproduce the same downsampling behavior at a given site across different partitions/environments. To this end I have implemented the ability to reset the random generator used in the ReservoirDownsampler based on the start position of the next reservoir of reads and the gatk default random seed. I'm interested to know what peoples thoughts are on this change, as theoretically it will make the gatk downsampling the same for each start position in the genome. . Fixes #5437",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5448
https://github.com/broadinstitute/gatk/pull/5448:156,Availability,down,downsampling,156,"I suspect this may be a somewhat controversial change. One of the issues we face for downsampling in spark is that we need to be able to reproduce the same downsampling behavior at a given site across different partitions/environments. To this end I have implemented the ability to reset the random generator used in the ReservoirDownsampler based on the start position of the next reservoir of reads and the gatk default random seed. I'm interested to know what peoples thoughts are on this change, as theoretically it will make the gatk downsampling the same for each start position in the genome. . Fixes #5437",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5448
https://github.com/broadinstitute/gatk/pull/5448:539,Availability,down,downsampling,539,"I suspect this may be a somewhat controversial change. One of the issues we face for downsampling in spark is that we need to be able to reproduce the same downsampling behavior at a given site across different partitions/environments. To this end I have implemented the ability to reset the random generator used in the ReservoirDownsampler based on the start position of the next reservoir of reads and the gatk default random seed. I'm interested to know what peoples thoughts are on this change, as theoretically it will make the gatk downsampling the same for each start position in the genome. . Fixes #5437",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5448
https://github.com/broadinstitute/gatk/issues/5449:2049,Energy Efficiency,power,power,2049,"> . . DP=2 GT:AD:DP:GQ:MIN_DP:PGT:PID:PL:SB ./.:.:0:0:0:.:.:0,0,0,0,0,0 ./.:0,2,0:2:6:.:0|1:65074846_T_C:90,6,0,90,6,90:0,0,2,0 ./.:.:1:0:0:.:.:0,0,0,0,0,0; chr7 65074876 . G *,<NON_REF> . . DP=2 GT:AD:DP:GQ:MIN_DP:PGT:PID:PL:SB ./.:.:0:0:0:.:.:0,0,0,0,0,0 ./.:0,2,0:2:6:.:0|1:65074846_T_C:90,6,0,90,6,90:0,0,2,0 ./.:.:1:0:0:.:.:0,0,0,0,0,0; chr7 65074877 . T *,<NON_REF> . . DP=2 GT:AD:DP:GQ:MIN_DP:PGT:PID:PL:SB ./.:.:0:0:0:.:.:0,0,0,0,0,0 ./.:0,2,0:2:6:.:0|1:65074846_T_C:90,6,0,90,6,90:0,0,2,0 ./.:.:1:0:0:.:.:0,0,0,0,0,0; chr7 65074878 . G <NON_REF> . . END=65074923 GT:DP:GQ:MIN_DP:PL ./.:0:0:0:0,0,0 ./.:2:0:0:0,0,0 ./.:1:0:0:0,0,0 ./.:2:0:0:0,0,0 ./.:0:0:0:0,0,0 ./.:0:0:0:0,0,0 ./.:0:0:0:0,0,0 ./.:1:0:0:0,0,; ```; Where the second genotype column with the `65074846_T_C` tag is for NA12891, which is the sample that has the deletion. I suspect that the GATK engine logic is smart enough to look upstream since those genotyped get annotations. This is admittedly sort of an ambiguous case, but GDB certainly should be able to power through. The stack trace looks like this:; ```; Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code); C [libtiledbgenomicsdb4636568691140868757.dylib+0x1a2cf] BroadCombinedGVCFOperator::handle_deletions(Variant&, VariantQueryConfig const&)+0xa7f; C [libtiledbgenomicsdb4636568691140868757.dylib+0x18f12] BroadCombinedGVCFOperator::operate(Variant&, VariantQueryConfig const&)+0x22; C [libtiledbgenomicsdb4636568691140868757.dylib+0x59d98] VariantQueryProcessor::handle_gvcf_ranges(std::__1::priority_queue<VariantCall*, std::__1::vector<VariantCall*, std::__1::allocator<VariantCall*> >, EndCmpVariantCallStruct>&, VariantQueryConfig const&, Variant&, SingleVariantOperatorBase&, long long&, long long, bool, unsigned long long&, GTProfileStats*) const+0xa8; C [libtiledbgenomicsdb4636568691140868757.dylib+0x5a8e2] VariantQueryProcessor::scan_handle_cell(VariantQueryConfig const&, unsigned int, Variant&, SingleVariantOperatorBase&",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5449
https://github.com/broadinstitute/gatk/issues/5449:5596,Performance,load,loadNextFeature,5596,"dbgenomicsdb4636568691140868757.dylib+0xbb2ad] GenomicsDBBCFGenerator::GenomicsDBBCFGenerator(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, char const*, int, int, int, unsigned long, unsigned long, char const*, bool, bool, bool)+0x3d; C [libtiledbgenomicsdb4636568691140868757.dylib+0xfad71] Java_com_intel_genomicsdb_reader_GenomicsDBQueryStream_jniGenomicsDBInit+0x171; j com.intel.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;IIIJJZZZZ)J+0; j com.intel.genomicsdb.reader.GenomicsDBQueryStream.<init>(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;IIIJJZZZZ)V+32; j com.intel.genomicsdb.reader.GenomicsDBQueryStream.<init>(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;IIIJJZZ)V+20; j com.intel.genomicsdb.reader.GenomicsDBQueryStream.<init>(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;IIZ)V+18; j com.intel.genomicsdb.reader.GenomicsDBFeatureIterator.setNextSourceAsCurrent()V+96; j com.intel.genomicsdb.reader.GenomicsDBFeatureIterator.<init>(Ljava/lang/String;Ljava/util/List;Lhtsjdk/tribble/FeatureCodecHeader;Lhtsjdk/tribble/FeatureCodec;Ljava/lang/String;Ljava/util/OptionalInt;Ljava/util/OptionalInt;Ljava/util/Optional;)V+122; j com.intel.genomicsdb.reader.GenomicsDBFeatureReader.query(Ljava/lang/String;II)Lhtsjdk/tribble/CloseableTribbleIterator;+67; j org.broadinstitute.hellbender.engine.FeatureIntervalIterator.queryNextInterval()Z+95; j org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextFeature()Lhtsjdk/tribble/Feature;+20; j org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextNovelFeature()Lhtsjdk/tribble/Feature;+1; j org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next()Lhtsjdk/tribble/Feature;+24; j org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next()Ljava/lang/Object;+1; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5449
https://github.com/broadinstitute/gatk/issues/5449:5705,Performance,load,loadNextNovelFeature,5705,"dbgenomicsdb4636568691140868757.dylib+0xbb2ad] GenomicsDBBCFGenerator::GenomicsDBBCFGenerator(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, char const*, int, int, int, unsigned long, unsigned long, char const*, bool, bool, bool)+0x3d; C [libtiledbgenomicsdb4636568691140868757.dylib+0xfad71] Java_com_intel_genomicsdb_reader_GenomicsDBQueryStream_jniGenomicsDBInit+0x171; j com.intel.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;IIIJJZZZZ)J+0; j com.intel.genomicsdb.reader.GenomicsDBQueryStream.<init>(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;IIIJJZZZZ)V+32; j com.intel.genomicsdb.reader.GenomicsDBQueryStream.<init>(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;IIIJJZZ)V+20; j com.intel.genomicsdb.reader.GenomicsDBQueryStream.<init>(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;IIZ)V+18; j com.intel.genomicsdb.reader.GenomicsDBFeatureIterator.setNextSourceAsCurrent()V+96; j com.intel.genomicsdb.reader.GenomicsDBFeatureIterator.<init>(Ljava/lang/String;Ljava/util/List;Lhtsjdk/tribble/FeatureCodecHeader;Lhtsjdk/tribble/FeatureCodec;Ljava/lang/String;Ljava/util/OptionalInt;Ljava/util/OptionalInt;Ljava/util/Optional;)V+122; j com.intel.genomicsdb.reader.GenomicsDBFeatureReader.query(Ljava/lang/String;II)Lhtsjdk/tribble/CloseableTribbleIterator;+67; j org.broadinstitute.hellbender.engine.FeatureIntervalIterator.queryNextInterval()Z+95; j org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextFeature()Lhtsjdk/tribble/Feature;+20; j org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextNovelFeature()Lhtsjdk/tribble/Feature;+1; j org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next()Lhtsjdk/tribble/Feature;+24; j org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next()Ljava/lang/Object;+1; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5449
https://github.com/broadinstitute/gatk/issues/5449:1889,Testability,log,logic,1889,"2:6:.:0|1:65074846_T_C:90,6,0,90,6,90:0,0,2,0 ./.:.:1:0:0:.:.:0,0,0,0,0,0; chr7 65074875 . C *,<NON_REF> . . DP=2 GT:AD:DP:GQ:MIN_DP:PGT:PID:PL:SB ./.:.:0:0:0:.:.:0,0,0,0,0,0 ./.:0,2,0:2:6:.:0|1:65074846_T_C:90,6,0,90,6,90:0,0,2,0 ./.:.:1:0:0:.:.:0,0,0,0,0,0; chr7 65074876 . G *,<NON_REF> . . DP=2 GT:AD:DP:GQ:MIN_DP:PGT:PID:PL:SB ./.:.:0:0:0:.:.:0,0,0,0,0,0 ./.:0,2,0:2:6:.:0|1:65074846_T_C:90,6,0,90,6,90:0,0,2,0 ./.:.:1:0:0:.:.:0,0,0,0,0,0; chr7 65074877 . T *,<NON_REF> . . DP=2 GT:AD:DP:GQ:MIN_DP:PGT:PID:PL:SB ./.:.:0:0:0:.:.:0,0,0,0,0,0 ./.:0,2,0:2:6:.:0|1:65074846_T_C:90,6,0,90,6,90:0,0,2,0 ./.:.:1:0:0:.:.:0,0,0,0,0,0; chr7 65074878 . G <NON_REF> . . END=65074923 GT:DP:GQ:MIN_DP:PL ./.:0:0:0:0,0,0 ./.:2:0:0:0,0,0 ./.:1:0:0:0,0,0 ./.:2:0:0:0,0,0 ./.:0:0:0:0,0,0 ./.:0:0:0:0,0,0 ./.:0:0:0:0,0,0 ./.:1:0:0:0,0,; ```; Where the second genotype column with the `65074846_T_C` tag is for NA12891, which is the sample that has the deletion. I suspect that the GATK engine logic is smart enough to look upstream since those genotyped get annotations. This is admittedly sort of an ambiguous case, but GDB certainly should be able to power through. The stack trace looks like this:; ```; Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code); C [libtiledbgenomicsdb4636568691140868757.dylib+0x1a2cf] BroadCombinedGVCFOperator::handle_deletions(Variant&, VariantQueryConfig const&)+0xa7f; C [libtiledbgenomicsdb4636568691140868757.dylib+0x18f12] BroadCombinedGVCFOperator::operate(Variant&, VariantQueryConfig const&)+0x22; C [libtiledbgenomicsdb4636568691140868757.dylib+0x59d98] VariantQueryProcessor::handle_gvcf_ranges(std::__1::priority_queue<VariantCall*, std::__1::vector<VariantCall*, std::__1::allocator<VariantCall*> >, EndCmpVariantCallStruct>&, VariantQueryConfig const&, Variant&, SingleVariantOperatorBase&, long long&, long long, bool, unsigned long long&, GTProfileStats*) const+0xa8; C [libtiledbgenomicsdb4636568691140868757.dylib+0x5a8e2] VariantQueryPr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5449
https://github.com/broadinstitute/gatk/pull/5451:126,Integrability,Depend,Depends,126,"This was a quick job, I think more than anything it highlighted that we are missing many features in spark as of right now. . Depends On #5416",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5451
https://github.com/broadinstitute/gatk/pull/5452:88,Modifiability,variab,variable,88,Resolved the issue of adding gs:// to the beginning and / to the end of the environment variable GATK_GCS_STAGING. Tested it locally.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5452
https://github.com/broadinstitute/gatk/pull/5452:115,Testability,Test,Tested,115,Resolved the issue of adding gs:// to the beginning and / to the end of the environment variable GATK_GCS_STAGING. Tested it locally.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5452
https://github.com/broadinstitute/gatk/pull/5455:10,Modifiability,variab,variable,10,"The debug variable of AsseemblyResultSet wasn't set anywhere, and therefore the command line argument debug didn't propagate to the function buildEventMapsForHaplotypes in EventMap.java. I added the function setDebug to AssemblyResultSet, and set its value in HaplotypeCallerEngine.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5455
https://github.com/broadinstitute/gatk/issues/5456:154,Modifiability,config,config,154,"With gnomAD support, we will need to be able to match an HG19 interval against the B37 data source. This can be done by adding a field to the data source config files `dataAreB37` (or similar). Then in the code, `DataSourceFuncotationFactory` sets that to an internal variable based on the config file and uses that setting to call either `getValues(mainSourceFileAsFeatureInput)` or `getValues(mainSourceFileAsFeatureInput, equivalentB37SimpleInterval)` where `equivalentHg19SimpleInterval` is the Hg19 equivalent interval.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5456
https://github.com/broadinstitute/gatk/issues/5456:268,Modifiability,variab,variable,268,"With gnomAD support, we will need to be able to match an HG19 interval against the B37 data source. This can be done by adding a field to the data source config files `dataAreB37` (or similar). Then in the code, `DataSourceFuncotationFactory` sets that to an internal variable based on the config file and uses that setting to call either `getValues(mainSourceFileAsFeatureInput)` or `getValues(mainSourceFileAsFeatureInput, equivalentB37SimpleInterval)` where `equivalentHg19SimpleInterval` is the Hg19 equivalent interval.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5456
https://github.com/broadinstitute/gatk/issues/5456:290,Modifiability,config,config,290,"With gnomAD support, we will need to be able to match an HG19 interval against the B37 data source. This can be done by adding a field to the data source config files `dataAreB37` (or similar). Then in the code, `DataSourceFuncotationFactory` sets that to an internal variable based on the config file and uses that setting to call either `getValues(mainSourceFileAsFeatureInput)` or `getValues(mainSourceFileAsFeatureInput, equivalentB37SimpleInterval)` where `equivalentHg19SimpleInterval` is the Hg19 equivalent interval.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5456
https://github.com/broadinstitute/gatk/issues/5458:212,Availability,error,error,212,"`PrintWriter` explicitly doesn't throw IOExceptions when it fails to write. This could cause silent corruption of output files. We should probably not use it because it's hard to remember to correctly check it's error status. Some places that use it: MafOutputRenderer, MetricsUtils, others...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5458
https://github.com/broadinstitute/gatk/issues/5459:95,Performance,optimiz,optimization,95,"In `CigarUtils.calculateCigar(final byte[] refSeq, final byte[] altSeq)` we have the following optimization to check for exact equality in order to avoid expensive Smith-Waterman alignment:. ```; if (Arrays.equals(refSeq, altSeq)){; return eg 101M; }; ```. We could similarly optimize a broader class of alt haplotypes by ruling out indels and returning M-only Cigars. To do this, line up the two sequences and see if there are a small number of non-contiguous mismatches. If so, each mismatch is a substitution. This optimization would shave about 5% off the runtime of both Mutect2 and HaplotypeCaller.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5459
https://github.com/broadinstitute/gatk/issues/5459:276,Performance,optimiz,optimize,276,"In `CigarUtils.calculateCigar(final byte[] refSeq, final byte[] altSeq)` we have the following optimization to check for exact equality in order to avoid expensive Smith-Waterman alignment:. ```; if (Arrays.equals(refSeq, altSeq)){; return eg 101M; }; ```. We could similarly optimize a broader class of alt haplotypes by ruling out indels and returning M-only Cigars. To do this, line up the two sequences and see if there are a small number of non-contiguous mismatches. If so, each mismatch is a substitution. This optimization would shave about 5% off the runtime of both Mutect2 and HaplotypeCaller.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5459
https://github.com/broadinstitute/gatk/issues/5459:518,Performance,optimiz,optimization,518,"In `CigarUtils.calculateCigar(final byte[] refSeq, final byte[] altSeq)` we have the following optimization to check for exact equality in order to avoid expensive Smith-Waterman alignment:. ```; if (Arrays.equals(refSeq, altSeq)){; return eg 101M; }; ```. We could similarly optimize a broader class of alt haplotypes by ruling out indels and returning M-only Cigars. To do this, line up the two sequences and see if there are a small number of non-contiguous mismatches. If so, each mismatch is a substitution. This optimization would shave about 5% off the runtime of both Mutect2 and HaplotypeCaller.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5459
https://github.com/broadinstitute/gatk/issues/5459:148,Safety,avoid,avoid,148,"In `CigarUtils.calculateCigar(final byte[] refSeq, final byte[] altSeq)` we have the following optimization to check for exact equality in order to avoid expensive Smith-Waterman alignment:. ```; if (Arrays.equals(refSeq, altSeq)){; return eg 101M; }; ```. We could similarly optimize a broader class of alt haplotypes by ruling out indels and returning M-only Cigars. To do this, line up the two sequences and see if there are a small number of non-contiguous mismatches. If so, each mismatch is a substitution. This optimization would shave about 5% off the runtime of both Mutect2 and HaplotypeCaller.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5459
https://github.com/broadinstitute/gatk/pull/5460:212,Safety,avoid,avoiding,212,"Closes #4833. This doesn't change the model, just the numerical implementation. It addresses some of the issues holding up #4614. From the discussion there, here is the essence of this PR:. >The general rule for avoiding finite precision problems with the qual score is: always calculate probabilities of alleles being absent. The problem with working in terms of alleles being present is that for very good GQs this probability is so close to 1 that quals can become infinite. . In this PR we add up the probabilities of genotypes that don't have the allele, which is small but non-zero and everything works fine. @ldgauthier care to review? Note that this does not close issue #4614, but it enables a subsequent PR to do so.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5460
https://github.com/broadinstitute/gatk/pull/5461:48,Availability,failure,failures,48,* PrintWriter doesn't throw exceptions on write failures and might therefore lead to unexpectedly trunacated data being produced without any notification; * replacing it's use in MetricsUtils and MafOutputRenderer where it might cause problems; * partial fix for https://github.com/broadinstitute/gatk/issues/5458,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5461
https://github.com/broadinstitute/gatk/pull/5462:819,Modifiability,extend,extended,819,"Closes #3561. @vruano could you review this? Note that I ended up implementing Dijkstra's algorithm instead of using a library, but it's only a few lines of code. This PR does not affect the outputs of HC or M2 at all. Also, @vruano, I recall your misgivings about the current haplotype enumeration (which this preserves):. >However, the current algorithm and the k-dijkstra still would show the same problems in terms of doing a suboptimal selection of haplotypes in terms of their coverage of plausible variation. I had implemented an alternative that fixed that issue . . . simulate haplotypes based on those same furcation likelihoods and wstop when we have not discovered anything new for a while... the problem of such an approach is to make it deterministic. Although this PR doesn't do that, it could easily be extended to do so just by running Dijkstra's algorithm until you have the amount of variation you want. That is, instead of terminating when the Dijkstra priority queue is empty or when we have discovered the maximum number of haplotypes, we could terminate based on some `Predicate<List<KBestHaplotype>>` on the list of haplotypes found so far. And it's deterministic since Dijkstra's algorithm is greedy. So basically, it's a nice refactoring for now but it also sets up some worthwhile extensions if we want.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5462
https://github.com/broadinstitute/gatk/pull/5462:1252,Modifiability,refactor,refactoring,1252,"Closes #3561. @vruano could you review this? Note that I ended up implementing Dijkstra's algorithm instead of using a library, but it's only a few lines of code. This PR does not affect the outputs of HC or M2 at all. Also, @vruano, I recall your misgivings about the current haplotype enumeration (which this preserves):. >However, the current algorithm and the k-dijkstra still would show the same problems in terms of doing a suboptimal selection of haplotypes in terms of their coverage of plausible variation. I had implemented an alternative that fixed that issue . . . simulate haplotypes based on those same furcation likelihoods and wstop when we have not discovered anything new for a while... the problem of such an approach is to make it deterministic. Although this PR doesn't do that, it could easily be extended to do so just by running Dijkstra's algorithm until you have the amount of variation you want. That is, instead of terminating when the Dijkstra priority queue is empty or when we have discovered the maximum number of haplotypes, we could terminate based on some `Predicate<List<KBestHaplotype>>` on the list of haplotypes found so far. And it's deterministic since Dijkstra's algorithm is greedy. So basically, it's a nice refactoring for now but it also sets up some worthwhile extensions if we want.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5462
https://github.com/broadinstitute/gatk/pull/5462:982,Performance,queue,queue,982,"Closes #3561. @vruano could you review this? Note that I ended up implementing Dijkstra's algorithm instead of using a library, but it's only a few lines of code. This PR does not affect the outputs of HC or M2 at all. Also, @vruano, I recall your misgivings about the current haplotype enumeration (which this preserves):. >However, the current algorithm and the k-dijkstra still would show the same problems in terms of doing a suboptimal selection of haplotypes in terms of their coverage of plausible variation. I had implemented an alternative that fixed that issue . . . simulate haplotypes based on those same furcation likelihoods and wstop when we have not discovered anything new for a while... the problem of such an approach is to make it deterministic. Although this PR doesn't do that, it could easily be extended to do so just by running Dijkstra's algorithm until you have the amount of variation you want. That is, instead of terminating when the Dijkstra priority queue is empty or when we have discovered the maximum number of haplotypes, we could terminate based on some `Predicate<List<KBestHaplotype>>` on the list of haplotypes found so far. And it's deterministic since Dijkstra's algorithm is greedy. So basically, it's a nice refactoring for now but it also sets up some worthwhile extensions if we want.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5462
https://github.com/broadinstitute/gatk/pull/5463:61,Deployability,update,update,61,* updating Intel-GKL from 8.5 -> 8.6; * this is a very minor update that only changes a log message; * fixes https://github.com/broadinstitute/gatk/issues/5393,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5463
https://github.com/broadinstitute/gatk/pull/5463:92,Integrability,message,message,92,* updating Intel-GKL from 8.5 -> 8.6; * this is a very minor update that only changes a log message; * fixes https://github.com/broadinstitute/gatk/issues/5393,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5463
https://github.com/broadinstitute/gatk/pull/5463:88,Testability,log,log,88,* updating Intel-GKL from 8.5 -> 8.6; * this is a very minor update that only changes a log message; * fixes https://github.com/broadinstitute/gatk/issues/5393,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5463
https://github.com/broadinstitute/gatk/pull/5464:208,Testability,test,testExcludeSelectionIDFromFile,208,Changed SelectVariants so that it can handle multiple rsIDs separated by ';' in a VCF file.; The corresponding entry gets removed if any of those rsIDs has been set by the user to be deleted.; I also changed testExcludeSelectionIDFromFile() in SelectVariantsIntegrationTest to check this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5464
https://github.com/broadinstitute/gatk/issues/5465:62,Modifiability,config,config,62,`DataSourceUtils` contains string constants for fields in the config file (e.g. `CONFIG_FILE_FIELD_NAME_NAME`). These should be rolled into an enum together. This will facilitate file validation by enabling them to be iterated over automatically using the enum's built in methods.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5465
https://github.com/broadinstitute/gatk/issues/5465:184,Security,validat,validation,184,`DataSourceUtils` contains string constants for fields in the config file (e.g. `CONFIG_FILE_FIELD_NAME_NAME`). These should be rolled into an enum together. This will facilitate file validation by enabling them to be iterated over automatically using the enum's built in methods.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5465
https://github.com/broadinstitute/gatk/issues/5468:513,Availability,error,errored,513,"## Feature request / documentation request. ### Tool(s) or class(es) involved; Reading files from non-public GCS paths. ### Description; I did not have Application Default Credentials set up when I tried to read from a private bucket. This failed, as expected. Could we add a comment explaining that running `gcloud auth application-default login` is the necessary step to making this work? I didn't see anything on the forum about how to solve this. The solution was in the comments to #2394. ### Observed; GATK errored out with a stack trace:; ```; code: 401; message: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; reason: required; location: Authorization; retryable: false; com.google.cloud.storage.StorageException: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:220); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:415); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:198); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:195); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:89); at com.google.cloud.RetryHelper.run(RetryHelper.java:74); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:51); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:195); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:673); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:429); at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:206); ```. ```; Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 401 Unauthorized; {; ""code"" : 401,; ""errors"" : [ {; ""domain"" : ""global"",; ""location"" : ""Authoriza",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5468
https://github.com/broadinstitute/gatk/issues/5468:1941,Availability,error,errors,1941,"h a stack trace:; ```; code: 401; message: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; reason: required; location: Authorization; retryable: false; com.google.cloud.storage.StorageException: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:220); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:415); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:198); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:195); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:89); at com.google.cloud.RetryHelper.run(RetryHelper.java:74); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:51); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:195); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:673); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:429); at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:206); ```. ```; Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 401 Unauthorized; {; ""code"" : 401,; ""errors"" : [ {; ""domain"" : ""global"",; ""location"" : ""Authorization"",; ""locationType"" : ""header"",; ""message"" : ""Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram."",; ""reason"" : ""required""; } ],; ""message"" : ""Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.""; }; ```. ### Desired; Something like ""Unable to read gs://joel-cram/SAM24339124.cram due to permissions. Have you enabled Google Cloud Application Default Credentials by running 'gcloud auth application-default login'? See [this forum post] for details.""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5468
https://github.com/broadinstitute/gatk/issues/5468:562,Integrability,message,message,562,"## Feature request / documentation request. ### Tool(s) or class(es) involved; Reading files from non-public GCS paths. ### Description; I did not have Application Default Credentials set up when I tried to read from a private bucket. This failed, as expected. Could we add a comment explaining that running `gcloud auth application-default login` is the necessary step to making this work? I didn't see anything on the forum about how to solve this. The solution was in the comments to #2394. ### Observed; GATK errored out with a stack trace:; ```; code: 401; message: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; reason: required; location: Authorization; retryable: false; com.google.cloud.storage.StorageException: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:220); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:415); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:198); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:195); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:89); at com.google.cloud.RetryHelper.run(RetryHelper.java:74); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:51); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:195); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:673); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:429); at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:206); ```. ```; Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 401 Unauthorized; {; ""code"" : 401,; ""errors"" : [ {; ""domain"" : ""global"",; ""location"" : ""Authoriza",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5468
https://github.com/broadinstitute/gatk/issues/5468:2038,Integrability,message,message,2038,"h a stack trace:; ```; code: 401; message: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; reason: required; location: Authorization; retryable: false; com.google.cloud.storage.StorageException: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:220); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:415); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:198); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:195); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:89); at com.google.cloud.RetryHelper.run(RetryHelper.java:74); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:51); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:195); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:673); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:429); at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:206); ```. ```; Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 401 Unauthorized; {; ""code"" : 401,; ""errors"" : [ {; ""domain"" : ""global"",; ""location"" : ""Authorization"",; ""locationType"" : ""header"",; ""message"" : ""Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram."",; ""reason"" : ""required""; } ],; ""message"" : ""Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.""; }; ```. ### Desired; Something like ""Unable to read gs://joel-cram/SAM24339124.cram due to permissions. Have you enabled Google Cloud Application Default Credentials by running 'gcloud auth application-default login'? See [this forum post] for details.""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5468
https://github.com/broadinstitute/gatk/issues/5468:2172,Integrability,message,message,2172,"h a stack trace:; ```; code: 401; message: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; reason: required; location: Authorization; retryable: false; com.google.cloud.storage.StorageException: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:220); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:415); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:198); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:195); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:89); at com.google.cloud.RetryHelper.run(RetryHelper.java:74); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:51); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:195); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:673); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:429); at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:206); ```. ```; Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 401 Unauthorized; {; ""code"" : 401,; ""errors"" : [ {; ""domain"" : ""global"",; ""location"" : ""Authorization"",; ""locationType"" : ""header"",; ""message"" : ""Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram."",; ""reason"" : ""required""; } ],; ""message"" : ""Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.""; }; ```. ### Desired; Something like ""Unable to read gs://joel-cram/SAM24339124.cram due to permissions. Have you enabled Google Cloud Application Default Credentials by running 'gcloud auth application-default login'? See [this forum post] for details.""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5468
https://github.com/broadinstitute/gatk/issues/5468:622,Security,access,access,622,"## Feature request / documentation request. ### Tool(s) or class(es) involved; Reading files from non-public GCS paths. ### Description; I did not have Application Default Credentials set up when I tried to read from a private bucket. This failed, as expected. Could we add a comment explaining that running `gcloud auth application-default login` is the necessary step to making this work? I didn't see anything on the forum about how to solve this. The solution was in the comments to #2394. ### Observed; GATK errored out with a stack trace:; ```; code: 401; message: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; reason: required; location: Authorization; retryable: false; com.google.cloud.storage.StorageException: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:220); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:415); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:198); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:195); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:89); at com.google.cloud.RetryHelper.run(RetryHelper.java:74); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:51); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:195); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:673); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:429); at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:206); ```. ```; Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 401 Unauthorized; {; ""code"" : 401,; ""errors"" : [ {; ""domain"" : ""global"",; ""location"" : ""Authoriza",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5468
https://github.com/broadinstitute/gatk/issues/5468:689,Security,Authoriz,Authorization,689,"## Feature request / documentation request. ### Tool(s) or class(es) involved; Reading files from non-public GCS paths. ### Description; I did not have Application Default Credentials set up when I tried to read from a private bucket. This failed, as expected. Could we add a comment explaining that running `gcloud auth application-default login` is the necessary step to making this work? I didn't see anything on the forum about how to solve this. The solution was in the comments to #2394. ### Observed; GATK errored out with a stack trace:; ```; code: 401; message: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; reason: required; location: Authorization; retryable: false; com.google.cloud.storage.StorageException: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:220); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:415); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:198); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:195); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:89); at com.google.cloud.RetryHelper.run(RetryHelper.java:74); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:51); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:195); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:673); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:429); at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:206); ```. ```; Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 401 Unauthorized; {; ""code"" : 401,; ""errors"" : [ {; ""domain"" : ""global"",; ""location"" : ""Authoriza",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5468
https://github.com/broadinstitute/gatk/issues/5468:816,Security,access,access,816,"## Feature request / documentation request. ### Tool(s) or class(es) involved; Reading files from non-public GCS paths. ### Description; I did not have Application Default Credentials set up when I tried to read from a private bucket. This failed, as expected. Could we add a comment explaining that running `gcloud auth application-default login` is the necessary step to making this work? I didn't see anything on the forum about how to solve this. The solution was in the comments to #2394. ### Observed; GATK errored out with a stack trace:; ```; code: 401; message: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; reason: required; location: Authorization; retryable: false; com.google.cloud.storage.StorageException: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:220); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:415); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:198); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:195); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:89); at com.google.cloud.RetryHelper.run(RetryHelper.java:74); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:51); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:195); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:673); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:429); at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:206); ```. ```; Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 401 Unauthorized; {; ""code"" : 401,; ""errors"" : [ {; ""domain"" : ""global"",; ""location"" : ""Authoriza",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5468
https://github.com/broadinstitute/gatk/issues/5468:1992,Security,Authoriz,Authorization,1992,"h a stack trace:; ```; code: 401; message: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; reason: required; location: Authorization; retryable: false; com.google.cloud.storage.StorageException: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:220); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:415); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:198); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:195); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:89); at com.google.cloud.RetryHelper.run(RetryHelper.java:74); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:51); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:195); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:673); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:429); at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:206); ```. ```; Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 401 Unauthorized; {; ""code"" : 401,; ""errors"" : [ {; ""domain"" : ""global"",; ""location"" : ""Authorization"",; ""locationType"" : ""header"",; ""message"" : ""Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram."",; ""reason"" : ""required""; } ],; ""message"" : ""Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.""; }; ```. ### Desired; Something like ""Unable to read gs://joel-cram/SAM24339124.cram due to permissions. Have you enabled Google Cloud Application Default Credentials by running 'gcloud auth application-default login'? See [this forum post] for details.""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5468
https://github.com/broadinstitute/gatk/issues/5468:2101,Security,access,access,2101,"h a stack trace:; ```; code: 401; message: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; reason: required; location: Authorization; retryable: false; com.google.cloud.storage.StorageException: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:220); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:415); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:198); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:195); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:89); at com.google.cloud.RetryHelper.run(RetryHelper.java:74); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:51); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:195); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:673); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:429); at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:206); ```. ```; Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 401 Unauthorized; {; ""code"" : 401,; ""errors"" : [ {; ""domain"" : ""global"",; ""location"" : ""Authorization"",; ""locationType"" : ""header"",; ""message"" : ""Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram."",; ""reason"" : ""required""; } ],; ""message"" : ""Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.""; }; ```. ### Desired; Something like ""Unable to read gs://joel-cram/SAM24339124.cram due to permissions. Have you enabled Google Cloud Application Default Credentials by running 'gcloud auth application-default login'? See [this forum post] for details.""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5468
https://github.com/broadinstitute/gatk/issues/5468:2235,Security,access,access,2235,"h a stack trace:; ```; code: 401; message: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; reason: required; location: Authorization; retryable: false; com.google.cloud.storage.StorageException: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:220); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:415); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:198); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:195); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:89); at com.google.cloud.RetryHelper.run(RetryHelper.java:74); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:51); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:195); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:673); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:429); at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:206); ```. ```; Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 401 Unauthorized; {; ""code"" : 401,; ""errors"" : [ {; ""domain"" : ""global"",; ""location"" : ""Authorization"",; ""locationType"" : ""header"",; ""message"" : ""Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram."",; ""reason"" : ""required""; } ],; ""message"" : ""Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.""; }; ```. ### Desired; Something like ""Unable to read gs://joel-cram/SAM24339124.cram due to permissions. Have you enabled Google Cloud Application Default Credentials by running 'gcloud auth application-default login'? See [this forum post] for details.""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5468
https://github.com/broadinstitute/gatk/issues/5468:341,Testability,log,login,341,"## Feature request / documentation request. ### Tool(s) or class(es) involved; Reading files from non-public GCS paths. ### Description; I did not have Application Default Credentials set up when I tried to read from a private bucket. This failed, as expected. Could we add a comment explaining that running `gcloud auth application-default login` is the necessary step to making this work? I didn't see anything on the forum about how to solve this. The solution was in the comments to #2394. ### Observed; GATK errored out with a stack trace:; ```; code: 401; message: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; reason: required; location: Authorization; retryable: false; com.google.cloud.storage.StorageException: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:220); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:415); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:198); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:195); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:89); at com.google.cloud.RetryHelper.run(RetryHelper.java:74); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:51); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:195); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:673); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:429); at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:206); ```. ```; Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 401 Unauthorized; {; ""code"" : 401,; ""errors"" : [ {; ""domain"" : ""global"",; ""location"" : ""Authoriza",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5468
https://github.com/broadinstitute/gatk/issues/5468:1670,Testability,assert,assertFileIsReadable,1670,"h a stack trace:; ```; code: 401; message: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; reason: required; location: Authorization; retryable: false; com.google.cloud.storage.StorageException: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:220); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:415); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:198); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:195); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:89); at com.google.cloud.RetryHelper.run(RetryHelper.java:74); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:51); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:195); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:673); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:429); at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:206); ```. ```; Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 401 Unauthorized; {; ""code"" : 401,; ""errors"" : [ {; ""domain"" : ""global"",; ""location"" : ""Authorization"",; ""locationType"" : ""header"",; ""message"" : ""Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram."",; ""reason"" : ""required""; } ],; ""message"" : ""Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.""; }; ```. ### Desired; Something like ""Unable to read gs://joel-cram/SAM24339124.cram due to permissions. Have you enabled Google Cloud Application Default Credentials by running 'gcloud auth application-default login'? See [this forum post] for details.""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5468
https://github.com/broadinstitute/gatk/issues/5468:2485,Testability,log,login,2485,"h a stack trace:; ```; code: 401; message: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; reason: required; location: Authorization; retryable: false; com.google.cloud.storage.StorageException: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:220); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:415); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:198); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:195); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:89); at com.google.cloud.RetryHelper.run(RetryHelper.java:74); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:51); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:195); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:673); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:429); at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:206); ```. ```; Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 401 Unauthorized; {; ""code"" : 401,; ""errors"" : [ {; ""domain"" : ""global"",; ""location"" : ""Authorization"",; ""locationType"" : ""header"",; ""message"" : ""Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram."",; ""reason"" : ""required""; } ],; ""message"" : ""Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.""; }; ```. ### Desired; Something like ""Unable to read gs://joel-cram/SAM24339124.cram due to permissions. Have you enabled Google Cloud Application Default Credentials by running 'gcloud auth application-default login'? See [this forum post] for details.""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5468
https://github.com/broadinstitute/gatk/pull/5469:125,Deployability,patch,patch,125,"This micro-optimization fell out of profiling of the HaplotypeCaller in GVCF mode. . Profiler view over an Exome before this patch:; <img width=""906"" alt=""screen shot 2018-11-30 at 2 06 34 pm"" src=""https://user-images.githubusercontent.com/16102845/49310230-bc44a380-f4ab-11e8-98aa-1c0b321223c0.png"">. Profiler view over the same Exome after this patch:; <img width=""886"" alt=""screen shot 2018-11-30 at 2 20 39 pm"" src=""https://user-images.githubusercontent.com/16102845/49310291-e4cc9d80-f4ab-11e8-9fb3-4d819fbce43a.png"">. I suspect given the remaining 9% runtime could be reduced further by looking more closely at the array operations in `isReadInformativeAboutIndelsOfSize()` . (It should be noted that these profiler results lie within the ReferenceModelForNoVariation codepath which since this is over an Exome we expect the runtime to overall be skewed towards no-variation blocks). Resolves #5648",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5469
https://github.com/broadinstitute/gatk/pull/5469:347,Deployability,patch,patch,347,"This micro-optimization fell out of profiling of the HaplotypeCaller in GVCF mode. . Profiler view over an Exome before this patch:; <img width=""906"" alt=""screen shot 2018-11-30 at 2 06 34 pm"" src=""https://user-images.githubusercontent.com/16102845/49310230-bc44a380-f4ab-11e8-98aa-1c0b321223c0.png"">. Profiler view over the same Exome after this patch:; <img width=""886"" alt=""screen shot 2018-11-30 at 2 20 39 pm"" src=""https://user-images.githubusercontent.com/16102845/49310291-e4cc9d80-f4ab-11e8-9fb3-4d819fbce43a.png"">. I suspect given the remaining 9% runtime could be reduced further by looking more closely at the array operations in `isReadInformativeAboutIndelsOfSize()` . (It should be noted that these profiler results lie within the ReferenceModelForNoVariation codepath which since this is over an Exome we expect the runtime to overall be skewed towards no-variation blocks). Resolves #5648",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5469
https://github.com/broadinstitute/gatk/pull/5469:574,Energy Efficiency,reduce,reduced,574,"This micro-optimization fell out of profiling of the HaplotypeCaller in GVCF mode. . Profiler view over an Exome before this patch:; <img width=""906"" alt=""screen shot 2018-11-30 at 2 06 34 pm"" src=""https://user-images.githubusercontent.com/16102845/49310230-bc44a380-f4ab-11e8-98aa-1c0b321223c0.png"">. Profiler view over the same Exome after this patch:; <img width=""886"" alt=""screen shot 2018-11-30 at 2 20 39 pm"" src=""https://user-images.githubusercontent.com/16102845/49310291-e4cc9d80-f4ab-11e8-9fb3-4d819fbce43a.png"">. I suspect given the remaining 9% runtime could be reduced further by looking more closely at the array operations in `isReadInformativeAboutIndelsOfSize()` . (It should be noted that these profiler results lie within the ReferenceModelForNoVariation codepath which since this is over an Exome we expect the runtime to overall be skewed towards no-variation blocks). Resolves #5648",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5469
https://github.com/broadinstitute/gatk/pull/5469:11,Performance,optimiz,optimization,11,"This micro-optimization fell out of profiling of the HaplotypeCaller in GVCF mode. . Profiler view over an Exome before this patch:; <img width=""906"" alt=""screen shot 2018-11-30 at 2 06 34 pm"" src=""https://user-images.githubusercontent.com/16102845/49310230-bc44a380-f4ab-11e8-98aa-1c0b321223c0.png"">. Profiler view over the same Exome after this patch:; <img width=""886"" alt=""screen shot 2018-11-30 at 2 20 39 pm"" src=""https://user-images.githubusercontent.com/16102845/49310291-e4cc9d80-f4ab-11e8-9fb3-4d819fbce43a.png"">. I suspect given the remaining 9% runtime could be reduced further by looking more closely at the array operations in `isReadInformativeAboutIndelsOfSize()` . (It should be noted that these profiler results lie within the ReferenceModelForNoVariation codepath which since this is over an Exome we expect the runtime to overall be skewed towards no-variation blocks). Resolves #5648",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5469
https://github.com/broadinstitute/gatk/issues/5472:261,Availability,error,errors,261,"@cmnbroad, [a researcher has pointed out](https://gatkforums.broadinstitute.org/gatk/discussion/1319/collected-faqs-about-interval-lists#latest) that although GATK accepts both types of intervals lists (Picard-style & BED), Picard tools called through the GATK errors with a BED intervals list. Is it possible to amend this behavior so any intervals list GATK accepts, Picard-called-through-GATK also accepts? If not, please let us know (myself and @rcmajovski) so that we can update documentation. . Given BED is the more widely-used intervals format, it would be great if we enabled its use consistently in our tools. The downside is the lack of reference match checking. However, it seems the decision has already been made with GATK's acceptance of BED intervals. Let me know your thoughts.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5472
https://github.com/broadinstitute/gatk/issues/5472:624,Availability,down,downside,624,"@cmnbroad, [a researcher has pointed out](https://gatkforums.broadinstitute.org/gatk/discussion/1319/collected-faqs-about-interval-lists#latest) that although GATK accepts both types of intervals lists (Picard-style & BED), Picard tools called through the GATK errors with a BED intervals list. Is it possible to amend this behavior so any intervals list GATK accepts, Picard-called-through-GATK also accepts? If not, please let us know (myself and @rcmajovski) so that we can update documentation. . Given BED is the more widely-used intervals format, it would be great if we enabled its use consistently in our tools. The downside is the lack of reference match checking. However, it seems the decision has already been made with GATK's acceptance of BED intervals. Let me know your thoughts.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5472
https://github.com/broadinstitute/gatk/issues/5472:477,Deployability,update,update,477,"@cmnbroad, [a researcher has pointed out](https://gatkforums.broadinstitute.org/gatk/discussion/1319/collected-faqs-about-interval-lists#latest) that although GATK accepts both types of intervals lists (Picard-style & BED), Picard tools called through the GATK errors with a BED intervals list. Is it possible to amend this behavior so any intervals list GATK accepts, Picard-called-through-GATK also accepts? If not, please let us know (myself and @rcmajovski) so that we can update documentation. . Given BED is the more widely-used intervals format, it would be great if we enabled its use consistently in our tools. The downside is the lack of reference match checking. However, it seems the decision has already been made with GATK's acceptance of BED intervals. Let me know your thoughts.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5472
https://github.com/broadinstitute/gatk/pull/5473:226,Performance,tune,tuned,226,"Closes #4867. @takutosato Here it is. I'm not quite ready to make it the M2 default, but it looks really good. @meganshand I have tested it on every mixture in your workspace and results look very similar to the previous hand-tuned pruning results. I'm hoping it's good enough to become best practices for mitochondria and would appreciate if you gave it a shot. You have the right to review if you wish but there's no pressure to do so. @ldgauthier HaplotypeCaller might also benefit from this. In particular, I wonder about #3697. I'll test it out.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5473
https://github.com/broadinstitute/gatk/pull/5473:130,Testability,test,tested,130,"Closes #4867. @takutosato Here it is. I'm not quite ready to make it the M2 default, but it looks really good. @meganshand I have tested it on every mixture in your workspace and results look very similar to the previous hand-tuned pruning results. I'm hoping it's good enough to become best practices for mitochondria and would appreciate if you gave it a shot. You have the right to review if you wish but there's no pressure to do so. @ldgauthier HaplotypeCaller might also benefit from this. In particular, I wonder about #3697. I'll test it out.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5473
https://github.com/broadinstitute/gatk/pull/5473:538,Testability,test,test,538,"Closes #4867. @takutosato Here it is. I'm not quite ready to make it the M2 default, but it looks really good. @meganshand I have tested it on every mixture in your workspace and results look very similar to the previous hand-tuned pruning results. I'm hoping it's good enough to become best practices for mitochondria and would appreciate if you gave it a shot. You have the right to review if you wish but there's no pressure to do so. @ldgauthier HaplotypeCaller might also benefit from this. In particular, I wonder about #3697. I'll test it out.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5473
https://github.com/broadinstitute/gatk/issues/5476:125,Availability,down,downsampling,125,"After #5416 one of the lingering potential sources of difference between HaplotypeCallerSpark and HaplotypeCaller are in the downsampling, which could cost Spark both correctness and performance at pathological sites. This likely requires #5437 or some equivalent change to be implemented so we can save ourselves from materializing and shuffling all the reads in their AssemblyRegions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5476
https://github.com/broadinstitute/gatk/issues/5476:183,Performance,perform,performance,183,"After #5416 one of the lingering potential sources of difference between HaplotypeCallerSpark and HaplotypeCaller are in the downsampling, which could cost Spark both correctness and performance at pathological sites. This likely requires #5437 or some equivalent change to be implemented so we can save ourselves from materializing and shuffling all the reads in their AssemblyRegions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5476
https://github.com/broadinstitute/gatk/pull/5477:244,Availability,error,error,244,"Also, update code to use Hellbender's IOUtils instead of htsjdk's IOUtil; for these checks. We have both, presumably there's a reason Hellbender has their own and we should use them (for example, we can only add the hinting in our own). Sample error now:. A USER ERROR has occurred: Couldn't read file gs://foo/sam/m54113_160913_184949.scraps.beginning.sam. Error was: Error 403: jp-testing@redacted.iam.gserviceaccount.com does not have storage.objects.get access to foo/sam/m54113_160913_184949.scraps.beginning.sam. Potential cause: incorrect Google Cloud configuration; see instructions in the README. Fixes: #5468",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5477
https://github.com/broadinstitute/gatk/pull/5477:263,Availability,ERROR,ERROR,263,"Also, update code to use Hellbender's IOUtils instead of htsjdk's IOUtil; for these checks. We have both, presumably there's a reason Hellbender has their own and we should use them (for example, we can only add the hinting in our own). Sample error now:. A USER ERROR has occurred: Couldn't read file gs://foo/sam/m54113_160913_184949.scraps.beginning.sam. Error was: Error 403: jp-testing@redacted.iam.gserviceaccount.com does not have storage.objects.get access to foo/sam/m54113_160913_184949.scraps.beginning.sam. Potential cause: incorrect Google Cloud configuration; see instructions in the README. Fixes: #5468",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5477
https://github.com/broadinstitute/gatk/pull/5477:358,Availability,Error,Error,358,"Also, update code to use Hellbender's IOUtils instead of htsjdk's IOUtil; for these checks. We have both, presumably there's a reason Hellbender has their own and we should use them (for example, we can only add the hinting in our own). Sample error now:. A USER ERROR has occurred: Couldn't read file gs://foo/sam/m54113_160913_184949.scraps.beginning.sam. Error was: Error 403: jp-testing@redacted.iam.gserviceaccount.com does not have storage.objects.get access to foo/sam/m54113_160913_184949.scraps.beginning.sam. Potential cause: incorrect Google Cloud configuration; see instructions in the README. Fixes: #5468",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5477
https://github.com/broadinstitute/gatk/pull/5477:369,Availability,Error,Error,369,"Also, update code to use Hellbender's IOUtils instead of htsjdk's IOUtil; for these checks. We have both, presumably there's a reason Hellbender has their own and we should use them (for example, we can only add the hinting in our own). Sample error now:. A USER ERROR has occurred: Couldn't read file gs://foo/sam/m54113_160913_184949.scraps.beginning.sam. Error was: Error 403: jp-testing@redacted.iam.gserviceaccount.com does not have storage.objects.get access to foo/sam/m54113_160913_184949.scraps.beginning.sam. Potential cause: incorrect Google Cloud configuration; see instructions in the README. Fixes: #5468",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5477
https://github.com/broadinstitute/gatk/pull/5477:6,Deployability,update,update,6,"Also, update code to use Hellbender's IOUtils instead of htsjdk's IOUtil; for these checks. We have both, presumably there's a reason Hellbender has their own and we should use them (for example, we can only add the hinting in our own). Sample error now:. A USER ERROR has occurred: Couldn't read file gs://foo/sam/m54113_160913_184949.scraps.beginning.sam. Error was: Error 403: jp-testing@redacted.iam.gserviceaccount.com does not have storage.objects.get access to foo/sam/m54113_160913_184949.scraps.beginning.sam. Potential cause: incorrect Google Cloud configuration; see instructions in the README. Fixes: #5468",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5477
https://github.com/broadinstitute/gatk/pull/5477:559,Deployability,configurat,configuration,559,"Also, update code to use Hellbender's IOUtils instead of htsjdk's IOUtil; for these checks. We have both, presumably there's a reason Hellbender has their own and we should use them (for example, we can only add the hinting in our own). Sample error now:. A USER ERROR has occurred: Couldn't read file gs://foo/sam/m54113_160913_184949.scraps.beginning.sam. Error was: Error 403: jp-testing@redacted.iam.gserviceaccount.com does not have storage.objects.get access to foo/sam/m54113_160913_184949.scraps.beginning.sam. Potential cause: incorrect Google Cloud configuration; see instructions in the README. Fixes: #5468",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5477
https://github.com/broadinstitute/gatk/pull/5477:559,Modifiability,config,configuration,559,"Also, update code to use Hellbender's IOUtils instead of htsjdk's IOUtil; for these checks. We have both, presumably there's a reason Hellbender has their own and we should use them (for example, we can only add the hinting in our own). Sample error now:. A USER ERROR has occurred: Couldn't read file gs://foo/sam/m54113_160913_184949.scraps.beginning.sam. Error was: Error 403: jp-testing@redacted.iam.gserviceaccount.com does not have storage.objects.get access to foo/sam/m54113_160913_184949.scraps.beginning.sam. Potential cause: incorrect Google Cloud configuration; see instructions in the README. Fixes: #5468",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5477
https://github.com/broadinstitute/gatk/pull/5477:458,Security,access,access,458,"Also, update code to use Hellbender's IOUtils instead of htsjdk's IOUtil; for these checks. We have both, presumably there's a reason Hellbender has their own and we should use them (for example, we can only add the hinting in our own). Sample error now:. A USER ERROR has occurred: Couldn't read file gs://foo/sam/m54113_160913_184949.scraps.beginning.sam. Error was: Error 403: jp-testing@redacted.iam.gserviceaccount.com does not have storage.objects.get access to foo/sam/m54113_160913_184949.scraps.beginning.sam. Potential cause: incorrect Google Cloud configuration; see instructions in the README. Fixes: #5468",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5477
https://github.com/broadinstitute/gatk/pull/5477:383,Testability,test,testing,383,"Also, update code to use Hellbender's IOUtils instead of htsjdk's IOUtil; for these checks. We have both, presumably there's a reason Hellbender has their own and we should use them (for example, we can only add the hinting in our own). Sample error now:. A USER ERROR has occurred: Couldn't read file gs://foo/sam/m54113_160913_184949.scraps.beginning.sam. Error was: Error 403: jp-testing@redacted.iam.gserviceaccount.com does not have storage.objects.get access to foo/sam/m54113_160913_184949.scraps.beginning.sam. Potential cause: incorrect Google Cloud configuration; see instructions in the README. Fixes: #5468",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5477
https://github.com/broadinstitute/gatk/issues/5481:452,Availability,error,error,452,"### Goal; Our team try to explore the usage of GATK Spark tool for our internal WES and WGS data. ## Bug Report. ### Affected tool(s) or class(es); BwaSpark and ReadsPipelineSpark. ### Affected version(s); - [ X ] Latest public release version [gatk 4.0.11.0]. ### Description ; For both tools, we encountered an issue when the driver shutdown the command as the screenshot. However, for the bwaspark, the alignment ratio seems to be unaffected by the error, but the lines number of the VCF file from ReadsPipelineSpark varies randomly, and quite different from the non-spark version of GATK 4.0.5.2. #### Steps to reproduce; Before running the tool, we generated the image index for whole genome by using the fasta file from GATK official ftp site, and uploaded the reference file to Hadoop HDFS. ``` bash; gatk-4.0.11.0/gatk BwaMemIndexImageCreator -I Homo_sapiens_assembly38.fasta -O Homo_sapiens_assembly38.fasta.img; ```. and then, we preprocess our pair end fastq files into unaligned ubam file as, ; ``` bash; java -jar picard.jar FastqToSam \; F1=R1.fastq.gz; F2=R2.fastq.gz; O=unaligned_reads.bam \; SM=sample001 \; PL=illumina \; RG=rg001; ```. For BwaSpark, we used,; ``` {bash}; ../gatk-4.0.11.0/gatk --java-options ""-Dgatk.spark.debug=true -XX:+PrintGCDetails"" BwaSpark -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test3.bam -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --spark-runner SPARK --spark-master spark://master:7077 -- --num-executors 4 --driver-memory 4g --executor-cores 10 --executor-memory 20g; ```. For ReadsPipelineSpark, we used, ; ``` {bash}; time_gatk ""ReadsPipelineSpark --tmp-dir /tmp --align true -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test10.vcf -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --known-sites hdfs://ns/user/root/Homo_sapiens_assembly38.dbsnp138.vcf -pairHMM AVX_LOGLESS_CACHING --max-reads-per-alignment-start 50"" 4 44 88g 12g; ```. #### Expected behavior; Both tool s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5481
https://github.com/broadinstitute/gatk/issues/5481:2046,Availability,error,error,2046,"cial ftp site, and uploaded the reference file to Hadoop HDFS. ``` bash; gatk-4.0.11.0/gatk BwaMemIndexImageCreator -I Homo_sapiens_assembly38.fasta -O Homo_sapiens_assembly38.fasta.img; ```. and then, we preprocess our pair end fastq files into unaligned ubam file as, ; ``` bash; java -jar picard.jar FastqToSam \; F1=R1.fastq.gz; F2=R2.fastq.gz; O=unaligned_reads.bam \; SM=sample001 \; PL=illumina \; RG=rg001; ```. For BwaSpark, we used,; ``` {bash}; ../gatk-4.0.11.0/gatk --java-options ""-Dgatk.spark.debug=true -XX:+PrintGCDetails"" BwaSpark -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test3.bam -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --spark-runner SPARK --spark-master spark://master:7077 -- --num-executors 4 --driver-memory 4g --executor-cores 10 --executor-memory 20g; ```. For ReadsPipelineSpark, we used, ; ``` {bash}; time_gatk ""ReadsPipelineSpark --tmp-dir /tmp --align true -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test10.vcf -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --known-sites hdfs://ns/user/root/Homo_sapiens_assembly38.dbsnp138.vcf -pairHMM AVX_LOGLESS_CACHING --max-reads-per-alignment-start 50"" 4 44 88g 12g; ```. #### Expected behavior; Both tool should end successfully without the specified error, and generated consistent result. #### Actual behavior; Both tool ends throwing out the same error, but the alignment ratio of the bam file from bwaspark is the same as the original bwa, and seems to be ok. The vcf variant number changes between each runs, and different from the result of stable non-spark official GATK 4 version. ``` bash; 2018-12-03 13:19:45 ERROR CoarseGrainedExecutorBackend:43 - RECEIVED SIGNAL TERM; ```. The vcf line number for several runs of the ReadsPipelineSpark,; ``` bash; 94488 test2.vcf.gz; 97426 test3.vcf.gz; 82279 test4.hg19.vcf.gz; 99403 test4.vcf.gz; 86384 test7_hg19.vcf.gz; 104854 test7.vcf.gz; 68824 test9.vcf.gz; 97810 test.vcf.gz; ```. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5481
https://github.com/broadinstitute/gatk/issues/5481:2145,Availability,error,error,2145,"cial ftp site, and uploaded the reference file to Hadoop HDFS. ``` bash; gatk-4.0.11.0/gatk BwaMemIndexImageCreator -I Homo_sapiens_assembly38.fasta -O Homo_sapiens_assembly38.fasta.img; ```. and then, we preprocess our pair end fastq files into unaligned ubam file as, ; ``` bash; java -jar picard.jar FastqToSam \; F1=R1.fastq.gz; F2=R2.fastq.gz; O=unaligned_reads.bam \; SM=sample001 \; PL=illumina \; RG=rg001; ```. For BwaSpark, we used,; ``` {bash}; ../gatk-4.0.11.0/gatk --java-options ""-Dgatk.spark.debug=true -XX:+PrintGCDetails"" BwaSpark -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test3.bam -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --spark-runner SPARK --spark-master spark://master:7077 -- --num-executors 4 --driver-memory 4g --executor-cores 10 --executor-memory 20g; ```. For ReadsPipelineSpark, we used, ; ``` {bash}; time_gatk ""ReadsPipelineSpark --tmp-dir /tmp --align true -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test10.vcf -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --known-sites hdfs://ns/user/root/Homo_sapiens_assembly38.dbsnp138.vcf -pairHMM AVX_LOGLESS_CACHING --max-reads-per-alignment-start 50"" 4 44 88g 12g; ```. #### Expected behavior; Both tool should end successfully without the specified error, and generated consistent result. #### Actual behavior; Both tool ends throwing out the same error, but the alignment ratio of the bam file from bwaspark is the same as the original bwa, and seems to be ok. The vcf variant number changes between each runs, and different from the result of stable non-spark official GATK 4 version. ``` bash; 2018-12-03 13:19:45 ERROR CoarseGrainedExecutorBackend:43 - RECEIVED SIGNAL TERM; ```. The vcf line number for several runs of the ReadsPipelineSpark,; ``` bash; 94488 test2.vcf.gz; 97426 test3.vcf.gz; 82279 test4.hg19.vcf.gz; 99403 test4.vcf.gz; 86384 test7_hg19.vcf.gz; 104854 test7.vcf.gz; 68824 test9.vcf.gz; 97810 test.vcf.gz; ```. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5481
https://github.com/broadinstitute/gatk/issues/5481:2414,Availability,ERROR,ERROR,2414,"cial ftp site, and uploaded the reference file to Hadoop HDFS. ``` bash; gatk-4.0.11.0/gatk BwaMemIndexImageCreator -I Homo_sapiens_assembly38.fasta -O Homo_sapiens_assembly38.fasta.img; ```. and then, we preprocess our pair end fastq files into unaligned ubam file as, ; ``` bash; java -jar picard.jar FastqToSam \; F1=R1.fastq.gz; F2=R2.fastq.gz; O=unaligned_reads.bam \; SM=sample001 \; PL=illumina \; RG=rg001; ```. For BwaSpark, we used,; ``` {bash}; ../gatk-4.0.11.0/gatk --java-options ""-Dgatk.spark.debug=true -XX:+PrintGCDetails"" BwaSpark -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test3.bam -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --spark-runner SPARK --spark-master spark://master:7077 -- --num-executors 4 --driver-memory 4g --executor-cores 10 --executor-memory 20g; ```. For ReadsPipelineSpark, we used, ; ``` {bash}; time_gatk ""ReadsPipelineSpark --tmp-dir /tmp --align true -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test10.vcf -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --known-sites hdfs://ns/user/root/Homo_sapiens_assembly38.dbsnp138.vcf -pairHMM AVX_LOGLESS_CACHING --max-reads-per-alignment-start 50"" 4 44 88g 12g; ```. #### Expected behavior; Both tool should end successfully without the specified error, and generated consistent result. #### Actual behavior; Both tool ends throwing out the same error, but the alignment ratio of the bam file from bwaspark is the same as the original bwa, and seems to be ok. The vcf variant number changes between each runs, and different from the result of stable non-spark official GATK 4 version. ``` bash; 2018-12-03 13:19:45 ERROR CoarseGrainedExecutorBackend:43 - RECEIVED SIGNAL TERM; ```. The vcf line number for several runs of the ReadsPipelineSpark,; ``` bash; 94488 test2.vcf.gz; 97426 test3.vcf.gz; 82279 test4.hg19.vcf.gz; 99403 test4.vcf.gz; 86384 test7_hg19.vcf.gz; 104854 test7.vcf.gz; 68824 test9.vcf.gz; 97810 test.vcf.gz; ```. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5481
https://github.com/broadinstitute/gatk/issues/5481:228,Deployability,release,release,228,"### Goal; Our team try to explore the usage of GATK Spark tool for our internal WES and WGS data. ## Bug Report. ### Affected tool(s) or class(es); BwaSpark and ReadsPipelineSpark. ### Affected version(s); - [ X ] Latest public release version [gatk 4.0.11.0]. ### Description ; For both tools, we encountered an issue when the driver shutdown the command as the screenshot. However, for the bwaspark, the alignment ratio seems to be unaffected by the error, but the lines number of the VCF file from ReadsPipelineSpark varies randomly, and quite different from the non-spark version of GATK 4.0.5.2. #### Steps to reproduce; Before running the tool, we generated the image index for whole genome by using the fasta file from GATK official ftp site, and uploaded the reference file to Hadoop HDFS. ``` bash; gatk-4.0.11.0/gatk BwaMemIndexImageCreator -I Homo_sapiens_assembly38.fasta -O Homo_sapiens_assembly38.fasta.img; ```. and then, we preprocess our pair end fastq files into unaligned ubam file as, ; ``` bash; java -jar picard.jar FastqToSam \; F1=R1.fastq.gz; F2=R2.fastq.gz; O=unaligned_reads.bam \; SM=sample001 \; PL=illumina \; RG=rg001; ```. For BwaSpark, we used,; ``` {bash}; ../gatk-4.0.11.0/gatk --java-options ""-Dgatk.spark.debug=true -XX:+PrintGCDetails"" BwaSpark -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test3.bam -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --spark-runner SPARK --spark-master spark://master:7077 -- --num-executors 4 --driver-memory 4g --executor-cores 10 --executor-memory 20g; ```. For ReadsPipelineSpark, we used, ; ``` {bash}; time_gatk ""ReadsPipelineSpark --tmp-dir /tmp --align true -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test10.vcf -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --known-sites hdfs://ns/user/root/Homo_sapiens_assembly38.dbsnp138.vcf -pairHMM AVX_LOGLESS_CACHING --max-reads-per-alignment-start 50"" 4 44 88g 12g; ```. #### Expected behavior; Both tool s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5481
https://github.com/broadinstitute/gatk/issues/5481:1306,Testability,test,test,1306,"oth tools, we encountered an issue when the driver shutdown the command as the screenshot. However, for the bwaspark, the alignment ratio seems to be unaffected by the error, but the lines number of the VCF file from ReadsPipelineSpark varies randomly, and quite different from the non-spark version of GATK 4.0.5.2. #### Steps to reproduce; Before running the tool, we generated the image index for whole genome by using the fasta file from GATK official ftp site, and uploaded the reference file to Hadoop HDFS. ``` bash; gatk-4.0.11.0/gatk BwaMemIndexImageCreator -I Homo_sapiens_assembly38.fasta -O Homo_sapiens_assembly38.fasta.img; ```. and then, we preprocess our pair end fastq files into unaligned ubam file as, ; ``` bash; java -jar picard.jar FastqToSam \; F1=R1.fastq.gz; F2=R2.fastq.gz; O=unaligned_reads.bam \; SM=sample001 \; PL=illumina \; RG=rg001; ```. For BwaSpark, we used,; ``` {bash}; ../gatk-4.0.11.0/gatk --java-options ""-Dgatk.spark.debug=true -XX:+PrintGCDetails"" BwaSpark -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test3.bam -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --spark-runner SPARK --spark-master spark://master:7077 -- --num-executors 4 --driver-memory 4g --executor-cores 10 --executor-memory 20g; ```. For ReadsPipelineSpark, we used, ; ``` {bash}; time_gatk ""ReadsPipelineSpark --tmp-dir /tmp --align true -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test10.vcf -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --known-sites hdfs://ns/user/root/Homo_sapiens_assembly38.dbsnp138.vcf -pairHMM AVX_LOGLESS_CACHING --max-reads-per-alignment-start 50"" 4 44 88g 12g; ```. #### Expected behavior; Both tool should end successfully without the specified error, and generated consistent result. #### Actual behavior; Both tool ends throwing out the same error, but the alignment ratio of the bam file from bwaspark is the same as the original bwa, and seems to be ok. The vcf variant number cha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5481
https://github.com/broadinstitute/gatk/issues/5481:1354,Testability,test,test,1354,"e command as the screenshot. However, for the bwaspark, the alignment ratio seems to be unaffected by the error, but the lines number of the VCF file from ReadsPipelineSpark varies randomly, and quite different from the non-spark version of GATK 4.0.5.2. #### Steps to reproduce; Before running the tool, we generated the image index for whole genome by using the fasta file from GATK official ftp site, and uploaded the reference file to Hadoop HDFS. ``` bash; gatk-4.0.11.0/gatk BwaMemIndexImageCreator -I Homo_sapiens_assembly38.fasta -O Homo_sapiens_assembly38.fasta.img; ```. and then, we preprocess our pair end fastq files into unaligned ubam file as, ; ``` bash; java -jar picard.jar FastqToSam \; F1=R1.fastq.gz; F2=R2.fastq.gz; O=unaligned_reads.bam \; SM=sample001 \; PL=illumina \; RG=rg001; ```. For BwaSpark, we used,; ``` {bash}; ../gatk-4.0.11.0/gatk --java-options ""-Dgatk.spark.debug=true -XX:+PrintGCDetails"" BwaSpark -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test3.bam -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --spark-runner SPARK --spark-master spark://master:7077 -- --num-executors 4 --driver-memory 4g --executor-cores 10 --executor-memory 20g; ```. For ReadsPipelineSpark, we used, ; ``` {bash}; time_gatk ""ReadsPipelineSpark --tmp-dir /tmp --align true -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test10.vcf -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --known-sites hdfs://ns/user/root/Homo_sapiens_assembly38.dbsnp138.vcf -pairHMM AVX_LOGLESS_CACHING --max-reads-per-alignment-start 50"" 4 44 88g 12g; ```. #### Expected behavior; Both tool should end successfully without the specified error, and generated consistent result. #### Actual behavior; Both tool ends throwing out the same error, but the alignment ratio of the bam file from bwaspark is the same as the original bwa, and seems to be ok. The vcf variant number changes between each runs, and different from the result of stab",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5481
https://github.com/broadinstitute/gatk/issues/5481:1694,Testability,test,test,1694," the tool, we generated the image index for whole genome by using the fasta file from GATK official ftp site, and uploaded the reference file to Hadoop HDFS. ``` bash; gatk-4.0.11.0/gatk BwaMemIndexImageCreator -I Homo_sapiens_assembly38.fasta -O Homo_sapiens_assembly38.fasta.img; ```. and then, we preprocess our pair end fastq files into unaligned ubam file as, ; ``` bash; java -jar picard.jar FastqToSam \; F1=R1.fastq.gz; F2=R2.fastq.gz; O=unaligned_reads.bam \; SM=sample001 \; PL=illumina \; RG=rg001; ```. For BwaSpark, we used,; ``` {bash}; ../gatk-4.0.11.0/gatk --java-options ""-Dgatk.spark.debug=true -XX:+PrintGCDetails"" BwaSpark -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test3.bam -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --spark-runner SPARK --spark-master spark://master:7077 -- --num-executors 4 --driver-memory 4g --executor-cores 10 --executor-memory 20g; ```. For ReadsPipelineSpark, we used, ; ``` {bash}; time_gatk ""ReadsPipelineSpark --tmp-dir /tmp --align true -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test10.vcf -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --known-sites hdfs://ns/user/root/Homo_sapiens_assembly38.dbsnp138.vcf -pairHMM AVX_LOGLESS_CACHING --max-reads-per-alignment-start 50"" 4 44 88g 12g; ```. #### Expected behavior; Both tool should end successfully without the specified error, and generated consistent result. #### Actual behavior; Both tool ends throwing out the same error, but the alignment ratio of the bam file from bwaspark is the same as the original bwa, and seems to be ok. The vcf variant number changes between each runs, and different from the result of stable non-spark official GATK 4 version. ``` bash; 2018-12-03 13:19:45 ERROR CoarseGrainedExecutorBackend:43 - RECEIVED SIGNAL TERM; ```. The vcf line number for several runs of the ReadsPipelineSpark,; ``` bash; 94488 test2.vcf.gz; 97426 test3.vcf.gz; 82279 test4.hg19.vcf.gz; 99403 test4.vcf.gz;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5481
https://github.com/broadinstitute/gatk/issues/5481:1742,Testability,test,test,1742,"icial ftp site, and uploaded the reference file to Hadoop HDFS. ``` bash; gatk-4.0.11.0/gatk BwaMemIndexImageCreator -I Homo_sapiens_assembly38.fasta -O Homo_sapiens_assembly38.fasta.img; ```. and then, we preprocess our pair end fastq files into unaligned ubam file as, ; ``` bash; java -jar picard.jar FastqToSam \; F1=R1.fastq.gz; F2=R2.fastq.gz; O=unaligned_reads.bam \; SM=sample001 \; PL=illumina \; RG=rg001; ```. For BwaSpark, we used,; ``` {bash}; ../gatk-4.0.11.0/gatk --java-options ""-Dgatk.spark.debug=true -XX:+PrintGCDetails"" BwaSpark -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test3.bam -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --spark-runner SPARK --spark-master spark://master:7077 -- --num-executors 4 --driver-memory 4g --executor-cores 10 --executor-memory 20g; ```. For ReadsPipelineSpark, we used, ; ``` {bash}; time_gatk ""ReadsPipelineSpark --tmp-dir /tmp --align true -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test10.vcf -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --known-sites hdfs://ns/user/root/Homo_sapiens_assembly38.dbsnp138.vcf -pairHMM AVX_LOGLESS_CACHING --max-reads-per-alignment-start 50"" 4 44 88g 12g; ```. #### Expected behavior; Both tool should end successfully without the specified error, and generated consistent result. #### Actual behavior; Both tool ends throwing out the same error, but the alignment ratio of the bam file from bwaspark is the same as the original bwa, and seems to be ok. The vcf variant number changes between each runs, and different from the result of stable non-spark official GATK 4 version. ``` bash; 2018-12-03 13:19:45 ERROR CoarseGrainedExecutorBackend:43 - RECEIVED SIGNAL TERM; ```. The vcf line number for several runs of the ReadsPipelineSpark,; ``` bash; 94488 test2.vcf.gz; 97426 test3.vcf.gz; 82279 test4.hg19.vcf.gz; 99403 test4.vcf.gz; 86384 test7_hg19.vcf.gz; 104854 test7.vcf.gz; 68824 test9.vcf.gz; 97810 test.vcf.gz; ```. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5481
https://github.com/broadinstitute/gatk/issues/5481:2713,Testability,test,test,2713,"cial ftp site, and uploaded the reference file to Hadoop HDFS. ``` bash; gatk-4.0.11.0/gatk BwaMemIndexImageCreator -I Homo_sapiens_assembly38.fasta -O Homo_sapiens_assembly38.fasta.img; ```. and then, we preprocess our pair end fastq files into unaligned ubam file as, ; ``` bash; java -jar picard.jar FastqToSam \; F1=R1.fastq.gz; F2=R2.fastq.gz; O=unaligned_reads.bam \; SM=sample001 \; PL=illumina \; RG=rg001; ```. For BwaSpark, we used,; ``` {bash}; ../gatk-4.0.11.0/gatk --java-options ""-Dgatk.spark.debug=true -XX:+PrintGCDetails"" BwaSpark -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test3.bam -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --spark-runner SPARK --spark-master spark://master:7077 -- --num-executors 4 --driver-memory 4g --executor-cores 10 --executor-memory 20g; ```. For ReadsPipelineSpark, we used, ; ``` {bash}; time_gatk ""ReadsPipelineSpark --tmp-dir /tmp --align true -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test10.vcf -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --known-sites hdfs://ns/user/root/Homo_sapiens_assembly38.dbsnp138.vcf -pairHMM AVX_LOGLESS_CACHING --max-reads-per-alignment-start 50"" 4 44 88g 12g; ```. #### Expected behavior; Both tool should end successfully without the specified error, and generated consistent result. #### Actual behavior; Both tool ends throwing out the same error, but the alignment ratio of the bam file from bwaspark is the same as the original bwa, and seems to be ok. The vcf variant number changes between each runs, and different from the result of stable non-spark official GATK 4 version. ``` bash; 2018-12-03 13:19:45 ERROR CoarseGrainedExecutorBackend:43 - RECEIVED SIGNAL TERM; ```. The vcf line number for several runs of the ReadsPipelineSpark,; ``` bash; 94488 test2.vcf.gz; 97426 test3.vcf.gz; 82279 test4.hg19.vcf.gz; 99403 test4.vcf.gz; 86384 test7_hg19.vcf.gz; 104854 test7.vcf.gz; 68824 test9.vcf.gz; 97810 test.vcf.gz; ```. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5481
https://github.com/broadinstitute/gatk/pull/5485:85,Deployability,release,released,85,"Relies on https://github.com/disq-bio/disq/pull/69, which has not yet been merged or released.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5485
https://github.com/broadinstitute/gatk/pull/5487:67,Security,validat,validation,67,"@takutosato I was checking the filter analysis outputs of every M2 validation and this filter hurts much, much more than it helps, probably because other developments have made it less necessary. Let's essentially turn it off by default.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5487
https://github.com/broadinstitute/gatk/issues/5488:1074,Availability,down,down,1074,"Based on my HaplotypeCaller GVCF performance evaluation, we are spending a significant amount of time in `ReferenceConfidenceModel.calcNIndelInformativeReads()', including upwards of 40% of the overall runtime on a bam under some conditions. To this end we have already done some performance work optimizing its constituent methods (#5469, #5470). Even with those changes it appears that the method can take upwards of 25% of the total runtime, which appears to be a consequence of the nature of the algorithm. It appears that the core of the problem appears to be associated with the calls we make to `isReadInformativeAboutIndelsOfSize()` which has a complexity overall of approximately `O(pileupsPerRegion * readsPerPileup * basesPerRead * maxIndelSize)` which ends up being a large number. One approach to fixing this problem be to rethink the repetitive operations we do for every pileup and instead do it on a per-read basis, and furthermore we could exploit the nature of the existing algorithm to avoid checking mismatches at the front of the read when we know that down the line we will fail out because of mismatches at the end of the read. Furthermore there is the broader philisophical question of whether there are changes that could be made to the algorithm that might carry a bigger risk of changing the results, like applying some heuristic based on the complexity of the reference sequence at a given site to reduce the amount of work we have to do.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5488
https://github.com/broadinstitute/gatk/issues/5488:1426,Energy Efficiency,reduce,reduce,1426,"Based on my HaplotypeCaller GVCF performance evaluation, we are spending a significant amount of time in `ReferenceConfidenceModel.calcNIndelInformativeReads()', including upwards of 40% of the overall runtime on a bam under some conditions. To this end we have already done some performance work optimizing its constituent methods (#5469, #5470). Even with those changes it appears that the method can take upwards of 25% of the total runtime, which appears to be a consequence of the nature of the algorithm. It appears that the core of the problem appears to be associated with the calls we make to `isReadInformativeAboutIndelsOfSize()` which has a complexity overall of approximately `O(pileupsPerRegion * readsPerPileup * basesPerRead * maxIndelSize)` which ends up being a large number. One approach to fixing this problem be to rethink the repetitive operations we do for every pileup and instead do it on a per-read basis, and furthermore we could exploit the nature of the existing algorithm to avoid checking mismatches at the front of the read when we know that down the line we will fail out because of mismatches at the end of the read. Furthermore there is the broader philisophical question of whether there are changes that could be made to the algorithm that might carry a bigger risk of changing the results, like applying some heuristic based on the complexity of the reference sequence at a given site to reduce the amount of work we have to do.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5488
https://github.com/broadinstitute/gatk/issues/5488:33,Performance,perform,performance,33,"Based on my HaplotypeCaller GVCF performance evaluation, we are spending a significant amount of time in `ReferenceConfidenceModel.calcNIndelInformativeReads()', including upwards of 40% of the overall runtime on a bam under some conditions. To this end we have already done some performance work optimizing its constituent methods (#5469, #5470). Even with those changes it appears that the method can take upwards of 25% of the total runtime, which appears to be a consequence of the nature of the algorithm. It appears that the core of the problem appears to be associated with the calls we make to `isReadInformativeAboutIndelsOfSize()` which has a complexity overall of approximately `O(pileupsPerRegion * readsPerPileup * basesPerRead * maxIndelSize)` which ends up being a large number. One approach to fixing this problem be to rethink the repetitive operations we do for every pileup and instead do it on a per-read basis, and furthermore we could exploit the nature of the existing algorithm to avoid checking mismatches at the front of the read when we know that down the line we will fail out because of mismatches at the end of the read. Furthermore there is the broader philisophical question of whether there are changes that could be made to the algorithm that might carry a bigger risk of changing the results, like applying some heuristic based on the complexity of the reference sequence at a given site to reduce the amount of work we have to do.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5488
https://github.com/broadinstitute/gatk/issues/5488:280,Performance,perform,performance,280,"Based on my HaplotypeCaller GVCF performance evaluation, we are spending a significant amount of time in `ReferenceConfidenceModel.calcNIndelInformativeReads()', including upwards of 40% of the overall runtime on a bam under some conditions. To this end we have already done some performance work optimizing its constituent methods (#5469, #5470). Even with those changes it appears that the method can take upwards of 25% of the total runtime, which appears to be a consequence of the nature of the algorithm. It appears that the core of the problem appears to be associated with the calls we make to `isReadInformativeAboutIndelsOfSize()` which has a complexity overall of approximately `O(pileupsPerRegion * readsPerPileup * basesPerRead * maxIndelSize)` which ends up being a large number. One approach to fixing this problem be to rethink the repetitive operations we do for every pileup and instead do it on a per-read basis, and furthermore we could exploit the nature of the existing algorithm to avoid checking mismatches at the front of the read when we know that down the line we will fail out because of mismatches at the end of the read. Furthermore there is the broader philisophical question of whether there are changes that could be made to the algorithm that might carry a bigger risk of changing the results, like applying some heuristic based on the complexity of the reference sequence at a given site to reduce the amount of work we have to do.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5488
https://github.com/broadinstitute/gatk/issues/5488:297,Performance,optimiz,optimizing,297,"Based on my HaplotypeCaller GVCF performance evaluation, we are spending a significant amount of time in `ReferenceConfidenceModel.calcNIndelInformativeReads()', including upwards of 40% of the overall runtime on a bam under some conditions. To this end we have already done some performance work optimizing its constituent methods (#5469, #5470). Even with those changes it appears that the method can take upwards of 25% of the total runtime, which appears to be a consequence of the nature of the algorithm. It appears that the core of the problem appears to be associated with the calls we make to `isReadInformativeAboutIndelsOfSize()` which has a complexity overall of approximately `O(pileupsPerRegion * readsPerPileup * basesPerRead * maxIndelSize)` which ends up being a large number. One approach to fixing this problem be to rethink the repetitive operations we do for every pileup and instead do it on a per-read basis, and furthermore we could exploit the nature of the existing algorithm to avoid checking mismatches at the front of the read when we know that down the line we will fail out because of mismatches at the end of the read. Furthermore there is the broader philisophical question of whether there are changes that could be made to the algorithm that might carry a bigger risk of changing the results, like applying some heuristic based on the complexity of the reference sequence at a given site to reduce the amount of work we have to do.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5488
https://github.com/broadinstitute/gatk/issues/5488:1005,Safety,avoid,avoid,1005,"Based on my HaplotypeCaller GVCF performance evaluation, we are spending a significant amount of time in `ReferenceConfidenceModel.calcNIndelInformativeReads()', including upwards of 40% of the overall runtime on a bam under some conditions. To this end we have already done some performance work optimizing its constituent methods (#5469, #5470). Even with those changes it appears that the method can take upwards of 25% of the total runtime, which appears to be a consequence of the nature of the algorithm. It appears that the core of the problem appears to be associated with the calls we make to `isReadInformativeAboutIndelsOfSize()` which has a complexity overall of approximately `O(pileupsPerRegion * readsPerPileup * basesPerRead * maxIndelSize)` which ends up being a large number. One approach to fixing this problem be to rethink the repetitive operations we do for every pileup and instead do it on a per-read basis, and furthermore we could exploit the nature of the existing algorithm to avoid checking mismatches at the front of the read when we know that down the line we will fail out because of mismatches at the end of the read. Furthermore there is the broader philisophical question of whether there are changes that could be made to the algorithm that might carry a bigger risk of changing the results, like applying some heuristic based on the complexity of the reference sequence at a given site to reduce the amount of work we have to do.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5488
https://github.com/broadinstitute/gatk/issues/5488:1298,Safety,risk,risk,1298,"Based on my HaplotypeCaller GVCF performance evaluation, we are spending a significant amount of time in `ReferenceConfidenceModel.calcNIndelInformativeReads()', including upwards of 40% of the overall runtime on a bam under some conditions. To this end we have already done some performance work optimizing its constituent methods (#5469, #5470). Even with those changes it appears that the method can take upwards of 25% of the total runtime, which appears to be a consequence of the nature of the algorithm. It appears that the core of the problem appears to be associated with the calls we make to `isReadInformativeAboutIndelsOfSize()` which has a complexity overall of approximately `O(pileupsPerRegion * readsPerPileup * basesPerRead * maxIndelSize)` which ends up being a large number. One approach to fixing this problem be to rethink the repetitive operations we do for every pileup and instead do it on a per-read basis, and furthermore we could exploit the nature of the existing algorithm to avoid checking mismatches at the front of the read when we know that down the line we will fail out because of mismatches at the end of the read. Furthermore there is the broader philisophical question of whether there are changes that could be made to the algorithm that might carry a bigger risk of changing the results, like applying some heuristic based on the complexity of the reference sequence at a given site to reduce the amount of work we have to do.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5488
https://github.com/broadinstitute/gatk/pull/5490:244,Availability,robust,robust,244,"Reverts the reversion in #5225, this time addressing the lexicographical ordering issue in #5217 at the WDL level by simply renaming gCNV output at the command line. If desired, we can eventually change gCNV itself to output filenames that are robust against lexicographic ordering, but this is low priority in my opinion. @vruano this is what we discussed last week. Tests pass on Travis, and I'm pretty sure this fix should work OK, but I have not done an actual run with enough samples to see the fix in action. Can I assign you to review once I get a chance to do this?. EDIT: Also went ahead and rolled an older PR #5304 into this one so I can test both at the same time. Closes #4724.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5490
https://github.com/broadinstitute/gatk/pull/5490:368,Testability,Test,Tests,368,"Reverts the reversion in #5225, this time addressing the lexicographical ordering issue in #5217 at the WDL level by simply renaming gCNV output at the command line. If desired, we can eventually change gCNV itself to output filenames that are robust against lexicographic ordering, but this is low priority in my opinion. @vruano this is what we discussed last week. Tests pass on Travis, and I'm pretty sure this fix should work OK, but I have not done an actual run with enough samples to see the fix in action. Can I assign you to review once I get a chance to do this?. EDIT: Also went ahead and rolled an older PR #5304 into this one so I can test both at the same time. Closes #4724.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5490
https://github.com/broadinstitute/gatk/pull/5490:649,Testability,test,test,649,"Reverts the reversion in #5225, this time addressing the lexicographical ordering issue in #5217 at the WDL level by simply renaming gCNV output at the command line. If desired, we can eventually change gCNV itself to output filenames that are robust against lexicographic ordering, but this is low priority in my opinion. @vruano this is what we discussed last week. Tests pass on Travis, and I'm pretty sure this fix should work OK, but I have not done an actual run with enough samples to see the fix in action. Can I assign you to review once I get a chance to do this?. EDIT: Also went ahead and rolled an older PR #5304 into this one so I can test both at the same time. Closes #4724.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5490
https://github.com/broadinstitute/gatk/pull/5490:117,Usability,simpl,simply,117,"Reverts the reversion in #5225, this time addressing the lexicographical ordering issue in #5217 at the WDL level by simply renaming gCNV output at the command line. If desired, we can eventually change gCNV itself to output filenames that are robust against lexicographic ordering, but this is low priority in my opinion. @vruano this is what we discussed last week. Tests pass on Travis, and I'm pretty sure this fix should work OK, but I have not done an actual run with enough samples to see the fix in action. Can I assign you to review once I get a chance to do this?. EDIT: Also went ahead and rolled an older PR #5304 into this one so I can test both at the same time. Closes #4724.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5490
https://github.com/broadinstitute/gatk/pull/5491:304,Performance,cache,cache,304,"- Fixed a NPE in the `problem` variant case, which is now resolved.; This was due to not filling out the dataSourceName field.; - Tested with b37 gnomAD matching against b37 variants with hg19 data; sources.; - Fixed some issues with the default problem variant annotations.; - Adding in per-data source cache settings.; - Fixing logger in DataSourceUtils. Fixes #5456",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5491
https://github.com/broadinstitute/gatk/pull/5491:130,Testability,Test,Tested,130,"- Fixed a NPE in the `problem` variant case, which is now resolved.; This was due to not filling out the dataSourceName field.; - Tested with b37 gnomAD matching against b37 variants with hg19 data; sources.; - Fixed some issues with the default problem variant annotations.; - Adding in per-data source cache settings.; - Fixing logger in DataSourceUtils. Fixes #5456",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5491
https://github.com/broadinstitute/gatk/pull/5491:330,Testability,log,logger,330,"- Fixed a NPE in the `problem` variant case, which is now resolved.; This was due to not filling out the dataSourceName field.; - Tested with b37 gnomAD matching against b37 variants with hg19 data; sources.; - Fixed some issues with the default problem variant annotations.; - Adding in per-data source cache settings.; - Fixing logger in DataSourceUtils. Fixes #5456",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5491
https://github.com/broadinstitute/gatk/issues/5492:319,Security,access,access,319,"When `ReadPosRankSumTest.getReadPosition` encounters the second of two deletions with two bases in between it hits the following code:; ```; if ( AlignmentUtils.isInsideDeletion(read.getCigar(), offset) ) {; return OptionalDouble.of(INVALID_ELEMENT_FROM_READ);; }; ```; which returns negative infinity. Those with TCGA access can reproduce the issue by running on the TCGA exome pair ESCA-IG-A3YB-TP-NB at 15:34525804-34525810.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5492
https://github.com/broadinstitute/gatk/issues/5494:29,Deployability,pipeline,pipelines,29,"For the automated regression pipelines, I would like to add `bc` and possibly other programs to the docker image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5494
https://github.com/broadinstitute/gatk/issues/5499:64,Testability,log,log,64,"i386 build fails on FreeBSD: ```Too many open files```. See the log: http://beefy11.nyi.freebsd.org/data/head-i386-default/p486731_s341608/logs/gatk-4.0.11.0.log. It seems that the build tools that you choose are broken, or something is broken in your build process.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499
https://github.com/broadinstitute/gatk/issues/5499:139,Testability,log,logs,139,"i386 build fails on FreeBSD: ```Too many open files```. See the log: http://beefy11.nyi.freebsd.org/data/head-i386-default/p486731_s341608/logs/gatk-4.0.11.0.log. It seems that the build tools that you choose are broken, or something is broken in your build process.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499
https://github.com/broadinstitute/gatk/issues/5499:158,Testability,log,log,158,"i386 build fails on FreeBSD: ```Too many open files```. See the log: http://beefy11.nyi.freebsd.org/data/head-i386-default/p486731_s341608/logs/gatk-4.0.11.0.log. It seems that the build tools that you choose are broken, or something is broken in your build process.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499
https://github.com/broadinstitute/gatk/pull/5501:423,Availability,failure,failure,423,"The more the best alignment mismatches its assigned reference location, the more stringent the filter becomes, that is, the lower the threshold for secondary alignments to constitute a multi-mapping becomes. If the best mapping mismatches at one base and the second best mismatches at three, that is very different from the best mismatching at four bases and the second best mismatching at six. @takutosato This corrects a failure to filter some very obvious clustered events mapping artifacts. For example, there were 11 ""events"" within 150 bp in one MC3 sample that were all being called as false negatives for M2 and true positives for M1. If these are real, I will eat dog food.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5501
https://github.com/broadinstitute/gatk/pull/5508:74,Testability,test,test,74,Fixes #5476. This change requires #5437 to work properly. Not sure how to test this yet.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5508
https://github.com/broadinstitute/gatk/issues/5509:10,Deployability,update,update,10,"We should update any public docs that contain recommendations on how to write JEXL expressions to reflect the align with the doc changes made in https://github.com/broadinstitute/gatk/pull/5422. Specifically, we should recommend that multiple simple expressions be used in place of a single compound expression. Here are @sooheelee 's notes on what docs need to change:. Here are the documents we should also update to reflect these updates:. https://gatkforums.broadinstitute.org/gatk/discussion/1255/using-jexl-to-apply-hard-filters-or-select-variants-based-on-annotation-values; https://software.broadinstitute.org/gatk/documentation/article?id=11080; Also, here is a list of documents with the jexl tag:; https://gatkforums.broadinstitute.org/gatk/discussions/tagged/jexl. I have put in a word of caution in https://gatkforums.broadinstitute.org/gatk/discussion/12350/how-to-filter-on-genotype-using-variantfiltration/p1?new=1.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5509
https://github.com/broadinstitute/gatk/issues/5509:409,Deployability,update,update,409,"We should update any public docs that contain recommendations on how to write JEXL expressions to reflect the align with the doc changes made in https://github.com/broadinstitute/gatk/pull/5422. Specifically, we should recommend that multiple simple expressions be used in place of a single compound expression. Here are @sooheelee 's notes on what docs need to change:. Here are the documents we should also update to reflect these updates:. https://gatkforums.broadinstitute.org/gatk/discussion/1255/using-jexl-to-apply-hard-filters-or-select-variants-based-on-annotation-values; https://software.broadinstitute.org/gatk/documentation/article?id=11080; Also, here is a list of documents with the jexl tag:; https://gatkforums.broadinstitute.org/gatk/discussions/tagged/jexl. I have put in a word of caution in https://gatkforums.broadinstitute.org/gatk/discussion/12350/how-to-filter-on-genotype-using-variantfiltration/p1?new=1.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5509
https://github.com/broadinstitute/gatk/issues/5509:433,Deployability,update,updates,433,"We should update any public docs that contain recommendations on how to write JEXL expressions to reflect the align with the doc changes made in https://github.com/broadinstitute/gatk/pull/5422. Specifically, we should recommend that multiple simple expressions be used in place of a single compound expression. Here are @sooheelee 's notes on what docs need to change:. Here are the documents we should also update to reflect these updates:. https://gatkforums.broadinstitute.org/gatk/discussion/1255/using-jexl-to-apply-hard-filters-or-select-variants-based-on-annotation-values; https://software.broadinstitute.org/gatk/documentation/article?id=11080; Also, here is a list of documents with the jexl tag:; https://gatkforums.broadinstitute.org/gatk/discussions/tagged/jexl. I have put in a word of caution in https://gatkforums.broadinstitute.org/gatk/discussion/12350/how-to-filter-on-genotype-using-variantfiltration/p1?new=1.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5509
https://github.com/broadinstitute/gatk/issues/5509:243,Usability,simpl,simple,243,"We should update any public docs that contain recommendations on how to write JEXL expressions to reflect the align with the doc changes made in https://github.com/broadinstitute/gatk/pull/5422. Specifically, we should recommend that multiple simple expressions be used in place of a single compound expression. Here are @sooheelee 's notes on what docs need to change:. Here are the documents we should also update to reflect these updates:. https://gatkforums.broadinstitute.org/gatk/discussion/1255/using-jexl-to-apply-hard-filters-or-select-variants-based-on-annotation-values; https://software.broadinstitute.org/gatk/documentation/article?id=11080; Also, here is a list of documents with the jexl tag:; https://gatkforums.broadinstitute.org/gatk/discussions/tagged/jexl. I have put in a word of caution in https://gatkforums.broadinstitute.org/gatk/discussion/12350/how-to-filter-on-genotype-using-variantfiltration/p1?new=1.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5509
https://github.com/broadinstitute/gatk/issues/5511:130,Availability,failure,failures,130,"Hello.; I am not sure if this is a bug or simply a tool version problem but when running the tests with `./gradlew test` I get 18 failures with the `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest` class. I wanted to make sure GATK was working right when compiled it from source to have a working base a I intend to try out some experimental modifications to the code. ## Bug Report. ### Affected tool(s) or class(es); `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest`. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of [11.12.18]. ### Description; The following commands were used to build and test GATK; ```; git pull; ./gradlew clean; ./gradlew bundle; ./gradlew test; ```; The tests resulted in 18 failed as can be seen in the attached file. ; [Test results - Class org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip](https://github.com/broadinstitute/gatk/files/2667609/Test.results.-.Class.org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip). If this is normal (expected) when building from the last commit on master you can close this issue. For experimental development should I use the most recent release or can I work from the most recent commit on master ?. #### Steps to reproduce; see commands above description. The problem could be from a library or java version maybe. I run a Ubuntu 16.04 LTS desktop.; ```; uname -a; Linux A13PC04 4.4.0-140-generic #166-Ubuntu SMP Wed Nov 14 20:09:47 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. javac -version; javac 1.8.0_102. java -version; java version ""1.8.0_102""; Java(TM) SE Runtime Environment (build 1.8.0_102-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode); ```. #### Expected behavior; I was expecting the tests to pass. #### Actual behavior; 18 tests failed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5511
https://github.com/broadinstitute/gatk/issues/5511:599,Deployability,release,release,599,"Hello.; I am not sure if this is a bug or simply a tool version problem but when running the tests with `./gradlew test` I get 18 failures with the `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest` class. I wanted to make sure GATK was working right when compiled it from source to have a working base a I intend to try out some experimental modifications to the code. ## Bug Report. ### Affected tool(s) or class(es); `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest`. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of [11.12.18]. ### Description; The following commands were used to build and test GATK; ```; git pull; ./gradlew clean; ./gradlew bundle; ./gradlew test; ```; The tests resulted in 18 failed as can be seen in the attached file. ; [Test results - Class org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip](https://github.com/broadinstitute/gatk/files/2667609/Test.results.-.Class.org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip). If this is normal (expected) when building from the last commit on master you can close this issue. For experimental development should I use the most recent release or can I work from the most recent commit on master ?. #### Steps to reproduce; see commands above description. The problem could be from a library or java version maybe. I run a Ubuntu 16.04 LTS desktop.; ```; uname -a; Linux A13PC04 4.4.0-140-generic #166-Ubuntu SMP Wed Nov 14 20:09:47 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. javac -version; javac 1.8.0_102. java -version; java version ""1.8.0_102""; Java(TM) SE Runtime Environment (build 1.8.0_102-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode); ```. #### Expected behavior; I was expecting the tests to pass. #### Actual behavior; 18 tests failed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5511
https://github.com/broadinstitute/gatk/issues/5511:1343,Deployability,release,release,1343,"Hello.; I am not sure if this is a bug or simply a tool version problem but when running the tests with `./gradlew test` I get 18 failures with the `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest` class. I wanted to make sure GATK was working right when compiled it from source to have a working base a I intend to try out some experimental modifications to the code. ## Bug Report. ### Affected tool(s) or class(es); `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest`. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of [11.12.18]. ### Description; The following commands were used to build and test GATK; ```; git pull; ./gradlew clean; ./gradlew bundle; ./gradlew test; ```; The tests resulted in 18 failed as can be seen in the attached file. ; [Test results - Class org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip](https://github.com/broadinstitute/gatk/files/2667609/Test.results.-.Class.org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip). If this is normal (expected) when building from the last commit on master you can close this issue. For experimental development should I use the most recent release or can I work from the most recent commit on master ?. #### Steps to reproduce; see commands above description. The problem could be from a library or java version maybe. I run a Ubuntu 16.04 LTS desktop.; ```; uname -a; Linux A13PC04 4.4.0-140-generic #166-Ubuntu SMP Wed Nov 14 20:09:47 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. javac -version; javac 1.8.0_102. java -version; java version ""1.8.0_102""; Java(TM) SE Runtime Environment (build 1.8.0_102-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode); ```. #### Expected behavior; I was expecting the tests to pass. #### Actual behavior; 18 tests failed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5511
https://github.com/broadinstitute/gatk/issues/5511:93,Testability,test,tests,93,"Hello.; I am not sure if this is a bug or simply a tool version problem but when running the tests with `./gradlew test` I get 18 failures with the `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest` class. I wanted to make sure GATK was working right when compiled it from source to have a working base a I intend to try out some experimental modifications to the code. ## Bug Report. ### Affected tool(s) or class(es); `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest`. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of [11.12.18]. ### Description; The following commands were used to build and test GATK; ```; git pull; ./gradlew clean; ./gradlew bundle; ./gradlew test; ```; The tests resulted in 18 failed as can be seen in the attached file. ; [Test results - Class org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip](https://github.com/broadinstitute/gatk/files/2667609/Test.results.-.Class.org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip). If this is normal (expected) when building from the last commit on master you can close this issue. For experimental development should I use the most recent release or can I work from the most recent commit on master ?. #### Steps to reproduce; see commands above description. The problem could be from a library or java version maybe. I run a Ubuntu 16.04 LTS desktop.; ```; uname -a; Linux A13PC04 4.4.0-140-generic #166-Ubuntu SMP Wed Nov 14 20:09:47 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. javac -version; javac 1.8.0_102. java -version; java version ""1.8.0_102""; Java(TM) SE Runtime Environment (build 1.8.0_102-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode); ```. #### Expected behavior; I was expecting the tests to pass. #### Actual behavior; 18 tests failed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5511
https://github.com/broadinstitute/gatk/issues/5511:115,Testability,test,test,115,"Hello.; I am not sure if this is a bug or simply a tool version problem but when running the tests with `./gradlew test` I get 18 failures with the `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest` class. I wanted to make sure GATK was working right when compiled it from source to have a working base a I intend to try out some experimental modifications to the code. ## Bug Report. ### Affected tool(s) or class(es); `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest`. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of [11.12.18]. ### Description; The following commands were used to build and test GATK; ```; git pull; ./gradlew clean; ./gradlew bundle; ./gradlew test; ```; The tests resulted in 18 failed as can be seen in the attached file. ; [Test results - Class org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip](https://github.com/broadinstitute/gatk/files/2667609/Test.results.-.Class.org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip). If this is normal (expected) when building from the last commit on master you can close this issue. For experimental development should I use the most recent release or can I work from the most recent commit on master ?. #### Steps to reproduce; see commands above description. The problem could be from a library or java version maybe. I run a Ubuntu 16.04 LTS desktop.; ```; uname -a; Linux A13PC04 4.4.0-140-generic #166-Ubuntu SMP Wed Nov 14 20:09:47 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. javac -version; javac 1.8.0_102. java -version; java version ""1.8.0_102""; Java(TM) SE Runtime Environment (build 1.8.0_102-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode); ```. #### Expected behavior; I was expecting the tests to pass. #### Actual behavior; 18 tests failed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5511
https://github.com/broadinstitute/gatk/issues/5511:735,Testability,test,test,735,"Hello.; I am not sure if this is a bug or simply a tool version problem but when running the tests with `./gradlew test` I get 18 failures with the `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest` class. I wanted to make sure GATK was working right when compiled it from source to have a working base a I intend to try out some experimental modifications to the code. ## Bug Report. ### Affected tool(s) or class(es); `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest`. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of [11.12.18]. ### Description; The following commands were used to build and test GATK; ```; git pull; ./gradlew clean; ./gradlew bundle; ./gradlew test; ```; The tests resulted in 18 failed as can be seen in the attached file. ; [Test results - Class org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip](https://github.com/broadinstitute/gatk/files/2667609/Test.results.-.Class.org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip). If this is normal (expected) when building from the last commit on master you can close this issue. For experimental development should I use the most recent release or can I work from the most recent commit on master ?. #### Steps to reproduce; see commands above description. The problem could be from a library or java version maybe. I run a Ubuntu 16.04 LTS desktop.; ```; uname -a; Linux A13PC04 4.4.0-140-generic #166-Ubuntu SMP Wed Nov 14 20:09:47 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. javac -version; javac 1.8.0_102. java -version; java version ""1.8.0_102""; Java(TM) SE Runtime Environment (build 1.8.0_102-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode); ```. #### Expected behavior; I was expecting the tests to pass. #### Actual behavior; 18 tests failed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5511
https://github.com/broadinstitute/gatk/issues/5511:806,Testability,test,test,806,"Hello.; I am not sure if this is a bug or simply a tool version problem but when running the tests with `./gradlew test` I get 18 failures with the `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest` class. I wanted to make sure GATK was working right when compiled it from source to have a working base a I intend to try out some experimental modifications to the code. ## Bug Report. ### Affected tool(s) or class(es); `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest`. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of [11.12.18]. ### Description; The following commands were used to build and test GATK; ```; git pull; ./gradlew clean; ./gradlew bundle; ./gradlew test; ```; The tests resulted in 18 failed as can be seen in the attached file. ; [Test results - Class org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip](https://github.com/broadinstitute/gatk/files/2667609/Test.results.-.Class.org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip). If this is normal (expected) when building from the last commit on master you can close this issue. For experimental development should I use the most recent release or can I work from the most recent commit on master ?. #### Steps to reproduce; see commands above description. The problem could be from a library or java version maybe. I run a Ubuntu 16.04 LTS desktop.; ```; uname -a; Linux A13PC04 4.4.0-140-generic #166-Ubuntu SMP Wed Nov 14 20:09:47 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. javac -version; javac 1.8.0_102. java -version; java version ""1.8.0_102""; Java(TM) SE Runtime Environment (build 1.8.0_102-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode); ```. #### Expected behavior; I was expecting the tests to pass. #### Actual behavior; 18 tests failed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5511
https://github.com/broadinstitute/gatk/issues/5511:821,Testability,test,tests,821,"Hello.; I am not sure if this is a bug or simply a tool version problem but when running the tests with `./gradlew test` I get 18 failures with the `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest` class. I wanted to make sure GATK was working right when compiled it from source to have a working base a I intend to try out some experimental modifications to the code. ## Bug Report. ### Affected tool(s) or class(es); `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest`. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of [11.12.18]. ### Description; The following commands were used to build and test GATK; ```; git pull; ./gradlew clean; ./gradlew bundle; ./gradlew test; ```; The tests resulted in 18 failed as can be seen in the attached file. ; [Test results - Class org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip](https://github.com/broadinstitute/gatk/files/2667609/Test.results.-.Class.org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip). If this is normal (expected) when building from the last commit on master you can close this issue. For experimental development should I use the most recent release or can I work from the most recent commit on master ?. #### Steps to reproduce; see commands above description. The problem could be from a library or java version maybe. I run a Ubuntu 16.04 LTS desktop.; ```; uname -a; Linux A13PC04 4.4.0-140-generic #166-Ubuntu SMP Wed Nov 14 20:09:47 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. javac -version; javac 1.8.0_102. java -version; java version ""1.8.0_102""; Java(TM) SE Runtime Environment (build 1.8.0_102-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode); ```. #### Expected behavior; I was expecting the tests to pass. #### Actual behavior; 18 tests failed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5511
https://github.com/broadinstitute/gatk/issues/5511:889,Testability,Test,Test,889,"Hello.; I am not sure if this is a bug or simply a tool version problem but when running the tests with `./gradlew test` I get 18 failures with the `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest` class. I wanted to make sure GATK was working right when compiled it from source to have a working base a I intend to try out some experimental modifications to the code. ## Bug Report. ### Affected tool(s) or class(es); `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest`. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of [11.12.18]. ### Description; The following commands were used to build and test GATK; ```; git pull; ./gradlew clean; ./gradlew bundle; ./gradlew test; ```; The tests resulted in 18 failed as can be seen in the attached file. ; [Test results - Class org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip](https://github.com/broadinstitute/gatk/files/2667609/Test.results.-.Class.org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip). If this is normal (expected) when building from the last commit on master you can close this issue. For experimental development should I use the most recent release or can I work from the most recent commit on master ?. #### Steps to reproduce; see commands above description. The problem could be from a library or java version maybe. I run a Ubuntu 16.04 LTS desktop.; ```; uname -a; Linux A13PC04 4.4.0-140-generic #166-Ubuntu SMP Wed Nov 14 20:09:47 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. javac -version; javac 1.8.0_102. java -version; java version ""1.8.0_102""; Java(TM) SE Runtime Environment (build 1.8.0_102-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode); ```. #### Expected behavior; I was expecting the tests to pass. #### Actual behavior; 18 tests failed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5511
https://github.com/broadinstitute/gatk/issues/5511:1063,Testability,Test,Test,1063,"Hello.; I am not sure if this is a bug or simply a tool version problem but when running the tests with `./gradlew test` I get 18 failures with the `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest` class. I wanted to make sure GATK was working right when compiled it from source to have a working base a I intend to try out some experimental modifications to the code. ## Bug Report. ### Affected tool(s) or class(es); `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest`. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of [11.12.18]. ### Description; The following commands were used to build and test GATK; ```; git pull; ./gradlew clean; ./gradlew bundle; ./gradlew test; ```; The tests resulted in 18 failed as can be seen in the attached file. ; [Test results - Class org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip](https://github.com/broadinstitute/gatk/files/2667609/Test.results.-.Class.org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip). If this is normal (expected) when building from the last commit on master you can close this issue. For experimental development should I use the most recent release or can I work from the most recent commit on master ?. #### Steps to reproduce; see commands above description. The problem could be from a library or java version maybe. I run a Ubuntu 16.04 LTS desktop.; ```; uname -a; Linux A13PC04 4.4.0-140-generic #166-Ubuntu SMP Wed Nov 14 20:09:47 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. javac -version; javac 1.8.0_102. java -version; java version ""1.8.0_102""; Java(TM) SE Runtime Environment (build 1.8.0_102-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode); ```. #### Expected behavior; I was expecting the tests to pass. #### Actual behavior; 18 tests failed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5511
https://github.com/broadinstitute/gatk/issues/5511:1925,Testability,test,tests,1925,"Hello.; I am not sure if this is a bug or simply a tool version problem but when running the tests with `./gradlew test` I get 18 failures with the `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest` class. I wanted to make sure GATK was working right when compiled it from source to have a working base a I intend to try out some experimental modifications to the code. ## Bug Report. ### Affected tool(s) or class(es); `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest`. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of [11.12.18]. ### Description; The following commands were used to build and test GATK; ```; git pull; ./gradlew clean; ./gradlew bundle; ./gradlew test; ```; The tests resulted in 18 failed as can be seen in the attached file. ; [Test results - Class org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip](https://github.com/broadinstitute/gatk/files/2667609/Test.results.-.Class.org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip). If this is normal (expected) when building from the last commit on master you can close this issue. For experimental development should I use the most recent release or can I work from the most recent commit on master ?. #### Steps to reproduce; see commands above description. The problem could be from a library or java version maybe. I run a Ubuntu 16.04 LTS desktop.; ```; uname -a; Linux A13PC04 4.4.0-140-generic #166-Ubuntu SMP Wed Nov 14 20:09:47 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. javac -version; javac 1.8.0_102. java -version; java version ""1.8.0_102""; Java(TM) SE Runtime Environment (build 1.8.0_102-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode); ```. #### Expected behavior; I was expecting the tests to pass. #### Actual behavior; 18 tests failed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5511
https://github.com/broadinstitute/gatk/issues/5511:1965,Testability,test,tests,1965,"Hello.; I am not sure if this is a bug or simply a tool version problem but when running the tests with `./gradlew test` I get 18 failures with the `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest` class. I wanted to make sure GATK was working right when compiled it from source to have a working base a I intend to try out some experimental modifications to the code. ## Bug Report. ### Affected tool(s) or class(es); `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest`. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of [11.12.18]. ### Description; The following commands were used to build and test GATK; ```; git pull; ./gradlew clean; ./gradlew bundle; ./gradlew test; ```; The tests resulted in 18 failed as can be seen in the attached file. ; [Test results - Class org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip](https://github.com/broadinstitute/gatk/files/2667609/Test.results.-.Class.org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip). If this is normal (expected) when building from the last commit on master you can close this issue. For experimental development should I use the most recent release or can I work from the most recent commit on master ?. #### Steps to reproduce; see commands above description. The problem could be from a library or java version maybe. I run a Ubuntu 16.04 LTS desktop.; ```; uname -a; Linux A13PC04 4.4.0-140-generic #166-Ubuntu SMP Wed Nov 14 20:09:47 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. javac -version; javac 1.8.0_102. java -version; java version ""1.8.0_102""; Java(TM) SE Runtime Environment (build 1.8.0_102-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode); ```. #### Expected behavior; I was expecting the tests to pass. #### Actual behavior; 18 tests failed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5511
https://github.com/broadinstitute/gatk/issues/5511:42,Usability,simpl,simply,42,"Hello.; I am not sure if this is a bug or simply a tool version problem but when running the tests with `./gradlew test` I get 18 failures with the `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest` class. I wanted to make sure GATK was working right when compiled it from source to have a working base a I intend to try out some experimental modifications to the code. ## Bug Report. ### Affected tool(s) or class(es); `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest`. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of [11.12.18]. ### Description; The following commands were used to build and test GATK; ```; git pull; ./gradlew clean; ./gradlew bundle; ./gradlew test; ```; The tests resulted in 18 failed as can be seen in the attached file. ; [Test results - Class org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip](https://github.com/broadinstitute/gatk/files/2667609/Test.results.-.Class.org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip). If this is normal (expected) when building from the last commit on master you can close this issue. For experimental development should I use the most recent release or can I work from the most recent commit on master ?. #### Steps to reproduce; see commands above description. The problem could be from a library or java version maybe. I run a Ubuntu 16.04 LTS desktop.; ```; uname -a; Linux A13PC04 4.4.0-140-generic #166-Ubuntu SMP Wed Nov 14 20:09:47 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. javac -version; javac 1.8.0_102. java -version; java version ""1.8.0_102""; Java(TM) SE Runtime Environment (build 1.8.0_102-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode); ```. #### Expected behavior; I was expecting the tests to pass. #### Actual behavior; 18 tests failed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5511
https://github.com/broadinstitute/gatk/issues/5512:447,Deployability,Integrat,Integrated,447,"Most tools that write vcf outputs call `GATKTool.createVCFWriter(final File outFile)` because they use `File` arguments. Once https://github.com/broadinstitute/gatk/pull/5378 is merged, these calls should be replaced with calls to `GATKTool.createVCFWriter(final Path outFile)`, and then the `File` method can be removed. It would be nice if during the same pass we:. - Change the type of the `File` arguments with use the new `GATKUri` class.; - Integrated the use of `getDefaultToolVCFHeaderLines` use of for all the writers (see https://github.com/broadinstitute/gatk/issues/5493); - Tag each of these outputs as ""gcs-enabled"".",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5512
https://github.com/broadinstitute/gatk/issues/5512:447,Integrability,Integrat,Integrated,447,"Most tools that write vcf outputs call `GATKTool.createVCFWriter(final File outFile)` because they use `File` arguments. Once https://github.com/broadinstitute/gatk/pull/5378 is merged, these calls should be replaced with calls to `GATKTool.createVCFWriter(final Path outFile)`, and then the `File` method can be removed. It would be nice if during the same pass we:. - Change the type of the `File` arguments with use the new `GATKUri` class.; - Integrated the use of `getDefaultToolVCFHeaderLines` use of for all the writers (see https://github.com/broadinstitute/gatk/issues/5493); - Tag each of these outputs as ""gcs-enabled"".",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5512
https://github.com/broadinstitute/gatk/issues/5513:104,Deployability,release,release,104,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); - [x] Latest public release version: 4.0.11.0; - [ ] Latest master branch as of [date of test?]. ### Description ; The output vcf for a few samples looks like this:. ```; chrM 151 . CT TC . PASS DP=3420;ECNT=23;POP_AF=4.000e-03;P_CONTAM=0.00;TLOD=14304.21 GT:AD:AF:DP:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:ORIGINAL_CONTIG_MISMATCH:SA_MAP_AF:SA_POST_PROB:SB 0/1:27,3242:0.992:3269:13,1545:14,1697:30,30:317,335:60:26:0:0.990,0.990,0.992:0.045,0.015,0.940:9,18,1541,1701; chrM 152 . T C . chimeric_original_alignment DP=3358;ECNT=23;POP_AF=4.000e-03;P_CONTAM=0.00;TLOD=46.40 GT:AD:AF:DP:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:ORIGINAL_CONTIG_MISMATCH:PGT:PID:POTENTIAL_POLYMORPHIC_NUMT:SA_MAP_AF:SA_POST_PROB:SB 0/1:250,25:0.099:275:119,13:131,12:30,30:336,317:60:29:25:0|1:8660_C_T:true:0.091,0.061,0.091:2.722e-03,0.035,0.962:112,138,7,18; ```. ```; chrM 151 . CT TC . PASS DP=1867;ECNT=20;POP_AF=4.000e-03;P_CONTAM=0.00;TLOD=6145.34 GT:AD:AF:DP:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:ORIGINAL_CONTIG_MISMATCH:SA_MAP_AF:SA_POST_PROB:SB 0/1:13,1792:0.993:1805:7,879:6,913:30,30:441,442:60:39:0:0.990,0.990,0.993:0.026,0.024,0.950:5,8,745,1047; chrM 152 . T C . PASS DP=1847;ECNT=20;POP_AF=4.000e-03;P_CONTAM=0.00;TLOD=12.96 GT:AD:AF:DP:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:ORIGINAL_CONTIG_MISMATCH:SA_MAP_AF:SA_POST_PROB:SB 0/1:0,1755:0.999:1755:0,862:0,893:0,30:0,442:60:39:6:0.990,0.990,1.00:0.027,0.027,0.946:0,0,726,1029; ```. Note that site 152 is a T->C that is also captured in the MNP at site 151 CT->TC. In one case site 152 is filtered, but in the other it passes, but in both cases the MNP passes. . #### Steps to reproduce; @klaricch Could you please post the input BAMs into the Mutect task as well as the output VCFs from that task? Could you also post the ""script"" generated by Cromwell that will show what command Cromwell actually ran at this point? Thanks!. #### Expected behavior; I'm not sure what should happen in this case, but the two o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5513
https://github.com/broadinstitute/gatk/issues/5513:173,Testability,test,test,173,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); - [x] Latest public release version: 4.0.11.0; - [ ] Latest master branch as of [date of test?]. ### Description ; The output vcf for a few samples looks like this:. ```; chrM 151 . CT TC . PASS DP=3420;ECNT=23;POP_AF=4.000e-03;P_CONTAM=0.00;TLOD=14304.21 GT:AD:AF:DP:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:ORIGINAL_CONTIG_MISMATCH:SA_MAP_AF:SA_POST_PROB:SB 0/1:27,3242:0.992:3269:13,1545:14,1697:30,30:317,335:60:26:0:0.990,0.990,0.992:0.045,0.015,0.940:9,18,1541,1701; chrM 152 . T C . chimeric_original_alignment DP=3358;ECNT=23;POP_AF=4.000e-03;P_CONTAM=0.00;TLOD=46.40 GT:AD:AF:DP:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:ORIGINAL_CONTIG_MISMATCH:PGT:PID:POTENTIAL_POLYMORPHIC_NUMT:SA_MAP_AF:SA_POST_PROB:SB 0/1:250,25:0.099:275:119,13:131,12:30,30:336,317:60:29:25:0|1:8660_C_T:true:0.091,0.061,0.091:2.722e-03,0.035,0.962:112,138,7,18; ```. ```; chrM 151 . CT TC . PASS DP=1867;ECNT=20;POP_AF=4.000e-03;P_CONTAM=0.00;TLOD=6145.34 GT:AD:AF:DP:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:ORIGINAL_CONTIG_MISMATCH:SA_MAP_AF:SA_POST_PROB:SB 0/1:13,1792:0.993:1805:7,879:6,913:30,30:441,442:60:39:0:0.990,0.990,0.993:0.026,0.024,0.950:5,8,745,1047; chrM 152 . T C . PASS DP=1847;ECNT=20;POP_AF=4.000e-03;P_CONTAM=0.00;TLOD=12.96 GT:AD:AF:DP:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:ORIGINAL_CONTIG_MISMATCH:SA_MAP_AF:SA_POST_PROB:SB 0/1:0,1755:0.999:1755:0,862:0,893:0,30:0,442:60:39:6:0.990,0.990,1.00:0.027,0.027,0.946:0,0,726,1029; ```. Note that site 152 is a T->C that is also captured in the MNP at site 151 CT->TC. In one case site 152 is filtered, but in the other it passes, but in both cases the MNP passes. . #### Steps to reproduce; @klaricch Could you please post the input BAMs into the Mutect task as well as the output VCFs from that task? Could you also post the ""script"" generated by Cromwell that will show what command Cromwell actually ran at this point? Thanks!. #### Expected behavior; I'm not sure what should happen in this case, but the two o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5513
https://github.com/broadinstitute/gatk/pull/5514:423,Deployability,Update,Updated,423,"* Added liftover chain file creation script.; * Added WDLs and some arguments to lift over gnomAD; * Added chain file for b37->hg38 and arguments for liftover.; * Limited to 1000 records in memory.; * Added stack trace option to all wdls and sub tasks.; * Fixed output to be consistent with local files for indexing.; * Added timing information on wdls.; * Added a wdl/json to create a TSV from gnomAD allele freq data.; * Updated indexFeatureFile wdl, added params for run to index gnomAD.; * Added json file for indexing a large gnomad file.; * Fixed critical issues with NIO data sources.; * Updates to the test script to save output and point to full cloud data.; * Added some logging hooks to SeekableByteChannelPrefetcher. I haven't reviewed this since I made the changes to it to see what should stay, so it may need a fair bit of work.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5514
https://github.com/broadinstitute/gatk/pull/5514:595,Deployability,Update,Updates,595,"* Added liftover chain file creation script.; * Added WDLs and some arguments to lift over gnomAD; * Added chain file for b37->hg38 and arguments for liftover.; * Limited to 1000 records in memory.; * Added stack trace option to all wdls and sub tasks.; * Fixed output to be consistent with local files for indexing.; * Added timing information on wdls.; * Added a wdl/json to create a TSV from gnomAD allele freq data.; * Updated indexFeatureFile wdl, added params for run to index gnomAD.; * Added json file for indexing a large gnomad file.; * Fixed critical issues with NIO data sources.; * Updates to the test script to save output and point to full cloud data.; * Added some logging hooks to SeekableByteChannelPrefetcher. I haven't reviewed this since I made the changes to it to see what should stay, so it may need a fair bit of work.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5514
https://github.com/broadinstitute/gatk/pull/5514:610,Testability,test,test,610,"* Added liftover chain file creation script.; * Added WDLs and some arguments to lift over gnomAD; * Added chain file for b37->hg38 and arguments for liftover.; * Limited to 1000 records in memory.; * Added stack trace option to all wdls and sub tasks.; * Fixed output to be consistent with local files for indexing.; * Added timing information on wdls.; * Added a wdl/json to create a TSV from gnomAD allele freq data.; * Updated indexFeatureFile wdl, added params for run to index gnomAD.; * Added json file for indexing a large gnomad file.; * Fixed critical issues with NIO data sources.; * Updates to the test script to save output and point to full cloud data.; * Added some logging hooks to SeekableByteChannelPrefetcher. I haven't reviewed this since I made the changes to it to see what should stay, so it may need a fair bit of work.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5514
https://github.com/broadinstitute/gatk/pull/5514:681,Testability,log,logging,681,"* Added liftover chain file creation script.; * Added WDLs and some arguments to lift over gnomAD; * Added chain file for b37->hg38 and arguments for liftover.; * Limited to 1000 records in memory.; * Added stack trace option to all wdls and sub tasks.; * Fixed output to be consistent with local files for indexing.; * Added timing information on wdls.; * Added a wdl/json to create a TSV from gnomAD allele freq data.; * Updated indexFeatureFile wdl, added params for run to index gnomAD.; * Added json file for indexing a large gnomad file.; * Fixed critical issues with NIO data sources.; * Updates to the test script to save output and point to full cloud data.; * Added some logging hooks to SeekableByteChannelPrefetcher. I haven't reviewed this since I made the changes to it to see what should stay, so it may need a fair bit of work.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5514
https://github.com/broadinstitute/gatk/issues/5515:154,Performance,perform,performs,154,"The prefetching code right now grabs one ""block"" at a time around the site of interest. It would be beneficial to make the prefetching smarter so that it performs better when there are large files involved. A couple of options:. * Slightly smarter (still simple):; * keep 1 block before, current block, and one block ahead at all times. * Much smarter:; * determine where the next position ( or next 10 positions ) will be based on driving reads/variants and asynchronously fetch in a small space around it",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5515
https://github.com/broadinstitute/gatk/issues/5515:255,Usability,simpl,simple,255,"The prefetching code right now grabs one ""block"" at a time around the site of interest. It would be beneficial to make the prefetching smarter so that it performs better when there are large files involved. A couple of options:. * Slightly smarter (still simple):; * keep 1 block before, current block, and one block ahead at all times. * Much smarter:; * determine where the next position ( or next 10 positions ) will be based on driving reads/variants and asynchronously fetch in a small space around it",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5515
https://github.com/broadinstitute/gatk/pull/5516:0,Deployability,update,update,0,update,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5516
https://github.com/broadinstitute/gatk/issues/5517:11,Security,Validat,ValidateSamFile,11,"Currently `ValidateSamFile` has a limited list of platforms which does not include `BGI`. We should add `BGI` to the list of valid platforms, and we should review our list of supported platforms to make sure there are no others we are missing.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5517
https://github.com/broadinstitute/gatk/pull/5518:314,Energy Efficiency,power,powerful,314,"@takutosato The extra strength of normal reads informing the ref allele's annotations improves results (very) slightly in all of our validations. The deeper reason for this change is in anticipation of multi-sample mode, where filtering based on a single INFO field will be simpler and probably statistically more powerful than filtering on a bunch of separate genotype fields.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5518
https://github.com/broadinstitute/gatk/pull/5518:133,Security,validat,validations,133,"@takutosato The extra strength of normal reads informing the ref allele's annotations improves results (very) slightly in all of our validations. The deeper reason for this change is in anticipation of multi-sample mode, where filtering based on a single INFO field will be simpler and probably statistically more powerful than filtering on a bunch of separate genotype fields.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5518
https://github.com/broadinstitute/gatk/pull/5518:274,Usability,simpl,simpler,274,"@takutosato The extra strength of normal reads informing the ref allele's annotations improves results (very) slightly in all of our validations. The deeper reason for this change is in anticipation of multi-sample mode, where filtering based on a single INFO field will be simpler and probably statistically more powerful than filtering on a bunch of separate genotype fields.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5518
https://github.com/broadinstitute/gatk/pull/5519:109,Availability,failure,failures,109,"Fix for https://github.com/broadinstitute/gatk/issues/5511, intermittent LeftAlignAndTrimVariants unit tests failures.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5519
https://github.com/broadinstitute/gatk/pull/5519:103,Testability,test,tests,103,"Fix for https://github.com/broadinstitute/gatk/issues/5511, intermittent LeftAlignAndTrimVariants unit tests failures.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5519
https://github.com/broadinstitute/gatk/pull/5522:130,Availability,avail,available,130,Store the NCBI build version in the Gencode datasource config files; in order to resolve an issue where this value was not always available; when annotating IGR variants. Resolves #4404,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5522
https://github.com/broadinstitute/gatk/pull/5522:55,Modifiability,config,config,55,Store the NCBI build version in the Gencode datasource config files; in order to resolve an issue where this value was not always available; when annotating IGR variants. Resolves #4404,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5522
https://github.com/broadinstitute/gatk/pull/5524:200,Deployability,update,updated,200,ModelSegments can currently deal with single sample segmentation. This branch contains the backend class (and the corresponding unit tests) that is able to segment based on multiple data samples. The updated version of the front-end class ModelSegments will be addressed in a different branch.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5524
https://github.com/broadinstitute/gatk/pull/5524:133,Testability,test,tests,133,ModelSegments can currently deal with single sample segmentation. This branch contains the backend class (and the corresponding unit tests) that is able to segment based on multiple data samples. The updated version of the front-end class ModelSegments will be addressed in a different branch.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5524
https://github.com/broadinstitute/gatk/pull/5525:111,Deployability,update,updated,111,Fixes the gatk doc part of https://github.com/broadinstitute/gatk/issues/5509 now that the public doc has been updated.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5525
https://github.com/broadinstitute/gatk/pull/5528:30,Availability,error,error,30,"Hello.; I believe there is an error in the description of the algorithm in the pairHMM alignment stage. It states that it does **local** alignment, however the code implements a **global** alignment.; Therefore I fixed the comments and added the correct reference to Durbin's book. (The figure referenced is not the Finite State Machine implemented). This was discussed on the GATK forum before I made the pull request.; See discussion here : [Discussion Thread](https://gatkforums.broadinstitute.org/gatk/discussion/23197/question-about-the-alignment-performed-in-the-haplotypecaller-pairhmm#latest). I believe this is important, because the difference between the two FSMs is as important as the difference between running the NeedlemanWunsch and Smith-Waterman algorithm (global vs local). Regards.; Rick",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5528
https://github.com/broadinstitute/gatk/pull/5528:552,Performance,perform,performed-in-the-haplotypecaller-pairhmm,552,"Hello.; I believe there is an error in the description of the algorithm in the pairHMM alignment stage. It states that it does **local** alignment, however the code implements a **global** alignment.; Therefore I fixed the comments and added the correct reference to Durbin's book. (The figure referenced is not the Finite State Machine implemented). This was discussed on the GATK forum before I made the pull request.; See discussion here : [Discussion Thread](https://gatkforums.broadinstitute.org/gatk/discussion/23197/question-about-the-alignment-performed-in-the-haplotypecaller-pairhmm#latest). I believe this is important, because the difference between the two FSMs is as important as the difference between running the NeedlemanWunsch and Smith-Waterman algorithm (global vs local). Regards.; Rick",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5528
https://github.com/broadinstitute/gatk/issues/5529:149,Performance,perform,performs,149,"Researcher reports a discrepancy in code comments (two occasions) vs. what is actually implemented in the code. The concern is that pairHMM actually performs global alignment (Durbin Figure 4.2) but the code comments indicate local alignment and Durbin Figure 4.3. . ---; Hello, thank you for your reply.; The algorithm is clear to me, from what I read of the code it is effectively the right of figure 4.1 (or 4.2 without the start and end states for simplicity) that is used and not 4.3. Therefore my concern, the comments in the source code clearly state that 4.3 is used and that it is local alignment, while the code in fact does global alignment. It makes more sense to do global alignment (sequence to sequence, like Needleman-Wunsch) and this is what the codes seems to do (does). Thank you for your answer. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/54930#Comment_54930",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5529
https://github.com/broadinstitute/gatk/issues/5529:323,Usability,clear,clear,323,"Researcher reports a discrepancy in code comments (two occasions) vs. what is actually implemented in the code. The concern is that pairHMM actually performs global alignment (Durbin Figure 4.2) but the code comments indicate local alignment and Durbin Figure 4.3. . ---; Hello, thank you for your reply.; The algorithm is clear to me, from what I read of the code it is effectively the right of figure 4.1 (or 4.2 without the start and end states for simplicity) that is used and not 4.3. Therefore my concern, the comments in the source code clearly state that 4.3 is used and that it is local alignment, while the code in fact does global alignment. It makes more sense to do global alignment (sequence to sequence, like Needleman-Wunsch) and this is what the codes seems to do (does). Thank you for your answer. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/54930#Comment_54930",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5529
https://github.com/broadinstitute/gatk/issues/5529:452,Usability,simpl,simplicity,452,"Researcher reports a discrepancy in code comments (two occasions) vs. what is actually implemented in the code. The concern is that pairHMM actually performs global alignment (Durbin Figure 4.2) but the code comments indicate local alignment and Durbin Figure 4.3. . ---; Hello, thank you for your reply.; The algorithm is clear to me, from what I read of the code it is effectively the right of figure 4.1 (or 4.2 without the start and end states for simplicity) that is used and not 4.3. Therefore my concern, the comments in the source code clearly state that 4.3 is used and that it is local alignment, while the code in fact does global alignment. It makes more sense to do global alignment (sequence to sequence, like Needleman-Wunsch) and this is what the codes seems to do (does). Thank you for your answer. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/54930#Comment_54930",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5529
https://github.com/broadinstitute/gatk/issues/5529:544,Usability,clear,clearly,544,"Researcher reports a discrepancy in code comments (two occasions) vs. what is actually implemented in the code. The concern is that pairHMM actually performs global alignment (Durbin Figure 4.2) but the code comments indicate local alignment and Durbin Figure 4.3. . ---; Hello, thank you for your reply.; The algorithm is clear to me, from what I read of the code it is effectively the right of figure 4.1 (or 4.2 without the start and end states for simplicity) that is used and not 4.3. Therefore my concern, the comments in the source code clearly state that 4.3 is used and that it is local alignment, while the code in fact does global alignment. It makes more sense to do global alignment (sequence to sequence, like Needleman-Wunsch) and this is what the codes seems to do (does). Thank you for your answer. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/54930#Comment_54930",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5529
https://github.com/broadinstitute/gatk/issues/5531:277,Usability,learn,learning,277,One of the most important filters in Mutect2 is the STR / polymerase slippage filter. It has some hard-coded parameters of PCR slippage rates as a function of STR repeat unit and length. We could probably increase indel sensitivity a lot (decrease filtered false negatives) by learning a model for each bam.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5531
https://github.com/broadinstitute/gatk/issues/5532:310,Deployability,toggle,toggle,310,"`HaplotypeCallerIntegrationTest` has two kinds of tests: exact-match tests against prior output to let us know when ANYTHING changes, and looser concordance tests against GATK3 output. The exact-match tests should stay, and are not a problem now that we've implemented an `UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS` toggle to update them all at once. The GATK3 concordance tests, however, are past their usefulness. We've now diverged sufficiently from GATK3 that we need a new truth set for `HaplotypeCaller`. We should change the tests to assert concordance against this new truth set (whatever it ends up being) rather than GATK 3 output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5532
https://github.com/broadinstitute/gatk/issues/5532:320,Deployability,update,update,320,"`HaplotypeCallerIntegrationTest` has two kinds of tests: exact-match tests against prior output to let us know when ANYTHING changes, and looser concordance tests against GATK3 output. The exact-match tests should stay, and are not a problem now that we've implemented an `UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS` toggle to update them all at once. The GATK3 concordance tests, however, are past their usefulness. We've now diverged sufficiently from GATK3 that we need a new truth set for `HaplotypeCaller`. We should change the tests to assert concordance against this new truth set (whatever it ends up being) rather than GATK 3 output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5532
https://github.com/broadinstitute/gatk/issues/5532:50,Testability,test,tests,50,"`HaplotypeCallerIntegrationTest` has two kinds of tests: exact-match tests against prior output to let us know when ANYTHING changes, and looser concordance tests against GATK3 output. The exact-match tests should stay, and are not a problem now that we've implemented an `UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS` toggle to update them all at once. The GATK3 concordance tests, however, are past their usefulness. We've now diverged sufficiently from GATK3 that we need a new truth set for `HaplotypeCaller`. We should change the tests to assert concordance against this new truth set (whatever it ends up being) rather than GATK 3 output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5532
https://github.com/broadinstitute/gatk/issues/5532:69,Testability,test,tests,69,"`HaplotypeCallerIntegrationTest` has two kinds of tests: exact-match tests against prior output to let us know when ANYTHING changes, and looser concordance tests against GATK3 output. The exact-match tests should stay, and are not a problem now that we've implemented an `UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS` toggle to update them all at once. The GATK3 concordance tests, however, are past their usefulness. We've now diverged sufficiently from GATK3 that we need a new truth set for `HaplotypeCaller`. We should change the tests to assert concordance against this new truth set (whatever it ends up being) rather than GATK 3 output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5532
https://github.com/broadinstitute/gatk/issues/5532:157,Testability,test,tests,157,"`HaplotypeCallerIntegrationTest` has two kinds of tests: exact-match tests against prior output to let us know when ANYTHING changes, and looser concordance tests against GATK3 output. The exact-match tests should stay, and are not a problem now that we've implemented an `UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS` toggle to update them all at once. The GATK3 concordance tests, however, are past their usefulness. We've now diverged sufficiently from GATK3 that we need a new truth set for `HaplotypeCaller`. We should change the tests to assert concordance against this new truth set (whatever it ends up being) rather than GATK 3 output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5532
https://github.com/broadinstitute/gatk/issues/5532:201,Testability,test,tests,201,"`HaplotypeCallerIntegrationTest` has two kinds of tests: exact-match tests against prior output to let us know when ANYTHING changes, and looser concordance tests against GATK3 output. The exact-match tests should stay, and are not a problem now that we've implemented an `UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS` toggle to update them all at once. The GATK3 concordance tests, however, are past their usefulness. We've now diverged sufficiently from GATK3 that we need a new truth set for `HaplotypeCaller`. We should change the tests to assert concordance against this new truth set (whatever it ends up being) rather than GATK 3 output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5532
https://github.com/broadinstitute/gatk/issues/5532:367,Testability,test,tests,367,"`HaplotypeCallerIntegrationTest` has two kinds of tests: exact-match tests against prior output to let us know when ANYTHING changes, and looser concordance tests against GATK3 output. The exact-match tests should stay, and are not a problem now that we've implemented an `UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS` toggle to update them all at once. The GATK3 concordance tests, however, are past their usefulness. We've now diverged sufficiently from GATK3 that we need a new truth set for `HaplotypeCaller`. We should change the tests to assert concordance against this new truth set (whatever it ends up being) rather than GATK 3 output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5532
https://github.com/broadinstitute/gatk/issues/5532:526,Testability,test,tests,526,"`HaplotypeCallerIntegrationTest` has two kinds of tests: exact-match tests against prior output to let us know when ANYTHING changes, and looser concordance tests against GATK3 output. The exact-match tests should stay, and are not a problem now that we've implemented an `UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS` toggle to update them all at once. The GATK3 concordance tests, however, are past their usefulness. We've now diverged sufficiently from GATK3 that we need a new truth set for `HaplotypeCaller`. We should change the tests to assert concordance against this new truth set (whatever it ends up being) rather than GATK 3 output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5532
https://github.com/broadinstitute/gatk/issues/5532:535,Testability,assert,assert,535,"`HaplotypeCallerIntegrationTest` has two kinds of tests: exact-match tests against prior output to let us know when ANYTHING changes, and looser concordance tests against GATK3 output. The exact-match tests should stay, and are not a problem now that we've implemented an `UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS` toggle to update them all at once. The GATK3 concordance tests, however, are past their usefulness. We've now diverged sufficiently from GATK3 that we need a new truth set for `HaplotypeCaller`. We should change the tests to assert concordance against this new truth set (whatever it ends up being) rather than GATK 3 output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5532
https://github.com/broadinstitute/gatk/issues/5533:52,Availability,avail,available,52,Running `gatk --version` produces a dump of all the available tools and then a user exception.; ```. ... UpdateVcfSequenceDictionary (Picard) Takes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.; VariantAnnotator (BETA Tool) Tool for adding annotations to VCF files; VcfFormatConverter (Picard) Converts VCF to BCF or BCF to VCF.; VcfToIntervalList (Picard) Converts a VCF or BCF file to a Picard Interval List. --------------------------------------------------------------------------------------. ***********************************************************************. A USER ERROR has occurred: '--version' is not a valid command. ***********************************************************************; ```; It should print the version instead.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5533
https://github.com/broadinstitute/gatk/issues/5533:647,Availability,ERROR,ERROR,647,Running `gatk --version` produces a dump of all the available tools and then a user exception.; ```. ... UpdateVcfSequenceDictionary (Picard) Takes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.; VariantAnnotator (BETA Tool) Tool for adding annotations to VCF files; VcfFormatConverter (Picard) Converts VCF to BCF or BCF to VCF.; VcfToIntervalList (Picard) Converts a VCF or BCF file to a Picard Interval List. --------------------------------------------------------------------------------------. ***********************************************************************. A USER ERROR has occurred: '--version' is not a valid command. ***********************************************************************; ```; It should print the version instead.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5533
https://github.com/broadinstitute/gatk/issues/5533:105,Deployability,Update,UpdateVcfSequenceDictionary,105,Running `gatk --version` produces a dump of all the available tools and then a user exception.; ```. ... UpdateVcfSequenceDictionary (Picard) Takes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.; VariantAnnotator (BETA Tool) Tool for adding annotations to VCF files; VcfFormatConverter (Picard) Converts VCF to BCF or BCF to VCF.; VcfToIntervalList (Picard) Converts a VCF or BCF file to a Picard Interval List. --------------------------------------------------------------------------------------. ***********************************************************************. A USER ERROR has occurred: '--version' is not a valid command. ***********************************************************************; ```; It should print the version instead.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5533
https://github.com/broadinstitute/gatk/issues/5533:212,Deployability,update,updates,212,Running `gatk --version` produces a dump of all the available tools and then a user exception.; ```. ... UpdateVcfSequenceDictionary (Picard) Takes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.; VariantAnnotator (BETA Tool) Tool for adding annotations to VCF files; VcfFormatConverter (Picard) Converts VCF to BCF or BCF to VCF.; VcfToIntervalList (Picard) Converts a VCF or BCF file to a Picard Interval List. --------------------------------------------------------------------------------------. ***********************************************************************. A USER ERROR has occurred: '--version' is not a valid command. ***********************************************************************; ```; It should print the version instead.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5533
https://github.com/broadinstitute/gatk/issues/5536:113,Deployability,release,release-,113,"Hi,. I report a bug here https://gatkforums.broadinstitute.org/gatk/discussion/23236/has-anyone-reported-the-new-release-4-0-12-0-calculatecontaminations-bug/p1?new=1.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5536
https://github.com/broadinstitute/gatk/pull/5538:63,Deployability,release,released,63,"@lbergelson @jamesemery The newest version of Barclay (not yet released) checks for and rejects dangling mutex argument references. `MarkDuplicatesSparkArgumentCollection` has a few args that are defined as mutually exclusive with other arguments that are defined directly in `MarkDuplicatesSpark`, and outside of the arg collection, but these fail in the other contexts where `MarkDuplicatesSparkArgumentCollection` collection is used (`ReadsPipelineSpark`, `BwaAndMarkDuplicatesPipelineSpark`, etc). This PR moves the referenced args into `MarkDuplicatesSparkArgumentCollection`, which resolves the compile time and parse time issues, but these args aren't actually honored in the other contexts, so this may not be the right fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5538
https://github.com/broadinstitute/gatk/pull/5540:36,Performance,perform,performance,36,order to dramatically improve exome performance. This makes an 81X speedup in the 1000interval test I put back in. Exomes are just about unrunnable without some sort of interval manipulation. The 65 sample exome joint calling callset output was exactly the same with this version. I also did a comparison for a chr1 and chr20 import (so that we were looking at two intervals that were far apart) and the runtime was the same. This is a huge improvement for some typical use cases.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5540
https://github.com/broadinstitute/gatk/pull/5540:95,Testability,test,test,95,order to dramatically improve exome performance. This makes an 81X speedup in the 1000interval test I put back in. Exomes are just about unrunnable without some sort of interval manipulation. The 65 sample exome joint calling callset output was exactly the same with this version. I also did a comparison for a chr1 and chr20 import (so that we were looking at two intervals that were far apart) and the runtime was the same. This is a huge improvement for some typical use cases.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5540
https://github.com/broadinstitute/gatk/issues/5543:84,Availability,error,error-read-max-length-must-be-,84,"Coming from https://gatkforums.broadinstitute.org/gatk/discussion/9358/gatk-runtime-error-read-max-length-must-be-0-but-got-0-with-1000g-bam#latest. There seems to be a bug somewhere in the implementation of pair hmm, which multiple users have run into. The most recent user reported running Mutect2 on two different machines with the same inputs, and same versions of GATK. One run was successful, while the other failed with ; ``` ; java.lang.IllegalArgumentException: readMaxLength must be > 0 but got 0; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); 	at org.broadinstitute.hellbender.utils.pairhmm.PairHMM.initialize(PairHMM.java:152); 	at org.broadinstitute.hellbender.utils.pairhmm.N2MemoryPairHMM.initialize(N2MemoryPairHMM.java:28); 	at org.broadinstitute.hellbender.utils.pairhmm.LoglessPairHMM.initialize(LoglessPairHMM.java:7); 	at org.broadinstitute.hellbender.utils.pairhmm.PairHMM.initialize(PairHMM.java:177); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.initializePairHMM(PairHMMLikelihoodCalculationEngine.java:242); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngine.java:177); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:207); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:212); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:979); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadins",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543
https://github.com/broadinstitute/gatk/issues/5543:2364,Performance,multi-thread,multi-threaded,2364,"rg.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:207); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:212); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:979); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289) ; ```; after an earlier warning ; ```; 10:31:03.566 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 10:31:03.566 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; ```; It seems like there is some sort of bug which is leading to ``pairhmm.initialize()`` being called with ``readMaxLength=0`` at https://github.com/broadinstitute/gatk/blob/95155e886caabf0ea4880ff255388dea33878cfa/src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/PairHMMLikelihoodCalculationEngine.java#L242. when ``VectorLoglessPairHmm`` is being used this doesn't cause any issue, because ``initialize()`` is overridden and the parameter ``readMaxLength`` is ignored. However, when ``LoglessPairHMM`` is used, ``PairHmm.initialize()`` is eventually called with ``readMaxLength=0``, which leads to the stack trace above.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543
https://github.com/broadinstitute/gatk/issues/5543:554,Security,validat,validateArg,554,"Coming from https://gatkforums.broadinstitute.org/gatk/discussion/9358/gatk-runtime-error-read-max-length-must-be-0-but-got-0-with-1000g-bam#latest. There seems to be a bug somewhere in the implementation of pair hmm, which multiple users have run into. The most recent user reported running Mutect2 on two different machines with the same inputs, and same versions of GATK. One run was successful, while the other failed with ; ``` ; java.lang.IllegalArgumentException: readMaxLength must be > 0 but got 0; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); 	at org.broadinstitute.hellbender.utils.pairhmm.PairHMM.initialize(PairHMM.java:152); 	at org.broadinstitute.hellbender.utils.pairhmm.N2MemoryPairHMM.initialize(N2MemoryPairHMM.java:28); 	at org.broadinstitute.hellbender.utils.pairhmm.LoglessPairHMM.initialize(LoglessPairHMM.java:7); 	at org.broadinstitute.hellbender.utils.pairhmm.PairHMM.initialize(PairHMM.java:177); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.initializePairHMM(PairHMMLikelihoodCalculationEngine.java:242); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngine.java:177); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:207); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:212); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:979); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadins",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543
https://github.com/broadinstitute/gatk/issues/5543:818,Testability,Log,LoglessPairHMM,818,"Coming from https://gatkforums.broadinstitute.org/gatk/discussion/9358/gatk-runtime-error-read-max-length-must-be-0-but-got-0-with-1000g-bam#latest. There seems to be a bug somewhere in the implementation of pair hmm, which multiple users have run into. The most recent user reported running Mutect2 on two different machines with the same inputs, and same versions of GATK. One run was successful, while the other failed with ; ``` ; java.lang.IllegalArgumentException: readMaxLength must be > 0 but got 0; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); 	at org.broadinstitute.hellbender.utils.pairhmm.PairHMM.initialize(PairHMM.java:152); 	at org.broadinstitute.hellbender.utils.pairhmm.N2MemoryPairHMM.initialize(N2MemoryPairHMM.java:28); 	at org.broadinstitute.hellbender.utils.pairhmm.LoglessPairHMM.initialize(LoglessPairHMM.java:7); 	at org.broadinstitute.hellbender.utils.pairhmm.PairHMM.initialize(PairHMM.java:177); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.initializePairHMM(PairHMMLikelihoodCalculationEngine.java:242); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngine.java:177); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:207); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:212); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:979); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadins",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543
https://github.com/broadinstitute/gatk/issues/5543:844,Testability,Log,LoglessPairHMM,844,"Coming from https://gatkforums.broadinstitute.org/gatk/discussion/9358/gatk-runtime-error-read-max-length-must-be-0-but-got-0-with-1000g-bam#latest. There seems to be a bug somewhere in the implementation of pair hmm, which multiple users have run into. The most recent user reported running Mutect2 on two different machines with the same inputs, and same versions of GATK. One run was successful, while the other failed with ; ``` ; java.lang.IllegalArgumentException: readMaxLength must be > 0 but got 0; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); 	at org.broadinstitute.hellbender.utils.pairhmm.PairHMM.initialize(PairHMM.java:152); 	at org.broadinstitute.hellbender.utils.pairhmm.N2MemoryPairHMM.initialize(N2MemoryPairHMM.java:28); 	at org.broadinstitute.hellbender.utils.pairhmm.LoglessPairHMM.initialize(LoglessPairHMM.java:7); 	at org.broadinstitute.hellbender.utils.pairhmm.PairHMM.initialize(PairHMM.java:177); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.initializePairHMM(PairHMMLikelihoodCalculationEngine.java:242); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngine.java:177); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:207); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:212); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:979); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadins",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543
https://github.com/broadinstitute/gatk/issues/5543:3155,Testability,Log,LoglessPairHMM,3155,"rg.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:207); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:212); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:979); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289) ; ```; after an earlier warning ; ```; 10:31:03.566 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 10:31:03.566 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; ```; It seems like there is some sort of bug which is leading to ``pairhmm.initialize()`` being called with ``readMaxLength=0`` at https://github.com/broadinstitute/gatk/blob/95155e886caabf0ea4880ff255388dea33878cfa/src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/PairHMMLikelihoodCalculationEngine.java#L242. when ``VectorLoglessPairHmm`` is being used this doesn't cause any issue, because ``initialize()`` is overridden and the parameter ``readMaxLength`` is ignored. However, when ``LoglessPairHMM`` is used, ``PairHmm.initialize()`` is eventually called with ``readMaxLength=0``, which leads to the stack trace above.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543
https://github.com/broadinstitute/gatk/pull/5544:121,Energy Efficiency,adapt,adaptive,121,This optimizes the defaults in mitochondria-mode for WGS mitochondria calling. It changes the `pruning-lod-threshold` in adaptive pruning and the `lod-divided-by-depth` threshold in `FilterMutectCalls`.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5544
https://github.com/broadinstitute/gatk/pull/5544:121,Modifiability,adapt,adaptive,121,This optimizes the defaults in mitochondria-mode for WGS mitochondria calling. It changes the `pruning-lod-threshold` in adaptive pruning and the `lod-divided-by-depth` threshold in `FilterMutectCalls`.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5544
https://github.com/broadinstitute/gatk/pull/5544:5,Performance,optimiz,optimizes,5,This optimizes the defaults in mitochondria-mode for WGS mitochondria calling. It changes the `pruning-lod-threshold` in adaptive pruning and the `lod-divided-by-depth` threshold in `FilterMutectCalls`.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5544
https://github.com/broadinstitute/gatk/issues/5545:4657,Availability,ERROR,ERROR,4657,"ionStream.readObject(KryoSerializer.scala:246); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); 	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. [Stage 21:> (0 + 60) / 3539]18/12/21 16:08:30 ERROR org.apache.spark.scheduler.TaskSetManager: Task 26 in stage 21.0 failed 4 times; aborting job; 18/12/21 16:08:30 ERROR org.apache.spark.internal.io.SparkHadoopMapReduceWriter: Aborting job job_20181221160412_0054.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 26 in stage 21.0 failed 4 times, most recent failure: Lost task 26.3 in stage 21.0 (TID 2498, readpipeline-w-4.c.broad-gatk-test.internal, executor 21): java.io.IOException: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:4776,Availability,ERROR,ERROR,4776,"ntBroadcast.scala:293); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); 	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. [Stage 21:> (0 + 60) / 3539]18/12/21 16:08:30 ERROR org.apache.spark.scheduler.TaskSetManager: Task 26 in stage 21.0 failed 4 times; aborting job; 18/12/21 16:08:30 ERROR org.apache.spark.internal.io.SparkHadoopMapReduceWriter: Aborting job job_20181221160412_0054.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 26 in stage 21.0 failed 4 times, most recent failure: Lost task 26.3 in stage 21.0 (TID 2498, readpipeline-w-4.c.broad-gatk-test.internal, executor 21): java.io.IOException: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:4936,Availability,failure,failure,4936,"ast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. [Stage 21:> (0 + 60) / 3539]18/12/21 16:08:30 ERROR org.apache.spark.scheduler.TaskSetManager: Task 26 in stage 21.0 failed 4 times; aborting job; 18/12/21 16:08:30 ERROR org.apache.spark.internal.io.SparkHadoopMapReduceWriter: Aborting job job_20181221160412_0054.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 26 in stage 21.0 failed 4 times, most recent failure: Lost task 26.3 in stage 21.0 (TID 2498, readpipeline-w-4.c.broad-gatk-test.internal, executor 21): java.io.IOException: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.disq_bio.disq.impl.form",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:4995,Availability,failure,failure,4995,"st$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. [Stage 21:> (0 + 60) / 3539]18/12/21 16:08:30 ERROR org.apache.spark.scheduler.TaskSetManager: Task 26 in stage 21.0 failed 4 times; aborting job; 18/12/21 16:08:30 ERROR org.apache.spark.internal.io.SparkHadoopMapReduceWriter: Aborting job job_20181221160412_0054.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 26 in stage 21.0 failed 4 times, most recent failure: Lost task 26.3 in stage 21.0 (TID 2498, readpipeline-w-4.c.broad-gatk-test.internal, executor 21): java.io.IOException: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.disq_bio.disq.impl.formats.sam.AnySamSinkMultiple.lambda$save$bddeb71b$1(AnySamSinkM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:12902,Availability,robust,robust,12902,"362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1084); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:1003); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:994); 	at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopFile(JavaPairRDD.scala:823); 	at org.disq_bio.disq.impl.formats.sam.AnySamSinkMultiple.save(AnySamSinkMultiple.java:96); 	at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:206); 	at org.broadinstitute.hellbender.engine.sp...; ```. heres the command line; bamIn=gs://broad-gatk-test-jenkins-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; refIn=gs://broad-gatk-test-jenkins-robust/human_g1k_v37.fasta; bamOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam.md.bqsr; knownIn=gs://broad-gatk-test-jenkins-robust/dbsnp_138.b37.excluding_sites_after_129.vcf; vcfOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.vcf; ./gatk ReadsPipelineSpark \; -I $bamIn \; -R $refIn \; --output-bam $bamOut \; -O $vcfOut \; --known-sites $knownIn \; --sharded-output true \; --emit-original-quals \; --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES \; --num-reducers 0 \; -- \; --spark-runner GCS \; --cluster $CLUSTERNAME \; --driver-memory 8G \; --conf 'spark.yarn.executor.memoryOverhead=2000' \; --executor-memory 18g \; --executor-cores 6 \; --conf spark.yarn.executor.memoryOverhead=2000""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:12992,Availability,robust,robust,12992,"362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1084); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:1003); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:994); 	at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopFile(JavaPairRDD.scala:823); 	at org.disq_bio.disq.impl.formats.sam.AnySamSinkMultiple.save(AnySamSinkMultiple.java:96); 	at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:206); 	at org.broadinstitute.hellbender.engine.sp...; ```. heres the command line; bamIn=gs://broad-gatk-test-jenkins-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; refIn=gs://broad-gatk-test-jenkins-robust/human_g1k_v37.fasta; bamOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam.md.bqsr; knownIn=gs://broad-gatk-test-jenkins-robust/dbsnp_138.b37.excluding_sites_after_129.vcf; vcfOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.vcf; ./gatk ReadsPipelineSpark \; -I $bamIn \; -R $refIn \; --output-bam $bamOut \; -O $vcfOut \; --known-sites $knownIn \; --sharded-output true \; --emit-original-quals \; --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES \; --num-reducers 0 \; -- \; --spark-runner GCS \; --cluster $CLUSTERNAME \; --driver-memory 8G \; --conf 'spark.yarn.executor.memoryOverhead=2000' \; --executor-memory 18g \; --executor-cores 6 \; --conf spark.yarn.executor.memoryOverhead=2000""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:13062,Availability,robust,robust,13062,"362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1084); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:1003); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:994); 	at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopFile(JavaPairRDD.scala:823); 	at org.disq_bio.disq.impl.formats.sam.AnySamSinkMultiple.save(AnySamSinkMultiple.java:96); 	at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:206); 	at org.broadinstitute.hellbender.engine.sp...; ```. heres the command line; bamIn=gs://broad-gatk-test-jenkins-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; refIn=gs://broad-gatk-test-jenkins-robust/human_g1k_v37.fasta; bamOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam.md.bqsr; knownIn=gs://broad-gatk-test-jenkins-robust/dbsnp_138.b37.excluding_sites_after_129.vcf; vcfOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.vcf; ./gatk ReadsPipelineSpark \; -I $bamIn \; -R $refIn \; --output-bam $bamOut \; -O $vcfOut \; --known-sites $knownIn \; --sharded-output true \; --emit-original-quals \; --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES \; --num-reducers 0 \; -- \; --spark-runner GCS \; --cluster $CLUSTERNAME \; --driver-memory 8G \; --conf 'spark.yarn.executor.memoryOverhead=2000' \; --executor-memory 18g \; --executor-cores 6 \; --conf spark.yarn.executor.memoryOverhead=2000""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:13162,Availability,robust,robust,13162,"362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1084); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:1003); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:994); 	at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopFile(JavaPairRDD.scala:823); 	at org.disq_bio.disq.impl.formats.sam.AnySamSinkMultiple.save(AnySamSinkMultiple.java:96); 	at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:206); 	at org.broadinstitute.hellbender.engine.sp...; ```. heres the command line; bamIn=gs://broad-gatk-test-jenkins-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; refIn=gs://broad-gatk-test-jenkins-robust/human_g1k_v37.fasta; bamOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam.md.bqsr; knownIn=gs://broad-gatk-test-jenkins-robust/dbsnp_138.b37.excluding_sites_after_129.vcf; vcfOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.vcf; ./gatk ReadsPipelineSpark \; -I $bamIn \; -R $refIn \; --output-bam $bamOut \; -O $vcfOut \; --known-sites $knownIn \; --sharded-output true \; --emit-original-quals \; --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES \; --num-reducers 0 \; -- \; --spark-runner GCS \; --cluster $CLUSTERNAME \; --driver-memory 8G \; --conf 'spark.yarn.executor.memoryOverhead=2000' \; --executor-memory 18g \; --executor-cores 6 \; --conf spark.yarn.executor.memoryOverhead=2000""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:13256,Availability,robust,robust,13256,"362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1084); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:1003); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:994); 	at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopFile(JavaPairRDD.scala:823); 	at org.disq_bio.disq.impl.formats.sam.AnySamSinkMultiple.save(AnySamSinkMultiple.java:96); 	at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:206); 	at org.broadinstitute.hellbender.engine.sp...; ```. heres the command line; bamIn=gs://broad-gatk-test-jenkins-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; refIn=gs://broad-gatk-test-jenkins-robust/human_g1k_v37.fasta; bamOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam.md.bqsr; knownIn=gs://broad-gatk-test-jenkins-robust/dbsnp_138.b37.excluding_sites_after_129.vcf; vcfOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.vcf; ./gatk ReadsPipelineSpark \; -I $bamIn \; -R $refIn \; --output-bam $bamOut \; -O $vcfOut \; --known-sites $knownIn \; --sharded-output true \; --emit-original-quals \; --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES \; --num-reducers 0 \; -- \; --spark-runner GCS \; --cluster $CLUSTERNAME \; --driver-memory 8G \; --conf 'spark.yarn.executor.memoryOverhead=2000' \; --executor-memory 18g \; --executor-cores 6 \; --conf spark.yarn.executor.memoryOverhead=2000""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:73,Energy Efficiency,schedul,scheduler,73,"```; [Stage 21:> (0 + 60) / 3539]18/12/21 16:08:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 46.0 in stage 21.0 (TID 2398, readpipeline-w-2.c.broad-gatk-test.internal, executor 12): java.io.IOException: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.disq_bio.disq.impl.formats.sam.AnySamSinkMultiple.lambda$save$bddeb71b$1(AnySamSinkMultiple.java:91); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:1891,Energy Efficiency,schedul,scheduler,1891,t.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.disq_bio.disq.impl.formats.sam.AnySamSinkMultiple.lambda$save$bddeb71b$1(AnySamSinkMultiple.java:91); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:144); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esoteric,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:1963,Energy Efficiency,schedul,scheduler,1963,rrentBroadcast.scala:96); 	at org.disq_bio.disq.impl.formats.sam.AnySamSinkMultiple.lambda$save$bddeb71b$1(AnySamSinkMultiple.java:91); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:144); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	at co,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:4680,Energy Efficiency,schedul,scheduler,4680," 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); 	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. [Stage 21:> (0 + 60) / 3539]18/12/21 16:08:30 ERROR org.apache.spark.scheduler.TaskSetManager: Task 26 in stage 21.0 failed 4 times; aborting job; 18/12/21 16:08:30 ERROR org.apache.spark.internal.io.SparkHadoopMapReduceWriter: Aborting job job_20181221160412_0054.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 26 in stage 21.0 failed 4 times, most recent failure: Lost task 26.3 in stage 21.0 (TID 2498, readpipeline-w-4.c.broad-gatk-test.internal, executor 21): java.io.IOException: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:6796,Energy Efficiency,schedul,scheduler,6796,t.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.disq_bio.disq.impl.formats.sam.AnySamSinkMultiple.lambda$save$bddeb71b$1(AnySamSinkMultiple.java:91); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:144); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esoteric,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:6868,Energy Efficiency,schedul,scheduler,6868,rrentBroadcast.scala:96); 	at org.disq_bio.disq.impl.formats.sam.AnySamSinkMultiple.lambda$save$bddeb71b$1(AnySamSinkMultiple.java:91); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:144); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	at co,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:9557,Energy Efficiency,schedul,scheduler,9557,t(KryoSerializer.scala:246); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); 	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:9597,Energy Efficiency,schedul,scheduler,9597,.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); 	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:9696,Energy Efficiency,schedul,scheduler,9696,park.util.Utils$.tryWithSafeFinally(Utils.scala:1337); 	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:9794,Energy Efficiency,schedul,scheduler,9794,cast$.unBlockifyObject(TorrentBroadcast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:10048,Energy Efficiency,schedul,scheduler,10048, by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:10129,Energy Efficiency,schedul,scheduler,10129,mon.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.internal.io.SparkHadoopM,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:10235,Energy Efficiency,schedul,scheduler,10235,er.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$.write(SparkHadoopMapReduceWriter.scala:88); 	at org.apache.spark.rdd.PairRDDFunctions$$ano,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:10385,Energy Efficiency,schedul,scheduler,10385,yo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$.write(SparkHadoopMapReduceWriter.scala:88); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1085); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDatas,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:10474,Energy Efficiency,schedul,scheduler,10474,ObjectField.java:125); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$.write(SparkHadoopMapReduceWriter.scala:88); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1085); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1085); 	at org.apache.spark.rdd.PairRDDFunctions$$anonf,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:10572,Energy Efficiency,schedul,scheduler,10572,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$.write(SparkHadoopMapReduceWriter.scala:88); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1085); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1085); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1085); 	at org.apache.spark.rdd.RDDOpe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:10668,Energy Efficiency,schedul,scheduler,10668,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$.write(SparkHadoopMapReduceWriter.scala:88); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1085); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1085); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1085); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:10833,Energy Efficiency,schedul,scheduler,10833,.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$.write(SparkHadoopMapReduceWriter.scala:88); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1085); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1085); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1085); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(P,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:13527,Energy Efficiency,reduce,reducers,13527,"362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1084); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:1003); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:994); 	at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopFile(JavaPairRDD.scala:823); 	at org.disq_bio.disq.impl.formats.sam.AnySamSinkMultiple.save(AnySamSinkMultiple.java:96); 	at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:206); 	at org.broadinstitute.hellbender.engine.sp...; ```. heres the command line; bamIn=gs://broad-gatk-test-jenkins-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; refIn=gs://broad-gatk-test-jenkins-robust/human_g1k_v37.fasta; bamOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam.md.bqsr; knownIn=gs://broad-gatk-test-jenkins-robust/dbsnp_138.b37.excluding_sites_after_129.vcf; vcfOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.vcf; ./gatk ReadsPipelineSpark \; -I $bamIn \; -R $refIn \; --output-bam $bamOut \; -O $vcfOut \; --known-sites $knownIn \; --sharded-output true \; --emit-original-quals \; --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES \; --num-reducers 0 \; -- \; --spark-runner GCS \; --cluster $CLUSTERNAME \; --driver-memory 8G \; --conf 'spark.yarn.executor.memoryOverhead=2000' \; --executor-memory 18g \; --executor-cores 6 \; --conf spark.yarn.executor.memoryOverhead=2000""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:2088,Performance,concurren,concurrent,2088,.java:91); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:144); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551); 	at com.esotericsoftware.kryo.Kryo.readObj,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:2173,Performance,concurren,concurrent,2173,e.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:144); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(Object,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:6993,Performance,concurren,concurrent,6993,.java:91); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:144); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551); 	at com.esotericsoftware.kryo.Kryo.readObj,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:7078,Performance,concurren,concurrent,7078,e.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:144); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(Object,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:4744,Safety,abort,aborting,4744,"ntBroadcast.scala:293); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); 	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. [Stage 21:> (0 + 60) / 3539]18/12/21 16:08:30 ERROR org.apache.spark.scheduler.TaskSetManager: Task 26 in stage 21.0 failed 4 times; aborting job; 18/12/21 16:08:30 ERROR org.apache.spark.internal.io.SparkHadoopMapReduceWriter: Aborting job job_20181221160412_0054.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 26 in stage 21.0 failed 4 times, most recent failure: Lost task 26.3 in stage 21.0 (TID 2498, readpipeline-w-4.c.broad-gatk-test.internal, executor 21): java.io.IOException: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:4839,Safety,Abort,Aborting,4839,":1337); 	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. [Stage 21:> (0 + 60) / 3539]18/12/21 16:08:30 ERROR org.apache.spark.scheduler.TaskSetManager: Task 26 in stage 21.0 failed 4 times; aborting job; 18/12/21 16:08:30 ERROR org.apache.spark.internal.io.SparkHadoopMapReduceWriter: Aborting job job_20181221160412_0054.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 26 in stage 21.0 failed 4 times, most recent failure: Lost task 26.3 in stage 21.0 (TID 2498, readpipeline-w-4.c.broad-gatk-test.internal, executor 21): java.io.IOException: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:4915,Safety,abort,aborted,4915,"ast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. [Stage 21:> (0 + 60) / 3539]18/12/21 16:08:30 ERROR org.apache.spark.scheduler.TaskSetManager: Task 26 in stage 21.0 failed 4 times; aborting job; 18/12/21 16:08:30 ERROR org.apache.spark.internal.io.SparkHadoopMapReduceWriter: Aborting job job_20181221160412_0054.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 26 in stage 21.0 failed 4 times, most recent failure: Lost task 26.3 in stage 21.0 (TID 2498, readpipeline-w-4.c.broad-gatk-test.internal, executor 21): java.io.IOException: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.disq_bio.disq.impl.form",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:9728,Safety,abort,abortStage,9728,thSafeFinally(Utils.scala:1337); 	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:9826,Safety,abort,abortStage,9826,(TorrentBroadcast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:10071,Safety,abort,abortStage,10071,ationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:169,Testability,test,test,169,"```; [Stage 21:> (0 + 60) / 3539]18/12/21 16:08:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 46.0 in stage 21.0 (TID 2398, readpipeline-w-2.c.broad-gatk-test.internal, executor 12): java.io.IOException: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.disq_bio.disq.impl.formats.sam.AnySamSinkMultiple.lambda$save$bddeb71b$1(AnySamSinkMultiple.java:91); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:5074,Testability,test,test,5074,"he.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. [Stage 21:> (0 + 60) / 3539]18/12/21 16:08:30 ERROR org.apache.spark.scheduler.TaskSetManager: Task 26 in stage 21.0 failed 4 times; aborting job; 18/12/21 16:08:30 ERROR org.apache.spark.internal.io.SparkHadoopMapReduceWriter: Aborting job job_20181221160412_0054.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 26 in stage 21.0 failed 4 times, most recent failure: Lost task 26.3 in stage 21.0 (TID 2498, readpipeline-w-4.c.broad-gatk-test.internal, executor 21): java.io.IOException: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.disq_bio.disq.impl.formats.sam.AnySamSinkMultiple.lambda$save$bddeb71b$1(AnySamSinkMultiple.java:91); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.appl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:12889,Testability,test,test-jenkins-robust,12889,"362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1084); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:1003); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:994); 	at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopFile(JavaPairRDD.scala:823); 	at org.disq_bio.disq.impl.formats.sam.AnySamSinkMultiple.save(AnySamSinkMultiple.java:96); 	at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:206); 	at org.broadinstitute.hellbender.engine.sp...; ```. heres the command line; bamIn=gs://broad-gatk-test-jenkins-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; refIn=gs://broad-gatk-test-jenkins-robust/human_g1k_v37.fasta; bamOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam.md.bqsr; knownIn=gs://broad-gatk-test-jenkins-robust/dbsnp_138.b37.excluding_sites_after_129.vcf; vcfOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.vcf; ./gatk ReadsPipelineSpark \; -I $bamIn \; -R $refIn \; --output-bam $bamOut \; -O $vcfOut \; --known-sites $knownIn \; --sharded-output true \; --emit-original-quals \; --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES \; --num-reducers 0 \; -- \; --spark-runner GCS \; --cluster $CLUSTERNAME \; --driver-memory 8G \; --conf 'spark.yarn.executor.memoryOverhead=2000' \; --executor-memory 18g \; --executor-cores 6 \; --conf spark.yarn.executor.memoryOverhead=2000""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:12979,Testability,test,test-jenkins-robust,12979,"362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1084); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:1003); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:994); 	at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopFile(JavaPairRDD.scala:823); 	at org.disq_bio.disq.impl.formats.sam.AnySamSinkMultiple.save(AnySamSinkMultiple.java:96); 	at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:206); 	at org.broadinstitute.hellbender.engine.sp...; ```. heres the command line; bamIn=gs://broad-gatk-test-jenkins-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; refIn=gs://broad-gatk-test-jenkins-robust/human_g1k_v37.fasta; bamOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam.md.bqsr; knownIn=gs://broad-gatk-test-jenkins-robust/dbsnp_138.b37.excluding_sites_after_129.vcf; vcfOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.vcf; ./gatk ReadsPipelineSpark \; -I $bamIn \; -R $refIn \; --output-bam $bamOut \; -O $vcfOut \; --known-sites $knownIn \; --sharded-output true \; --emit-original-quals \; --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES \; --num-reducers 0 \; -- \; --spark-runner GCS \; --cluster $CLUSTERNAME \; --driver-memory 8G \; --conf 'spark.yarn.executor.memoryOverhead=2000' \; --executor-memory 18g \; --executor-cores 6 \; --conf spark.yarn.executor.memoryOverhead=2000""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:13043,Testability,test,test-jenkins-write-robust,13043,"362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1084); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:1003); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:994); 	at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopFile(JavaPairRDD.scala:823); 	at org.disq_bio.disq.impl.formats.sam.AnySamSinkMultiple.save(AnySamSinkMultiple.java:96); 	at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:206); 	at org.broadinstitute.hellbender.engine.sp...; ```. heres the command line; bamIn=gs://broad-gatk-test-jenkins-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; refIn=gs://broad-gatk-test-jenkins-robust/human_g1k_v37.fasta; bamOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam.md.bqsr; knownIn=gs://broad-gatk-test-jenkins-robust/dbsnp_138.b37.excluding_sites_after_129.vcf; vcfOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.vcf; ./gatk ReadsPipelineSpark \; -I $bamIn \; -R $refIn \; --output-bam $bamOut \; -O $vcfOut \; --known-sites $knownIn \; --sharded-output true \; --emit-original-quals \; --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES \; --num-reducers 0 \; -- \; --spark-runner GCS \; --cluster $CLUSTERNAME \; --driver-memory 8G \; --conf 'spark.yarn.executor.memoryOverhead=2000' \; --executor-memory 18g \; --executor-cores 6 \; --conf spark.yarn.executor.memoryOverhead=2000""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:13149,Testability,test,test-jenkins-robust,13149,"362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1084); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:1003); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:994); 	at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopFile(JavaPairRDD.scala:823); 	at org.disq_bio.disq.impl.formats.sam.AnySamSinkMultiple.save(AnySamSinkMultiple.java:96); 	at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:206); 	at org.broadinstitute.hellbender.engine.sp...; ```. heres the command line; bamIn=gs://broad-gatk-test-jenkins-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; refIn=gs://broad-gatk-test-jenkins-robust/human_g1k_v37.fasta; bamOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam.md.bqsr; knownIn=gs://broad-gatk-test-jenkins-robust/dbsnp_138.b37.excluding_sites_after_129.vcf; vcfOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.vcf; ./gatk ReadsPipelineSpark \; -I $bamIn \; -R $refIn \; --output-bam $bamOut \; -O $vcfOut \; --known-sites $knownIn \; --sharded-output true \; --emit-original-quals \; --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES \; --num-reducers 0 \; -- \; --spark-runner GCS \; --cluster $CLUSTERNAME \; --driver-memory 8G \; --conf 'spark.yarn.executor.memoryOverhead=2000' \; --executor-memory 18g \; --executor-cores 6 \; --conf spark.yarn.executor.memoryOverhead=2000""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5545:13237,Testability,test,test-jenkins-write-robust,13237,"362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1084); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:1003); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:994); 	at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopFile(JavaPairRDD.scala:823); 	at org.disq_bio.disq.impl.formats.sam.AnySamSinkMultiple.save(AnySamSinkMultiple.java:96); 	at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:206); 	at org.broadinstitute.hellbender.engine.sp...; ```. heres the command line; bamIn=gs://broad-gatk-test-jenkins-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; refIn=gs://broad-gatk-test-jenkins-robust/human_g1k_v37.fasta; bamOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam.md.bqsr; knownIn=gs://broad-gatk-test-jenkins-robust/dbsnp_138.b37.excluding_sites_after_129.vcf; vcfOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.vcf; ./gatk ReadsPipelineSpark \; -I $bamIn \; -R $refIn \; --output-bam $bamOut \; -O $vcfOut \; --known-sites $knownIn \; --sharded-output true \; --emit-original-quals \; --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES \; --num-reducers 0 \; -- \; --spark-runner GCS \; --cluster $CLUSTERNAME \; --driver-memory 8G \; --conf 'spark.yarn.executor.memoryOverhead=2000' \; --executor-memory 18g \; --executor-cores 6 \; --conf spark.yarn.executor.memoryOverhead=2000""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545
https://github.com/broadinstitute/gatk/issues/5547:162,Availability,error,error,162,## Bug Report. ### Affected tool(s) or class(es); CountReadsSpark. ### Affected version(s); gatk-4.0.12.0. ### Description ; Reading cram generates the following error when running CountReadsSpark on yarn. . ```; ./gatk-4.0.12.0/gatk CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -- --spark-runner SPARK --spark-master yarn; Using GATK jar /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-spark.jar; Running:; /share/pkg/spark/2.1.0/install/bin/spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 13:13:11.050 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:13:11.275 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:12366,Availability,ERROR,ERROR,12366,"2); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:09 ERROR scheduler.TaskSetManager: Task 16 in stage 0.0 failed 4 times; aborting job; 13:14:09.675 INFO CountReadsSpark - Shutting down engine; [December 21, 2018 1:14:09 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.97 minutes.; Runtime.totalMemory()=937426944; org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 0.0 failed 4 times, most recent failure: Lost task 16.3 in stage 0.0 (TID 11, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:12494,Availability,down,down,12494,"at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:09 ERROR scheduler.TaskSetManager: Task 16 in stage 0.0 failed 4 times; aborting job; 13:14:09.675 INFO CountReadsSpark - Shutting down engine; [December 21, 2018 1:14:09 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.97 minutes.; Runtime.totalMemory()=937426944; org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 0.0 failed 4 times, most recent failure: Lost task 16.3 in stage 0.0 (TID 11, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scal",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:12736,Availability,failure,failure,12736,"DD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:09 ERROR scheduler.TaskSetManager: Task 16 in stage 0.0 failed 4 times; aborting job; 13:14:09.675 INFO CountReadsSpark - Shutting down engine; [December 21, 2018 1:14:09 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.97 minutes.; Runtime.totalMemory()=937426944; org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 0.0 failed 4 times, most recent failure: Lost task 16.3 in stage 0.0 (TID 11, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anon",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:12794,Availability,failure,failure,12794,"k.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:09 ERROR scheduler.TaskSetManager: Task 16 in stage 0.0 failed 4 times; aborting job; 13:14:09.675 INFO CountReadsSpark - Shutting down engine; [December 21, 2018 1:14:09 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.97 minutes.; Runtime.totalMemory()=937426944; org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 0.0 failed 4 times, most recent failure: Lost task 16.3 in stage 0.0 (TID 11, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.Spark",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:625,Deployability,install,install,625,## Bug Report. ### Affected tool(s) or class(es); CountReadsSpark. ### Affected version(s); gatk-4.0.12.0. ### Description ; Reading cram generates the following error when running CountReadsSpark on yarn. . ```; ./gatk-4.0.12.0/gatk CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -- --spark-runner SPARK --spark-master yarn; Using GATK jar /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-spark.jar; Running:; /share/pkg/spark/2.1.0/install/bin/spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 13:13:11.050 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:13:11.275 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:12584,Deployability,pipeline,pipelines,12584,"util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:09 ERROR scheduler.TaskSetManager: Task 16 in stage 0.0 failed 4 times; aborting job; 13:14:09.675 INFO CountReadsSpark - Shutting down engine; [December 21, 2018 1:14:09 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.97 minutes.; Runtime.totalMemory()=937426944; org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 0.0 failed 4 times, most recent failure: Lost task 16.3 in stage 0.0 (TID 11, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Ut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:16213,Deployability,pipeline,pipelines,16213,uler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958); at org.apache.spark.rdd.RDD.count(RDD.scala:1157); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark.runTool(CountReadsSpark.java:80); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:470); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:17306,Deployability,deploy,deploy,17306,"ine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:470); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:17343,Deployability,deploy,deploy,17343,"Tool.java:470); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(I",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:17415,Deployability,deploy,deploy,17415,"arkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:17491,Deployability,deploy,deploy,17491,"nstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RD",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:17562,Deployability,deploy,deploy,17562,"am.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:17631,Deployability,deploy,deploy,17631,"gram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:4454,Energy Efficiency,schedul,scheduler,4454,"tReadsSpark - Inflater: IntelInflater; 13:13:13.004 INFO CountReadsSpark - GCS max retries/reopens: 20; 13:13:13.004 INFO CountReadsSpark - Requester pays: disabled; 13:13:13.005 WARN CountReadsSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CountReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 13:13:13.005 INFO CountReadsSpark - Initializing engine; 13:13:13.005 INFO CountReadsSpark - Done initializing engine; 18/12/21 13:13:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/12/21 13:13:16 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 18/12/21 13:13:19 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; [Stage 0:> (0 + 2) / 155]18/12/21 13:13:57 WARN scheduler.TaskSetManager: Lost task 10.0 in stage 0.0 (TID 1, scc-q15.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 2485550, span 40026, expected MD5 106b97c463c8a19ce0f92bbd488ac81d; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:11",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:5625,Energy Efficiency,schedul,scheduler,5625,"quence id 1, start 2485550, span 40026, expected MD5 106b97c463c8a19ce0f92bbd488ac81d; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:13:58 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 9999, span 21707, expected MD5 059b07ed1e0589040ada9b236b88b509; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:5696,Energy Efficiency,schedul,scheduler,5696,"f92bbd488ac81d; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:13:58 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 9999, span 21707, expected MD5 059b07ed1e0589040ada9b236b88b509; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:6038,Energy Efficiency,schedul,scheduler,6038,"2); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:13:58 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 9999, span 21707, expected MD5 059b07ed1e0589040ada9b236b88b509; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apach",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:7205,Energy Efficiency,schedul,scheduler,7205," sequence id 0, start 9999, span 21707, expected MD5 059b07ed1e0589040ada9b236b88b509; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:00 WARN scheduler.TaskSetManager: Lost task 16.0 in stage 0.0 (TID 2, scc-q15.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:7276,Energy Efficiency,schedul,scheduler,7276,"da9b236b88b509; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:00 WARN scheduler.TaskSetManager: Lost task 16.0 in stage 0.0 (TID 2, scc-q15.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.coll",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:7618,Energy Efficiency,schedul,scheduler,7618,"2); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:00 WARN scheduler.TaskSetManager: Lost task 16.0 in stage 0.0 (TID 2, scc-q15.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:8791,Energy Efficiency,schedul,scheduler,8791,"ence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:00 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 0.0 (TID 3, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 124511724, span 7265, expected MD5 cf58e0adc447a66b188474efc3c84a43; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.co",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:8862,Energy Efficiency,schedul,scheduler,8862,"bfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:00 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 0.0 (TID 3, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 124511724, span 7265, expected MD5 cf58e0adc447a66b188474efc3c84a43; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:9204,Energy Efficiency,schedul,scheduler,9204,"2); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:00 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 0.0 (TID 3, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 124511724, span 7265, expected MD5 cf58e0adc447a66b188474efc3c84a43; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:10375,Energy Efficiency,schedul,scheduler,10375,"uence id 0, start 124511724, span 7265, expected MD5 cf58e0adc447a66b188474efc3c84a43; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:03 WARN scheduler.TaskSetManager: Lost task 23.0 in stage 0.0 (TID 6, scc-q15.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 2, start 93470412, span 157, expected MD5 56b7844faa4e0c4f61fd6774df454b09; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.col",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:10446,Energy Efficiency,schedul,scheduler,10446,"8474efc3c84a43; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:03 WARN scheduler.TaskSetManager: Lost task 23.0 in stage 0.0 (TID 6, scc-q15.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 2, start 93470412, span 157, expected MD5 56b7844faa4e0c4f61fd6774df454b09; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collect",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:10788,Energy Efficiency,schedul,scheduler,10788,"2); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:03 WARN scheduler.TaskSetManager: Lost task 23.0 in stage 0.0 (TID 6, scc-q15.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 2, start 93470412, span 157, expected MD5 56b7844faa4e0c4f61fd6774df454b09; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.ap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:11958,Energy Efficiency,schedul,scheduler,11958,"equence id 2, start 93470412, span 157, expected MD5 56b7844faa4e0c4f61fd6774df454b09; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:09 ERROR scheduler.TaskSetManager: Task 16 in stage 0.0 failed 4 times; aborting job; 13:14:09.675 INFO CountReadsSpark - Shutting down engine; [December 21, 2018 1:14:09 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.97 minutes.; Runtime.totalMemory()=937426944; org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 0.0 failed 4 times, most recent failure: Lost task 16.3 in stage 0.0 (TID 11, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:12029,Energy Efficiency,schedul,scheduler,12029,"fd6774df454b09; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:09 ERROR scheduler.TaskSetManager: Task 16 in stage 0.0 failed 4 times; aborting job; 13:14:09.675 INFO CountReadsSpark - Shutting down engine; [December 21, 2018 1:14:09 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.97 minutes.; Runtime.totalMemory()=937426944; org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 0.0 failed 4 times, most recent failure: Lost task 16.3 in stage 0.0 (TID 11, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:12372,Energy Efficiency,schedul,scheduler,12372,"2); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:09 ERROR scheduler.TaskSetManager: Task 16 in stage 0.0 failed 4 times; aborting job; 13:14:09.675 INFO CountReadsSpark - Shutting down engine; [December 21, 2018 1:14:09 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.97 minutes.; Runtime.totalMemory()=937426944; org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 0.0 failed 4 times, most recent failure: Lost task 16.3 in stage 0.0 (TID 11, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:13951,Energy Efficiency,schedul,scheduler,13951,"ence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:14022,Energy Efficiency,schedul,scheduler,14022,bfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:8,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:14381,Energy Efficiency,schedul,scheduler,14381,tor$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:14421,Energy Efficiency,schedul,scheduler,14421,on.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:14519,Energy Efficiency,schedul,scheduler,14519,$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:14616,Energy Efficiency,schedul,scheduler,14616,:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:14867,Energy Efficiency,schedul,scheduler,14867,kContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:14947,Energy Efficiency,schedul,scheduler,14947,cheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.SparkContext.runJob(SparkContext.sca,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:15052,Energy Efficiency,schedul,scheduler,15052,at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958); at org.apache.spark.rdd.RDD.count(RDD.scala:1157); at org.apache.spark.api.java.JavaRDDLike$cla,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:15200,Energy Efficiency,schedul,scheduler,15200,va:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958); at org.apache.spark.rdd.RDD.count(RDD.scala:1157); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tool,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:15288,Energy Efficiency,schedul,scheduler,15288,617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958); at org.apache.spark.rdd.RDD.count(RDD.scala:1157); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark.runTool(CountReadsSpark.java:80); at org.broadinstitut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:15385,Energy Efficiency,schedul,scheduler,15385,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958); at org.apache.spark.rdd.RDD.count(RDD.scala:1157); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark.runTool(CountReadsSpark.java:80); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:470); at org.broadinstitut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:15480,Energy Efficiency,schedul,scheduler,15480,.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958); at org.apache.spark.rdd.RDD.count(RDD.scala:1157); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark.runTool(CountReadsSpark.java:80); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:470); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at o,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:15643,Energy Efficiency,schedul,scheduler,15643,abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958); at org.apache.spark.rdd.RDD.count(RDD.scala:1157); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark.runTool(CountReadsSpark.java:80); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:470); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceM,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:18764,Energy Efficiency,schedul,scheduler,18764,"t.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ```; #### Steps to reproduce; /gatk-4.0.12.0/gatk CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -- --spark-runner SPARK --spark-master yarn. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:18835,Energy Efficiency,schedul,scheduler,18835,"t.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ```; #### Steps to reproduce; /gatk-4.0.12.0/gatk CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -- --spark-runner SPARK --spark-master yarn. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:4981,Integrability,Wrap,Wrappers,4981,"tiveCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/12/21 13:13:16 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 18/12/21 13:13:19 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; [Stage 0:> (0 + 2) / 155]18/12/21 13:13:57 WARN scheduler.TaskSetManager: Lost task 10.0 in stage 0.0 (TID 1, scc-q15.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 2485550, span 40026, expected MD5 106b97c463c8a19ce0f92bbd488ac81d; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.r",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:5015,Integrability,Wrap,Wrappers,5015,"e to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/12/21 13:13:16 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 18/12/21 13:13:19 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; [Stage 0:> (0 + 2) / 155]18/12/21 13:13:57 WARN scheduler.TaskSetManager: Lost task 10.0 in stage 0.0 (TID 1, scc-q15.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 2485550, span 40026, expected MD5 106b97c463c8a19ce0f92bbd488ac81d; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:6561,Integrability,Wrap,Wrappers,6561,"pply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:13:58 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 9999, span 21707, expected MD5 059b07ed1e0589040ada9b236b88b509; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.r",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:6595,Integrability,Wrap,Wrappers,6595,"la:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:13:58 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 9999, span 21707, expected MD5 059b07ed1e0589040ada9b236b88b509; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:8147,Integrability,Wrap,Wrappers,8147,"parkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:00 WARN scheduler.TaskSetManager: Lost task 16.0 in stage 0.0 (TID 2, scc-q15.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.r",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:8181,Integrability,Wrap,Wrappers,8181,"4); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:00 WARN scheduler.TaskSetManager: Lost task 16.0 in stage 0.0 (TID 2, scc-q15.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:9731,Integrability,Wrap,Wrappers,9731,"(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:00 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 0.0 (TID 3, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 124511724, span 7265, expected MD5 cf58e0adc447a66b188474efc3c84a43; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.r",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:9765,Integrability,Wrap,Wrappers,9765,"944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:00 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 0.0 (TID 3, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 124511724, span 7265, expected MD5 cf58e0adc447a66b188474efc3c84a43; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:11314,Integrability,Wrap,Wrappers,11314,"y(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:03 WARN scheduler.TaskSetManager: Lost task 23.0 in stage 0.0 (TID 6, scc-q15.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 2, start 93470412, span 157, expected MD5 56b7844faa4e0c4f61fd6774df454b09; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.r",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:11348,Integrability,Wrap,Wrappers,11348,"1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:03 WARN scheduler.TaskSetManager: Lost task 23.0 in stage 0.0 (TID 6, scc-q15.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 2, start 93470412, span 157, expected MD5 56b7844faa4e0c4f61fd6774df454b09; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:13307,Integrability,Wrap,Wrappers,13307,"hread.run(Thread.java:745). 18/12/21 13:14:09 ERROR scheduler.TaskSetManager: Task 16 in stage 0.0 failed 4 times; aborting job; 13:14:09.675 INFO CountReadsSpark - Shutting down engine; [December 21, 2018 1:14:09 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.97 minutes.; Runtime.totalMemory()=937426944; org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 0.0 failed 4 times, most recent failure: Lost task 16.3 in stage 0.0 (TID 11, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.r",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:13341,Integrability,Wrap,Wrappers,13341,":745). 18/12/21 13:14:09 ERROR scheduler.TaskSetManager: Task 16 in stage 0.0 failed 4 times; aborting job; 13:14:09.675 INFO CountReadsSpark - Shutting down engine; [December 21, 2018 1:14:09 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.97 minutes.; Runtime.totalMemory()=937426944; org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 0.0 failed 4 times, most recent failure: Lost task 16.3 in stage 0.0 (TID 11, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). D",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:18120,Integrability,Wrap,Wrappers,18120,"java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.r",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:18154,Integrability,Wrap,Wrappers,18154,"ect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). `",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:1713,Modifiability,variab,variables,1713,"lassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 13:13:11.050 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:13:11.275 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:13:12.999 INFO CountReadsSpark - ------------------------------------------------------------; 13:13:13.000 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 13:13:13.000 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:13:13.000 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 13:13:13.001 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:13:13.001 INFO CountReadsSpark - Start Date/Time: December 21, 2018 1:13:11 PM EST; 13:13:13.001 INFO CountReadsSpark - ----------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:1839,Modifiability,config,configured,1839,"lassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 13:13:11.050 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:13:11.275 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:13:12.999 INFO CountReadsSpark - ------------------------------------------------------------; 13:13:13.000 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 13:13:13.000 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:13:13.000 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 13:13:13.001 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:13:13.001 INFO CountReadsSpark - Start Date/Time: December 21, 2018 1:13:11 PM EST; 13:13:13.001 INFO CountReadsSpark - ----------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:1900,Performance,Load,Loading,1900,"JavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 13:13:11.050 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:13:11.275 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:13:12.999 INFO CountReadsSpark - ------------------------------------------------------------; 13:13:13.000 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 13:13:13.000 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:13:13.000 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 13:13:13.001 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:13:13.001 INFO CountReadsSpark - Start Date/Time: December 21, 2018 1:13:11 PM EST; 13:13:13.001 INFO CountReadsSpark - ------------------------------------------------------------; 13:13:13.001 INFO CountReadsSpark - ------------------------------------------------------------; 13:13:13.003 INFO Co",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:4020,Performance,load,load,4020,"park - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 13:13:13.004 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 13:13:13.004 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 13:13:13.004 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:13:13.004 INFO CountReadsSpark - Deflater: IntelDeflater; 13:13:13.004 INFO CountReadsSpark - Inflater: IntelInflater; 13:13:13.004 INFO CountReadsSpark - GCS max retries/reopens: 20; 13:13:13.004 INFO CountReadsSpark - Requester pays: disabled; 13:13:13.005 WARN CountReadsSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CountReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 13:13:13.005 INFO CountReadsSpark - Initializing engine; 13:13:13.005 INFO CountReadsSpark - Done initializing engine; 18/12/21 13:13:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/12/21 13:13:16 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 18/12/21 13:13:19 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; [Stage 0:> (0 + 2) / 155]18/12/21 13:13:57 WARN scheduler.TaskSetManager: Lost task 10.0 in stage 0.0 (TID 1, scc-q15.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 2485550, span 40026, expected MD5 106b97c463c8a19ce0f92bbd488ac81d; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:4251,Performance,load,loaded,4251,"TSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 13:13:13.004 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:13:13.004 INFO CountReadsSpark - Deflater: IntelDeflater; 13:13:13.004 INFO CountReadsSpark - Inflater: IntelInflater; 13:13:13.004 INFO CountReadsSpark - GCS max retries/reopens: 20; 13:13:13.004 INFO CountReadsSpark - Requester pays: disabled; 13:13:13.005 WARN CountReadsSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CountReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 13:13:13.005 INFO CountReadsSpark - Initializing engine; 13:13:13.005 INFO CountReadsSpark - Done initializing engine; 18/12/21 13:13:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/12/21 13:13:16 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 18/12/21 13:13:19 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; [Stage 0:> (0 + 2) / 155]18/12/21 13:13:57 WARN scheduler.TaskSetManager: Lost task 10.0 in stage 0.0 (TID 1, scc-q15.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 2485550, span 40026, expected MD5 106b97c463c8a19ce0f92bbd488ac81d; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:5818,Performance,concurren,concurrent,5818,"ext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:13:58 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 9999, span 21707, expected MD5 059b07ed1e0589040ada9b236b88b509; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:5902,Performance,concurren,concurrent,5902,"er.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:13:58 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 9999, span 21707, expected MD5 059b07ed1e0589040ada9b236b88b509; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.R",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:7398,Performance,concurren,concurrent,7398,"ext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:00 WARN scheduler.TaskSetManager: Lost task 16.0 in stage 0.0 (TID 2, scc-q15.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:7482,Performance,concurren,concurrent,7482,"er.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:00 WARN scheduler.TaskSetManager: Lost task 16.0 in stage 0.0 (TID 2, scc-q15.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:8984,Performance,concurren,concurrent,8984,"ext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:00 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 0.0 (TID 3, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 124511724, span 7265, expected MD5 cf58e0adc447a66b188474efc3c84a43; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:9068,Performance,concurren,concurrent,9068,"er.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:00 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 0.0 (TID 3, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 124511724, span 7265, expected MD5 cf58e0adc447a66b188474efc3c84a43; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.r",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:10568,Performance,concurren,concurrent,10568,"ext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:03 WARN scheduler.TaskSetManager: Lost task 23.0 in stage 0.0 (TID 6, scc-q15.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 2, start 93470412, span 157, expected MD5 56b7844faa4e0c4f61fd6774df454b09; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at or",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:10652,Performance,concurren,concurrent,10652,"er.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:03 WARN scheduler.TaskSetManager: Lost task 23.0 in stage 0.0 (TID 6, scc-q15.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 2, start 93470412, span 157, expected MD5 56b7844faa4e0c4f61fd6774df454b09; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:12151,Performance,concurren,concurrent,12151,"ext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:09 ERROR scheduler.TaskSetManager: Task 16 in stage 0.0 failed 4 times; aborting job; 13:14:09.675 INFO CountReadsSpark - Shutting down engine; [December 21, 2018 1:14:09 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.97 minutes.; Runtime.totalMemory()=937426944; org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 0.0 failed 4 times, most recent failure: Lost task 16.3 in stage 0.0 (TID 11, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:12235,Performance,concurren,concurrent,12235,"er.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:09 ERROR scheduler.TaskSetManager: Task 16 in stage 0.0 failed 4 times; aborting job; 13:14:09.675 INFO CountReadsSpark - Shutting down engine; [December 21, 2018 1:14:09 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.97 minutes.; Runtime.totalMemory()=937426944; org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 0.0 failed 4 times, most recent failure: Lost task 16.3 in stage 0.0 (TID 11, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:14144,Performance,concurren,concurrent,14144,ext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Opti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:14228,Performance,concurren,concurrent,14228,er.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:18957,Performance,concurren,concurrent,18957,"t.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ```; #### Steps to reproduce; /gatk-4.0.12.0/gatk CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -- --spark-runner SPARK --spark-master yarn. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:19041,Performance,concurren,concurrent,19041,"t.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ```; #### Steps to reproduce; /gatk-4.0.12.0/gatk CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -- --spark-runner SPARK --spark-master yarn. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:12435,Safety,abort,aborting,12435," scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:09 ERROR scheduler.TaskSetManager: Task 16 in stage 0.0 failed 4 times; aborting job; 13:14:09.675 INFO CountReadsSpark - Shutting down engine; [December 21, 2018 1:14:09 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.97 minutes.; Runtime.totalMemory()=937426944; org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 0.0 failed 4 times, most recent failure: Lost task 16.3 in stage 0.0 (TID 11, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:12715,Safety,abort,aborted,12715,"DD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:09 ERROR scheduler.TaskSetManager: Task 16 in stage 0.0 failed 4 times; aborting job; 13:14:09.675 INFO CountReadsSpark - Shutting down engine; [December 21, 2018 1:14:09 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.97 minutes.; Runtime.totalMemory()=937426944; org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 0.0 failed 4 times, most recent failure: Lost task 16.3 in stage 0.0 (TID 11, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anon",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:14551,Safety,abort,abortStage,14551,tor.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:14648,Safety,abort,abortStage,14648,spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/issues/5547:14890,Safety,abort,abortStage,14890,ly(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547
https://github.com/broadinstitute/gatk/pull/5548:98,Modifiability,config,config,98,Add PEP8 python style with type hints and use model directories instead of separate arguments for config and weights.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5548
https://github.com/broadinstitute/gatk/issues/5550:401,Integrability,wrap,wrapper,401,"## Feature request. ### Tool(s) or class(es) involved; gatk-completion.sh in docker container. ### Description; When using the GATK docker for interactive work (eg in a workshop), currently we have to do `source gatk-completion.sh` to activate tab completion, or add something to that effect to the bash profile. Would be great to have that already set up in the docker by default (just like the gatk wrapper being in the PATH).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5550
https://github.com/broadinstitute/gatk/pull/5551:20,Deployability,update,updates,20,Fixes #5421. Picard updates are in https://github.com/broadinstitute/picard/pull/1259.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5551
https://github.com/broadinstitute/gatk/pull/5552:58,Testability,Test,Tested,58,Fixes https://github.com/broadinstitute/gatk/issues/5550. Tested manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5552
https://github.com/broadinstitute/gatk/issues/5553:47,Availability,error,error,47,"Hello,. I'd like to report a FilterMutectCalls error. It was run using mutect2.wdl with gatk 4.0.12.0.; FilterMutectCalls of gatk 4.0.10.1 succeeded using same input.; Please let me know if there is something else I can provide. Thank you. ```; java.lang.IllegalArgumentException: errorRateLog10 must be good probability but got NaN; at org.broadinstitute.hellbender.utils.QualityUtils.phredScaleLog10ErrorRate(QualityUtils.java:321); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.lambda$applyGermlineVariantFilter$10(Mutect2FilteringEngine.java:207); at java.util.stream.DoublePipeline$3$1.accept(DoublePipeline.java:231); at java.util.Spliterators$DoubleArraySpliterator.forEachRemaining(Spliterators.java:1198); at java.util.Spliterator$OfDouble.forEachRemaining(Spliterator.java:822); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545); at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); at java.util.stream.IntPipeline.toArray(IntPipeline.java:502); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.applyGermlineVariantFilter(Mutect2FilteringEngine.java:207); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.calculateFilters(Mutect2FilteringEngine.java:436); at org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls.firstPassApply(FilterMutectCalls.java:120); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.lambda$traverseVariants$0(TwoPassVariantWalker.java:76); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5553
https://github.com/broadinstitute/gatk/issues/5553:931,Integrability,wrap,wrapAndCopyInto,931,"Hello,. I'd like to report a FilterMutectCalls error. It was run using mutect2.wdl with gatk 4.0.12.0.; FilterMutectCalls of gatk 4.0.10.1 succeeded using same input.; Please let me know if there is something else I can provide. Thank you. ```; java.lang.IllegalArgumentException: errorRateLog10 must be good probability but got NaN; at org.broadinstitute.hellbender.utils.QualityUtils.phredScaleLog10ErrorRate(QualityUtils.java:321); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.lambda$applyGermlineVariantFilter$10(Mutect2FilteringEngine.java:207); at java.util.stream.DoublePipeline$3$1.accept(DoublePipeline.java:231); at java.util.Spliterators$DoubleArraySpliterator.forEachRemaining(Spliterators.java:1198); at java.util.Spliterator$OfDouble.forEachRemaining(Spliterator.java:822); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545); at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); at java.util.stream.IntPipeline.toArray(IntPipeline.java:502); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.applyGermlineVariantFilter(Mutect2FilteringEngine.java:207); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.calculateFilters(Mutect2FilteringEngine.java:436); at org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls.firstPassApply(FilterMutectCalls.java:120); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.lambda$traverseVariants$0(TwoPassVariantWalker.java:76); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5553
https://github.com/broadinstitute/gatk/issues/5553:2109,Integrability,wrap,wrapAndCopyInto,2109,va:260); at java.util.stream.IntPipeline.toArray(IntPipeline.java:502); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.applyGermlineVariantFilter(Mutect2FilteringEngine.java:207); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.calculateFilters(Mutect2FilteringEngine.java:436); at org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls.firstPassApply(FilterMutectCalls.java:120); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.lambda$traverseVariants$0(TwoPassVariantWalker.java:76); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.traverseVariants(TwoPassVariantWalker.java:74); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.traverse(TwoPassVariantWalker.java:27); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5553
https://github.com/broadinstitute/gatk/issues/5553:3481,Modifiability,variab,variable,3481,broadinstitute.hellbender.engine.TwoPassVariantWalker.traverseVariants(TwoPassVariantWalker.java:74); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.traverse(TwoPassVariantWalker.java:27); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/392945202/gatk-package-4.0.12.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6500m -jar /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/392945202/gatk-package-4.0.12.0-local.jar FilterMutectCalls -V /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/-356078842/Ameloblastoma_FFPE_P5-unfiltered.vcf.gz -O Ameloblastoma_FFPE_P5-filtered.vcf.gz --contamination-table /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/51739658/contamination.table --tumor-segmentation /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/51739658/segments.table --max-events-in-region 6. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5553
https://github.com/broadinstitute/gatk/issues/5553:3318,Testability,log,log,3318,ipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.traverseVariants(TwoPassVariantWalker.java:74); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.traverse(TwoPassVariantWalker.java:27); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/392945202/gatk-package-4.0.12.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6500m -jar /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/392945202/gatk-package-4.0.12.0-local.jar FilterMutectCalls -V /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/-356078842/Ameloblastoma_FFPE_P5-unfiltered.vcf.gz -O Ameloblastoma_FFPE_P5-filtered.vcf.gz --contamination-table /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/51739658/contamination.table --tumor-segmentation /gpfs/data/software/cromwell/log/cromwell-executions/,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5553
https://github.com/broadinstitute/gatk/issues/5553:3720,Testability,log,log,3720,broadinstitute.hellbender.engine.TwoPassVariantWalker.traverseVariants(TwoPassVariantWalker.java:74); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.traverse(TwoPassVariantWalker.java:27); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/392945202/gatk-package-4.0.12.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6500m -jar /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/392945202/gatk-package-4.0.12.0-local.jar FilterMutectCalls -V /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/-356078842/Ameloblastoma_FFPE_P5-unfiltered.vcf.gz -O Ameloblastoma_FFPE_P5-filtered.vcf.gz --contamination-table /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/51739658/contamination.table --tumor-segmentation /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/51739658/segments.table --max-events-in-region 6. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5553
https://github.com/broadinstitute/gatk/issues/5553:3910,Testability,log,log,3910,broadinstitute.hellbender.engine.TwoPassVariantWalker.traverseVariants(TwoPassVariantWalker.java:74); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.traverse(TwoPassVariantWalker.java:27); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/392945202/gatk-package-4.0.12.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6500m -jar /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/392945202/gatk-package-4.0.12.0-local.jar FilterMutectCalls -V /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/-356078842/Ameloblastoma_FFPE_P5-unfiltered.vcf.gz -O Ameloblastoma_FFPE_P5-filtered.vcf.gz --contamination-table /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/51739658/contamination.table --tumor-segmentation /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/51739658/segments.table --max-events-in-region 6. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5553
https://github.com/broadinstitute/gatk/issues/5553:4151,Testability,log,log,4151,broadinstitute.hellbender.engine.TwoPassVariantWalker.traverseVariants(TwoPassVariantWalker.java:74); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.traverse(TwoPassVariantWalker.java:27); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/392945202/gatk-package-4.0.12.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6500m -jar /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/392945202/gatk-package-4.0.12.0-local.jar FilterMutectCalls -V /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/-356078842/Ameloblastoma_FFPE_P5-unfiltered.vcf.gz -O Ameloblastoma_FFPE_P5-filtered.vcf.gz --contamination-table /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/51739658/contamination.table --tumor-segmentation /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/51739658/segments.table --max-events-in-region 6. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5553
https://github.com/broadinstitute/gatk/issues/5553:4328,Testability,log,log,4328,broadinstitute.hellbender.engine.TwoPassVariantWalker.traverseVariants(TwoPassVariantWalker.java:74); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.traverse(TwoPassVariantWalker.java:27); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/392945202/gatk-package-4.0.12.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6500m -jar /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/392945202/gatk-package-4.0.12.0-local.jar FilterMutectCalls -V /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/-356078842/Ameloblastoma_FFPE_P5-unfiltered.vcf.gz -O Ameloblastoma_FFPE_P5-filtered.vcf.gz --contamination-table /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/51739658/contamination.table --tumor-segmentation /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/51739658/segments.table --max-events-in-region 6. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5553
https://github.com/broadinstitute/gatk/pull/5555:60,Availability,down,down,60,"Closes #5088. @takutosato Not a high priority, just cutting down the list of open issues.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5555
https://github.com/broadinstitute/gatk/pull/5556:92,Availability,recover,recover,92,"Also fixed some minor style issues in argument variable names and the WDL. This should help recover some deletions and might possibly clear up some issues with MAF estimation when the number of hets is small. @LeeTL1220 can you run on some test cases to check the effect? (Note that the changes to fix estimation of the posterior widths, which will in turn affect similar-segment smoothing, are in another branch; we should test those changes as well.). Note that the default threshold of zero for the tumor in matched-normal mode should ensure that the sites genotyped as het should always match in the tumor and the normal. (This will ultimately make multisample segmentation, as enabled by #5524, more straightforward.) There was previously a check for this condition in the integration test; however, it wasn't actually activated by the test data. I could modify the test data to add a proper regression test, but since these test files are generated by running another tool on a test BAM in the repo, this could be misleading. I'm OK with punting in this case. @jonn-smith do you mind reviewing, since this resulted from your turn as liaison? Should be super quick. Thanks again for raising the issue!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5556
https://github.com/broadinstitute/gatk/pull/5556:778,Deployability,integrat,integration,778,"Also fixed some minor style issues in argument variable names and the WDL. This should help recover some deletions and might possibly clear up some issues with MAF estimation when the number of hets is small. @LeeTL1220 can you run on some test cases to check the effect? (Note that the changes to fix estimation of the posterior widths, which will in turn affect similar-segment smoothing, are in another branch; we should test those changes as well.). Note that the default threshold of zero for the tumor in matched-normal mode should ensure that the sites genotyped as het should always match in the tumor and the normal. (This will ultimately make multisample segmentation, as enabled by #5524, more straightforward.) There was previously a check for this condition in the integration test; however, it wasn't actually activated by the test data. I could modify the test data to add a proper regression test, but since these test files are generated by running another tool on a test BAM in the repo, this could be misleading. I'm OK with punting in this case. @jonn-smith do you mind reviewing, since this resulted from your turn as liaison? Should be super quick. Thanks again for raising the issue!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5556
https://github.com/broadinstitute/gatk/pull/5556:778,Integrability,integrat,integration,778,"Also fixed some minor style issues in argument variable names and the WDL. This should help recover some deletions and might possibly clear up some issues with MAF estimation when the number of hets is small. @LeeTL1220 can you run on some test cases to check the effect? (Note that the changes to fix estimation of the posterior widths, which will in turn affect similar-segment smoothing, are in another branch; we should test those changes as well.). Note that the default threshold of zero for the tumor in matched-normal mode should ensure that the sites genotyped as het should always match in the tumor and the normal. (This will ultimately make multisample segmentation, as enabled by #5524, more straightforward.) There was previously a check for this condition in the integration test; however, it wasn't actually activated by the test data. I could modify the test data to add a proper regression test, but since these test files are generated by running another tool on a test BAM in the repo, this could be misleading. I'm OK with punting in this case. @jonn-smith do you mind reviewing, since this resulted from your turn as liaison? Should be super quick. Thanks again for raising the issue!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5556
https://github.com/broadinstitute/gatk/pull/5556:47,Modifiability,variab,variable,47,"Also fixed some minor style issues in argument variable names and the WDL. This should help recover some deletions and might possibly clear up some issues with MAF estimation when the number of hets is small. @LeeTL1220 can you run on some test cases to check the effect? (Note that the changes to fix estimation of the posterior widths, which will in turn affect similar-segment smoothing, are in another branch; we should test those changes as well.). Note that the default threshold of zero for the tumor in matched-normal mode should ensure that the sites genotyped as het should always match in the tumor and the normal. (This will ultimately make multisample segmentation, as enabled by #5524, more straightforward.) There was previously a check for this condition in the integration test; however, it wasn't actually activated by the test data. I could modify the test data to add a proper regression test, but since these test files are generated by running another tool on a test BAM in the repo, this could be misleading. I'm OK with punting in this case. @jonn-smith do you mind reviewing, since this resulted from your turn as liaison? Should be super quick. Thanks again for raising the issue!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5556
https://github.com/broadinstitute/gatk/pull/5556:92,Safety,recover,recover,92,"Also fixed some minor style issues in argument variable names and the WDL. This should help recover some deletions and might possibly clear up some issues with MAF estimation when the number of hets is small. @LeeTL1220 can you run on some test cases to check the effect? (Note that the changes to fix estimation of the posterior widths, which will in turn affect similar-segment smoothing, are in another branch; we should test those changes as well.). Note that the default threshold of zero for the tumor in matched-normal mode should ensure that the sites genotyped as het should always match in the tumor and the normal. (This will ultimately make multisample segmentation, as enabled by #5524, more straightforward.) There was previously a check for this condition in the integration test; however, it wasn't actually activated by the test data. I could modify the test data to add a proper regression test, but since these test files are generated by running another tool on a test BAM in the repo, this could be misleading. I'm OK with punting in this case. @jonn-smith do you mind reviewing, since this resulted from your turn as liaison? Should be super quick. Thanks again for raising the issue!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5556
https://github.com/broadinstitute/gatk/pull/5556:240,Testability,test,test,240,"Also fixed some minor style issues in argument variable names and the WDL. This should help recover some deletions and might possibly clear up some issues with MAF estimation when the number of hets is small. @LeeTL1220 can you run on some test cases to check the effect? (Note that the changes to fix estimation of the posterior widths, which will in turn affect similar-segment smoothing, are in another branch; we should test those changes as well.). Note that the default threshold of zero for the tumor in matched-normal mode should ensure that the sites genotyped as het should always match in the tumor and the normal. (This will ultimately make multisample segmentation, as enabled by #5524, more straightforward.) There was previously a check for this condition in the integration test; however, it wasn't actually activated by the test data. I could modify the test data to add a proper regression test, but since these test files are generated by running another tool on a test BAM in the repo, this could be misleading. I'm OK with punting in this case. @jonn-smith do you mind reviewing, since this resulted from your turn as liaison? Should be super quick. Thanks again for raising the issue!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5556
https://github.com/broadinstitute/gatk/pull/5556:424,Testability,test,test,424,"Also fixed some minor style issues in argument variable names and the WDL. This should help recover some deletions and might possibly clear up some issues with MAF estimation when the number of hets is small. @LeeTL1220 can you run on some test cases to check the effect? (Note that the changes to fix estimation of the posterior widths, which will in turn affect similar-segment smoothing, are in another branch; we should test those changes as well.). Note that the default threshold of zero for the tumor in matched-normal mode should ensure that the sites genotyped as het should always match in the tumor and the normal. (This will ultimately make multisample segmentation, as enabled by #5524, more straightforward.) There was previously a check for this condition in the integration test; however, it wasn't actually activated by the test data. I could modify the test data to add a proper regression test, but since these test files are generated by running another tool on a test BAM in the repo, this could be misleading. I'm OK with punting in this case. @jonn-smith do you mind reviewing, since this resulted from your turn as liaison? Should be super quick. Thanks again for raising the issue!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5556
https://github.com/broadinstitute/gatk/pull/5556:790,Testability,test,test,790,"Also fixed some minor style issues in argument variable names and the WDL. This should help recover some deletions and might possibly clear up some issues with MAF estimation when the number of hets is small. @LeeTL1220 can you run on some test cases to check the effect? (Note that the changes to fix estimation of the posterior widths, which will in turn affect similar-segment smoothing, are in another branch; we should test those changes as well.). Note that the default threshold of zero for the tumor in matched-normal mode should ensure that the sites genotyped as het should always match in the tumor and the normal. (This will ultimately make multisample segmentation, as enabled by #5524, more straightforward.) There was previously a check for this condition in the integration test; however, it wasn't actually activated by the test data. I could modify the test data to add a proper regression test, but since these test files are generated by running another tool on a test BAM in the repo, this could be misleading. I'm OK with punting in this case. @jonn-smith do you mind reviewing, since this resulted from your turn as liaison? Should be super quick. Thanks again for raising the issue!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5556
https://github.com/broadinstitute/gatk/pull/5556:841,Testability,test,test,841,"Also fixed some minor style issues in argument variable names and the WDL. This should help recover some deletions and might possibly clear up some issues with MAF estimation when the number of hets is small. @LeeTL1220 can you run on some test cases to check the effect? (Note that the changes to fix estimation of the posterior widths, which will in turn affect similar-segment smoothing, are in another branch; we should test those changes as well.). Note that the default threshold of zero for the tumor in matched-normal mode should ensure that the sites genotyped as het should always match in the tumor and the normal. (This will ultimately make multisample segmentation, as enabled by #5524, more straightforward.) There was previously a check for this condition in the integration test; however, it wasn't actually activated by the test data. I could modify the test data to add a proper regression test, but since these test files are generated by running another tool on a test BAM in the repo, this could be misleading. I'm OK with punting in this case. @jonn-smith do you mind reviewing, since this resulted from your turn as liaison? Should be super quick. Thanks again for raising the issue!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5556
https://github.com/broadinstitute/gatk/pull/5556:871,Testability,test,test,871,"Also fixed some minor style issues in argument variable names and the WDL. This should help recover some deletions and might possibly clear up some issues with MAF estimation when the number of hets is small. @LeeTL1220 can you run on some test cases to check the effect? (Note that the changes to fix estimation of the posterior widths, which will in turn affect similar-segment smoothing, are in another branch; we should test those changes as well.). Note that the default threshold of zero for the tumor in matched-normal mode should ensure that the sites genotyped as het should always match in the tumor and the normal. (This will ultimately make multisample segmentation, as enabled by #5524, more straightforward.) There was previously a check for this condition in the integration test; however, it wasn't actually activated by the test data. I could modify the test data to add a proper regression test, but since these test files are generated by running another tool on a test BAM in the repo, this could be misleading. I'm OK with punting in this case. @jonn-smith do you mind reviewing, since this resulted from your turn as liaison? Should be super quick. Thanks again for raising the issue!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5556
https://github.com/broadinstitute/gatk/pull/5556:908,Testability,test,test,908,"Also fixed some minor style issues in argument variable names and the WDL. This should help recover some deletions and might possibly clear up some issues with MAF estimation when the number of hets is small. @LeeTL1220 can you run on some test cases to check the effect? (Note that the changes to fix estimation of the posterior widths, which will in turn affect similar-segment smoothing, are in another branch; we should test those changes as well.). Note that the default threshold of zero for the tumor in matched-normal mode should ensure that the sites genotyped as het should always match in the tumor and the normal. (This will ultimately make multisample segmentation, as enabled by #5524, more straightforward.) There was previously a check for this condition in the integration test; however, it wasn't actually activated by the test data. I could modify the test data to add a proper regression test, but since these test files are generated by running another tool on a test BAM in the repo, this could be misleading. I'm OK with punting in this case. @jonn-smith do you mind reviewing, since this resulted from your turn as liaison? Should be super quick. Thanks again for raising the issue!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5556
https://github.com/broadinstitute/gatk/pull/5556:930,Testability,test,test,930,"Also fixed some minor style issues in argument variable names and the WDL. This should help recover some deletions and might possibly clear up some issues with MAF estimation when the number of hets is small. @LeeTL1220 can you run on some test cases to check the effect? (Note that the changes to fix estimation of the posterior widths, which will in turn affect similar-segment smoothing, are in another branch; we should test those changes as well.). Note that the default threshold of zero for the tumor in matched-normal mode should ensure that the sites genotyped as het should always match in the tumor and the normal. (This will ultimately make multisample segmentation, as enabled by #5524, more straightforward.) There was previously a check for this condition in the integration test; however, it wasn't actually activated by the test data. I could modify the test data to add a proper regression test, but since these test files are generated by running another tool on a test BAM in the repo, this could be misleading. I'm OK with punting in this case. @jonn-smith do you mind reviewing, since this resulted from your turn as liaison? Should be super quick. Thanks again for raising the issue!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5556
https://github.com/broadinstitute/gatk/pull/5556:984,Testability,test,test,984,"Also fixed some minor style issues in argument variable names and the WDL. This should help recover some deletions and might possibly clear up some issues with MAF estimation when the number of hets is small. @LeeTL1220 can you run on some test cases to check the effect? (Note that the changes to fix estimation of the posterior widths, which will in turn affect similar-segment smoothing, are in another branch; we should test those changes as well.). Note that the default threshold of zero for the tumor in matched-normal mode should ensure that the sites genotyped as het should always match in the tumor and the normal. (This will ultimately make multisample segmentation, as enabled by #5524, more straightforward.) There was previously a check for this condition in the integration test; however, it wasn't actually activated by the test data. I could modify the test data to add a proper regression test, but since these test files are generated by running another tool on a test BAM in the repo, this could be misleading. I'm OK with punting in this case. @jonn-smith do you mind reviewing, since this resulted from your turn as liaison? Should be super quick. Thanks again for raising the issue!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5556
https://github.com/broadinstitute/gatk/pull/5556:134,Usability,clear,clear,134,"Also fixed some minor style issues in argument variable names and the WDL. This should help recover some deletions and might possibly clear up some issues with MAF estimation when the number of hets is small. @LeeTL1220 can you run on some test cases to check the effect? (Note that the changes to fix estimation of the posterior widths, which will in turn affect similar-segment smoothing, are in another branch; we should test those changes as well.). Note that the default threshold of zero for the tumor in matched-normal mode should ensure that the sites genotyped as het should always match in the tumor and the normal. (This will ultimately make multisample segmentation, as enabled by #5524, more straightforward.) There was previously a check for this condition in the integration test; however, it wasn't actually activated by the test data. I could modify the test data to add a proper regression test, but since these test files are generated by running another tool on a test BAM in the repo, this could be misleading. I'm OK with punting in this case. @jonn-smith do you mind reviewing, since this resulted from your turn as liaison? Should be super quick. Thanks again for raising the issue!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5556
https://github.com/broadinstitute/gatk/issues/5558:212,Deployability,update,updates,212,Funcotator currently ignores transcript version numbers when doing internal comparisons. . There should be a flag to enable transcript ID version checking (but it should remain off by default). This will involve updates to:; - Funcotator.java; - GencodeFuncotationFactory.java; - SimpleXsvFuncotationFactory.java. And possibly other classes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5558
https://github.com/broadinstitute/gatk/issues/5558:280,Usability,Simpl,SimpleXsvFuncotationFactory,280,Funcotator currently ignores transcript version numbers when doing internal comparisons. . There should be a flag to enable transcript ID version checking (but it should remain off by default). This will involve updates to:; - Funcotator.java; - GencodeFuncotationFactory.java; - SimpleXsvFuncotationFactory.java. And possibly other classes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5558
https://github.com/broadinstitute/gatk/pull/5560:320,Deployability,release,release,320,"@takutosato Most of this PR is refactoring to make filtering work for multiple samples while leaving single-pair output unchanged. For example, moving FORMAT annotations to the INFO field, keeping track of the sample of orientation bias priors etc. I'm leaving the wdl unchanged for now. It will still work with the new release.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5560
https://github.com/broadinstitute/gatk/pull/5560:31,Modifiability,refactor,refactoring,31,"@takutosato Most of this PR is refactoring to make filtering work for multiple samples while leaving single-pair output unchanged. For example, moving FORMAT annotations to the INFO field, keeping track of the sample of orientation bias priors etc. I'm leaving the wdl unchanged for now. It will still work with the new release.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5560
https://github.com/broadinstitute/gatk/issues/5561:186,Performance,optimiz,optimization,186,The somatic likelihoods model becomes very expensive when evaluated at every reference position. MT calling takes about 40 minutes in GVCF mode and 5 minutes without ref conf. Ideas for optimization include:; - Reducing the convergence threshold for `alleleFractionsPosterior` method (my initial attempts here didn't make a big improvement); - Using a simplified likelihoods model for reference sites (similar to `lnLikelihoodRatio` used for active region determination); - Keeping LOD calculation from active region determination and only calculating it once,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5561
https://github.com/broadinstitute/gatk/issues/5561:352,Usability,simpl,simplified,352,The somatic likelihoods model becomes very expensive when evaluated at every reference position. MT calling takes about 40 minutes in GVCF mode and 5 minutes without ref conf. Ideas for optimization include:; - Reducing the convergence threshold for `alleleFractionsPosterior` method (my initial attempts here didn't make a big improvement); - Using a simplified likelihoods model for reference sites (similar to `lnLikelihoodRatio` used for active region determination); - Keeping LOD calculation from active region determination and only calculating it once,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5561
https://github.com/broadinstitute/gatk/issues/5564:12,Deployability,update,updated,12,"We recently updated (PR #4858) the default Smith-Waterman parameters for realigning reads to their best haplotype. Although there is no reason for the alignment of haplotypes to the reference to use the same parameters, it seems like we are similarly favoring indels too much. There is a forum discussion to this effect:. https://gatkforums.broadinstitute.org/gatk/discussion/23230/gatk-haplotypecaller-mnp-output-problem. Here are the parameters we use:. * match: 200; * substitution: -150; * indel start: -260; * indel extend: -11. These parameters, which are essentially a prior on biological variation, prefer an indel, with a cost of 260, to a SNP, with a cost of 350. This does not seem correct. It almost never comes up because the correct alignment is usually unambiguous, but when it does, shouldn't we break the tie in favor of the SNP?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564
https://github.com/broadinstitute/gatk/issues/5564:521,Modifiability,extend,extend,521,"We recently updated (PR #4858) the default Smith-Waterman parameters for realigning reads to their best haplotype. Although there is no reason for the alignment of haplotypes to the reference to use the same parameters, it seems like we are similarly favoring indels too much. There is a forum discussion to this effect:. https://gatkforums.broadinstitute.org/gatk/discussion/23230/gatk-haplotypecaller-mnp-output-problem. Here are the parameters we use:. * match: 200; * substitution: -150; * indel start: -260; * indel extend: -11. These parameters, which are essentially a prior on biological variation, prefer an indel, with a cost of 260, to a SNP, with a cost of 350. This does not seem correct. It almost never comes up because the correct alignment is usually unambiguous, but when it does, shouldn't we break the tie in favor of the SNP?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564
https://github.com/broadinstitute/gatk/pull/5567:450,Testability,test,test,450,"The documentation date is currently displayed incorrectly, and nonsensically, and sometimes from the future, in the generated doc. See the bottom of the page at https://software.broadinstitute.org/gatk/documentation/tooldocs/4.0.12.0/#, for example. Before:; `GATK version 4.0.11.0-92-gf9a2e5c-SNAPSHOT built at 09-10-2019 06:10:16`; After:; `GATK version 4.0.11.0-92-gf9a2e5c-SNAPSHOT built at Wed, 9 Jan 2019 18:13:38 -0500`. @bhanugandham Can you test this when you get a chance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5567
https://github.com/broadinstitute/gatk/issues/5568:284,Deployability,update,updated,284,"@nalinigans I have some very long-running jobs that I'm trying to track, but my stderr is choked with buffer resize outputs. Would it be possible to turn those off by default or add a flag to turn them off? It makes for huge log files and it's hard to track if the log is still being updated for jobs that run >24hrs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5568
https://github.com/broadinstitute/gatk/issues/5568:225,Testability,log,log,225,"@nalinigans I have some very long-running jobs that I'm trying to track, but my stderr is choked with buffer resize outputs. Would it be possible to turn those off by default or add a flag to turn them off? It makes for huge log files and it's hard to track if the log is still being updated for jobs that run >24hrs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5568
https://github.com/broadinstitute/gatk/issues/5568:265,Testability,log,log,265,"@nalinigans I have some very long-running jobs that I'm trying to track, but my stderr is choked with buffer resize outputs. Would it be possible to turn those off by default or add a flag to turn them off? It makes for huge log files and it's hard to track if the log is still being updated for jobs that run >24hrs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5568
https://github.com/broadinstitute/gatk/issues/5571:18,Availability,robust,robust,18,"There should be a robust mechanism to check whether an index file is up-to-date with respect to the file it indexes (eg., UUIDs, hashes, etc.). Modification time alone is not sufficient, since files can get uploaded out-of-order in cloud environments.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5571
https://github.com/broadinstitute/gatk/issues/5571:129,Security,hash,hashes,129,"There should be a robust mechanism to check whether an index file is up-to-date with respect to the file it indexes (eg., UUIDs, hashes, etc.). Modification time alone is not sufficient, since files can get uploaded out-of-order in cloud environments.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5571
